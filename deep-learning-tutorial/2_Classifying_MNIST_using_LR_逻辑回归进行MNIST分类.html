
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>2.逻辑回归进行MNIST分类 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="3_Multilayer_Perceptron_多层感知机.html" />
    
    
    <link rel="prev" href="1_Getting_Started_入门.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    简介
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="1_Getting_Started_入门.html">
            
                <a href="1_Getting_Started_入门.html">
            
                    
                    1.入门
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3" data-path="2_Classifying_MNIST_using_LR_逻辑回归进行MNIST分类.html">
            
                <a href="2_Classifying_MNIST_using_LR_逻辑回归进行MNIST分类.html">
            
                    
                    2.逻辑回归进行MNIST分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="3_Multilayer_Perceptron_多层感知机.html">
            
                <a href="3_Multilayer_Perceptron_多层感知机.html">
            
                    
                    3.多层感知机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="4_Convoltional_Neural_Networks_LeNet_卷积神经网络.html">
            
                <a href="4_Convoltional_Neural_Networks_LeNet_卷积神经网络.html">
            
                    
                    4.卷积神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="5_Denoising_Autoencoders_降噪自动编码.html">
            
                <a href="5_Denoising_Autoencoders_降噪自动编码.html">
            
                    
                    5.降噪自动编码
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="6_Stacked_Denoising_Autoencoders_层叠降噪自动编码机.html">
            
                <a href="6_Stacked_Denoising_Autoencoders_层叠降噪自动编码机.html">
            
                    
                    6.层叠降噪自动编码机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="7_Restricted_Boltzmann_Machine_受限波尔兹曼机.html">
            
                <a href="7_Restricted_Boltzmann_Machine_受限波尔兹曼机.html">
            
                    
                    7.受限波尔兹曼机
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >2.逻辑回归进行MNIST分类</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="&#x4F7F;&#x7528;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x8FDB;&#x884C;mnist&#x5206;&#x7C7B;&#xFF08;classifying-mnist-using-logistic-regressing&#xFF09;">&#x4F7F;&#x7528;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x8FDB;&#x884C;MNIST&#x5206;&#x7C7B;&#xFF08;Classifying MNIST using Logistic Regressing&#xFF09;</h1>
<p>&#x672C;&#x8282;&#x5047;&#x5B9A;&#x8BFB;&#x8005;&#x5C5E;&#x6027;&#x4E86;&#x4E0B;&#x9762;&#x7684;Theano&#x6982;&#x5FF5;&#xFF1A;<a href="http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables" target="_blank">&#x5171;&#x4EAB;&#x53D8;&#x91CF;&#xFF08;shared variable&#xFF09;</a>, <a href="http://deeplearning.net/software/theano/tutorial/adding.html#adding-two-scalars" target="_blank">&#x57FA;&#x672C;&#x6570;&#x5B66;&#x7B97;&#x5B50;&#xFF08;basic arithmetic ops&#xFF09;</a>, <a href="http://deeplearning.net/software/theano/tutorial/examples.html#computing-gradients" target="_blank">Theano&#x7684;&#x8FDB;&#x9636;&#xFF08;T.grad&#xFF09;</a>, <a href="http://deeplearning.net/software/theano/library/config.html#config.floatX" target="_blank">floatX(&#x9ED8;&#x8BA4;&#x4E3A;float64)</a>&#x3002;&#x5047;&#x5982;&#x4F60;&#x60F3;&#x8981;&#x5728;&#x4F60;&#x7684;GPU&#x4E0A;&#x8DD1;&#x4F60;&#x7684;&#x4EE3;&#x7801;&#xFF0C;&#x4F60;&#x4E5F;&#x9700;&#x8981;&#x770B;<a href="http://deeplearning.net/software/theano/tutorial/using_gpu.html" target="_blank">GPU</a>&#x3002;</p>
<p>&#x672C;&#x8282;&#x7684;&#x6240;&#x6709;&#x4EE3;&#x7801;&#x53EF;&#x4EE5;&#x5728;<a href="http://deeplearning.net/tutorial/code/logistic_sgd.py" target="_blank">&#x8FD9;&#x91CC;</a>&#x4E0B;&#x8F7D;&#x3002;</p>
<p>&#x5728;&#x8FD9;&#x4E00;&#x8282;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x5C55;&#x793A;Theano&#x5982;&#x4F55;&#x5B9E;&#x73B0;&#x6700;&#x57FA;&#x672C;&#x7684;&#x5206;&#x7C7B;&#x5668;&#xFF1A;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x5206;&#x7C7B;&#x5668;&#x3002;&#x6211;&#x4EEC;&#x4EE5;&#x6A21;&#x578B;&#x7684;&#x5FEB;&#x901F;&#x5165;&#x95E8;&#x5F00;&#x59CB;&#xFF0C;&#x590D;&#x4E60;(refresher)&#x548C;&#x5DE9;&#x56FA;(anchor)&#x6570;&#x5B66;&#x8D1F;&#x53F7;&#xFF0C;&#x4E5F;&#x5C55;&#x793A;&#x4E86;&#x6570;&#x5B66;&#x8868;&#x8FBE;&#x5F0F;&#x5982;&#x4F55;&#x6620;&#x5C04;&#x5230;Theano&#x56FE;&#x4E2D;&#x3002;</p>
<h2 id="&#x6A21;&#x578B;">&#x6A21;&#x578B;</h2>
<p>&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x6A21;&#x578B;&#x662F;&#x4E00;&#x4E2A;&#x7EBF;&#x6027;&#x6982;&#x7387;&#x6A21;&#x578B;&#x3002;&#x5B83;&#x7531;&#x4E00;&#x4E2A;&#x6743;&#x503C;&#x77E9;&#x9635;W&#x548C;&#x504F;&#x7F6E;&#x5411;&#x91CF;b&#x53C2;&#x6570;&#x5316;&#x3002;&#x5206;&#x7C7B;&#x901A;&#x8FC7;&#x5C06;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x63D0;&#x4EA4;&#x5230;&#x4E00;&#x7EC4;&#x8D85;&#x5E73;&#x9762;&#xFF0C;&#x6BCF;&#x4E2A;&#x8D85;&#x5E73;&#x9762;&#x5BF9;&#x5E94;&#x4E00;&#x4E2A;&#x7C7B;&#x3002;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x548C;&#x8D85;&#x5E73;&#x9762;&#x7684;&#x8DDD;&#x79BB;&#x662F;&#x8FD9;&#x4E2A;&#x8F93;&#x5165;&#x5C5E;&#x4E8E;&#x8BE5;&#x7C7B;&#x7684;&#x4E00;&#x4E2A;&#x6982;&#x7387;&#x91CF;&#x5316;&#x3002;
&#x5728;&#x7ED9;&#x5B9A;&#x6A21;&#x578B;&#x4E0B;&#xFF0C;&#x8F93;&#x5165;x&#xFF0C;&#x8F93;&#x51FA;&#x4E3A;y&#x7684;&#x6982;&#x7387;&#xFF0C;&#x53EF;&#x4EE5;&#x7528;&#x5982;&#x4E0B;&#x516C;&#x5F0F;&#x8868;&#x793A;</p>
<center>![probality](/images/2_the_model_1.png)</center>

<center>![y_prediction](/images/2_the_model_2.png)</center>

<p>Theano&#x4EE3;&#x7801;&#x5982;&#x4E0B;&#x3002;</p>
<pre><code class="lang-Python">        <span class="hljs-comment"># initialize with 0 the weights W as a matrix of shape (n_in, n_out)</span>
        self.W = theano.shared(
            value=numpy.zeros(
                (n_in, n_out),
                dtype=theano.config.floatX
            ),
            name=<span class="hljs-string">&apos;W&apos;</span>,
            borrow=<span class="hljs-keyword">True</span>
        )
        <span class="hljs-comment"># initialize the baises b as a vector of n_out 0s</span>
        self.b = theano.shared(
            value=numpy.zeros(
                (n_out,),
                dtype=theano.config.floatX
            ),
            name=<span class="hljs-string">&apos;b&apos;</span>,
            borrow=<span class="hljs-keyword">True</span>
        )

        <span class="hljs-comment"># symbolic expression for computing the matrix of class-membership</span>
        <span class="hljs-comment"># probabilities</span>
        <span class="hljs-comment"># Where:</span>
        <span class="hljs-comment"># W is a matrix where column-k represent the separation hyper plain for</span>
        <span class="hljs-comment"># class-k</span>
        <span class="hljs-comment"># x is a matrix where row-j  represents input training sample-j</span>
        <span class="hljs-comment"># b is a vector where element-k represent the free parameter of hyper</span>
        <span class="hljs-comment"># plain-k</span>
        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)

        <span class="hljs-comment"># symbolic description of how to compute prediction as class whose</span>
        <span class="hljs-comment"># probability is maximal</span>
        self.y_pred = T.argmax(self.p_y_given_x, axis=<span class="hljs-number">1</span>)
</code></pre>
<p>&#x7531;&#x4E8E;&#x6A21;&#x578B;&#x7684;&#x53C2;&#x6570;&#x9700;&#x8981;&#x4E0D;&#x65AD;&#x7684;&#x5B58;&#x53D6;&#x548C;&#x4FEE;&#x6B63;&#xFF0C;&#x6240;&#x4EE5;&#x6211;&#x4EEC;&#x628A;W&#x548C;b&#x5B9A;&#x4E49;&#x4E3A;&#x5171;&#x4EAB;&#x53D8;&#x91CF;&#x3002;&#x8FD9;&#x4E2A;dot&#xFF08;&#x70B9;&#x4E58;&#xFF09;&#x548C;softmax&#x8FD0;&#x7B97;&#x7528;&#x4EE5;&#x8BA1;&#x7B97;&#x8FD9;&#x4E2A;P(Y|x,W,b)&#x3002;&#x8FD9;&#x4E2A;&#x7ED3;&#x679C;<code>p_y_given_x</code>(probability)&#x662F;&#x4E00;&#x4E2A;vector&#x7C7B;&#x578B;&#x7684;&#x6982;&#x7387;&#x5411;&#x91CF;&#x3002;
&#x4E3A;&#x4E86;&#x83B7;&#x5F97;&#x5B9E;&#x9645;&#x7684;&#x6A21;&#x578B;&#x9884;&#x6D4B;&#xFF0C;&#x6211;&#x4EEC;&#x4F7F;&#x7528;<code>T_argmax</code>&#x64CD;&#x4F5C;&#xFF0C;&#x6765;&#x8FD4;&#x56DE;<code>p_y_given_x</code>&#x7684;&#x6700;&#x5927;&#x503C;&#x5BF9;&#x5E94;&#x7684;y&#x3002;
    &#x5982;&#x679C;&#x60F3;&#x8981;&#x83B7;&#x5F97;&#x5B8C;&#x6574;&#x7684;Theano&#x7B97;&#x5B50;&#xFF0C;&#x770B;<a href="http://deeplearning.net/software/theano/library/tensor/basic.html#basic-tensor-functionality" target="_blank">&#x7B97;&#x5B50;&#x5217;&#x8868;</a></p>
<h2 id="&#x5B9A;&#x4E49;&#x4E00;&#x4E2A;&#x635F;&#x5931;&#x51FD;&#x6570;">&#x5B9A;&#x4E49;&#x4E00;&#x4E2A;&#x635F;&#x5931;&#x51FD;&#x6570;</h2>
<p>&#x5B66;&#x4E60;&#x4F18;&#x5316;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x9700;&#x8981;&#x6700;&#x5C0F;&#x5316;&#x4E00;&#x4E2A;&#x635F;&#x5931;&#x53C2;&#x6570;&#x3002;&#x5728;&#x591A;&#x5206;&#x7C7B;&#x7684;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x4E2D;&#xFF0C;&#x5F88;&#x663E;&#x7136;&#x662F;&#x4F7F;&#x7528;&#x8D1F;&#x5BF9;&#x6570;&#x4F3C;&#x7136;&#x51FD;&#x6570;&#x4F5C;&#x4E3A;&#x635F;&#x5931;&#x51FD;&#x6570;&#x3002;&#x4F3C;&#x7136;&#x51FD;&#x6570;&#x548C;&#x635F;&#x5931;&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<center>![loss_function](/images/2_defining_a_loss_function_1.png)</center>

<p>&#x867D;&#x7136;&#x6574;&#x672C;&#x4E66;&#x90FD;&#x81F4;&#x529B;&#x4E8E;&#x63A2;&#x8BA8;&#x6700;&#x5C0F;&#x5316;&#x8BDD;&#x9898;&#xFF0C;&#x4F46;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x662F;&#x8FC4;&#x4ECA;&#x4E3A;&#x6B62;&#x6700;&#x7B80;&#x5355;&#x7684;&#x6700;&#x5C0F;&#x5316;&#x975E;&#x7EBF;&#x6027;&#x51FD;&#x6570;&#x7684;&#x65B9;&#x6CD5;&#x3002;&#x5728;&#x8FD9;&#x4E2A;&#x6559;&#x7A0B;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x4F7F;&#x7528;minibatch&#x968F;&#x673A;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x7B97;&#x6CD5;&#x3002;&#x53EF;&#x4EE5;&#x770B;<a href="http://deeplearning.net/tutorial/gettingstarted.html#opt-sgd" target="_blank">&#x968F;&#x673A;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;</a>&#x6765;&#x83B7;&#x5F97;&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x3002;
&#x4E0B;&#x9762;&#x7684;&#x4EE3;&#x7801;&#x5B9A;&#x4E49;&#x4E86;&#x4E00;&#x4E2A;&#x5BF9;&#x7ED9;&#x5B9A;&#x7684;minibatch&#x7684;&#x635F;&#x5931;&#x51FD;&#x6570;&#x3002;</p>
<pre><code class="lang-Python">        <span class="hljs-comment"># y.shape[0] is (symbolically) the number of rows in y, i.e.,</span>
        <span class="hljs-comment"># number of examples (call it n) in the minibatch</span>
        <span class="hljs-comment"># T.arange(y.shape[0]) is a symbolic vector which will contain</span>
        <span class="hljs-comment"># [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of</span>
        <span class="hljs-comment"># Log-Probabilities (call it LP) with one row per example and</span>
        <span class="hljs-comment"># one column per class LP[T.arange(y.shape[0]),y] is a vector</span>
        <span class="hljs-comment"># v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,</span>
        <span class="hljs-comment"># LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is</span>
        <span class="hljs-comment"># the mean (across minibatch examples) of the elements in v,</span>
        <span class="hljs-comment"># i.e., the mean log-likelihood across the minibatch.</span>
        <span class="hljs-keyword">return</span> -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[<span class="hljs-number">0</span>]), y])
</code></pre>
<pre><code>&#x5728;&#x8FD9;&#x91CC;&#x6211;&#x4EEC;&#x4F7F;&#x7528;&#x9519;&#x8BEF;&#x7684;&#x5E73;&#x5747;&#x6765;&#x8868;&#x793A;&#x635F;&#x5931;&#x51FD;&#x6570;&#xFF0C;&#x4EE5;&#x51CF;&#x5C11;minibatch&#x5C3A;&#x5BF8;&#x5BF9;&#x6211;&#x4EEC;&#x7684;&#x5F71;&#x54CD;&#x3002;
</code></pre><h2 id="&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x7C7B;">&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x7C7B;</h2>
<p>&#x73B0;&#x5728;&#xFF0C;&#x6211;&#x4EEC;&#x8981;&#x5B9A;&#x4E49;&#x4E00;&#x4E2A;<code>&#x903B;&#x8F91;&#x56DE;&#x5F52;</code>&#x7684;&#x7C7B;&#xFF0C;&#x6765;&#x6982;&#x62EC;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x7684;&#x57FA;&#x672C;&#x884C;&#x4E3A;&#x3002;&#x4EE3;&#x7801;&#x5DF2;&#x7ECF;&#x662F;&#x6211;&#x4EEC;&#x4E4B;&#x524D;&#x6DB5;&#x76D6;&#x7684;&#x4E86;&#xFF0C;&#x4E0D;&#x518D;&#x8FDB;&#x884C;&#x8FC7;&#x591A;&#x89E3;&#x91CA;&#x3002;</p>
<pre><code class="lang-Python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LogisticRegression</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;Multi-class Logistic Regression Class

    The logistic regression is fully described by a weight matrix :math:`W`
    and bias vector :math:`b`. Classification is done by projecting data
    points onto a set of hyperplanes, the distance to which is used to
    determine a class membership probability.
    &quot;&quot;&quot;</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, input, n_in, n_out)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Initialize the parameters of the logistic regression

        :type input: theano.tensor.TensorType
        :param input: symbolic variable that describes the input of the
                      architecture (one minibatch)

        :type n_in: int
        :param n_in: number of input units, the dimension of the space in
                     which the datapoints lie

        :type n_out: int
        :param n_out: number of output units, the dimension of the space in
                      which the labels lie

        &quot;&quot;&quot;</span>
        <span class="hljs-comment"># start-snippet-1</span>
        <span class="hljs-comment"># initialize with 0 the weights W as a matrix of shape (n_in, n_out)</span>
        self.W = theano.shared(
            value=numpy.zeros(
                (n_in, n_out),
                dtype=theano.config.floatX
            ),
            name=<span class="hljs-string">&apos;W&apos;</span>,
            borrow=<span class="hljs-keyword">True</span>
        )
        <span class="hljs-comment"># initialize the baises b as a vector of n_out 0s</span>
        self.b = theano.shared(
            value=numpy.zeros(
                (n_out,),
                dtype=theano.config.floatX
            ),
            name=<span class="hljs-string">&apos;b&apos;</span>,
            borrow=<span class="hljs-keyword">True</span>
        )

        <span class="hljs-comment"># symbolic expression for computing the matrix of class-membership</span>
        <span class="hljs-comment"># probabilities</span>
        <span class="hljs-comment"># Where:</span>
        <span class="hljs-comment"># W is a matrix where column-k represent the separation hyper plain for</span>
        <span class="hljs-comment"># class-k</span>
        <span class="hljs-comment"># x is a matrix where row-j  represents input training sample-j</span>
        <span class="hljs-comment"># b is a vector where element-k represent the free parameter of hyper</span>
        <span class="hljs-comment"># plain-k</span>
        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)

        <span class="hljs-comment"># symbolic description of how to compute prediction as class whose</span>
        <span class="hljs-comment"># probability is maximal</span>
        self.y_pred = T.argmax(self.p_y_given_x, axis=<span class="hljs-number">1</span>)
        <span class="hljs-comment"># end-snippet-1</span>

        <span class="hljs-comment"># parameters of the model</span>
        self.params = [self.W, self.b]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">negative_log_likelihood</span><span class="hljs-params">(self, y)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Return the mean of the negative log-likelihood of the prediction
        of this model under a given target distribution.

        .. math::

            \frac{1}{|\mathcal{D}|} \mathcal{L} (\theta=\{W,b\}, \mathcal{D}) =
            \frac{1}{|\mathcal{D}|} \sum_{i=0}^{|\mathcal{D}|}
                \log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\
            \ell (\theta=\{W,b\}, \mathcal{D})

        :type y: theano.tensor.TensorType
        :param y: corresponds to a vector that gives for each example the
                  correct label

        Note: we use the mean instead of the sum so that
              the learning rate is less dependent on the batch size
        &quot;&quot;&quot;</span>
        <span class="hljs-comment"># start-snippet-2</span>
        <span class="hljs-comment"># y.shape[0] is (symbolically) the number of rows in y, i.e.,</span>
        <span class="hljs-comment"># number of examples (call it n) in the minibatch</span>
        <span class="hljs-comment"># T.arange(y.shape[0]) is a symbolic vector which will contain</span>
        <span class="hljs-comment"># [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of</span>
        <span class="hljs-comment"># Log-Probabilities (call it LP) with one row per example and</span>
        <span class="hljs-comment"># one column per class LP[T.arange(y.shape[0]),y] is a vector</span>
        <span class="hljs-comment"># v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,</span>
        <span class="hljs-comment"># LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is</span>
        <span class="hljs-comment"># the mean (across minibatch examples) of the elements in v,</span>
        <span class="hljs-comment"># i.e., the mean log-likelihood across the minibatch.</span>
        <span class="hljs-keyword">return</span> -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[<span class="hljs-number">0</span>]), y])
        <span class="hljs-comment"># end-snippet-2</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">errors</span><span class="hljs-params">(self, y)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Return a float representing the number of errors in the minibatch
        over the total number of examples of the minibatch ; zero one
        loss over the size of the minibatch

        :type y: theano.tensor.TensorType
        :param y: corresponds to a vector that gives for each example the
                  correct label
        &quot;&quot;&quot;</span>

        <span class="hljs-comment"># check if y has same dimension of y_pred</span>
        <span class="hljs-keyword">if</span> y.ndim != self.y_pred.ndim:
            <span class="hljs-keyword">raise</span> TypeError(
                <span class="hljs-string">&apos;y should have the same shape as self.y_pred&apos;</span>,
                (<span class="hljs-string">&apos;y&apos;</span>, y.type, <span class="hljs-string">&apos;y_pred&apos;</span>, self.y_pred.type)
            )
        <span class="hljs-comment"># check if y is of the correct datatype</span>
        <span class="hljs-keyword">if</span> y.dtype.startswith(<span class="hljs-string">&apos;int&apos;</span>):
            <span class="hljs-comment"># the T.neq operator returns a vector of 0s and 1s, where 1</span>
            <span class="hljs-comment"># represents a mistake in prediction</span>
            <span class="hljs-keyword">return</span> T.mean(T.neq(self.y_pred, y))
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">raise</span> NotImplementedError()
</code></pre>
<p>&#x6211;&#x4EEC;&#x901A;&#x8FC7;&#x5982;&#x4E0B;&#x4EE3;&#x7801;&#x6765;&#x5B9E;&#x4F8B;&#x5316;&#x8FD9;&#x4E2A;&#x7C7B;&#x3002;</p>
<pre><code class="lang-Pyhton">    # generate symbolic variables for input (x and y represent a
    # minibatch)
    x = T.matrix(&apos;x&apos;)  # data, presented as rasterized images
    y = T.ivector(&apos;y&apos;)  # labels, presented as 1D vector of [int] labels

    # construct the logistic regression class
    # Each MNIST image has size 28*28
    classifier = LogisticRegression(input=x, n_in=28 * 28, n_out=10)
</code></pre>
<p>&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;&#xFF0C;&#x8F93;&#x5165;&#x5411;&#x91CF;x&#xFF0C;&#x548C;&#x5176;&#x76F8;&#x5173;&#x7684;&#x6807;&#x7B7E;y&#x90FD;&#x662F;&#x5B9A;&#x4E49;&#x5728;<code>LogisticRegression</code>&#x5B9E;&#x4F53;&#x5916;&#x7684;&#x3002;&#x8FD9;&#x4E2A;&#x7C7B;&#x9700;&#x8981;&#x5C06;&#x8F93;&#x5165;&#x6570;&#x636E;&#x4F5C;&#x4E3A;<code>__init__</code>&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;&#x3002;&#x8FD9;&#x5728;&#x5C06;&#x8FD9;&#x4E9B;&#x7C7B;&#x7684;&#x5B9E;&#x4F8B;&#x8FDE;&#x63A5;&#x8D77;&#x6765;&#x6784;&#x5EFA;&#x6DF1;&#x7F51;&#x7EDC;&#x65B9;&#x9762;&#x975E;&#x5E38;&#x6709;&#x7528;&#x3002;&#x4E00;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x53EF;&#x4EE5;&#x4F5C;&#x4E3A;&#x4E0B;&#x4E00;&#x5C42;&#x7684;&#x8F93;&#x5165;&#x3002;
&#x6700;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x5B9A;&#x4E49;&#x4E86;&#x4E00;&#x4E2A;<code>cost</code>&#x53D8;&#x91CF;&#x6765;&#x6700;&#x5C0F;&#x5316;&#x3002;</p>
<pre><code class="lang-Python">    <span class="hljs-comment"># the cost we minimize during training is the negative log likelihood of</span>
    <span class="hljs-comment"># the model in symbolic format</span>
    cost = classifier.negative_log_likelihood(y)
</code></pre>
<h2 id="&#x5B66;&#x4E60;&#x6A21;&#x578B;">&#x5B66;&#x4E60;&#x6A21;&#x578B;</h2>
<p>&#x5728;&#x5B9E;&#x73B0;MSGD&#x7684;&#x8BB8;&#x591A;&#x8BED;&#x8A00;&#x4E2D;&#xFF0C;&#x9700;&#x8981;&#x901A;&#x8FC7;&#x624B;&#x52A8;&#x6C42;&#x89E3;&#x635F;&#x5931;&#x51FD;&#x6570;&#x5BF9;&#x6BCF;&#x4E2A;&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#xFF08;&#x5FAE;&#x5206;&#xFF09;&#x6765;&#x5B9E;&#x73B0;&#x3002;
&#x5728;Theano&#x4E2D;&#x5462;&#xFF0C;&#x8FD9;&#x662F;&#x975E;&#x5E38;&#x7B80;&#x5355;&#x7684;&#x3002;&#x5B83;&#x81EA;&#x52A8;&#x5FAE;&#x5206;&#xFF0C;&#x5E76;&#x4E14;&#x4F7F;&#x7528;&#x4E86;&#x4E00;&#x5B9A;&#x7684;&#x6570;&#x5B66;&#x8F6C;&#x6362;&#x6765;&#x63D0;&#x9AD8;&#x6570;&#x5B66;&#x7A33;&#x5B9A;&#x6027;&#x3002;</p>
<pre><code class="lang-Pyhton">    g_W = T.grad(cost=cost, wrt=classifier.W)
    g_b = T.grad(cost=cost, wrt=classifier.b)
</code></pre>
<p>&#x8FD9;&#x4E2A;&#x51FD;&#x6570;<code>train_model</code>&#x53EF;&#x4EE5;&#x88AB;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#x3002;</p>
<pre><code class="lang-Python">    <span class="hljs-comment"># specify how to update the parameters of the model as a list of</span>
    <span class="hljs-comment"># (variable, update expression) pairs.</span>
    updates = [(classifier.W, classifier.W - learning_rate * g_W),
               (classifier.b, classifier.b - learning_rate * g_b)]

    <span class="hljs-comment"># compiling a Theano function `train_model` that returns the cost, but in</span>
    <span class="hljs-comment"># the same time updates the parameter of the model based on the rules</span>
    <span class="hljs-comment"># defined in `updates`</span>
    train_model = theano.function(
        inputs=[index],
        outputs=cost,
        updates=updates,
        givens={
            x: train_set_x[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size],
            y: train_set_y[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size]
        }
    )
</code></pre>
<p><code>update</code>&#x662F;&#x4E00;&#x4E2A;list&#xFF0C;&#x7528;&#x4EE5;&#x66F4;&#x65B0;&#x6BCF;&#x4E00;&#x6B65;&#x7684;&#x53C2;&#x6570;&#x3002;<code>given</code>&#x662F;&#x4E00;&#x4E2A;&#x5B57;&#x5178;&#xFF0C;&#x7528;&#x4EE5;&#x8868;&#x793A;&#x8C61;&#x5F81;&#x53D8;&#x91CF;&#xFF0C;&#x548C;&#x4F60;&#x5728;&#x8BE5;&#x6B65;&#x4E2D;&#x8868;&#x793A;&#x7684;&#x6570;&#x636E;&#x3002;&#x8FD9;&#x4E2A;<code>train_model</code>&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#x662F;minibatch&#x7684;<code>index</code>&#xFF0C;batch&#x7684;&#x5927;&#x5C0F;&#x4E4B;&#x524D;&#x5DF2;&#x7ECF;&#x56FA;&#x5B9A;&#xFF0C;&#x4EE5;&#x6B64;&#x88AB;&#x5B9A;&#x4E49;&#x4E3A;x&#xFF0C;&#x4EE5;&#x53CA;&#x5176;&#x76F8;&#x5173;&#x7684;y&#x3002;</li>
<li>&#x8FD4;&#x56DE;&#x662F;&#x8BE5;<code>index</code>&#x4E0B;&#x4E0E;x&#xFF0C;y&#x76F8;&#x5173;&#x7684;<code>cost</code>/&#x635F;&#x5931;&#x51FD;&#x6570;&#x3002;</li>
<li>&#x6BCF;&#x4E00;&#x6B21;&#x51FD;&#x6570;&#x8C03;&#x7528;&#xFF0C;&#x5B83;&#x90FD;&#x5148;&#x7528;index&#x5BF9;&#x5E94;&#x7684;&#x8BAD;&#x7EC3;&#x96C6;&#x7684;&#x5207;&#x7247;&#x6765;&#x66F4;&#x65B0;x&#xFF0C;y&#x3002;&#x7136;&#x540E;&#x8BA1;&#x7B97;&#x8BE5;minibatch&#x4E0B;&#x7684;cost&#xFF0C;&#x4EE5;&#x53CA;&#x7533;&#x8BF7;<code>update</code>&#x64CD;&#x4F5C;&#x3002;
&#x6BCF;&#x6B21;<code>train_model(inedx)</code>&#x88AB;&#x8C03;&#x7528;&#xFF0C;&#x5B83;&#x90FD;&#x8BA1;&#x7B97;&#x5E76;&#x8FD4;&#x56DE;&#x8BE5;minibatch&#x7684;cost&#xFF0C;&#x5F53;&#x7136;&#x8FD9;&#x4E5F;&#x662F;MSGD&#x7684;&#x4E00;&#x6B65;&#x3002;&#x6574;&#x4E2A;&#x5B66;&#x4E60;&#x7B97;&#x6CD5;&#x56E0;&#x5FAA;&#x73AF;&#x4E86;&#x6570;&#x636E;&#x96C6;&#x6240;&#x6709;&#x6837;&#x4F8B;&#x3002;</li>
</ul>
<h2 id="&#x8BAD;&#x7EC3;&#x6A21;&#x578B;">&#x8BAD;&#x7EC3;&#x6A21;&#x578B;</h2>
<p>&#x5728;&#x4E4B;&#x524D;&#x8BBA;&#x8FF0;&#x4E2D;&#x6240;&#x8BF4;&#xFF0C;&#x6211;&#x4EEC;&#x5BF9;&#x5206;&#x7C7B;&#x9519;&#x8BEF;&#x7684;&#x6837;&#x672C;&#x611F;&#x5174;&#x8DA3;&#xFF08;&#x4E0D;&#x4EC5;&#x4EC5;&#x662F;&#x53EF;&#x80FD;&#x6027;&#xFF09;&#x3002;&#x56E0;&#x6B64;&#x6A21;&#x578B;&#x4E2D;&#x589E;&#x52A0;&#x4E86;&#x4E00;&#x4E2A;&#x989D;&#x5916;&#x7684;&#x5B9E;&#x4F8B;&#x65B9;&#x6CD5;&#xFF0C;&#x6765;&#x7EAA;&#x5F55;&#x6BCF;&#x4E2A;minibatch&#x4E2D;&#x7684;&#x9519;&#x8BEF;&#x5206;&#x7C7B;&#x6837;&#x4F8B;&#x6570;&#x3002;</p>
<pre><code class="lang-Python">   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">errors</span><span class="hljs-params">(self, y)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Return a float representing the number of errors in the minibatch
        over the total number of examples of the minibatch ; zero one
        loss over the size of the minibatch

        :type y: theano.tensor.TensorType
        :param y: corresponds to a vector that gives for each example the
                  correct label
        &quot;&quot;&quot;</span>

        <span class="hljs-comment"># check if y has same dimension of y_pred</span>
        <span class="hljs-keyword">if</span> y.ndim != self.y_pred.ndim:
            <span class="hljs-keyword">raise</span> TypeError(
                <span class="hljs-string">&apos;y should have the same shape as self.y_pred&apos;</span>,
                (<span class="hljs-string">&apos;y&apos;</span>, y.type, <span class="hljs-string">&apos;y_pred&apos;</span>, self.y_pred.type)
            )
        <span class="hljs-comment"># check if y is of the correct datatype</span>
        <span class="hljs-keyword">if</span> y.dtype.startswith(<span class="hljs-string">&apos;int&apos;</span>):
            <span class="hljs-comment"># the T.neq operator returns a vector of 0s and 1s, where 1</span>
            <span class="hljs-comment"># represents a mistake in prediction</span>
            <span class="hljs-keyword">return</span> T.mean(T.neq(self.y_pred, y))
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">raise</span> NotImplementedError()
</code></pre>
<p>&#x6211;&#x4EEC;&#x521B;&#x5EFA;&#x4E86;<code>test_model</code>&#x51FD;&#x6570;&#xFF0C;&#x7136;&#x540E;&#x4E5F;&#x521B;&#x5EFA;&#x4E86;<code>validate_model</code>&#x6765;&#x8C03;&#x7528;&#x53BB;&#x4FEE;&#x6B63;&#x8FD9;&#x4E2A;&#x503C;&#x3002;&#x5F53;&#x7136;<code>validate_model</code>&#x662F;early-stopping&#x7684;&#x5173;&#x952E;&#x3002;&#x5B83;&#x4EEC;&#x90FD;&#x662F;&#x6765;&#x7EDF;&#x8BA1;minibatch&#x4E2D;&#x5206;&#x7C7B;&#x9519;&#x8BEF;&#x7684;&#x6837;&#x4F8B;&#x6570;&#x3002;</p>
<pre><code class="lang-Python">    <span class="hljs-comment"># compiling a Theano function that computes the mistakes that are made by</span>
    <span class="hljs-comment"># the model on a minibatch</span>
    test_model = theano.function(
        inputs=[index],
        outputs=classifier.errors(y),
        givens={
            x: test_set_x[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size],
            y: test_set_y[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size]
        }
    )

    validate_model = theano.function(
        inputs=[index],
        outputs=classifier.errors(y),
        givens={
            x: valid_set_x[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size],
            y: valid_set_y[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size]
        }
    )
</code></pre>
<h2 id="&#x628A;&#x5B83;&#x4EEC;&#x7EC4;&#x5408;&#x8D77;&#x6765;">&#x628A;&#x5B83;&#x4EEC;&#x7EC4;&#x5408;&#x8D77;&#x6765;</h2>
<p>&#x6700;&#x540E;&#x7684;&#x4EE3;&#x7801;&#x5982;&#x4E0B;&#x3002;</p>
<pre><code class="lang-Python"><span class="hljs-string">&quot;&quot;&quot;
This tutorial introduces logistic regression using Theano and stochastic
gradient descent.

Logistic regression is a probabilistic, linear classifier. It is parametrized
by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is
done by projecting data points onto a set of hyperplanes, the distance to
which is used to determine a class membership probability.

Mathematically, this can be written as:

.. math::
  P(Y=i|x, W,b) &amp;= softmax_i(W x + b) \\
                &amp;= \frac {e^{W_i x + b_i}} {\sum_j e^{W_j x + b_j}}


The output of the model or prediction is then done by taking the argmax of
the vector whose i&apos;th element is P(Y=i|x).

.. math::

  y_{pred} = argmax_i P(Y=i|x,W,b)


This tutorial presents a stochastic gradient descent optimization method
suitable for large datasets.


References:

    - textbooks: &quot;Pattern Recognition and Machine Learning&quot; -
                 Christopher M. Bishop, section 4.3.2

&quot;&quot;&quot;</span>
__docformat__ = <span class="hljs-string">&apos;restructedtext en&apos;</span>

<span class="hljs-keyword">import</span> cPickle
<span class="hljs-keyword">import</span> gzip
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">import</span> time

<span class="hljs-keyword">import</span> numpy

<span class="hljs-keyword">import</span> theano
<span class="hljs-keyword">import</span> theano.tensor <span class="hljs-keyword">as</span> T


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LogisticRegression</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;Multi-class Logistic Regression Class

    The logistic regression is fully described by a weight matrix :math:`W`
    and bias vector :math:`b`. Classification is done by projecting data
    points onto a set of hyperplanes, the distance to which is used to
    determine a class membership probability.
    &quot;&quot;&quot;</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, input, n_in, n_out)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Initialize the parameters of the logistic regression

        :type input: theano.tensor.TensorType
        :param input: symbolic variable that describes the input of the
                      architecture (one minibatch)

        :type n_in: int
        :param n_in: number of input units, the dimension of the space in
                     which the datapoints lie

        :type n_out: int
        :param n_out: number of output units, the dimension of the space in
                      which the labels lie

        &quot;&quot;&quot;</span>
        <span class="hljs-comment"># start-snippet-1</span>
        <span class="hljs-comment"># initialize with 0 the weights W as a matrix of shape (n_in, n_out)</span>
        self.W = theano.shared(
            value=numpy.zeros(
                (n_in, n_out),
                dtype=theano.config.floatX
            ),
            name=<span class="hljs-string">&apos;W&apos;</span>,
            borrow=<span class="hljs-keyword">True</span>
        )
        <span class="hljs-comment"># initialize the baises b as a vector of n_out 0s</span>
        self.b = theano.shared(
            value=numpy.zeros(
                (n_out,),
                dtype=theano.config.floatX
            ),
            name=<span class="hljs-string">&apos;b&apos;</span>,
            borrow=<span class="hljs-keyword">True</span>
        )

        <span class="hljs-comment"># symbolic expression for computing the matrix of class-membership</span>
        <span class="hljs-comment"># probabilities</span>
        <span class="hljs-comment"># Where:</span>
        <span class="hljs-comment"># W is a matrix where column-k represent the separation hyper plain for</span>
        <span class="hljs-comment"># class-k</span>
        <span class="hljs-comment"># x is a matrix where row-j  represents input training sample-j</span>
        <span class="hljs-comment"># b is a vector where element-k represent the free parameter of hyper</span>
        <span class="hljs-comment"># plain-k</span>
        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)

        <span class="hljs-comment"># symbolic description of how to compute prediction as class whose</span>
        <span class="hljs-comment"># probability is maximal</span>
        self.y_pred = T.argmax(self.p_y_given_x, axis=<span class="hljs-number">1</span>)
        <span class="hljs-comment"># end-snippet-1</span>

        <span class="hljs-comment"># parameters of the model</span>
        self.params = [self.W, self.b]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">negative_log_likelihood</span><span class="hljs-params">(self, y)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Return the mean of the negative log-likelihood of the prediction
        of this model under a given target distribution.

        .. math::

            \frac{1}{|\mathcal{D}|} \mathcal{L} (\theta=\{W,b\}, \mathcal{D}) =
            \frac{1}{|\mathcal{D}|} \sum_{i=0}^{|\mathcal{D}|}
                \log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\
            \ell (\theta=\{W,b\}, \mathcal{D})

        :type y: theano.tensor.TensorType
        :param y: corresponds to a vector that gives for each example the
                  correct label

        Note: we use the mean instead of the sum so that
              the learning rate is less dependent on the batch size
        &quot;&quot;&quot;</span>
        <span class="hljs-comment"># start-snippet-2</span>
        <span class="hljs-comment"># y.shape[0] is (symbolically) the number of rows in y, i.e.,</span>
        <span class="hljs-comment"># number of examples (call it n) in the minibatch</span>
        <span class="hljs-comment"># T.arange(y.shape[0]) is a symbolic vector which will contain</span>
        <span class="hljs-comment"># [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of</span>
        <span class="hljs-comment"># Log-Probabilities (call it LP) with one row per example and</span>
        <span class="hljs-comment"># one column per class LP[T.arange(y.shape[0]),y] is a vector</span>
        <span class="hljs-comment"># v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,</span>
        <span class="hljs-comment"># LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is</span>
        <span class="hljs-comment"># the mean (across minibatch examples) of the elements in v,</span>
        <span class="hljs-comment"># i.e., the mean log-likelihood across the minibatch.</span>
        <span class="hljs-keyword">return</span> -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[<span class="hljs-number">0</span>]), y])
        <span class="hljs-comment"># end-snippet-2</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">errors</span><span class="hljs-params">(self, y)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Return a float representing the number of errors in the minibatch
        over the total number of examples of the minibatch ; zero one
        loss over the size of the minibatch

        :type y: theano.tensor.TensorType
        :param y: corresponds to a vector that gives for each example the
                  correct label
        &quot;&quot;&quot;</span>

        <span class="hljs-comment"># check if y has same dimension of y_pred</span>
        <span class="hljs-keyword">if</span> y.ndim != self.y_pred.ndim:
            <span class="hljs-keyword">raise</span> TypeError(
                <span class="hljs-string">&apos;y should have the same shape as self.y_pred&apos;</span>,
                (<span class="hljs-string">&apos;y&apos;</span>, y.type, <span class="hljs-string">&apos;y_pred&apos;</span>, self.y_pred.type)
            )
        <span class="hljs-comment"># check if y is of the correct datatype</span>
        <span class="hljs-keyword">if</span> y.dtype.startswith(<span class="hljs-string">&apos;int&apos;</span>):
            <span class="hljs-comment"># the T.neq operator returns a vector of 0s and 1s, where 1</span>
            <span class="hljs-comment"># represents a mistake in prediction</span>
            <span class="hljs-keyword">return</span> T.mean(T.neq(self.y_pred, y))
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">raise</span> NotImplementedError()


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_data</span><span class="hljs-params">(dataset)</span>:</span>
    <span class="hljs-string">&apos;&apos;&apos; Loads the dataset

    :type dataset: string
    :param dataset: the path to the dataset (here MNIST)
    &apos;&apos;&apos;</span>

    <span class="hljs-comment">#############</span>
    <span class="hljs-comment"># LOAD DATA #</span>
    <span class="hljs-comment">#############</span>

    <span class="hljs-comment"># Download the MNIST dataset if it is not present</span>
    data_dir, data_file = os.path.split(dataset)
    <span class="hljs-keyword">if</span> data_dir == <span class="hljs-string">&quot;&quot;</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> os.path.isfile(dataset):
        <span class="hljs-comment"># Check if dataset is in the data directory.</span>
        new_path = os.path.join(
            os.path.split(__file__)[<span class="hljs-number">0</span>],
            <span class="hljs-string">&quot;..&quot;</span>,
            <span class="hljs-string">&quot;data&quot;</span>,
            dataset
        )
        <span class="hljs-keyword">if</span> os.path.isfile(new_path) <span class="hljs-keyword">or</span> data_file == <span class="hljs-string">&apos;mnist.pkl.gz&apos;</span>:
            dataset = new_path

    <span class="hljs-keyword">if</span> (<span class="hljs-keyword">not</span> os.path.isfile(dataset)) <span class="hljs-keyword">and</span> data_file == <span class="hljs-string">&apos;mnist.pkl.gz&apos;</span>:
        <span class="hljs-keyword">import</span> urllib
        origin = (
            <span class="hljs-string">&apos;http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz&apos;</span>
        )
        <span class="hljs-keyword">print</span> <span class="hljs-string">&apos;Downloading data from %s&apos;</span> % origin
        urllib.urlretrieve(origin, dataset)

    <span class="hljs-keyword">print</span> <span class="hljs-string">&apos;... loading data&apos;</span>

    <span class="hljs-comment"># Load the dataset</span>
    f = gzip.open(dataset, <span class="hljs-string">&apos;rb&apos;</span>)
    train_set, valid_set, test_set = cPickle.load(f)
    f.close()
    <span class="hljs-comment">#train_set, valid_set, test_set format: tuple(input, target)</span>
    <span class="hljs-comment">#input is an numpy.ndarray of 2 dimensions (a matrix)</span>
    <span class="hljs-comment">#witch row&apos;s correspond to an example. target is a</span>
    <span class="hljs-comment">#numpy.ndarray of 1 dimensions (vector)) that have the same length as</span>
    <span class="hljs-comment">#the number of rows in the input. It should give the target</span>
    <span class="hljs-comment">#target to the example with the same index in the input.</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">shared_dataset</span><span class="hljs-params">(data_xy, borrow=True)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Function that loads the dataset into shared variables

        The reason we store our dataset in shared variables is to allow
        Theano to copy it into the GPU memory (when code is run on GPU).
        Since copying data into the GPU is slow, copying a minibatch everytime
        is needed (the default behaviour if the data is not in a shared
        variable) would lead to a large decrease in performance.
        &quot;&quot;&quot;</span>
        data_x, data_y = data_xy
        shared_x = theano.shared(numpy.asarray(data_x,
                                               dtype=theano.config.floatX),
                                 borrow=borrow)
        shared_y = theano.shared(numpy.asarray(data_y,
                                               dtype=theano.config.floatX),
                                 borrow=borrow)
        <span class="hljs-comment"># When storing data on the GPU it has to be stored as floats</span>
        <span class="hljs-comment"># therefore we will store the labels as ``floatX`` as well</span>
        <span class="hljs-comment"># (``shared_y`` does exactly that). But during our computations</span>
        <span class="hljs-comment"># we need them as ints (we use labels as index, and if they are</span>
        <span class="hljs-comment"># floats it doesn&apos;t make sense) therefore instead of returning</span>
        <span class="hljs-comment"># ``shared_y`` we will have to cast it to int. This little hack</span>
        <span class="hljs-comment"># lets ous get around this issue</span>
        <span class="hljs-keyword">return</span> shared_x, T.cast(shared_y, <span class="hljs-string">&apos;int32&apos;</span>)

    test_set_x, test_set_y = shared_dataset(test_set)
    valid_set_x, valid_set_y = shared_dataset(valid_set)
    train_set_x, train_set_y = shared_dataset(train_set)

    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),
            (test_set_x, test_set_y)]
    <span class="hljs-keyword">return</span> rval


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sgd_optimization_mnist</span><span class="hljs-params">(learning_rate=<span class="hljs-number">0.13</span>, n_epochs=<span class="hljs-number">1000</span>,
                           dataset=<span class="hljs-string">&apos;mnist.pkl.gz&apos;</span>,
                           batch_size=<span class="hljs-number">600</span>)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;
    Demonstrate stochastic gradient descent optimization of a log-linear
    model

    This is demonstrated on MNIST.

    :type learning_rate: float
    :param learning_rate: learning rate used (factor for the stochastic
                          gradient)

    :type n_epochs: int
    :param n_epochs: maximal number of epochs to run the optimizer

    :type dataset: string
    :param dataset: the path of the MNIST dataset file from
                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz

    &quot;&quot;&quot;</span>
    datasets = load_data(dataset)

    train_set_x, train_set_y = datasets[<span class="hljs-number">0</span>]
    valid_set_x, valid_set_y = datasets[<span class="hljs-number">1</span>]
    test_set_x, test_set_y = datasets[<span class="hljs-number">2</span>]

    <span class="hljs-comment"># compute number of minibatches for training, validation and testing</span>
    n_train_batches = train_set_x.get_value(borrow=<span class="hljs-keyword">True</span>).shape[<span class="hljs-number">0</span>] / batch_size
    n_valid_batches = valid_set_x.get_value(borrow=<span class="hljs-keyword">True</span>).shape[<span class="hljs-number">0</span>] / batch_size
    n_test_batches = test_set_x.get_value(borrow=<span class="hljs-keyword">True</span>).shape[<span class="hljs-number">0</span>] / batch_size

    <span class="hljs-comment">######################</span>
    <span class="hljs-comment"># BUILD ACTUAL MODEL #</span>
    <span class="hljs-comment">######################</span>
    <span class="hljs-keyword">print</span> <span class="hljs-string">&apos;... building the model&apos;</span>

    <span class="hljs-comment"># allocate symbolic variables for the data</span>
    index = T.lscalar()  <span class="hljs-comment"># index to a [mini]batch</span>

    <span class="hljs-comment"># generate symbolic variables for input (x and y represent a</span>
    <span class="hljs-comment"># minibatch)</span>
    x = T.matrix(<span class="hljs-string">&apos;x&apos;</span>)  <span class="hljs-comment"># data, presented as rasterized images</span>
    y = T.ivector(<span class="hljs-string">&apos;y&apos;</span>)  <span class="hljs-comment"># labels, presented as 1D vector of [int] labels</span>

    <span class="hljs-comment"># construct the logistic regression class</span>
    <span class="hljs-comment"># Each MNIST image has size 28*28</span>
    classifier = LogisticRegression(input=x, n_in=<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, n_out=<span class="hljs-number">10</span>)

    <span class="hljs-comment"># the cost we minimize during training is the negative log likelihood of</span>
    <span class="hljs-comment"># the model in symbolic format</span>
    cost = classifier.negative_log_likelihood(y)

    <span class="hljs-comment"># compiling a Theano function that computes the mistakes that are made by</span>
    <span class="hljs-comment"># the model on a minibatch</span>
    test_model = theano.function(
        inputs=[index],
        outputs=classifier.errors(y),
        givens={
            x: test_set_x[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size],
            y: test_set_y[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size]
        }
    )

    validate_model = theano.function(
        inputs=[index],
        outputs=classifier.errors(y),
        givens={
            x: valid_set_x[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size],
            y: valid_set_y[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size]
        }
    )

    <span class="hljs-comment"># compute the gradient of cost with respect to theta = (W,b)</span>
    g_W = T.grad(cost=cost, wrt=classifier.W)
    g_b = T.grad(cost=cost, wrt=classifier.b)

    <span class="hljs-comment"># start-snippet-3</span>
    <span class="hljs-comment"># specify how to update the parameters of the model as a list of</span>
    <span class="hljs-comment"># (variable, update expression) pairs.</span>
    updates = [(classifier.W, classifier.W - learning_rate * g_W),
               (classifier.b, classifier.b - learning_rate * g_b)]

    <span class="hljs-comment"># compiling a Theano function `train_model` that returns the cost, but in</span>
    <span class="hljs-comment"># the same time updates the parameter of the model based on the rules</span>
    <span class="hljs-comment"># defined in `updates`</span>
    train_model = theano.function(
        inputs=[index],
        outputs=cost,
        updates=updates,
        givens={
            x: train_set_x[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size],
            y: train_set_y[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size]
        }
    )
    <span class="hljs-comment"># end-snippet-3</span>

    <span class="hljs-comment">###############</span>
    <span class="hljs-comment"># TRAIN MODEL #</span>
    <span class="hljs-comment">###############</span>
    <span class="hljs-keyword">print</span> <span class="hljs-string">&apos;... training the model&apos;</span>
    <span class="hljs-comment"># early-stopping parameters</span>
    patience = <span class="hljs-number">5000</span>  <span class="hljs-comment"># look as this many examples regardless</span>
    patience_increase = <span class="hljs-number">2</span>  <span class="hljs-comment"># wait this much longer when a new best is</span>
                                  <span class="hljs-comment"># found</span>
    improvement_threshold = <span class="hljs-number">0.995</span>  <span class="hljs-comment"># a relative improvement of this much is</span>
                                  <span class="hljs-comment"># considered significant</span>
    validation_frequency = min(n_train_batches, patience / <span class="hljs-number">2</span>)
                                  <span class="hljs-comment"># go through this many</span>
                                  <span class="hljs-comment"># minibatche before checking the network</span>
                                  <span class="hljs-comment"># on the validation set; in this case we</span>
                                  <span class="hljs-comment"># check every epoch</span>

    best_validation_loss = numpy.inf
    test_score = <span class="hljs-number">0.</span>
    start_time = time.clock()

    done_looping = <span class="hljs-keyword">False</span>
    epoch = <span class="hljs-number">0</span>
    <span class="hljs-keyword">while</span> (epoch &lt; n_epochs) <span class="hljs-keyword">and</span> (<span class="hljs-keyword">not</span> done_looping):
        epoch = epoch + <span class="hljs-number">1</span>
        <span class="hljs-keyword">for</span> minibatch_index <span class="hljs-keyword">in</span> xrange(n_train_batches):

            minibatch_avg_cost = train_model(minibatch_index)
            <span class="hljs-comment"># iteration number</span>
            iter = (epoch - <span class="hljs-number">1</span>) * n_train_batches + minibatch_index

            <span class="hljs-keyword">if</span> (iter + <span class="hljs-number">1</span>) % validation_frequency == <span class="hljs-number">0</span>:
                <span class="hljs-comment"># compute zero-one loss on validation set</span>
                validation_losses = [validate_model(i)
                                     <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> xrange(n_valid_batches)]
                this_validation_loss = numpy.mean(validation_losses)

                print(
                    <span class="hljs-string">&apos;epoch %i, minibatch %i/%i, validation error %f %%&apos;</span> %
                    (
                        epoch,
                        minibatch_index + <span class="hljs-number">1</span>,
                        n_train_batches,
                        this_validation_loss * <span class="hljs-number">100.</span>
                    )
                )

                <span class="hljs-comment"># if we got the best validation score until now</span>
                <span class="hljs-keyword">if</span> this_validation_loss &lt; best_validation_loss:
                    <span class="hljs-comment">#improve patience if loss improvement is good enough</span>
                    <span class="hljs-keyword">if</span> this_validation_loss &lt; best_validation_loss *  \
                       improvement_threshold:
                        patience = max(patience, iter * patience_increase)

                    best_validation_loss = this_validation_loss
                    <span class="hljs-comment"># test it on the test set</span>

                    test_losses = [test_model(i)
                                   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> xrange(n_test_batches)]
                    test_score = numpy.mean(test_losses)

                    print(
                        (
                            <span class="hljs-string">&apos;     epoch %i, minibatch %i/%i, test error of&apos;</span>
                            <span class="hljs-string">&apos; best model %f %%&apos;</span>
                        ) %
                        (
                            epoch,
                            minibatch_index + <span class="hljs-number">1</span>,
                            n_train_batches,
                            test_score * <span class="hljs-number">100.</span>
                        )
                    )

            <span class="hljs-keyword">if</span> patience &lt;= iter:
                done_looping = <span class="hljs-keyword">True</span>
                <span class="hljs-keyword">break</span>

    end_time = time.clock()
    print(
        (
            <span class="hljs-string">&apos;Optimization complete with best validation score of %f %%,&apos;</span>
            <span class="hljs-string">&apos;with test performance %f %%&apos;</span>
        )
        % (best_validation_loss * <span class="hljs-number">100.</span>, test_score * <span class="hljs-number">100.</span>)
    )
    <span class="hljs-keyword">print</span> <span class="hljs-string">&apos;The code run for %d epochs, with %f epochs/sec&apos;</span> % (
        epoch, <span class="hljs-number">1.</span> * epoch / (end_time - start_time))
    <span class="hljs-keyword">print</span> &gt;&gt; sys.stderr, (<span class="hljs-string">&apos;The code for file &apos;</span> +
                          os.path.split(__file__)[<span class="hljs-number">1</span>] +
                          <span class="hljs-string">&apos; ran for %.1fs&apos;</span> % ((end_time - start_time)))

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&apos;__main__&apos;</span>:
    sgd_optimization_mnist()
</code></pre>
<p>&#x8FD9;&#x4E2A;&#x8F93;&#x51FA;&#x5C06;&#x662F;&#x5982;&#x4E0B;&#x7684;&#x683C;&#x5F0F;</p>
<pre><code>...
epoch 72, minibatch 83/83, validation error 7.510417 %
epoch 72, minibatch 83/83, test error of best model 7.510417 %
epoch 73, minibatch 83/83, validation error 7.500000 %
epoch 73, minibatch 83/83, test error of best model 7.489583 %
Optimization complete with best validation score of 7.500000 %,with test performance 7.489583 %
The code run for 74 epochs, with 1.936983 epochs/sec
</code></pre><p>&#x5728;<code>Intel(R) Core(TM)2 Duo CPU E8400 @ 3.00 Ghz</code>&#x4E0A;&#xFF0C;&#x8FD9;&#x4E2A;&#x4EE3;&#x7801;&#x7684;&#x901F;&#x5EA6;&#x662F;1.936 epochs/sec&#x7136;&#x540E;&#x8DD1;75 epochs&#xFF0C;&#x5F97;&#x5230;&#x6D4B;&#x8BD5;&#x9519;&#x8BEF;&#x7387;&#x4E3A;7.489%&#x3002;&#x5728;GPU&#x4E0A;&#x4E3A;10.0 epochs/sec&#x5728;&#x8FD9;&#x4E2A;&#x5B9E;&#x4F8B;&#x4E2D;&#x6211;&#x4EEC;&#x5B9A;&#x4E49;&#x4E3A;batch&#x7684;&#x5927;&#x5C0F;&#x4E3A;600&#x3002;</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="1_Getting_Started_入门.html" class="navigation navigation-prev " aria-label="Previous page: 1.入门">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="3_Multilayer_Perceptron_多层感知机.html" class="navigation navigation-next " aria-label="Next page: 3.多层感知机">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"2.逻辑回归进行MNIST分类","level":"1.3","depth":1,"next":{"title":"3.多层感知机","level":"1.4","depth":1,"path":"3_Multilayer_Perceptron_多层感知机.md","ref":"3_Multilayer_Perceptron_多层感知机.md","articles":[]},"previous":{"title":"1.入门","level":"1.2","depth":1,"path":"1_Getting_Started_入门.md","ref":"1_Getting_Started_入门.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"2_Classifying_MNIST_using_LR_逻辑回归进行MNIST分类.md","mtime":"2017-07-12T16:21:46.000Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2021-05-10T15:44:26.709Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

