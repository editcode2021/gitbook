
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>3.多层感知机 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="4_Convoltional_Neural_Networks_LeNet_卷积神经网络.html" />
    
    
    <link rel="prev" href="2_Classifying_MNIST_using_LR_逻辑回归进行MNIST分类.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    简介
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="1_Getting_Started_入门.html">
            
                <a href="1_Getting_Started_入门.html">
            
                    
                    1.入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="2_Classifying_MNIST_using_LR_逻辑回归进行MNIST分类.html">
            
                <a href="2_Classifying_MNIST_using_LR_逻辑回归进行MNIST分类.html">
            
                    
                    2.逻辑回归进行MNIST分类
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.4" data-path="3_Multilayer_Perceptron_多层感知机.html">
            
                <a href="3_Multilayer_Perceptron_多层感知机.html">
            
                    
                    3.多层感知机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="4_Convoltional_Neural_Networks_LeNet_卷积神经网络.html">
            
                <a href="4_Convoltional_Neural_Networks_LeNet_卷积神经网络.html">
            
                    
                    4.卷积神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="5_Denoising_Autoencoders_降噪自动编码.html">
            
                <a href="5_Denoising_Autoencoders_降噪自动编码.html">
            
                    
                    5.降噪自动编码
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="6_Stacked_Denoising_Autoencoders_层叠降噪自动编码机.html">
            
                <a href="6_Stacked_Denoising_Autoencoders_层叠降噪自动编码机.html">
            
                    
                    6.层叠降噪自动编码机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="7_Restricted_Boltzmann_Machine_受限波尔兹曼机.html">
            
                <a href="7_Restricted_Boltzmann_Machine_受限波尔兹曼机.html">
            
                    
                    7.受限波尔兹曼机
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >3.多层感知机</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="&#x591A;&#x5C42;&#x611F;&#x77E5;&#x673A;&#xFF08;multilayer-perceptron&#xFF09;">&#x591A;&#x5C42;&#x611F;&#x77E5;&#x673A;&#xFF08;Multilayer Perceptron&#xFF09;</h1>
<p>&#x5728;&#x672C;&#x8282;&#x4E2D;&#xFF0C;&#x5047;&#x8BBE;&#x4F60;&#x5DF2;&#x7ECF;&#x4E86;&#x89E3;&#x4E86;<a href="https://github.com/Syndrome777/DeepLearningTutorial/blob/master/2_Classifying_MNIST_using_LR_&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x8FDB;&#x884C;MNIST&#x5206;&#x7C7B;.md" target="_blank">&#x4F7F;&#x7528;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x8FDB;&#x884C;MNIST&#x5206;&#x7C7B;</a>&#x3002;&#x540C;&#x65F6;&#x672C;&#x8282;&#x7684;&#x6240;&#x6709;&#x4EE3;&#x7801;&#x53EF;&#x4EE5;&#x5728;<a href="http://deeplearning.net/tutorial/code/mlp.py" target="_blank">&#x8FD9;&#x91CC;</a>&#x4E0B;&#x8F7D;.</p>
<p>&#x4E0B;&#x4E00;&#x4E2A;&#x6211;&#x4EEC;&#x5C06;&#x5728;Theano&#x4E2D;&#x4F7F;&#x7528;&#x7684;&#x7ED3;&#x6784;&#x662F;&#x5355;&#x9690;&#x5C42;&#x7684;&#x591A;&#x5C42;&#x611F;&#x77E5;&#x673A;&#xFF08;MLP&#xFF09;&#x3002;MLP&#x53EF;&#x4EE5;&#x88AB;&#x770B;&#x4F5C;&#x4E00;&#x4E2A;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x5206;&#x7C7B;&#x5668;&#x3002;&#x8FD9;&#x4E2A;&#x4E2D;&#x95F4;&#x5C42;&#x88AB;&#x79F0;&#x4E3A;&#x9690;&#x85CF;&#x5C42;&#x3002;&#x4E00;&#x4E2A;&#x5355;&#x9690;&#x5C42;&#x5BF9;&#x4E8E;MLP&#x6210;&#x4E3A;&#x901A;&#x7528;&#x8FD1;&#x4F3C;&#x5668;&#x662F;&#x6709;&#x6548;&#x7684;&#x3002;&#x7136;&#x800C;&#x5728;&#x540E;&#x9762;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x8BB2;&#x8FF0;&#x4F7F;&#x7528;&#x591A;&#x4E2A;&#x9690;&#x85CF;&#x5C42;&#x7684;&#x597D;&#x5904;&#xFF0C;&#x4F8B;&#x5982;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x7684;&#x524D;&#x63D0;&#x3002;&#x8FD9;&#x4E2A;&#x8BFE;&#x7A0B;&#x4ECB;&#x7ECD;&#x4E86;<a href="http://www.iro.umontreal.ca/~pift6266/H10/notes/mlp.html" target="_blank">MLP&#xFF0C;&#x53CD;&#x5411;&#x8BEF;&#x5DEE;&#x4F20;&#x5BFC;&#xFF0C;&#x5982;&#x4F55;&#x8BAD;&#x7EC3;MLPs</a>&#x3002;</p>
<h2 id="&#x6A21;&#x578B;">&#x6A21;&#x578B;</h2>
<p>&#x4E00;&#x4E2A;&#x591A;&#x5C42;&#x611F;&#x77E5;&#x673A;&#xFF08;&#x6216;&#x8005;&#x8BF4;&#x4EBA;&#x5DE5;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x2014;&#x2014;ANN&#xFF09;,&#x5728;&#x53EA;&#x6709;&#x4E00;&#x4E2A;&#x9690;&#x85CF;&#x5C42;&#x65F6;&#x53EF;&#x4EE5;&#x88AB;&#x8868;&#x793A;&#x4E3A;&#x5982;&#x4E0B;&#x7684;&#x56FE;&#xFF1A;</p>
<p><img src="images/3_the_model_1.png" alt="mlp_model_1"></p>
<p>&#x4E8B;&#x5B9E;&#x4E0A;&#xFF0C;&#x4E00;&#x4E2A;&#x5355;&#x9690;&#x85CF;&#x5C42;&#x7684;MLP&#x662F;&#x4E00;&#x4E2A;&#x5982;&#x4E0B;&#x7684;&#x51FD;&#x6570;<img src="images/3_the_model_2.png" alt="mlp_model_2">&#xFF0C;&#x5176;&#x4E2D;x&#x662F;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x7684;&#x7EF4;&#x5EA6;&#xFF0C;L&#x662F;&#x8F93;&#x51FA;&#x5411;&#x91CF;&#x7684;&#x7EF4;&#x5EA6;&#x3002;&#x6211;&#x4EEC;&#x7528;&#x4E0B;&#x9762;&#x7684;&#x516C;&#x5F0F;&#x6765;&#x8868;&#x793A;MLP&#x6A21;&#x578B;&#xFF1A;</p>
<p><img src="images/3_the_model_3.png" alt="mlp_model_3"></p>
<p>&#x5176;&#x4E2D;b_1&#xFF0C;W_1&#x662F;&#x8F93;&#x51FA;&#x5C42;&#x5230;&#x9690;&#x85CF;&#x5C42;&#x7684;&#x504F;&#x7F6E;&#x5411;&#x91CF;&#x548C;&#x6743;&#x503C;&#x77E9;&#x9635;&#xFF0C;s&#x662F;&#x8BE5;&#x5C42;&#x7684;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x3002;&#x800C;b_2&#xFF0C;W_2&#x662F;&#x9690;&#x85CF;&#x5C42;&#x5230;&#x8F93;&#x51FA;&#x5C42;&#x7684;&#x504F;&#x7F6E;&#x5411;&#x91CF;&#x548C;&#x6743;&#x503C;&#x77E9;&#x9635;&#xFF0C;G&#x662F;&#x8BE5;&#x5C42;&#x7684;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x3002;&#x901A;&#x5E38;&#x9009;&#x62E9;s&#x4E3A;sigmoid&#x51FD;&#x6570;&#xFF0C;G&#x4E3A;softmax&#x51FD;&#x6570;&#x3002;
&#x5728;&#x8BAD;&#x7EC3;MLP&#x6A21;&#x578B;&#x7684;&#x53C2;&#x6570;&#x65F6;&#xFF0C;&#x6211;&#x4EEC;&#x4F7F;&#x7528;minibatch&#x7684;&#x968F;&#x673A;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#xFF0C;&#x5728;&#x83B7;&#x5F97;&#x68AF;&#x5EA6;&#x540E;&#x4F7F;&#x7528;&#x53CD;&#x5411;&#x8BEF;&#x5DEE;&#x4F20;&#x5BFC;&#x7B97;&#x6CD5;&#x6765;&#x5B9E;&#x73B0;&#x53C2;&#x6570;&#x7684;&#x8BAD;&#x7EC3;&#x3002;&#x7531;&#x4E8E;Theano&#x63D0;&#x4F9B;&#x81EA;&#x52A8;&#x7684;&#x5FAE;&#x5206;&#xFF0C;&#x6211;&#x4EEC;&#x4E0D;&#x9700;&#x8981;&#x5728;&#x8FD9;&#x4E2A;&#x6559;&#x7A0B;&#x91CC;&#x9762;&#x8C08;&#x53CA;&#x8FD9;&#x4E2A;&#x65B9;&#x9762;&#x3002;</p>
<h2 id="&#x4ECE;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x5230;&#x591A;&#x5C42;&#x611F;&#x77E5;&#x673A;">&#x4ECE;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x5230;&#x591A;&#x5C42;&#x611F;&#x77E5;&#x673A;</h2>
<p>&#x672C;&#x6559;&#x7A0B;&#x5C06;&#x4E13;&#x6CE8;&#x4E8E;&#x5355;&#x9690;&#x85CF;&#x5C42;&#x7684;MLP&#x3002;&#x6211;&#x4EEC;&#x4EE5;&#x9690;&#x85CF;&#x5C42;&#x7684;&#x7C7B;&#x7684;&#x5B9E;&#x73B0;&#x5F00;&#x59CB;&#xFF0C;&#x5982;&#x679C;&#x8981;&#x6784;&#x5EFA;&#x4E00;&#x4E2A;MLP&#xFF0C;&#x53EA;&#x9700;&#x8981;&#x5728;&#x6B64;&#x57FA;&#x7840;&#x4E0A;&#x6DFB;&#x52A0;&#x4E00;&#x4E2A;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x5C31;&#x597D;&#x3002;</p>
<pre><code class="lang-Python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HiddenLayer</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, rng, input, n_in, n_out, W=None, b=None,
                 activation=T.tanh)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        Typical hidden layer of a MLP: units are fully-connected and have
        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)
        and the bias vector b is of shape (n_out,).

        NOTE : The nonlinearity used here is tanh

        Hidden unit activation is given by: tanh(dot(input,W) + b)

        :type rng: numpy.random.RandomState
        :param rng: a random number generator used to initialize weights

        :type input: theano.tensor.dmatrix
        :param input: a symbolic tensor of shape (n_examples, n_in)

        :type n_in: int
        :param n_in: dimensionality of input

        :type n_out: int
        :param n_out: number of hidden units

        :type activation: theano.Op or function
        :param activation: Non linearity to be applied in the hidden
                           layer
        &quot;&quot;&quot;</span>
        self.input = input
</code></pre>
<p>&#x4E00;&#x4E2A;&#x9690;&#x85CF;&#x5C42;&#x7684;&#x6743;&#x503C;&#x521D;&#x59CB;&#x5316;&#xFF0C;&#x5E94;&#x5F53;&#x4ECE;&#x57FA;&#x4E8E;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x7684;&#x5747;&#x5300;&#x95F4;&#x9694;&#x4E2D;&#x5747;&#x5300;&#x91C7;&#x6837;&#x3002;&#x5BF9;&#x4E8E;sigmoid&#x51FD;&#x6570;&#x800C;&#x8A00;&#xFF0C;&#x8FD9;&#x4E2A;&#x95F4;&#x9694;&#x662F;<img src="images/3_from_lr_to_mlp_1.png" alt="interval">&#x3002;&#x5176;&#x4E2D;fan_in&#x662F;&#x7B2C;&#xFF08;i&#xFF0D;1&#xFF09;&#x5C42;&#x7684;&#x5355;&#x5143;&#x6570;&#x76EE;&#xFF0C;fan_out&#x662F;&#x7B2C;&#xFF08;i&#xFF09;&#x5C42;&#x5355;&#x5143;&#x7684;&#x6570;&#x76EE;&#xFF0C;<a href="http://deeplearning.net/tutorial/references.html#xavier10" target="_blank">&#x7ED3;&#x8BBA;&#x51FA;&#x81EA;&#x8FD9;&#x91CC;</a>&#x3002;
&#x8FD9;&#x6837;&#x7684;&#x521D;&#x59CB;&#x5316;&#xFF0C;&#x4FDD;&#x8BC1;&#x4E86;&#x5728;&#x8BAD;&#x7EC3;&#x7684;&#x65E9;&#x671F;&#xFF0C;&#x6BCF;&#x4E2A;&#x795E;&#x7ECF;&#x5143;&#x90FD;&#x53EF;&#x4EE5;&#x5DE5;&#x4F5C;&#x5728;&#x5B83;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x7684;&#x63A7;&#x5236;&#x8303;&#x56F4;&#x5185;&#xFF0C;&#x4ECE;&#x800C;&#x4F7F;&#x5F97;&#x4FE1;&#x606F;&#x53EF;&#x4EE5;&#x66F4;&#x7B80;&#x5355;&#x7684;&#x524D;&#x5411;&#x4F20;&#x5BFC;&#xFF08;&#x4ECE;&#x8F93;&#x5165;&#x5230;&#x8F93;&#x51FA;&#x7684;&#x6FC0;&#x6D3B;&#xFF09;&#x548C;&#x540E;&#x5411;&#x4F20;&#x5BFC;&#xFF08;&#x4ECE;&#x8F93;&#x51FA;&#x5230;&#x8F93;&#x5165;&#x7684;&#x68AF;&#x5EA6;&#xFF09;&#x3002;</p>
<pre><code class="lang-Python">        <span class="hljs-comment"># `W` is initialized with `W_values` which is uniformely sampled</span>
        <span class="hljs-comment"># from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))</span>
        <span class="hljs-comment"># for tanh activation function</span>
        <span class="hljs-comment"># the output of uniform if converted using asarray to dtype</span>
        <span class="hljs-comment"># theano.config.floatX so that the code is runable on GPU</span>
        <span class="hljs-comment"># Note : optimal initialization of weights is dependent on the</span>
        <span class="hljs-comment">#        activation function used (among other things).</span>
        <span class="hljs-comment">#        For example, results presented in [Xavier10] suggest that you</span>
        <span class="hljs-comment">#        should use 4 times larger initial weights for sigmoid</span>
        <span class="hljs-comment">#        compared to tanh</span>
        <span class="hljs-comment">#        We have no info for other function, so we use the same as</span>
        <span class="hljs-comment">#        tanh.</span>
        <span class="hljs-keyword">if</span> W <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:
            W_values = numpy.asarray(
                rng.uniform(
                    low=-numpy.sqrt(<span class="hljs-number">6.</span> / (n_in + n_out)),
                    high=numpy.sqrt(<span class="hljs-number">6.</span> / (n_in + n_out)),
                    size=(n_in, n_out)
                ),
                dtype=theano.config.floatX
            )
            <span class="hljs-keyword">if</span> activation == theano.tensor.nnet.sigmoid:
                W_values *= <span class="hljs-number">4</span>

            W = theano.shared(value=W_values, name=<span class="hljs-string">&apos;W&apos;</span>, borrow=<span class="hljs-keyword">True</span>)

        <span class="hljs-keyword">if</span> b <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:
            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)
            b = theano.shared(value=b_values, name=<span class="hljs-string">&apos;b&apos;</span>, borrow=<span class="hljs-keyword">True</span>)

        self.W = W
        self.b = b
</code></pre>
<p>&#x6CE8;&#x610F;&#xFF0C;&#x6211;&#x4EEC;&#x901A;&#x8FC7;&#x4F1A;&#x5C06;&#x4E00;&#x4E2A;&#x7ED9;&#x5B9A;&#x7684;&#x975E;&#x7EBF;&#x6027;&#x51FD;&#x6570;&#x4F5C;&#x4E3A;&#x9690;&#x85CF;&#x5C42;&#x7684;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x3002;&#x9ED8;&#x8BA4;&#x662F;<code>tanh</code>&#x51FD;&#x6570;&#xFF0C;&#x5F53;&#x7136;&#x5F88;&#x591A;&#x65F6;&#x5019;&#x4F60;&#x53EF;&#x80FD;&#x9700;&#x8981;&#x5176;&#x4ED6;&#x51FD;&#x6570;&#x3002;</p>
<pre><code class="lang-Python">        lin_output = T.dot(input, self.W) + self.b
        self.output = (
            lin_output <span class="hljs-keyword">if</span> activation <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>
            <span class="hljs-keyword">else</span> activation(lin_output)
        )
</code></pre>
<p>&#x5982;&#x679C;&#x4F60;&#x5DF2;&#x7ECF;&#x9605;&#x8BFB;&#x4E86;&#x4E0A;&#x9762;&#x7684;&#x9690;&#x85CF;&#x5C42;&#x8F93;&#x51FA;&#x548C;<a href="https://github.com/Syndrome777/DeepLearningTutorial/blob/master/2_Classifying_MNIST_using_LR_&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x8FDB;&#x884C;MNIST&#x5206;&#x7C7B;.md" target="_blank">&#x4F7F;&#x7528;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x8FDB;&#x884C;MNIST&#x5206;&#x7C7B;</a>&#x3002;&#x90A3;&#x4E48;&#x4F60;&#x53EF;&#x4EE5;&#x770B;&#x4E0B;&#x9762;&#x7684;MLP&#x7C7B;&#x7684;&#x5B9E;&#x73B0;&#x4E86;&#x3002;</p>
<pre><code class="lang-Python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLP</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;Multi-Layer Perceptron Class

    A multilayer perceptron is a feedforward artificial neural network model
    that has one layer or more of hidden units and nonlinear activations.
    Intermediate layers usually have as activation function tanh or the
    sigmoid function (defined here by a ``HiddenLayer`` class)  while the
    top layer is a softamx layer (defined here by a ``LogisticRegression``
    class).
    &quot;&quot;&quot;</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, rng, input, n_in, n_hidden, n_out)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Initialize the parameters for the multilayer perceptron

        :type rng: numpy.random.RandomState
        :param rng: a random number generator used to initialize weights

        :type input: theano.tensor.TensorType
        :param input: symbolic variable that describes the input of the
        architecture (one minibatch)

        :type n_in: int
        :param n_in: number of input units, the dimension of the space in
        which the datapoints lie

        :type n_hidden: int
        :param n_hidden: number of hidden units

        :type n_out: int
        :param n_out: number of output units, the dimension of the space in
        which the labels lie

        &quot;&quot;&quot;</span>

        <span class="hljs-comment"># Since we are dealing with a one hidden layer MLP, this will translate</span>
        <span class="hljs-comment"># into a HiddenLayer with a tanh activation function connected to the</span>
        <span class="hljs-comment"># LogisticRegression layer; the activation function can be replaced by</span>
        <span class="hljs-comment"># sigmoid or any other nonlinear function</span>
        self.hiddenLayer = HiddenLayer(
            rng=rng,
            input=input,
            n_in=n_in,
            n_out=n_hidden,
            activation=T.tanh
        )

        <span class="hljs-comment"># The logistic regression layer gets as input the hidden units</span>
        <span class="hljs-comment"># of the hidden layer</span>
        self.logRegressionLayer = LogisticRegression(
            input=self.hiddenLayer.output,
            n_in=n_hidden,
            n_out=n_out
        )
</code></pre>
<p>&#x5728;&#x672C;&#x8282;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x4E5F;&#x4F7F;&#x7528;L1/L2&#x6B63;&#x5219;&#x5316;&#xFF08;<a href="http://deeplearning.net/tutorial/gettingstarted.html#l1-l2-regularization" target="_blank">L1/L2&#x6B63;&#x5219;&#x5316;</a>&#xFF09;&#x3002;&#x6240;&#x4EE5;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x53BB;&#x8BA1;&#x7B97;W_1&#x548C;W_2&#x77E9;&#x9635;&#x7684;L1&#x6B63;&#x5219;&#x548C;L2&#x5E73;&#x65B9;&#x6B63;&#x5219;&#x3002;</p>
<pre><code class="lang-Python">        <span class="hljs-comment"># L1 norm ; one regularization option is to enforce L1 norm to</span>
        <span class="hljs-comment"># be small</span>
        self.L1 = (
            abs(self.hiddenLayer.W).sum()
            + abs(self.logRegressionLayer.W).sum()
        )

        <span class="hljs-comment"># square of L2 norm ; one regularization option is to enforce</span>
        <span class="hljs-comment"># square of L2 norm to be small</span>
        self.L2_sqr = (
            (self.hiddenLayer.W ** <span class="hljs-number">2</span>).sum()
            + (self.logRegressionLayer.W ** <span class="hljs-number">2</span>).sum()
        )

        <span class="hljs-comment"># negative log likelihood of the MLP is given by the negative</span>
        <span class="hljs-comment"># log likelihood of the output of the model, computed in the</span>
        <span class="hljs-comment"># logistic regression layer</span>
        self.negative_log_likelihood = (
            self.logRegressionLayer.negative_log_likelihood
        )
        <span class="hljs-comment"># same holds for the function computing the number of errors</span>
        self.errors = self.logRegressionLayer.errors

        <span class="hljs-comment"># the parameters of the model are the parameters of the two layer it is</span>
        <span class="hljs-comment"># made out of</span>
        self.params = self.hiddenLayer.params + self.logRegressionLayer.params
</code></pre>
<p>&#x5728;&#x6B64;&#x4E4B;&#x524D;&#xFF0C;&#x6211;&#x4EEC;&#x4F7F;&#x7528;minibatch&#x7684;&#x968F;&#x673A;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x6765;&#x8BAD;&#x7EC3;&#x8FD9;&#x4E2A;&#x6A21;&#x578B;&#x3002;&#x4E0D;&#x540C;&#x7684;&#x662F;&#xFF0C;&#x6211;&#x4EEC;&#x73B0;&#x5728;&#x5728;<code>cost</code>&#x51FD;&#x6570;&#x91CC;&#x9762;&#x6DFB;&#x52A0;&#x4E86;&#x6B63;&#x5219;&#x9879;&#x3002;<code>L1_reg</code>&#x548C;<code>L2_reg</code>&#x53EF;&#x4EE5;&#x63A7;&#x5236;&#x6743;&#x503C;&#x77E9;&#x9635;&#x7684;&#x6B63;&#x5219;&#x5316;&#x3002;&#x8BA1;&#x7B97;&#x65B0;cost&#x7684;&#x4EE3;&#x7801;&#x5982;&#x4E0B;&#xFF1A;</p>
<pre><code class="lang-Python">    <span class="hljs-comment"># the cost we minimize during training is the negative log likelihood of</span>
    <span class="hljs-comment"># the model plus the regularization terms (L1 and L2); cost is expressed</span>
    <span class="hljs-comment"># here symbolically</span>
    cost = (
        classifier.negative_log_likelihood(y)
        + L1_reg * classifier.L1
        + L2_reg * classifier.L2_sqr
    )
</code></pre>
<p>&#x6211;&#x4EEC;&#x4F7F;&#x7528;&#x68AF;&#x5EA6;&#x6765;&#x66F4;&#x65B0;&#x6A21;&#x578B;&#x53C2;&#x6570;&#xFF0C;&#x8FD9;&#x57FA;&#x672C;&#x548C;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x91CC;&#x9762;&#x7684;&#x4E00;&#x6837;&#x3002;&#x6211;&#x4EEC;&#x4ECE;&#x6A21;&#x578B;&#x7684;<code>params</code>&#x4E2D;&#x83B7;&#x53D6;&#x53C2;&#x6570;&#x5217;&#x8868;&#xFF0C;&#x7136;&#x540E;&#x5206;&#x6790;&#x5B83;&#xFF0C;&#x5E76;&#x8BA1;&#x7B97;&#x6BCF;&#x4E00;&#x6B65;&#x7684;&#x68AF;&#x5EA6;&#x3002;</p>
<pre><code class="lang-Python">    <span class="hljs-comment"># compute the gradient of cost with respect to theta (sotred in params)</span>
    <span class="hljs-comment"># the resulting gradients will be stored in a list gparams</span>
    gparams = [T.grad(cost, param) <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> classifier.params]

    <span class="hljs-comment"># specify how to update the parameters of the model as a list of</span>
    <span class="hljs-comment"># (variable, update expression) pairs</span>

    <span class="hljs-comment"># given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of</span>
    <span class="hljs-comment"># same length, zip generates a list C of same size, where each element</span>
    <span class="hljs-comment"># is a pair formed from the two lists :</span>
    <span class="hljs-comment">#    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]</span>
    updates = [
        (param, param - learning_rate * gparam)
        <span class="hljs-keyword">for</span> param, gparam <span class="hljs-keyword">in</span> zip(classifier.params, gparams)
    ]

    <span class="hljs-comment"># compiling a Theano function `train_model` that returns the cost, but</span>
    <span class="hljs-comment"># in the same time updates the parameter of the model based on the rules</span>
    <span class="hljs-comment"># defined in `updates`</span>
    train_model = theano.function(
        inputs=[index],
        outputs=cost,
        updates=updates,
        givens={
            x: train_set_x[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size],
            y: train_set_y[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size]
        }
    )
</code></pre>
<h2 id="&#x628A;&#x5B83;&#x7EC4;&#x5408;&#x8D77;&#x6765;">&#x628A;&#x5B83;&#x7EC4;&#x5408;&#x8D77;&#x6765;</h2>
<p>&#x5DF2;&#x7ECF;&#x89E3;&#x91CA;&#x4E86;&#x6240;&#x6709;&#x7684;&#x57FA;&#x672C;&#x8BE5;&#x6982;&#x5FF5;&#xFF0C;&#x4E0B;&#x9762;&#x7684;&#x4EE3;&#x7801;&#x5C31;&#x662F;&#x4E00;&#x4E2A;&#x5B8C;&#x6574;&#x7684;MLP&#x7C7B;&#x3002;</p>
<pre><code class="lang-Python"><span class="hljs-string">&quot;&quot;&quot;
This tutorial introduces the multilayer perceptron using Theano.

 A multilayer perceptron is a logistic regressor where
instead of feeding the input to the logistic regression you insert a
intermediate layer, called the hidden layer, that has a nonlinear
activation function (usually tanh or sigmoid) . One can use many such
hidden layers making the architecture deep. The tutorial will also tackle
the problem of MNIST digit classification.

.. math::

    f(x) = G( b^{(2)} + W^{(2)}( s( b^{(1)} + W^{(1)} x))),

References:

    - textbooks: &quot;Pattern Recognition and Machine Learning&quot; -
                 Christopher M. Bishop, section 5

&quot;&quot;&quot;</span>
__docformat__ = <span class="hljs-string">&apos;restructedtext en&apos;</span>


<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">import</span> time

<span class="hljs-keyword">import</span> numpy

<span class="hljs-keyword">import</span> theano
<span class="hljs-keyword">import</span> theano.tensor <span class="hljs-keyword">as</span> T


<span class="hljs-keyword">from</span> logistic_sgd <span class="hljs-keyword">import</span> LogisticRegression, load_data


<span class="hljs-comment"># start-snippet-1</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HiddenLayer</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, rng, input, n_in, n_out, W=None, b=None,
                 activation=T.tanh)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        Typical hidden layer of a MLP: units are fully-connected and have
        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)
        and the bias vector b is of shape (n_out,).

        NOTE : The nonlinearity used here is tanh

        Hidden unit activation is given by: tanh(dot(input,W) + b)

        :type rng: numpy.random.RandomState
        :param rng: a random number generator used to initialize weights

        :type input: theano.tensor.dmatrix
        :param input: a symbolic tensor of shape (n_examples, n_in)

        :type n_in: int
        :param n_in: dimensionality of input

        :type n_out: int
        :param n_out: number of hidden units

        :type activation: theano.Op or function
        :param activation: Non linearity to be applied in the hidden
                           layer
        &quot;&quot;&quot;</span>
        self.input = input
        <span class="hljs-comment"># end-snippet-1</span>

        <span class="hljs-comment"># `W` is initialized with `W_values` which is uniformely sampled</span>
        <span class="hljs-comment"># from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))</span>
        <span class="hljs-comment"># for tanh activation function</span>
        <span class="hljs-comment"># the output of uniform if converted using asarray to dtype</span>
        <span class="hljs-comment"># theano.config.floatX so that the code is runable on GPU</span>
        <span class="hljs-comment"># Note : optimal initialization of weights is dependent on the</span>
        <span class="hljs-comment">#        activation function used (among other things).</span>
        <span class="hljs-comment">#        For example, results presented in [Xavier10] suggest that you</span>
        <span class="hljs-comment">#        should use 4 times larger initial weights for sigmoid</span>
        <span class="hljs-comment">#        compared to tanh</span>
        <span class="hljs-comment">#        We have no info for other function, so we use the same as</span>
        <span class="hljs-comment">#        tanh.</span>
        <span class="hljs-keyword">if</span> W <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:
            W_values = numpy.asarray(
                rng.uniform(
                    low=-numpy.sqrt(<span class="hljs-number">6.</span> / (n_in + n_out)),
                    high=numpy.sqrt(<span class="hljs-number">6.</span> / (n_in + n_out)),
                    size=(n_in, n_out)
                ),
                dtype=theano.config.floatX
            )
            <span class="hljs-keyword">if</span> activation == theano.tensor.nnet.sigmoid:
                W_values *= <span class="hljs-number">4</span>

            W = theano.shared(value=W_values, name=<span class="hljs-string">&apos;W&apos;</span>, borrow=<span class="hljs-keyword">True</span>)

        <span class="hljs-keyword">if</span> b <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:
            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)
            b = theano.shared(value=b_values, name=<span class="hljs-string">&apos;b&apos;</span>, borrow=<span class="hljs-keyword">True</span>)

        self.W = W
        self.b = b

        lin_output = T.dot(input, self.W) + self.b
        self.output = (
            lin_output <span class="hljs-keyword">if</span> activation <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>
            <span class="hljs-keyword">else</span> activation(lin_output)
        )
        <span class="hljs-comment"># parameters of the model</span>
        self.params = [self.W, self.b]


<span class="hljs-comment"># start-snippet-2</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLP</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;Multi-Layer Perceptron Class

    A multilayer perceptron is a feedforward artificial neural network model
    that has one layer or more of hidden units and nonlinear activations.
    Intermediate layers usually have as activation function tanh or the
    sigmoid function (defined here by a ``HiddenLayer`` class)  while the
    top layer is a softamx layer (defined here by a ``LogisticRegression``
    class).
    &quot;&quot;&quot;</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, rng, input, n_in, n_hidden, n_out)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Initialize the parameters for the multilayer perceptron

        :type rng: numpy.random.RandomState
        :param rng: a random number generator used to initialize weights

        :type input: theano.tensor.TensorType
        :param input: symbolic variable that describes the input of the
        architecture (one minibatch)

        :type n_in: int
        :param n_in: number of input units, the dimension of the space in
        which the datapoints lie

        :type n_hidden: int
        :param n_hidden: number of hidden units

        :type n_out: int
        :param n_out: number of output units, the dimension of the space in
        which the labels lie

        &quot;&quot;&quot;</span>

        <span class="hljs-comment"># Since we are dealing with a one hidden layer MLP, this will translate</span>
        <span class="hljs-comment"># into a HiddenLayer with a tanh activation function connected to the</span>
        <span class="hljs-comment"># LogisticRegression layer; the activation function can be replaced by</span>
        <span class="hljs-comment"># sigmoid or any other nonlinear function</span>
        self.hiddenLayer = HiddenLayer(
            rng=rng,
            input=input,
            n_in=n_in,
            n_out=n_hidden,
            activation=T.tanh
        )

        <span class="hljs-comment"># The logistic regression layer gets as input the hidden units</span>
        <span class="hljs-comment"># of the hidden layer</span>
        self.logRegressionLayer = LogisticRegression(
            input=self.hiddenLayer.output,
            n_in=n_hidden,
            n_out=n_out
        )
        <span class="hljs-comment"># end-snippet-2 start-snippet-3</span>
        <span class="hljs-comment"># L1 norm ; one regularization option is to enforce L1 norm to</span>
        <span class="hljs-comment"># be small</span>
        self.L1 = (
            abs(self.hiddenLayer.W).sum()
            + abs(self.logRegressionLayer.W).sum()
        )

        <span class="hljs-comment"># square of L2 norm ; one regularization option is to enforce</span>
        <span class="hljs-comment"># square of L2 norm to be small</span>
        self.L2_sqr = (
            (self.hiddenLayer.W ** <span class="hljs-number">2</span>).sum()
            + (self.logRegressionLayer.W ** <span class="hljs-number">2</span>).sum()
        )

        <span class="hljs-comment"># negative log likelihood of the MLP is given by the negative</span>
        <span class="hljs-comment"># log likelihood of the output of the model, computed in the</span>
        <span class="hljs-comment"># logistic regression layer</span>
        self.negative_log_likelihood = (
            self.logRegressionLayer.negative_log_likelihood
        )
        <span class="hljs-comment"># same holds for the function computing the number of errors</span>
        self.errors = self.logRegressionLayer.errors

        <span class="hljs-comment"># the parameters of the model are the parameters of the two layer it is</span>
        <span class="hljs-comment"># made out of</span>
        self.params = self.hiddenLayer.params + self.logRegressionLayer.params
        <span class="hljs-comment"># end-snippet-3</span>


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_mlp</span><span class="hljs-params">(learning_rate=<span class="hljs-number">0.01</span>, L1_reg=<span class="hljs-number">0.00</span>, L2_reg=<span class="hljs-number">0.0001</span>, n_epochs=<span class="hljs-number">1000</span>,
             dataset=<span class="hljs-string">&apos;mnist.pkl.gz&apos;</span>, batch_size=<span class="hljs-number">20</span>, n_hidden=<span class="hljs-number">500</span>)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;
    Demonstrate stochastic gradient descent optimization for a multilayer
    perceptron

    This is demonstrated on MNIST.

    :type learning_rate: float
    :param learning_rate: learning rate used (factor for the stochastic
    gradient

    :type L1_reg: float
    :param L1_reg: L1-norm&apos;s weight when added to the cost (see
    regularization)

    :type L2_reg: float
    :param L2_reg: L2-norm&apos;s weight when added to the cost (see
    regularization)

    :type n_epochs: int
    :param n_epochs: maximal number of epochs to run the optimizer

    :type dataset: string
    :param dataset: the path of the MNIST dataset file from
                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz


   &quot;&quot;&quot;</span>
    datasets = load_data(dataset)

    train_set_x, train_set_y = datasets[<span class="hljs-number">0</span>]
    valid_set_x, valid_set_y = datasets[<span class="hljs-number">1</span>]
    test_set_x, test_set_y = datasets[<span class="hljs-number">2</span>]

    <span class="hljs-comment"># compute number of minibatches for training, validation and testing</span>
    n_train_batches = train_set_x.get_value(borrow=<span class="hljs-keyword">True</span>).shape[<span class="hljs-number">0</span>] / batch_size
    n_valid_batches = valid_set_x.get_value(borrow=<span class="hljs-keyword">True</span>).shape[<span class="hljs-number">0</span>] / batch_size
    n_test_batches = test_set_x.get_value(borrow=<span class="hljs-keyword">True</span>).shape[<span class="hljs-number">0</span>] / batch_size

    <span class="hljs-comment">######################</span>
    <span class="hljs-comment"># BUILD ACTUAL MODEL #</span>
    <span class="hljs-comment">######################</span>
    <span class="hljs-keyword">print</span> <span class="hljs-string">&apos;... building the model&apos;</span>

    <span class="hljs-comment"># allocate symbolic variables for the data</span>
    index = T.lscalar()  <span class="hljs-comment"># index to a [mini]batch</span>
    x = T.matrix(<span class="hljs-string">&apos;x&apos;</span>)  <span class="hljs-comment"># the data is presented as rasterized images</span>
    y = T.ivector(<span class="hljs-string">&apos;y&apos;</span>)  <span class="hljs-comment"># the labels are presented as 1D vector of</span>
                        <span class="hljs-comment"># [int] labels</span>

    rng = numpy.random.RandomState(<span class="hljs-number">1234</span>)

    <span class="hljs-comment"># construct the MLP class</span>
    classifier = MLP(
        rng=rng,
        input=x,
        n_in=<span class="hljs-number">28</span> * <span class="hljs-number">28</span>,
        n_hidden=n_hidden,
        n_out=<span class="hljs-number">10</span>
    )

    <span class="hljs-comment"># start-snippet-4</span>
    <span class="hljs-comment"># the cost we minimize during training is the negative log likelihood of</span>
    <span class="hljs-comment"># the model plus the regularization terms (L1 and L2); cost is expressed</span>
    <span class="hljs-comment"># here symbolically</span>
    cost = (
        classifier.negative_log_likelihood(y)
        + L1_reg * classifier.L1
        + L2_reg * classifier.L2_sqr
    )
    <span class="hljs-comment"># end-snippet-4</span>

    <span class="hljs-comment"># compiling a Theano function that computes the mistakes that are made</span>
    <span class="hljs-comment"># by the model on a minibatch</span>
    test_model = theano.function(
        inputs=[index],
        outputs=classifier.errors(y),
        givens={
            x: test_set_x[index * batch_size:(index + <span class="hljs-number">1</span>) * batch_size],
            y: test_set_y[index * batch_size:(index + <span class="hljs-number">1</span>) * batch_size]
        }
    )

    validate_model = theano.function(
        inputs=[index],
        outputs=classifier.errors(y),
        givens={
            x: valid_set_x[index * batch_size:(index + <span class="hljs-number">1</span>) * batch_size],
            y: valid_set_y[index * batch_size:(index + <span class="hljs-number">1</span>) * batch_size]
        }
    )

    <span class="hljs-comment"># start-snippet-5</span>
    <span class="hljs-comment"># compute the gradient of cost with respect to theta (sotred in params)</span>
    <span class="hljs-comment"># the resulting gradients will be stored in a list gparams</span>
    gparams = [T.grad(cost, param) <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> classifier.params]

    <span class="hljs-comment"># specify how to update the parameters of the model as a list of</span>
    <span class="hljs-comment"># (variable, update expression) pairs</span>

    <span class="hljs-comment"># given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of</span>
    <span class="hljs-comment"># same length, zip generates a list C of same size, where each element</span>
    <span class="hljs-comment"># is a pair formed from the two lists :</span>
    <span class="hljs-comment">#    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]</span>
    updates = [
        (param, param - learning_rate * gparam)
        <span class="hljs-keyword">for</span> param, gparam <span class="hljs-keyword">in</span> zip(classifier.params, gparams)
    ]

    <span class="hljs-comment"># compiling a Theano function `train_model` that returns the cost, but</span>
    <span class="hljs-comment"># in the same time updates the parameter of the model based on the rules</span>
    <span class="hljs-comment"># defined in `updates`</span>
    train_model = theano.function(
        inputs=[index],
        outputs=cost,
        updates=updates,
        givens={
            x: train_set_x[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size],
            y: train_set_y[index * batch_size: (index + <span class="hljs-number">1</span>) * batch_size]
        }
    )
    <span class="hljs-comment"># end-snippet-5</span>

    <span class="hljs-comment">###############</span>
    <span class="hljs-comment"># TRAIN MODEL #</span>
    <span class="hljs-comment">###############</span>
    <span class="hljs-keyword">print</span> <span class="hljs-string">&apos;... training&apos;</span>

    <span class="hljs-comment"># early-stopping parameters</span>
    patience = <span class="hljs-number">10000</span>  <span class="hljs-comment"># look as this many examples regardless</span>
    patience_increase = <span class="hljs-number">2</span>  <span class="hljs-comment"># wait this much longer when a new best is</span>
                           <span class="hljs-comment"># found</span>
    improvement_threshold = <span class="hljs-number">0.995</span>  <span class="hljs-comment"># a relative improvement of this much is</span>
                                   <span class="hljs-comment"># considered significant</span>
    validation_frequency = min(n_train_batches, patience / <span class="hljs-number">2</span>)
                                  <span class="hljs-comment"># go through this many</span>
                                  <span class="hljs-comment"># minibatche before checking the network</span>
                                  <span class="hljs-comment"># on the validation set; in this case we</span>
                                  <span class="hljs-comment"># check every epoch</span>

    best_validation_loss = numpy.inf
    best_iter = <span class="hljs-number">0</span>
    test_score = <span class="hljs-number">0.</span>
    start_time = time.clock()

    epoch = <span class="hljs-number">0</span>
    done_looping = <span class="hljs-keyword">False</span>

    <span class="hljs-keyword">while</span> (epoch &lt; n_epochs) <span class="hljs-keyword">and</span> (<span class="hljs-keyword">not</span> done_looping):
        epoch = epoch + <span class="hljs-number">1</span>
        <span class="hljs-keyword">for</span> minibatch_index <span class="hljs-keyword">in</span> xrange(n_train_batches):

            minibatch_avg_cost = train_model(minibatch_index)
            <span class="hljs-comment"># iteration number</span>
            iter = (epoch - <span class="hljs-number">1</span>) * n_train_batches + minibatch_index

            <span class="hljs-keyword">if</span> (iter + <span class="hljs-number">1</span>) % validation_frequency == <span class="hljs-number">0</span>:
                <span class="hljs-comment"># compute zero-one loss on validation set</span>
                validation_losses = [validate_model(i) <span class="hljs-keyword">for</span> i
                                     <span class="hljs-keyword">in</span> xrange(n_valid_batches)]
                this_validation_loss = numpy.mean(validation_losses)

                print(
                    <span class="hljs-string">&apos;epoch %i, minibatch %i/%i, validation error %f %%&apos;</span> %
                    (
                        epoch,
                        minibatch_index + <span class="hljs-number">1</span>,
                        n_train_batches,
                        this_validation_loss * <span class="hljs-number">100.</span>
                    )
                )

                <span class="hljs-comment"># if we got the best validation score until now</span>
                <span class="hljs-keyword">if</span> this_validation_loss &lt; best_validation_loss:
                    <span class="hljs-comment">#improve patience if loss improvement is good enough</span>
                    <span class="hljs-keyword">if</span> (
                        this_validation_loss &lt; best_validation_loss *
                        improvement_threshold
                    ):
                        patience = max(patience, iter * patience_increase)

                    best_validation_loss = this_validation_loss
                    best_iter = iter

                    <span class="hljs-comment"># test it on the test set</span>
                    test_losses = [test_model(i) <span class="hljs-keyword">for</span> i
                                   <span class="hljs-keyword">in</span> xrange(n_test_batches)]
                    test_score = numpy.mean(test_losses)

                    print((<span class="hljs-string">&apos;     epoch %i, minibatch %i/%i, test error of &apos;</span>
                           <span class="hljs-string">&apos;best model %f %%&apos;</span>) %
                          (epoch, minibatch_index + <span class="hljs-number">1</span>, n_train_batches,
                           test_score * <span class="hljs-number">100.</span>))

            <span class="hljs-keyword">if</span> patience &lt;= iter:
                done_looping = <span class="hljs-keyword">True</span>
                <span class="hljs-keyword">break</span>

    end_time = time.clock()
    print((<span class="hljs-string">&apos;Optimization complete. Best validation score of %f %% &apos;</span>
           <span class="hljs-string">&apos;obtained at iteration %i, with test performance %f %%&apos;</span>) %
          (best_validation_loss * <span class="hljs-number">100.</span>, best_iter + <span class="hljs-number">1</span>, test_score * <span class="hljs-number">100.</span>))
    <span class="hljs-keyword">print</span> &gt;&gt; sys.stderr, (<span class="hljs-string">&apos;The code for file &apos;</span> +
                          os.path.split(__file__)[<span class="hljs-number">1</span>] +
                          <span class="hljs-string">&apos; ran for %.2fm&apos;</span> % ((end_time - start_time) / <span class="hljs-number">60.</span>))


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&apos;__main__&apos;</span>:
    test_mlp()
</code></pre>
<p>&#x9884;&#x8BA1;&#x5C06;&#x4F1A;&#x5F97;&#x5230;&#x8FD9;&#x6837;&#x7684;&#x8F93;&#x51FA;&#xFF1A;</p>
<pre><code>Optimization complete. Best validation score of 1.690000 % obtained at iteration 2070000, with test performance 1.650000 %
The code for file mlp.py ran for 97.34m
</code></pre><p>&#x5728;&#x4E00;&#x53F0;Intel(R) Core(TM) i7-2600K CPU @ 3.40GHz&#x7684;&#x673A;&#x5668;&#x4E0A;&#xFF0C;&#x8FD9;&#x4E2A;&#x4EE3;&#x7801;&#x8DD1;&#x4E86;10.3 epoch/minute&#x7136;&#x540E;&#x82B1;&#x4E86;828 epochs&#x5F97;&#x5230;&#x4E86;1.65%&#x7684;&#x6D4B;&#x8BD5;&#x9519;&#x8BEF;&#x7387;&#x3002;
&#x8BFB;&#x8005;&#x4E5F;&#x53EF;&#x4EE5;&#x5728;<a href="http://yann.lecun.com/exdb/mnist" target="_blank">&#x8FD9;&#x4E2A;&#x9875;&#x9762;</a>&#x67E5;&#x770B;MNIST&#x7684;&#x8BC6;&#x522B;&#x7ED3;&#x679C;&#x3002;</p>
<h2 id="&#x8BAD;&#x7EC3;mlps&#x7684;&#x6280;&#x5DE7;">&#x8BAD;&#x7EC3;MLPs&#x7684;&#x6280;&#x5DE7;</h2>
<p>&#x5728;&#x4E0A;&#x9762;&#x7684;&#x4EE3;&#x7801;&#x4E2D;&#x56FD;&#xFF0C;&#x6709;&#x4E00;&#x4E9B;&#x662F;&#x4E0D;&#x80FD;&#x8FDB;&#x884C;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x6765;&#x4F18;&#x5316;&#x7684;&#x3002;&#x4E25;&#x683C;&#x610F;&#x4E49;&#x4E0A;&#x5C06;&#xFF0C;&#x53D1;&#x73B0;&#x6700;&#x4F18;&#x7684;&#x8D85;&#x53C2;&#x96C6;&#x5408;&#x662F;&#x4E0D;&#x53EF;&#x80FD;&#x7684;&#x4EFB;&#x52A1;&#x3002;&#x7B2C;&#x4E00;&#xFF0C;&#x6211;&#x4EEC;&#x4E0D;&#x80FD;&#x72EC;&#x7ACB;&#x7684;&#x4F18;&#x5316;&#x6BCF;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x3002;&#x7B2C;&#x4E8C;&#xFF0C;&#x6211;&#x4EEC;&#x4E0D;&#x80FD;&#x5F88;&#x5BB9;&#x6613;&#x7684;&#x6C42;&#x89E3;&#x6240;&#x6709;&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#xFF08;&#x6709;&#x4E9B;&#x662F;&#x79BB;&#x6563;&#x7684;&#x503C;&#xFF0C;&#x6709;&#x4E9B;&#x662F;&#x5B9E;&#x6570;&#xFF09;&#x3002;&#x7B2C;&#x4E09;&#xFF0C;&#x8FD9;&#x4E2A;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x662F;&#x975E;&#x51F8;&#x7684;&#xFF0C;&#x5BB9;&#x6613;&#x9677;&#x5165;&#x5C40;&#x90E8;&#x6700;&#x4F18;&#x3002;
&#x597D;&#x6D88;&#x606F;&#x662F;&#xFF0C;&#x8FC7;&#x53BB;25&#x5E74;&#xFF0C;&#x7814;&#x7A76;&#x8005;&#x53D1;&#x660E;&#x4E86;&#x4E00;&#x4E9B;&#x5728;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x4E2D;&#x9009;&#x62E9;&#x8D85;&#x53C2;&#x6570;&#x7684;&#x65B9;&#x6CD5;&#x548C;&#x89C4;&#x5219;&#x3002;&#x4F60;&#x53EF;&#x4EE5;&#x5728;LeCun&#x7B49;&#x4EBA;&#x7684;<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank">Efficient BackPro</a>&#x4E2D;&#x9605;&#x8BFB;&#xFF0C;&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x597D;&#x7684;&#x7EFC;&#x8FF0;&#x3002;&#x8FD9;&#x91CC;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x603B;&#x7ED3;&#x4E0B;&#x6211;&#x4EEC;&#x7684;&#x4EE3;&#x7801;&#x4E2D;&#x7528;&#x5230;&#x7684;&#x51E0;&#x4E2A;&#x91CD;&#x8981;&#x7684;&#x65B9;&#x6CD5;&#x548C;&#x6280;&#x672F;&#x3002;</p>
<h3 id="&#x975E;&#x7EBF;&#x6027;">&#x975E;&#x7EBF;&#x6027;</h3>
<p>&#x6700;&#x5E38;&#x89C1;&#x7684;&#x5C31;&#x662F;<code>sigmoid</code>&#x548C;<code>tanh</code>&#x51FD;&#x6570;&#x3002;&#x5728;<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank">&#x7B2C;4.4&#x8282;</a>&#x4E2D;&#x89E3;&#x91CA;&#x7684;&#xFF0C;&#x975E;&#x7EBF;&#x6027;&#x662F;&#x5173;&#x4E8E;&#x539F;&#x70B9;&#x5BF9;&#x79F0;&#x7684;&#xFF0C;&#x5B83;&#x503E;&#x5411;&#x53BB;&#x8F93;&#x51FA;0&#x5747;&#x503C;&#x7684;&#x8F93;&#x51FA;&#xFF08;&#x8FD9;&#x662F;&#x88AB;&#x671F;&#x671B;&#x7684;&#x5C5E;&#x6027;&#xFF09;&#x3002;&#x6839;&#x636E;&#x6211;&#x4EEC;&#x7684;&#x7ECF;&#x9A8C;&#xFF0C;tanh(&#x53CC;&#x66F2;&#x51FD;&#x6570;)&#x62E5;&#x6709;&#x66F4;&#x597D;&#x7684;&#x6536;&#x655B;&#x6027;&#x3002;</p>
<h3 id="&#x6743;&#x503C;&#x521D;&#x59CB;&#x5316;">&#x6743;&#x503C;&#x521D;&#x59CB;&#x5316;</h3>
<p>&#x5728;&#x521D;&#x59CB;&#x5316;&#x6743;&#x503C;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x6211;&#x4EEC;&#x4E00;&#x822C;&#x9700;&#x8981;&#x5B83;&#x4EEC;&#x5728;0&#x9644;&#x8FD1;&#xFF0C;&#x8981;&#x8DB3;&#x591F;&#x5C0F;&#xFF08;&#x5728;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x7684;&#x8FD1;&#x4F3C;&#x7EBF;&#x6027;&#x533A;&#x57DF;&#x53EF;&#x4EE5;&#x83B7;&#x5F97;&#x6700;&#x5927;&#x7684;&#x68AF;&#x5EA6;&#xFF09;&#x3002;&#x53E6;&#x4E00;&#x4E2A;&#x7279;&#x6027;&#xFF0C;&#x5C24;&#x5176;&#x5BF9;&#x6DF1;&#x5EA6;&#x7F51;&#x7EDC;&#x800C;&#x8A00;&#xFF0C;&#x662F;&#x53EF;&#x4EE5;&#x51CF;&#x5C0F;&#x5C42;&#x4E0E;&#x5C42;&#x4E4B;&#x95F4;&#x7684;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x7684;&#x65B9;&#x5DEE;&#x548C;&#x53CD;&#x5411;&#x4F20;&#x5BFC;&#x68AF;&#x5EA6;&#x7684;&#x65B9;&#x5DEE;&#x3002;&#x8FD9;&#x5C31;&#x53EF;&#x4EE5;&#x8BA9;&#x4FE1;&#x606F;&#x66F4;&#x597D;&#x7684;&#x5411;&#x4E0B;&#x548C;&#x5411;&#x4E0A;&#x7684;&#x4F20;&#x5BFC;&#xFF0C;&#x51CF;&#x5C11;&#x5C42;&#x95F4;&#x5DEE;&#x5F02;&#x3002;&#x6570;&#x5B66;&#x63A8;&#x5012;&#xFF0C;&#x8BF7;&#x770B;<a href="http://deeplearning.net/tutorial/references.html#xavier10" target="_blank">Xavier10</a>&#x3002;</p>
<h3 id="&#x5B66;&#x4E60;&#x7387;">&#x5B66;&#x4E60;&#x7387;</h3>
<p>&#x6709;&#x8BB8;&#x591A;&#x6587;&#x732E;&#x4E13;&#x6CE8;&#x5728;&#x597D;&#x7684;&#x5B66;&#x4E60;&#x901F;&#x7387;&#x7684;&#x9009;&#x62E9;&#x4E0A;&#x3002;&#x6700;&#x7B80;&#x5355;&#x7684;&#x65B9;&#x6848;&#x5C31;&#x662F;&#x9009;&#x62E9;&#x4E00;&#x4E2A;&#x56FA;&#x5B9A;&#x901F;&#x7387;&#x3002;&#x7ECF;&#x9A8C;&#x6CD5;&#x5219;&#xFF1A;&#x5C1D;&#x8BD5;&#x5BF9;&#x6570;&#x95F4;&#x9694;&#x7684;&#x503C;(0.1&#xFF0C;001&#xFF0C;&#x3002;&#x3002;)&#xFF0C;&#x7136;&#x540E;&#x7F29;&#x5C0F;&#xFF08;&#x5BF9;&#x6570;&#xFF09;&#x7F51;&#x7EDC;&#x641C;&#x7D22;&#x7684;&#x8303;&#x56F4;&#xFF08;&#x4F60;&#x83B7;&#x5F97;&#x6700;&#x4F4E;&#x9A8C;&#x8BC1;&#x9519;&#x8BEF;&#x7684;&#x533A;&#x57DF;&#xFF09;&#x3002;
&#x968F;&#x7740;&#x65F6;&#x95F4;&#x7684;&#x63A8;&#x79FB;&#x51CF;&#x5C0F;&#x5B66;&#x4E60;&#x901F;&#x7387;&#x6709;&#x65F6;&#x5019;&#x4E5F;&#x662F;&#x4E00;&#x4E2A;&#x597D;&#x4E3B;&#x610F;&#x3002;&#x4E00;&#x4E2A;&#x7B80;&#x5355;&#x7684;&#x65B9;&#x6CD5;&#x662F;&#x4F7F;&#x7528;&#x8FD9;&#x4E2A;&#x516C;&#x5F0F;&#xFF1A;u/(1+d*t)&#xFF0C;u&#x662F;&#x521D;&#x59CB;&#x901F;&#x7387;&#xFF08;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x4E0A;&#x9762;&#x8BB2;&#x7684;&#x7F51;&#x683C;&#x641C;&#x7D22;&#x9009;&#x62E9;&#xFF09;&#xFF0C;d&#x662F;&#x51CF;&#x5C0F;&#x5E38;&#x91CF;&#xFF0C;&#x7528;&#x4EE5;&#x63A7;&#x5236;&#x5B66;&#x4E60;&#x901F;&#x7387;&#xFF0C;&#x53EF;&#x4EE5;&#x8BBE;&#x4E3A;0.001&#x6216;&#x8005;&#x66F4;&#x5C0F;&#xFF0C;t&#x662F;&#x8FED;&#x4EE3;&#x6B21;&#x6570;&#x6216;&#x8005;&#x65F6;&#x95F4;&#x3002;
<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank">4.7&#x8282;</a>&#x8BB2;&#x8FF0;&#x4E86;&#x7F51;&#x7EDC;&#x4E2D;&#x6BCF;&#x4E2A;&#x53C2;&#x6570;&#x5B66;&#x4E60;&#x901F;&#x7387;&#x9009;&#x62E9;&#x7684;&#x65B9;&#x6CD5;&#xFF0C;&#x7136;&#x540E;&#x57FA;&#x4E8E;&#x5206;&#x7C7B;&#x9519;&#x8BEF;&#x7387;&#x81EA;&#x9002;&#x5E94;&#x7684;&#x9009;&#x62E9;&#x5B83;&#x4EEC;&#x3002;</p>
<h3 id="&#x9690;&#x85CF;&#x8282;&#x70B9;&#x6570;">&#x9690;&#x85CF;&#x8282;&#x70B9;&#x6570;</h3>
<p>&#x8FD9;&#x4E2A;&#x8D85;&#x53C2;&#x6570;&#x662F;&#x975E;&#x5E38;&#x57FA;&#x4E8E;&#x6570;&#x636E;&#x96C6;&#x7684;&#x3002;&#x6A21;&#x7CCA;&#x7684;&#x6765;&#x8BF4;&#x5C31;&#x662F;&#xFF0C;&#x8F93;&#x5165;&#x5206;&#x5E03;&#x8D8A;&#x590D;&#x6742;&#xFF0C;&#x53BB;&#x6A21;&#x62DF;&#x5B83;&#x7684;&#x7F51;&#x7EDC;&#x5C31;&#x9700;&#x8981;&#x66F4;&#x5927;&#x7684;&#x5BB9;&#x91CF;&#xFF0C;&#x90A3;&#x4E48;&#x9690;&#x85CF;&#x5355;&#x5143;&#x7684;&#x6570;&#x76EE;&#x5C31;&#x8981;&#x66F4;&#x5927;&#x3002;&#x4E8B;&#x5B9E;&#x4E0A;&#xFF0C;&#x4E00;&#x4E2A;&#x5C42;&#x7684;&#x6743;&#x503C;&#x77E9;&#x9635;&#x5C31;&#x662F;&#x53EF;&#x4EE5;&#x76F4;&#x63A5;&#x5EA6;&#x91CF;&#x7684;&#xFF08;&#x8F93;&#x5165;&#x7EF4;&#x5EA6;&#xFF0A;&#x8F93;&#x51FA;&#x7EF4;&#x5EA6;&#xFF09;&#x3002;
&#x9664;&#x975E;&#x6211;&#x4EEC;&#x53BB;&#x4F7F;&#x7528;&#x6B63;&#x5219;&#x9009;&#x9879;&#xFF08;early-stopping&#x6216;L1/L2&#x60E9;&#x7F5A;&#xFF09;&#xFF0C;&#x9690;&#x85CF;&#x8282;&#x70B9;&#x6570;&#x548C;&#x6CDB;&#x5316;&#x8868;&#x73B0;&#x7684;&#x5206;&#x5E03;&#x56FE;&#xFF0C;&#x5C06;&#x5448;&#x73B0;U&#x578B;&#xFF08;&#x5373;&#x9690;&#x85CF;&#x8282;&#x70B9;&#x8D8A;&#x591A;&#xFF0C;&#x5728;&#x540E;&#x671F;&#x5E76;&#x4E0D;&#x80FD;&#x63D0;&#x9AD8;&#x6CDB;&#x5316;&#x6027;&#xFF09;&#x3002;</p>
<h3 id="&#x6B63;&#x5219;&#x5316;&#x53C2;&#x6570;">&#x6B63;&#x5219;&#x5316;&#x53C2;&#x6570;</h3>
<p>&#x5178;&#x578B;&#x7684;&#x65B9;&#x6CD5;&#x662F;&#x4F7F;&#x7528;L1/L2&#x6B63;&#x5219;&#x5316;&#xFF0C;&#x540C;&#x65F6;lambda&#x8BBE;&#x4E3A;0.01&#xFF0C;0.001&#x7B49;&#x3002;&#x5C3D;&#x7BA1;&#x5728;&#x6211;&#x4EEC;&#x4E4B;&#x524D;&#x63D0;&#x53CA;&#x7684;&#x6846;&#x67B6;&#x91CC;&#x9762;&#xFF0C;&#x5B83;&#x5E76;&#x6CA1;&#x6709;&#x663E;&#x8457;&#x63D0;&#x9AD8;&#x6027;&#x80FD;&#xFF0C;&#x4F46;&#x5B83;&#x4ECD;&#x7136;&#x662F;&#x4E00;&#x4E2A;&#x503C;&#x5F97;&#x63A2;&#x8BA8;&#x7684;&#x65B9;&#x6CD5;&#x3002;</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="2_Classifying_MNIST_using_LR_逻辑回归进行MNIST分类.html" class="navigation navigation-prev " aria-label="Previous page: 2.逻辑回归进行MNIST分类">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="4_Convoltional_Neural_Networks_LeNet_卷积神经网络.html" class="navigation navigation-next " aria-label="Next page: 4.卷积神经网络">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"3.多层感知机","level":"1.4","depth":1,"next":{"title":"4.卷积神经网络","level":"1.5","depth":1,"path":"4_Convoltional_Neural_Networks_LeNet_卷积神经网络.md","ref":"4_Convoltional_Neural_Networks_LeNet_卷积神经网络.md","articles":[]},"previous":{"title":"2.逻辑回归进行MNIST分类","level":"1.3","depth":1,"path":"2_Classifying_MNIST_using_LR_逻辑回归进行MNIST分类.md","ref":"2_Classifying_MNIST_using_LR_逻辑回归进行MNIST分类.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"3_Multilayer_Perceptron_多层感知机.md","mtime":"2017-07-12T16:21:46.000Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2021-05-10T15:44:26.709Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

