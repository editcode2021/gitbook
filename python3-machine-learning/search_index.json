{"index":{"version":"0.5.12","fields":[{"name":"title","boost":10},{"name":"keywords","boost":15},{"name":"body","boost":1}],"ref":"url","documentStore":{"store":{"./":["#","algorithm","classifi","confusion_matrix,","datasets.load_boston()","datasets.load_digits()","datasets.load_iris()","datasets可以用来加载真实数据进行模型训练的测试","decomposition提供了降维相关算法的实现","fetch_lfw_peopl","fetch_lfw_people用于加载人脸数据集","fetch_mldata","fetch_mldata用于加载mnist数据集","gridsearchcv","gridsearchcv用于进行参数搜索，寻找合适的超参数","guide是对每一个算法的概述介绍。api是每一个算法的使用文档（也可以在首页大搜中搜索）","import","kneighborsclassifi","kneighborsclassifier是knn算法解决分类问题的实现","kneighborsclassifier是knn算法解决回归问题的实现","kneighborsregressor","learn","learn.org/stable/","learn.org/stable/documentation.html","learn中提供的算法","learn中的precis","learn中的roc曲线","learn中的混淆矩阵，精准率和召回率,f1_scor","learn和numpy为技术栈，学习了机器学习入门的基本算法，并自己实现了部分sikit","learn官网：http://scikit","learn算法","linear_model提供了线性模型相关算法的实现","linearregress","linearregression是线性回归算法的实现","linearsvc","linearsvr","logisticregress","logisticregression是逻辑回归的实现，默认使用了l2正则化","machin","mae的实现","mean_absolute_error","mean_squared_error","metrics模块提供了数据之间的度量相关运算","model_selection模块提供了模型选择的相关操作","mse的实现","multiclass模块提供了多分类问题的相关实现","neighbors模块提供了近邻相关的算法实现","onevsoneclassifi","onevsoneclassifier是ovo的实现","onevsrestclassifi","onevsrestclassifier是ovr的实现","pca","pca给出了主成分分析法的相关实现","polynomialfeatur","polynomialfeatures进行多项式曾维处理，使用线性回归的方法解决非线性问题","precision_recall_curv","precision_score,","preprocessing模块提供了数据预处理的相关操作","python3入门机器学习","r2_score","r2_score的实现","recal","recall_score,f1_scor","regress","ridg","ridge是岭回归的实现","roc_curve,roc_auc_scor","scikit","scr","sgdregressor","sgdregressor是梯度下降法相关的实现","sklearn.dataset","sklearn.decomposit","sklearn.linear_model","sklearn.liner_model","sklearn.metr","sklearn.model_select","sklearn.multiclass","sklearn.neighbor","sklearn.preprocess","sklearn.svm","standardscal","standardscaler提供数据归一化运算","support","svc","svm提供了支持向量机相关算法的实现","svr","train_test_split","train_test_split用于分割测试数据集和训练数据集","vector","中的user","主要以sikit","以下列出本笔记（课程）学习使用到的sikit","使用支持向量机思想解决分类问题","使用支持向量机思想解决回归问题","使用核函数的支撑向量机","使用核函数的解决回归问题的支撑向量机","官方github代码：https://github.com/liuyubobobo/play","曲线","本笔记记录笔者观看慕课网入门机器学习课程的笔记过程","用于加载手写识别的数据集","用于加载波士顿房价的数据集","用于加载鸢尾花数据集","笔记整理","简介","重点看http://scikit"],"ji-qi-xue-xi-ji-ben-gai-nian.html":["1.机器学习基本概念"],"chapter1/you-guan-shu-ju-de-yi-xie-zhu-yu.html":["有关数据的一些术语"],"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":["一些情况下，回归任务可以简化成分类任务，比如学生的具体成绩预测转换成评级，无人车驾驶，转换成油门，刹车，方向盘的程度","一些算法只支持完成二分类的任务","一部分数据有“标记”或者“答案”，另一部分数据没有","二分类任务","但是多分类的任务可以转换成二分类的任务","分类任务","判断发放给客户信用卡有风险；没有风险","判断发放给客户信用卡的风险评级","判断某支股票涨；跌","判断病患良性肿瘤；恶性肿瘤","判断邮件是垃圾邮件；不是垃圾邮件","医院已经积累了一定的病人信息和他们最终确诊是否患病的情况","半监督学习","回归任务","图像已经拥有了标记信息","图像识别","增强学习","多分类任务","学生成绩","对数据进行降维处理","对没有“标记”的数据进行分类","将给定的数据进行分类，比如区分猫和狗","市场分析","市场积累了房屋的基本信息和最终成交的金额","异常检测","房屋价格","数字识别","方便可视化","无人驾驶","更常见：各种原因产生的标记缺失","有一些算法只能解决分类问题","有一些算法只能解决回归问题","有一些算法天然可以完成多分类任务","有一些算法既能解决回归问题，也能解决分类问题","机器人","机器学习中监督学习的基本任务","根据周围环境的情况，采取行动，根据采取行动的结果，学习行动方式","特征压缩：pca","特征提取：信用卡的信用评级和人的胖瘦无关","监督学习","监督学习和半监督学习是基础","结果是一个连续数字的值，而非一个类别","给机器的训练数据拥有“标记”或者“答案”，人类已经给机器对数据进行了正确答案的划分，这个答案的划分本身就是监督的信息","给机器的训练数据没有任何的“答案”和“标记”","聚类分析","股票价格","银行已经积累了一定的客户信息和他们信息卡的信用情况","非监督学习"],"jupyter-notebookyu-numpy-de-shi-yong.html":["2.jupyt","notebook与numpy的使用"],"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":["%%timeit","%run","%timeit","./hello.pi","1.%run","2.%timeit","3.%time","4.其他","5.7","import","jupyt","mymodule.firstml","notebook与numpy的使用","notebook自动决定的）","μs（有多少次循环是由jupyt","一个陷阱","使用%lsmagic查看所有的魔法命令","使用%timeit","使用%time让测试只执行一次","可以直接使用import命令导入本机目录下的包","在使用方法后面加?查看文档","执行python脚本，并将脚本中的函数加载","本次测试时间比上面的测试时间会多，是因为只测试了一次。可能不够准确","测试一个排序算法，由于第一次执行完毕后数组已经排好序，那么在后面执行的时候，如果使用插入排序等算法就会导致后面999次的时间非常短，导致测试值不准确","测试代码的性能","测试多次在每次测试的执行性能不一样的时候测试结果会不准确。","测试整个代码块","测试结果表明，运行了一千次，取有价值的7次，平均每次耗时324+/","考虑用%timeit"],"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":["1.加载numpy与查看版本","2.python","3.numpy.array","list","list的元素可以存任何类型，在灵活度提升的同时，也导致性能下降了","numpi","一直替代方法是是使用array可以在构造数组的时候限定类型，但是由于array只是把元素当成一个一维或者多维数组，而并没有当做矩阵，向量，所以也没有提供相应的方法函数，使得在机器学习中非常的不方便","使用numpy创建数组(和python的array中几乎一样)","数据基础","查看数组元素类型","由于python","的特点"],"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":["1.创建“0”数组","2.创建全\"1\"矩阵和创建全\"n\"矩阵","3.生成等步长数组","4.random","arang","linspac","其他创建numpy.array","创建n维0数组,第一个参数shape是数组维度，第二个参数是类型","创建一维0数组","注意full","的方法","随机整数","随机浮点数","默认的数据类型是整形"],"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html":["1.基本属性","2.","3.reshap","numpy.array","改变维度","的基本操作","访问数组"],"knnsuan-fa-de-xue-xi-yu-shi-yong.html":["3.knn算法的学习与使用"],"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":[",3>0,所以判断这个新病人幻的事恶性肿瘤","1.knn算法的原理介绍","3：0","=","优点","假设现在设计一个程序判断一个新的肿瘤病人是良性肿瘤还是恶性肿瘤","先基于原有的肿瘤病人的发现时间和肿瘤大小（特征）对应的良性/恶性（值）建立了一张散点图，横坐标是肿瘤大小，纵坐标是发现时间，红色代表良性，蓝色代表恶性，现在要预测的病人的颜色为绿色","原理案例介绍","如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。","本质","然后用第一步中取到的三个点进行投票，比如本例中投票结果就是蓝：红","缺点","首先需要取一个k值（这个k值的取法后面会介绍），然后找到距离要预测的病人的点（绿点）距离最近的k个点"],"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":["\"\"\"初始化knn分类器\"\"\"","\"\"\"根据训练数据集x_train和y_train训练knn分类器\"\"\"","\"k","\"the","#","**2","0.3064319992975,","0]","1","1,","1.3761132675144652,","1.4900114024329525,","1:","2.354574897431513,","2.5786840957478887]","2.knn算法的一个简单实现","4.6986266144110695,","5)]","5.83460014556857,","5})","6","6.189696362066091,","6.749798999160064,","=","==",">=","[(1,","[0,0,0,0,0,1,1,1,1,1]","[1,","[1.343808831,3.368360954],","[2.110073483,1.781539638],","[2.280362439,2.866990263],","[3.582294042,4.679179110],","[4.812566907609877,","[5.745051997,3.533989803],","[7.423436942,4.696522875],","[7.792783481,3.424088941],","[7.939820817,0.791637231]","[9.172168622,2.511101045],","[[3.393533211,2.331273381],","[]","[sqrt(np.sum((x_train","[y_train[i]","\\","]","__init__(self,k):","assert","class","collect","collections的counter方法可以求出一个数组的相同元素的个数，返回一个dict【key=元素名，value=元素个数】","counter","counter(topk_y)","counter({0:","d","def","distanc","distances.append(d)","equal","fit(self,","import","k","knn","knnclassifier:","knn算法的封装","k近邻算法是非常特殊的，可以被认为是没有模型的算法","math","matplotlib.pyplot","most_common方法求出最多的元素对应的那个键值对","nearset","nearset[:k]]","none","np","np.argsort(distances)","np.array([8.093607318,3.365731514])","np.array(raw_data_x)","np.array(raw_data_y)","numpi","plt","plt.scatter(x[0],x[1],color='b')","plt.scatter(x_train[y_train==0,0],x_train[y_train==0,1],color='g')","plt.scatter(x_train[y_train==1,0],x_train[y_train==1,1],color='r')","predict_i","raw_data_i","raw_data_x=","self._x_train","self._y_train","self.k","size","sqrt","sqrt(np.sum((x_train","topk_i","valid\"","vote","votes.most_common(1)","votes.most_common(1)[0][0]","x","x)**2))","x_train","x_train,","x_train.shape[0]","x_train:","x_train]","y_train","y_train\"","y_train):","y_train.shape[0],","为了和其他算法统一，可以认为训练数据集就是模型","再看机器学习","原始集合","可以说knn是一个不需要训练过程的算法","实现过程简单编码","所述类别","投票","欧拉","求出距离测试点最近的6个点的类别","求平方","特征","生成表达式","绘制数据集及要预测的点","要预测的点","训练集合","返回排序后的结果的索引,也就是距离测试点距离最近的点的排序坐标数组"],"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":["\"\"\"将数据x和y按照test_radio分割成x_train,y_train,x_test,y_test\"\"\"","\"the","#","(112,","(112,)","(113,","(113,)","(150,","(150,)","(37,","(37,)","(38,","(38,)","*","0,","0.0","0.2","0.9473684210526315","1,","10,","100,","101,","102,","103,","104,","105,","106,","107,","108,","109,","11,","110,","111,","112,","113,","114,","115,","116,","117,","118,","119,","12,","120,","121,","122,","123,","124,","125,","126,","127,","128,","129,","13,","130,","131,","132,","133,","134,","135,","136,","137,","138,","14,","140,","141,","142,","143,","144,","145,","146,","147,","148,","149,","15,","16,","17,","18,","19,","1的一个随机排列","2,","20,","21,","22])","23,","24,","25,","26,","27,","28,","29,","2])","3,","3.判断机器学习算法的性能","30","30,","31,","32,","33,","34,","35,","36,","37,","38,","39,","4)","4,","40,","41,","42,","43,","44,","45,","46,","47,","48,","49,","5,","50,","51,","52,","53,","54,","55,","56,","57,","58,","59,","6,","60,","61,","62,","63,","64,","65,","66,","67,","68,","69,","7,","70,","71,","72,","73,","74,","75,","76,","77,","78,","79,","8,","80,","81,","82,","83,","84,","85,","86,","87,","88,","89,","9,","90,","91,","92,","93,","94,","95,","96,","97,","98,","99,","=","==","array([139,","array([2,","assert","dataset","datasets.load_iris()","def","equal","import","int(len(x)","iri","iris.data","iris.target","kneighborsclassifi","kneighborsclassifier(algorithm='auto',","kneighborsclassifier(n_neighbors=6)","knnclassifi","knnclassifier(k=6)","leaf_size=30,","machine_learn","machine_learning.knn","machine_learning.module_select","matplotlib.pyplot","metric='minkowski',","metric_params=none,","my_knn_clf","my_knn_clf.fit(x_train,y_train)","my_knn_clf.predict(x_test)","n_jobs=1,","n_neighbors=6,","np","np.random.permutation(len(x))","numpi","p=2,","permutation(n)","plt","print(x_test.shape)","print(x_train.shape)","print(y_test.shape)","print(y_train.shape)","seed=none):","shuffle_index","shuffle_indexes[:tets_size]","shuffle_indexes[tets_size:]","size","sklearn","sklearn.model_select","sklearn.neighbor","sklearn_knn_clf","sklearn_knn_clf.fit(x_train,y_train)","sklearn_knn_clf.predict(x_test)","split","sum(y_predict==y_test)/len(y_test)","test","test_index","test_radio=0.2,","test_ratio","test_ratio)","tets_siz","train","train_index","train_test_spilt","train_test_split","train_test_split(x,","train_test_split(x,y)","train_test_split(x,y,test_radio=0.25)","weights='uniform')","x","x.shape","x.shape[0]","x[test_indexes]","x[train_indexes]","x_test","x_train","x_train,x_test,y_train,y_test","x_train,y_train,x_test,y_test","y","y\"","y,","y.shape","y.shape[0],\\","y[test_indexes]","y[train_indexes]","y_predict","y_test","y_train","使用我们自己封装的测试分割函数分割训练集","加载鸢尾花数据集","封装我们自己的","求出准确率","测试我们的knn算法","测试数据集的比例","给出从0到n","获取测试数据集","预测结果"],"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":["\"\"","#","%%time","'distance'}","'n_neighbors':","'p':","'weights':","'weights':['distance'],","'weights':['uniform'],","0.0","0.9833333333333333","0.9853862212943633","1","1,verbose=2)","10],","10]},","11，分别拿每一个k去调用算法，得出分数，取得分最高的那个k","1为使用所有的核","2,","3,","4,","4.","5,","5]}],","6,","7,","8,","9,","=","=[",">","[\"uniform\",\"distance\"]:","['distance'],","['uniform'],","[1,","[i","]","array>","best_k","best_method","best_p","best_scor","best_score:","error_score='raise',","estimator=kneighborsclassifier(algorithm='auto',","fit_params=none,","grid","grid_search","grid_search.best_estimator_","grid_search.best_params_","grid_search.best_score_","grid_search.fit(x_train,y_train)","gridsearchcv","gridsearchcv(cv=none,","gridsearchcv(knn_clf,param_grid)","gridsearchcv(knn_clf,param_grid,n_jobs=","iid=true,","import","k","kneighborsclassifier()","kneighborsclassifier(algorithm='auto',","kneighborsclassifier(n_neighbors=k)","kneighborsclassifier(n_neighbors=k,weights='distance',p=p)","kneighborsclassifier(n_neighbors=k,weights=method)","knn_clf","knn_clf.fit(x_train,y_train)","knn_clf.score(x_test,y_test)","knn的另外一个超参数：距离的权重","leaf_size=30,","method","metric='minkowski',","metric_params=none,","n_job","n_jobs=1,","n_neighbors=3,","n_neighbors=5,","p","p=2,","p=3,","param_grid","param_grid=[{'weights':","pre_dispatch='2*n_jobs',","print(\"best_k=\",best_k)","print(\"best_method=\",best_method)","print(\"best_p=\",best_p)","print(\"best_score=\",best_score)","print(\"best_score=0.0.\",best_score)","range(1,11):","range(1,11)]","range(1,11)],","range(1,6):","range(1,6)]","refit=true,","return_train_score='warn',","score","scoring=none,","search","sklearn.model_select","verbos","verbose=0)","weights='distance')","weights='uniform'),","{","{'n_neighbors':","{'weights':","}","},","一般情况下使用距离的导数作为权证","不是用户传入的参数，而是根据用户传入的参数计算出来的结果，以_结尾","两种距离的整理对比","什么是距离","先new一个默认的classifier对象","到这里，我们获得了一个新的超参数","多线程并行处理，占用几个核，","如果k=10，则有必要对10以上的数字进行搜索","实验搜索","寻找好的超参数","寻找最好的k","思路，遍历1","搜索明可夫斯基距离相应的p","明克夫斯基距离","是否打印搜索信息,传入值越大，输出信息越详细","更多的距离定义","曼哈顿距离","最好的分数","最好的参数","最好的评估结果，返回的是kneighborsclassifier对象","欧拉距离","经验数值","网格搜索","考虑距离？不考虑距离","调用fit方法执行网格搜索","调用gridsearchcv创建网格搜索对象，传入参数为classifier对象以及参数列表","超参数和模型参数","领域知识"],"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":["\"\"\"将x根据这个standardscaler进行均值方差归一化处理\"\"\"","\"\"\"根据测试数据集x获得数据的均值和方差\"\"\"","\"must","\"the","#","##","((x[:,1]","(x2[:,0]","(x2[:,1]","(x[:,0]","(x[:col]",",","/","0,","0.","0.00519961,","0.02020202],","0.03970336,","0.04087684],","0.04217092,","0.05152343,","0.06060606,","0.06192266,","0.07070707,","0.08080808,","0.08125616,","0.09090909,","0.0916967","0.1010101","0.10824648,","0.1186457","0.12121212,","0.15151515,","0.15151515],","0.16161616,","0.16496953,","0.16559799,","0.17345038],","0.18181818,","0.18717298,","0.19191919,","0.2020202","0.20468323,","0.21212121,","0.22169257,","0.22427024],","0.2320918","0.2665797","0.27272727,","0.27841562,","0.2805277286657274","0.28282828,","0.28902506,","0.30602392],","0.31313131],","0.3156554505030807","0.32323232,","0.3281103","0.33333333,","0.33513866,","0.34343434,","0.35353535,","0.37373737,","0.39186171,","0.39393939,","0.40226093,","0.4040404","0.41245214,","0.41404933,","0.42424242,","0.42424242],","0.43434343,","0.43859746],","0.44076874,","0.44444444,","0.44858475,","0.45153738,","0.45454545,","0.46464646,","0.46848484848484845","0.47474747,","0.48484848,","0.48484848],","0.4917171717171717","0.49345605,","0.49494949,","0.5053078","0.52525253,","0.53535354,","0.53587921,","0.54545455,","0.55555556,","0.55555556])]","0.56203085,","0.56565657,","0.571171","0.57496445,","0.57575758,","0.57575758],","0.58585859,","0.5959596","0.60606061,","0.61875389,","0.62626263],","0.63636364,","0.64092567,","0.64646465,","0.65930628,","0.66666667,","0.67547694,","0.67676768,","0.68686869,","0.6969697","0.69839152,","0.70374454],","0.70707071,","0.72033239,","0.72727273,","0.73219998,","0.73737374,","0.74747475,","0.75429833])","0.7545644","0.76767677,","0.77777778,","0.78273335,","0.78892303,","0.80808081,","0.81818182,","0.82181859,","0.82828283]])","0.83631808],","0.83838384,","0.84564608,","0.84848485,","0.85858586,","0.86780201,","0.86868687,","0.87878788,","0.88713794],","0.8989899","0.90236912,","0.90616043,","0.91919192,","0.92929293,","0.93939394],","0.94524567,","0.94720873,","0.94949495,","0.95909217,","0.95959596,","0.96889162],","0.96889162]])","0.96969697,","0.97979798,","1.","1.0","1.01581521,","1.01971148],","1.02621444,","1.0295875","1.06867274,","1.07253826,","1.09467835,","1.10146516],","1.12926131,","1.13966053,","1.15228502],","1.15301457,","1.16916667])","1.17408507,","1.18598435,","1.19209981,","1.19638358,","1.2340387","1.2427074","1.25310662,","1.27644165,","1.28485856],","1.29943044,","1.30982967,","1.31552689,","1.3215547","1.35615349,","1.36655271,","1.36661224],","1.39986872,","1.40096142,","1.4174321","1.42327576,","1.43895396,","1.46959958,","1.47999881,","1.49918578],","1.49918578]])","1.52329579,","1.52632263,","1.53672185,","1.54843104,","1.56238103,","1.62783776,","1.63175932],","1.63976872,","1.64672287,","1.68580811,","1.69649176,","1.76295187,","1.76433286],","1.77014994,","1.77530738,","1.7763568394002505e","1.80923518,","1.80993786,","1.8547141","1.89357701,","1.93266225,","1.将这个数据映射到0~xmax","10,","12,","15,","15.],","15],","16,","17","18,","19,","1之间","2\"","2,","2.00218372,","2.08159044,","2.1795164","2.30294347,","2.30846679,","2.45593641,","2.53534313,","2.54979762,","2.98909581,","2.],","2.然后对于每个x相比于整个范围所占的比例","20,","21,","27,","28,","2],","3.0825","3.1086244689504386e","3.70916667,","31.],","31],","32,","33,","34,","35,","37,","39,","40,","42,","42.],","42],","43,","44,","45,","46,","47,","48,","48.],","48],","49,","5.数据归一化","52,","53,","54,","55,","55])","56,","57,","57.],","57],","58,","59,","6,","6.,","60,","62.],","62],","63,","64,","66,","67,","68,","69,","7,","70,","72,","73,","74,","76,","77,","8,","80,","81,","82.]])","82]])","83,","84,","85,","86,","87,","89,","9,","91,","92,","93.],","93],","94,","95,","96,","97,","99,","=","==","[","[(x","[0.06060606,","[0.15151515,","[0.25252525,","[0.27272727,","[0.33333333,","[0.39393939,","[0.49494949,","[0.73737374,","[0.98989899,","[15,","[15.,","[25,","[25.,","[27,","[27.,","[33,","[33.,","[39,","[39.,","[49,","[49.,","[73,","[73.,","[98,","[98.,","[array([0.95959596,","\\","],","__init__(self):","array([0.81019502,","array([5.83416667,","array([95,","array([[","array([[0.52525253,","array([[52,","array([[52.,","assert","befor","class","col","dataset","datasets.load_iris()","def","dimens","equal","featur","fit","fit(self,","import","iri","iris.data","iris.target","kneighborsclassifi","kneighborsclassifier(algorithm='auto',","kneighborsclassifier(n_neighbors=3)","knn_clf","knn_clf.fit(x_train,y_train)","knn_clf.score(x_test,y_test)","leaf_size=30,","learn","learn中使用scal","len(self.mean_),","matplotlib","matplotlib.pyplot","mean_","metric='minkowski',","metric_params=none,","n_jobs=1,","n_neighbors=3,","none","none,\\","normal","normalization：把所有数据映射到0","np","np.array([np.mean(x[:i])","np.array([np.std(x[:i])","np.array(x,dtype=float)","np.array(x2,dtype=float)","np.empty(shape=x.shape,dtype=float)","np.mean(x2[:,0])","np.mean(x2[:,0]))/np.std(x2[:,0])","np.mean(x2[:,1])","np.mean(x2[:,1]))/np.std(x2[:,1])","np.mean(x[:,0])","np.mean(x[:,1])","np.min(x))/np.max(x)","np.min(x)]","np.min(x[:,0]))","np.min(x[:,0]))/(np.max(x[:,0])","np.min(x[:,1])))","np.min(x[:,1]))/(np.max(x[:,1])","np.random.randint(0,100,(50,2))","np.random.randint(0,100,size=100)","np.std(x2[:,0])","np.std(x2[:,1])","np.std(x[:,0])","np.std(x[:,1])","number","numpi","p=2,","plt","plt.scatter(x2[:,0],x2[:,1])","range(x.shape(1))])","range(x.shape[1]):","resx","resx[:col]","return","scikit","self","self.mean_","self.mean_[col])","self.scale_","self.scale_[col]","sklearn","sklearn.model_select","sklearn.neighbor","sklearn.preprocess","standard","standardscal","standardscaler()","standardscaler(copy=true,","standardscaler.fit(x_train)","standardscaler.mean_","standardscaler.scale_","standardscaler.transform(x_test)","standardscaler.transform(x_train)","standardscaler:","std_\"","train_test_split","train_test_split(x,y,test_size=0.2,random_state=666)","transform!\"","transform(self,","weights='uniform')","with_mean=true,","with_std=true)","x","x):","x.ndim","x.shape[1]","x2","x2[:,0]","x2[:,1]","x[:,0]","x[:,1]","x[:10,:]","x_test","x_train","x_train,x_test,y_train,y_test","xmin","y","中的scaler","中的standardscal","之间","在scikit","均值","均值方差归一化","均值，可以看出现在的数据集是均匀分布的","存放了均值方差归一化所对应的信息","实现自己的standardscal","对测试数据集如何归一化？","把所有数据归一到均值为0方差为1的分布中","描述数据的分布范围（标准差）","方差","最值归一化","样本间的距离被一个字段所主导","生成一个一维向量进行归一化","生成一个二维矩阵进行归一化","解决方案","适用于分布有明显边界的情况；受outlier影响较大","适用于数据分布没有明显边界；有可能存在极端情况值","：将所有的数据映射到同一尺度"],"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":["#","%%time","'n_neighbors':","'p':","'weights':['distance'],","'weights':['uniform'],","1.将数据集分割成测试数据集合训练数据集","1.将数据集分成训练数据集合测试数据集","2.将数据集进行归一化处理","2.将训练数据集进行归一化","3.使用训练数据集的均值和方差将测试数据集归一化","3.创建一个kneighborsclassifi","4.使用kneighborsclassifi","4.使用训练数集训练处模型","5","5.使用归一化后的测试数据集测试分类的准确度（accuracy）","5.使用训练数据集得出分类准确度","6.sklearn","6.使用我们的模型预测新的数据","6.使用网格搜索寻找最好的超参数，然后回到1","7.探索超参数","=","=[","[i","]","array>","grid_search","grid_search.best_estimator_","grid_search.best_params_","grid_search.best_score_","grid_search.fit(x_train,y_train)","gridsearchcv","gridsearchcv(knn_clf,param_grid)","import","kneighborsclassifi","kneighborsclassifier()","kneighborsclassifier(n_neighbors=6)","knn_clf","param_grid","range(1,11)]","range(1,11)],","range(1,6)]","sklearn.model_select","sklearn.neighbor","sklearn.preprocess","sklearn_knn_clf","sklearn_knn_clf.fit(x_train,y_train)","sklearn_knn_clf.predict(x_test)","sklearn_knn_clf.score(x_test,y_test)","standardscal","standardscaler()","standardscaler.fit(x_train)","standardscaler.transform(x_test)","standardscaler.transform(x_train)","train_test_split","train_test_split(x,y)","x_test","x_train","x_train,x_test,y_train,y_test","y_predict","{","}","},","中使用knn算法的总结整理","先new一个默认的classifier对象","存放了均值方差归一化所对应的信息","对象","对象进行fit创建出模型","数据归一化总过程","最好的分数","最好的参数","机器学习总过程","机器学习流程回顾","获得最好的评估结果，返回的是kneighborsclassifier对象，可以直接拿来做机器学习预测了","调用fit方法执行网格搜索","调用gridsearchcv创建网格搜索对象，传入参数为classifier对象以及参数列表"],"线性回归算法/":["4.线性回归算法"],"线性回归算法/1.线性回归算法简介.html":[",y(i)）,那么我们期望寻找的直线就是y=ax+b，当给出一个新的点x(j)的时候，我们希望预测的y^(j)=ax(j)+b","1.线性回归算法简介","不使用直接相减的方式，由于差值有正有负，会抵消","不适用绝对值的方式，由于绝对值函数存在不可导的点","将横坐标作为x轴，纵坐标作为y轴，每一个点为（x(i)","房屋面积","样本特征只有一个的线性回归问题，为简单线性回归，如房屋价格","线性回归算法以一个坐标系里一个维度为结果，其他维度为特征（如二维平面坐标系中横轴为特征，纵轴为结果），无数的训练集放在坐标系中，发现他们是围绕着一条执行分布。线性回归算法的期望，就是寻找一条直线，最大程度的“拟合”样本特征和样本输出标记的关系","通过上面的推导，我们可以归纳出一类机器学习算法的基本思路，如下图；其中损失函数是计算期望值和预测值的差值，期望其差值（也就是损失）越来越小，而效用函数则是描述拟合度，期望契合度越来越好"],"线性回归算法/2.简单线性回归的实现.html":["\"\"\"初始化simpl","\"\"\"根据训练集x_train，y_train","\"\"\"给定单个待预测数据x_single，返回x_single对应的预测结果值\"\"\"","\"\"\"给定待预测集x_predict，返回x_predict对应的预测结果值\"\"\"","\"must","\"simpl","\"simplelinearregression1()\"","\"the","#","%timeit","(mean","(x_i","(x_train","*","**","+","+=","0,","0.0","0.39999999999999947","0.8","1","1,\\","100","1000000","11.3","2","2.0","2.1","2.2","2.简单线性回归的实现","3.0","6,","6.93","6]","7","826","84.6","=","==","[0,","__init__(self):","__repr__(self):","_predict(self,","a,b公式","array([5.2])","assert","befor","big_i","big_x","class","d","data\"","def","dev.","each)","equal","featur","fit","fit(self,","for循环方式实现","import","len(x_train)","len(y_train),\\","linear","loop","m","machine_learning.simplelinearregression1","matplotlib.pyplot","ms","none","none,\\","np","np.array([1.,2.,3.,4.,5.])","np.array([1.,3.,2.,3.,5.])","np.array([self._predict(x)","np.random.normal(size=m)","np.random.random(size=m)","num","num/d","numpi","per","plt","plt.axis([0,6,0,6])","plt.plot(x,y_hat,color='r')","plt.scatter(x,y)","predict!\"","predict(self,","reg1","reg1.a_","reg1.b_","reg1.fit(big_x,big_y)","reg1.fit(x,y)","reg1.predict(np.array([6.]))","reg1.predict(x)","reg2.fit(big_x,big_y)","regress","return","runs,","self","self.a_","self.a_*x_single+self.b_","self.b_","simpl","simplelinearregression1","simplelinearregression1()","simplelinearregression1:","size","solv","std.","train","x","x_i,","x_mean","x_mean)","x_mean)*(y_i","x_mean).dot(x_train","x_mean).dot(y_train","x_predict):","x_predict.ndim","x_predict])","x_single):","x_train","x_train,","x_train.mean()","x_train.ndim","y","y_hat","y_i","y_mean","y_mean)","y_predict","y_train\"","y_train):","y_train.mean()","zip(x_train,","±","µs","使用向量化点乘计算分子和分母","使用我们自己的simplelinearregression1","分子","分母","可以看出，向量化的运行速度比循环的形式速度要快80倍","向量化","向量化实现的性能测试","向量化改进num,d的计算方法","实现","模型\"\"\"","求均值","测试","简单自定义一个训练集并描绘","计算分子分母","计算参数a和b","训练simpl","输出"],"线性回归算法/3.衡量线性回归算法的指标.html":["\"\"\"计算y_true和y_predict之间的mse\"\"\"","\"\"\"计算y_true和y_predict之间的rmse\"\"\"","\"the","/","3.1","3.2","3.3","3.衡量线性回归算法的指标","==","\\","assert","def","equal","import","learn","len(y_predict),","len(y_true)","mean_absolute_error","mean_absolute_error(y_true,","mean_squared_error","mean_squared_error(y_test,y_predict)","mean_squared_error(y_true,","mse","mse,rmse,mae的实现","np.sum((y_tru","np.sum(np.absolute(y_tru","return","rmse","root_mean_squared_error(y_true,","size","sklearn.metr","sqrt(mean_squared_error(y_true,","y_predict\"","y_predict))","y_predict)**2)","y_predict):","y_true","其中衡量标准是和m有关的，因为越多的数据量产生的误差和可能会更大，但是毫无疑问越多的数据量训练出来的模型更好，为此需要一个取消误差的方法，如下","平方累加后再开根号，如果某些预测结果和真实结果相差非常大，那么rmse的结果会相对变大，所以rmse有放大误差的趋势，而mae没有，他直接就反应的是预测结果和真实结果直接的差距，正因如此，从某种程度上来说，想办法我们让rmse变的更小小对于我们来说比较有意义，因为这意味着整个样本的错误中，那个最值相对比较小，而且我们之前训练样本的目标，就是rmse根号里面1/m的这一部分，而这一部分的本质和优化rmse是一样的","的实现","的缺点，量纲不准确，如果y的单位是万元，平方后就变成了万元的平方，这可能会给我们带来一些麻烦","衡量标准","调用sikit"],"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":["\"\"\"根据测试数据集","\"\"\"计算y_true和y_predict之间的r",".metric","1","4.1","4.1.1","4.1.2","4.最好的衡量线性回归法的指标","=","def","import","learn","mae的局限性","mean_squared_error(y_true,","model产生的错误会很大，使用我们的模型预测产生的错误会相对少些（因为我们的模型充分的考虑了y和x之间的关系），用这两者相减，结果就是拟合了我们的错误指标，用1减去这个商结果就是我们的模型没有产生错误的指标","r","r2_score","r2_score(y_test,","r2_score(y_test,y_predict)","r2_score(y_true,","return","rmse","score(self,","self.predict(x_test)","sikit","sklearn.metr","squar","square\"\"\"","squared简介","x_test","x_test,","y_predict","y_predict)","y_predict)/np.var(y_true)","y_predict):","y_test","y_test):","使用baselin","可能预测房源准确度，rmse或者mae的值为5，预测学生的分数，结果的误差是10，这个5和10没有判断性，因为5和10对应不同的单位和量纲，无法比较","和","实现","将计算分数方法封装到我们的simplelinearregression中","意义","确定当前模型的准确度\"\"\"","解决办法"],"线性回归算法/5.多元线性回归.html":["\"\"\"初始化linear","\"\"\"根据测试数据集","\"\"\"根据训练数据集x_train，y_train","\"\"\"给定待预测数据集x_predict，返回表示x_predict的结果向量\"\"\"","\"linearregression()\"","\"must","\"n_neighbors\":[i","\"p\":[i","\"the","\"weights\"","#","'distance'}","'n_neighbors':","'p':","'weights':","(θ0)",".metric","0.5","0.602674505080953","0.6060327991735741","0.7354244906092771","0.8008916199519077","01,","01])","02,","03,","1))","1)),","1)]:","1,","1,verbose=1)","1.06270960e+00,","1.09940036e+01,","1.14235739e","1.23179738e","1.40778005e","10],","10]},","180","2,","2.45307516e","3","3,","3.12783163e","3.49155727e+00,","3.99667727e","32.64566083965224","4,","4.2","4.30926281e","5,","5.1","5.多元线性回归","5]}],","6,","60","7,","8,","8.43243544e","8.80618320e","9,","9.16425531e",":","=","==","[","[\"distance\"],","[\"uniform\"],","['distance'],","['uniform'],","[1,","[parallel(n_jobs=","\\","]","__init__(self):","__repr__(self):","array([","assert","a的每一行与b的每一列相乘再相加，等到结果是m行n列的）","befor","boston","boston.data","boston.target","candidates,","class","dataset","datasets.load_boston()","def","done","dot()","each","elapsed:","equal","error_score='raise',","estimator=kneighborsregressor(algorithm='auto',","featur","finish","fit","fit_intercept=true,","fit_normal(self,","fit_params=none,","fold","grid_search","grid_search.best_estimator_.score(x_test,y_test)","grid_search.best_params_","grid_search.best_score_","grid_search.fit(x_train,y_train)","gridsearchcv","gridsearchcv(cv=none,","gridsearchcv(knn_reg,param_grid,n_jobs=","https://blog.csdn.net/nomadlx53/article/details/50849941","iid=true,","import","kneighborsregressor","kneighborsregressor()","knn","knn_reg","knn_reg.fit(x_train,y_train)","knn_reg.score(x_test,y_test)","leaf_size=30,","learn","learn中的回归问题","len(self.coef_),\\","lin_reg","lin_reg.coef_","lin_reg.fit(x_train,y_train)","lin_reg.intercept_","lin_reg.score(x_test,y_test)","linearregress","linearregression()","linearregression(copy_x=true,","linearregression:","matplotlib.pyplot","metric='minkowski',","metric_params=none,","n_jobs=","n_jobs=1,","n_neighbors=5,","none","none,\\","normalize=false)","np","np.hstack","np.hstack([np.ones((len(x_predict),","np.hstack([np.ones((len(x_train),","np.linalg.inv()","np.linalg.inv(x_b.t.dot(x_b)).dot(x_b.t).dot(y_train)","np.ones((len(x_train),","number","numpi","out","p=2,","param_grid","param_grid=[{'weights':","plot","pre_dispatch='2*n_jobs',","predict\"","predict(self,","r2_score","r2_score(y_test,","range(1,11)]","range(1,11)],","range(1,6)]","refit=true,","regression模型\"\"\"","regressor","return","return_train_score='warn',","scikit","score(self,","scoring=none,","self","self._theta","self._theta[0]","self._theta[1:]","self.coef_","self.interception_","self.predict(x_test)","size","sklearn","sklearn.linear_model","sklearn.model_select","sklearn.neighbor","total","train_test_split","train_test_split(x,y,random_state=666)","verbose=1)","weights='uniform'),","x","x[y4.3","x_b","x_b.dot(self._theta)","x_b.t","x_predict","x_predict):","x_predict.shape[1]","x_predict])","x_test","x_test,","x_train","x_train\"","x_train,","x_train,x_test,y_train,y_test","x_train.shape[0]","x_train])","y","y_predict","y_predict)","y_test","y_test):","y_train\"","y_train):","y_train.shape[0],","{","{'n_neighbors':","{'weights':","|","}","},","θ向量","中的线性回归","加载波士顿房价数据","同样行数的，只有一列的全是1的矩阵","和","多元线性回归实现","多元线性回归简介和正规方程解","实现线性回归","截距","拼接矩阵","推导过程参考","构造一个和x_train","由于训练数据集和测试数据集的分割和我们的稍有不同，所以结果会略有不同","矩阵点乘","确定当前模型的准确度\"\"\"","系数向量（θ1,θ2,.....θn）","网格搜索超参数","获取矩阵的转置","获取矩阵的逆","补充（一个1xm的行向量乘以一个mx1的列向量等于一个数）","补充（矩阵点乘：a（m行）·b（n列）","训练linear","运用了cv交叉验证的方式","预测波士顿房价的测试"],"线性回归法/6线性回归的可解性和更多思考.html":["6.线性回归的可解性和更多思考"],"梯度下降法/":["5.梯度下降法","梯度下降法"],"梯度下降法/1.梯度下降法简介.html":["1.","1.梯度下降法简介","η太大，甚至导致不收敛","η太小，会减慢收敛学习速度","以下是定义了一个损失函数以后，参数theta对应的损失函数j的值对应的示例图，我们需要找到使得损失函数值j取得最小值对应的theta（这里是二维平面，也就是我们的参数只有一个）","其他注意事项","在曲线方程中，导数代表切线斜率","在直线方程中，导数代表斜率","多次运行，随机化初始点","导数代表theta单位变化时，j相应的变化","并不是所有函数都有唯一的极值点","梯度下降法的初始点也是一个超参数","梯度下降法简介","解决方案："],"梯度下降法/2.梯度下降法模拟.html":["\"\"\"","\"\"\"损失函数\"\"\"","\"\"\"损失函数的导数\"\"\"","#","(plot_x","(theta","0","0.001","0.01","0.1","0.8","1","1,6,141)","1.1","10)","1]","1e4,epsilon=1","2","2*(theta","2.1","2.5)","2.5)**2","2.梯度下降法模拟","8):","=","[]","def","dj(theta):","epsilon:","eta","eta:学习率η","except:","float('inf')","gradient_descent(0.,eta)","gradient_descent(0.,eta,n_it","gradient_descent(initial_theta,eta,n_it","i_it","i_iters2.2","import","initial_theta","initial_theta:初始化的theta值","j(theta):","matplotlib.pyplot","n_iters:","nan（not","np","np.linspace(","number）","numpi","plot_i","plot_theta_history()","plot_x","plt","plt.plot(plot_x,plot_y)","print(len(theta_history))","return","theta","theta_histori","theta_history.append(initial_theta)","theta_history[","try:","下面来看一下，如果eta取较大值1.1，会出现什么情况","使用不同η学习率测试并观察我们的梯度下降法的结果","保存theta的变化值","可以发现，只要η不超过一个限度，我们编写的函数都可以在有限次数之后找到最优解，并且η越小，学习的次数越多","可以看出当我们的eta取1.1，函数会循环直至终止，这是由于，我们的η设置过大，导致每次循环过后，损失函数j的值都向大的方向变化","实现","数据量太大会报错","最大循环次数","梯度下降法封装","梯度下降法模拟","简单模拟一个损失函数","精度","绘制我们模拟的损失函数","输出","输出10001"],"梯度下降法/3.多元线性回归中的梯度下降法.html":["3","3.多元线性回归中的梯度下降法","一个三维空间中的梯度下降法（x,y为系数，z为损失函数）","上面推导出的式子的大小是和样本数有关的，m越大，结果越大，这是不合理的，我们希望和m无关","多元线性回归中的梯度下降法","推导过程"],"梯度下降法/4.线性回归中的梯度下降法的实现.html":["\"\"\"","\"\"\"根据训练数据集x_train,y_train,","\"the","#","*","+","/","0","1,1)","100)","1e4):","2","2*np.random.random(s","2.","3.","4","4.","4.021457858204859","4.1","4.2","4.3","4.4","4.线性回归中的梯度下降法的实现","8):","=","==","\\","array([3.00706277])","assert","def","dj(theta,","epsilon:","epsilon=1","equal","eta,","eta:学习率η","eta=0.01,","except:","fit_gd(self,","float('inf')","gradient_descent(x_b,","i]))","i_it","import","initial_theta","initial_theta,","initial_theta:初始化的theta值","j(theta,","len(theta)):","len(x_b)","lin_reg","lin_reg.coef_","lin_reg.fit_gd(x,y)","lin_reg.interception_","linearregress","linearregression()","machine_learning.linearregress","matplotlib.pyplot","n_iter","n_iters:","n_iters=n_iters,","np","np.empty(len(theta))","np.random.normal(size=100)","np.random.seed(666)","np.sum((i","np.sum((x_b.dot(theta)","np.sum(x_b.dot(theta)","numpi","plt","plt.scatter(x,y)","range(1,","re","regress","res[0]","res[i]","return","size","theta","try:","x","x.reshape(","x_b,","x_b.dot(theta))**2)","x_b.t.dot(x_b.dot(theta)","x_b:","x_train","x_train,","x_train.shape[0]","x特征矩阵","y","y)","y).dot(x_b[:,","y):","y,","y:","y_train\"","y_train,","y_train.shape[0],","使用梯度下降法训练linear","使用真实的数据测试","使用真实的数据，调整eta和iters，要么由于eta太小导致无法得出真实的结果，要么由于eta太大导致训练时间加长，这是由于数据的规模在不同的特征上不同，所以我们需要对数据进行归一化","修改之前的求导函数","向量化","如果样本数非常多，那么即使使用梯度下降法也会导致速度比较慢，因为在梯度下降法中，每一个样本都要参与运算。这时候需要采用随机梯度下降法，我们将在下一小节进行介绍","定义截距为4","数据归一化","斜率为3","最大循环次数","梯度下降法封装","模型\"\"\"","比较笨的方法实现","测试我们的算法","精度","线性回归中的梯度下降法的实现","结果向量"],"梯度下降法/5.随机梯度下降法.html":["\"\"\"","\"the","#","%%time","(t","(x_b_i.dot(theta)","*","+","/","0.01","1","1)),","2","2.","3.93118623])","318","323","337","5","5.1","5.2","5.22","5.3","5.4","5.随机梯度下降法","50",":param",":return:","=","==",">=","\\","_theta","_theta:","array([3.03182269,","assert","cpu","cur_it","def","dj_sgd(theta,","dj_sgd(theta,x_b[rand_i],y[rand_i])","dj_sgd(theta,x_b_i,y_i):","equal","eta","fit_sgd(self,","gradient","i)","index","initial_theta","initial_theta,","learning_rate(cur_it","learning_rate(cur_iter)","learning_rate(t):","len(x_b)","m","ms","ms,","n_iter","n_iters,","n_iters:","n_iters=5,","n_iters=len(x_b)//3)","np.hstack([np.ones((len(x),","np.hstack([np.ones((len(x_train),","np.random.permutation(m)","np.random.randint(len(x_b))","np.random.randn(x_b.shape[1])","np.zeros(x_b.shape[1])","rand_i","range(m):","range(n_iters):","regression模型","return","self","self._theta","self._theta[0]","self._theta[1:]","self.coef_","self.interception_","sgd(x_b,","sgd(x_b,y,initial_theta,n_iters):","size","sys:","t0","t0,","t0:","t0=5,","t1","t1)","t1:","t1=50):","t:","theta","theta:","time:","times:","total:","user","wall","x])","x_b","x_b[indexes]","x_b_i","x_b_i,","x_b_i.t.dot(x_b_i.dot(theta)","x_b_i:","x_b_new","x_b_new[i],","x_train","x_train,","x_train.shape[0]","x_train:","x_train])","y,","y[indexes]","y_i)","y_i):","y_i:","y_new","y_new[i])","y_train\"","y_train,","y_train.shape[0],","y_train:","中的随机一个元素进行导数公式的计算","为了减慢变化速度，t0为了增加随机性","使用sklearn中的","使用随机梯度下降法训练linear","去x_b,i","在随机梯度下降法中，n_iters代表所有的样本会被看几圈","对x_b进行一个乱序的排序","对整个数据集看一遍","批量梯度下降法带来的一个问题是η的值需要设置的比较小，在样本数比较多的时候导致不是速度特别慢，这时候观察随机梯度下降法损失函数的求导公式，可以发现，我们对每一个xb都做了求和操作，又在最外面除以了m，那么可以考虑将求和和除以m的两个运算约掉，采用每次使用一个随机的xb","根据训练数据集x_train,","模拟数据进行测试","由于我们使用的事随机梯度下降法，所以导致我们的最终结果不会像批量梯度下降法一样准确的朝着一个方向运算，而是曲线行下降，这时候我们就希望，越到下面，η值相应减小，事运算次数变多，从而精确计算结果","真实数据波士顿房价进行测试","第t次循环","计算学习率，t1","输出","这里使用了模拟退火的思想","随机梯度下降法","随机梯度下降法介绍","随机梯度下降法实现","随机梯度下降法的封装和测试","随机的检查了3分之一个样本总量的样本","需要注意的是sklearn中的梯度下降法比我们自己的算法要复杂的多，性能和计算准确度上都比我们的要好，我们的算法只是用来演示过程，具体生产上的使用还是应该使用sklearn提供的"],"梯度下降法/6.梯度下降法的调试.html":["#","%time","(j(theta_1,x_b,y)","*","+","+=","/len(x_b)","0","0.01","0.94575233","1)),","1.98082712","10.99758484]","2.","214","27.6","3.06882065","3.94835863","4.97139932","5.9859077","531","6","6.1","6.梯度下降法的调试","613","67","7.01077392","7.99250414","745","76.7","8):","8.99151383","9.97525811","94.6","=","[","cpu","d_j_debug(theta,x_b,y,epsilon=0.01):","d_j_debug是通用的，可以放在任何求导的debug过程中，所以可以作为我们机器学习的工具箱来使用","d_j_main(theta,x_b,y):","def","epsilon","epsilon=1","eta","eta)","eta,","except:","float('inf')","gradient_descent(d_j,x_b,","gradient_descent(d_j_debug,x_b,","gradient_descent(d_j_main,x_b,","i_it","initial_theta","initial_theta,","j(theta,x_b,y):","j(theta_2,x_b,y))/(2*epsilon)","ms","ms,","n_iters=1e4,","np.arange(1,12,dtype=float)","np.empty(len(theta))","np.hstack([np.ones((len(x),","np.hstack([np.ones((len(x),1)),x])","np.random.normal(size=(1000,10))","np.random.normal(size=1000)","np.random.seed(666)","np.sum((i","np.zeros(x_b.shape[1])","print(theta)","range(len(theta)):","re","res[i]","return","sys:","theta","theta.copy()","theta_1","theta_1[i]","theta_2","theta_2[i]","time:","times:","total:","true_theta","try:","user","wall","x","x])","x_b","x_b.dot(theta))**2)/len(x_b)","x_b.dot(true_theta)","x_b.t.dot(x_b.dot(theta)","y","y)","y,","事实上，这也正是导数的定义，当函数y=f(x)的自变量x在一点x0上产生一个增量δx时，函数输出值的增量δy与自变量增量δx的比值在δx趋于0时的极限a如果存在，a即为在x0处的导数，记作f'(x0)或df(x0)/dx","使用d_j_debug调试模式求出theta","使用数学解求出theta","可能我们计算出梯度下降法的公式，并使用python编程实现，预测的过程中程序并没有报错，但是可能我们需要求的梯度的结果是错误的，这个时候需要怎么样去调试发现错误呢。","实现debug模式的dj(θ)","实现j(θ)函数","实现数学推导出的dj(θ)","我们可以在真正的机器学习之前，先使用d_j_debug这种调试方式来验证一下我们的d_j_main的结果是否正确，然后再进行机器学习。","扩展到多维维度则如下","批量梯度下降法，d_j为求导函数，作为一个参数传入，用于切换求导策略","梯度下降法","梯度下降法调试的原理","梯度下降法调试的实现","添加噪音","由此可以看出，我们的d_j_debug和d_j_main的结果是相近的，所以我们的d_j_main的数学推导是没问题的。","的调试","真实的θ值","输出结果","首先以二维坐标平面为例，一个点（o）的导数就是曲线在这个点的切线的斜率，在这个点两侧各取一个点（ab），那么ab两点对应的直线的斜率应该大体等于o的切线的斜率，并且这a和b的距离越近，那么两条直线的斜率就越接近"],"梯度下降法/7.梯度下降法的总结.html":["\"\"\"","\"the","#","(t","(x_b_k.dot(theta)","))*","*","+","/","1","2/len(x_b_k).","7.1","7.2","7.3","7.梯度下降法的总结",":param",":return:","=","=0","==",">=","\\","assert","cur_it","def","dj_sgd(theta,","equal","fit_lit_sgd(self,","initial_theta","initial_theta,","k:","learning_rate(t):","len(x_b)","m","n_iter","n_iters,","n_iters:","n_iters=5,","np.sum((x_b_k","range(n_iters):","regression模型","return","sgd(x_b,","size","t0","t0:","t0=5,","t1)","t1:","t1=50):","t1=50,k=10):","t:","theta","theta:","x_b_i:","x_b_k,","x_train","x_train,","x_train.shape[0]","x_train:","y,","y_i:","y_k)","y_k):","y_train\"","y_train,","y_train.shape[0],","y_train:","下面来看下二者的对比","中的随机选择k个元素进行导数公式的计算","为了减慢变化速度，t0为了增加随机性","低，每一次的方式不确定，甚至向反方向前进","使用随机梯度下降法训练linear","去x_b,i","在随机梯度下降法中，n_iters代表所有的样本会被看几圈","对于小批量梯度下降法，由多了一个超参数","小批量","小批量梯度下降法：即，我们每一次不看全部样本那么多，也不是只看一次样本那么少，每次只看k个样本","小批量随机下降法的超参数k","快","慢","批量梯度下降法","根据训练数据集x_train,","梯度上升法","每一次只需观察一个样本","每次对所有的样本看一遍才可以计算出梯度","每次看k个元素","稳定性","第t次循环","维度","综合二者的优缺点，有一种新的梯度下降法","计算学习率，t1","计算方式","速度","随机","随机梯度下降法","随机梯度下降法的优点","高，一定可以先向损失函数下降的方式前进"],"PCA/":["6.主成分分析法","pca","主成分分析法"],"PCA/1.PCA简介.html":["(demean)（归0：所有样本都减去他们的均值），使得均值为0，这样可以简化方差的公式","1.1","1.2","1.3","1.4","1.5","1.pca简介","1.主成分分析法的两个轴都是特征，线性回归y轴是目标结果值","2.主成分分析法的点是垂直于方差轴直线的，线性回归的点事垂直于x轴的","analysis）：也是一个梯度分析的应用，不仅是机器学习的算法，也是统计学的经典算法","compon","pca（princip","x(i)映射到w的距离实际上就是x(i)与w的点乘（蓝色的线），根据定义推导，其值实际上就是xproject","一个矩阵可以把一个向量拉伸或者缩短λ倍，这个向量就是特征向量，λ是特征值；","一组线性不相关的特征向量可以组成一个特征向量空间；","下图分别是扔掉了特征一和特征二的两种方案，很明显右边这种的效果会更好一些，因为访问二扔掉特征二以后，点之间的分布情况更接近与原图，但是这不是更好的","与线性回归的区别","举个栗子","事实上有一个指标可以之间定义样本间的距离，就是方差（variance）（方差：描述样本整体之间的疏密的一个指标，方差越大，代表样本之间越稀疏，方差越小，代表样本之间越紧密）","例如下面一个两个特征的一个训练集，我们可以选择一个特征，扔掉一个特征","如何定义样本间间距?","对pca和线性代数中的特征向量的对比","将样例的均值归为0","当然我们也可以之间使用数学原理推导出结果，这里我们主要关注使用搜索的策略来求解主成分分析法，这样我们对梯度上升发和梯度下降法也可以有一个更深刻的认识","总结","我们希望有一根直线，是斜着的，我们希望将所有的点都映射到这条直线上，那么这个时候我们就成功的将二维降到了一维，与此同时，这些点更加趋近与原来的点的分布情况，换句话说，点和点之间的距离比无论是映射到x还是映射到y周，他们之间的区分度都更加的大，也就更加容易区分","所以主成分分析实际上就是把矩阵转换到了一个特殊的特征向量空间；","推导","此时我们的目标函数就可以化简成","注：|xproject|的平均值也是一个向量","第一步:","这个特征向量空间是由能够被拉伸最长的前k个特征向量组成的；而且这k个特征向量相互正交；","这是一个目标函数的最优化问题，使用梯度上升法解决。","进行均值归0操作以后，就是下面的式子","那么如何找到这个让样本间间距最大的轴?","那么能够拉伸最长的特征向量，实际上就是这个矩阵的第一主成分；"],"PCA/2.使用梯度上升法解决PCA问题.html":["((ab)t=btat)转换成最后的结果","1.注意上面式子里的每一个(x1(i)·w1+x2(i)·w2+......xn(i)·wn)都是一个x(i)和w的点乘，所以式子可以进一步化解，","2.使用梯度上升法解决pca问题","2.化简过后可以进行向量化，即每一个∑(x(i)·w1)·x1(i)","3.最后根据转置法则","可以看成是(x·w)这个向量的转置（本来是个行向量，转置后是1行m列的列向量）与x这个矩阵（m行n列）做点乘等到的其中一项的相乘相加的结果"],"PCA/3.主成分PCA的实现.html":["\"\"\"计算单位向量\"\"\"","#","##","(2*epsilon)","(f(w_1,x)","*","+=","/","/len(x)","0","0.01","0.65069321])","0.75*x[:,0]+3.+np.random.normal(0.,10.,size=100)","0.75倍的x[:,0]加上3加上一个噪音","1e4,epsilon=1","2.","3.主成分pca的实现","8):","=","array([0.75934073,","cur_it","deamean(x)","def","demean(x):","df_debug(w,x,epsilon=0.0001):","df_math(w,x):","direction(inital_w)","direction(w):","epsilon","eta","f(w,x):","f(w_2,x))","gradient_ascent(df,x,inital_w,eta,n_it","gradient_ascent(df_debug,x_demean,inital_w,eta)","gradient_ascent(df_math,x_demean,inital_w,eta)","import","inital_w","matplotlib.pyplot","np","np.empty((100,2))","np.empty(len(w))","np.linalg.norm(w)","np.mean(x,axis=0)","np.random.random(x.shape[1])","np.random.uniform(0.,100.,size=100)","np.sum((x.dot(w)**2))/len(x)","numpi","plt","plt.plot([0,w[0]*30],[0,w[1]*30],color='r')","plt.scatter(x[:,0],x[:,1])","plt.scatter(x_demean[:,0],x_demean[:,1])","range(len(w)):","re","res[i]","return","w","w.copy()","w_1","w_1[i]","w_2","w_2[i]","x","x.t.dot(x.dot(w))","x[:,0]","x[:,1]","x_demean","使用debug模式","使用math数学解","使用梯度上升法求解主成分","初始值不能为0，因为将0带入求导公式，会发现得0，没有任何方向","因为对于我们的目标函数来说，w=0本身就是一个最小值点","因为我们本来就是要使得方差最大，而标准化的目的是使得方差为1","均值归0","构造一个两个样本之间有基本线性关系的数据集，可以使得我们降维的效果更加明显","梯度上升法","注意2：不能从0向量开始","注意3：不能使用standardscaler标准化数据","输出","这个轴就是我们求出的第一个主成分"],"PCA/4.求数据的前N个主成分.html":["\"\"\"计算单位向量\"\"\"","#","*","*w","/","/len(x)","0","0.01","0.01,n_iter","0.65068927,","0.65068972,","0.65069316]),","0.65069317])","0.75934372])]","0.75934411])","06","1,1)","1,1)*w","1.数据进行改变，将数据在第一个主成分上的分量去掉","1e","1e4,epsilon","1e4,epsilon=1","2.","2.在新的数据上求第一主成分","4.1","4.求数据的前n个主成分","8):","=","[]","[array([0.75934077,","array([","array([0.75934077,","cur_it","def","demean(x_pca)","df(w,x):","direction(inital_w)","direction(w):","eta","f(w,x):","first_componet(","first_componet(x2,inital_w,eta)","first_componet(x_demean,inital_w,eta)","first_componet(x_pca,initial_w,eta)","first_n_componet(2,x)","first_n_componet(n,x,eta","inital_w","initial_w","np.empty(x.shape)","np.linalg.norm(w)","np.random.random(x.shape[1])","np.random.random(x_pca.shape[1])","np.sum((x.dot(w)**2))/len(x)","plt.scatter(x2[:,0],x2[:,1])","range(len(x)):","range(n):","re","res.append(w)","return","w","w.dot(w2)","w2","x","x(i)","x(i)·w","x,inital_w,eta,n_it","x.copy()","x.dot(w).reshape(","x.t.dot(x.dot(w))","x2","x2[i]","x[i]","x[i].dot(w)*w","x_pca","x_pca.dot(w).reshape(","xproject(i)","xproject(i)就可以实现将x样本在xproject相应上的分量去掉，相减之后的集合意义就是讲x样本映射到了xproject向量相垂直的一个轴上，记为x`(i)","||xproject(i)||","·w","·w（方向）就是x(i)在w上的分向量记为xproject(i)=","几乎为0","即x(i)映射到w上的值，那么||xproject(i)||（大小）","又因为w和w2应该是互相垂直的，所以他们夹角的cos值等于0","向量化，x.dot(w)为m*1的向量，reshape后变成了1*m的列向量，再乘以w（方向）就是x的每一个值在w上","因为w和w2都是单位向量，所以他们两个点乘得到的结果就是他们夹角的cos值，","得到的x`","是x中的所有样本都去除了第一主成分上的分量得到的结果，要求第二主成分，只要在新的数据上，重新求一下第一主成分","求出第一主成分以后，如何求出下一个主成分?","求出第二主成分","的分量矩阵","相减得到的样本分布几乎垂直于原来的样本分布","获得前n个主成分实现","输出","输出5.1820671385094386e"],"PCA/5.高维数据向低维数据进行映射.html":["\"\"\"初始化pca\"\"\"","\"\"\"获得数据集x的前n个元素\"\"\"","\"n_compon","5.","5.高维数据向低维数据进行映射","=","__init__(self,n_components):","assert","class","def","fit(self,x,eta=0.01,n_iters=1e4):","n_compon","n_components>=1,","none","pca:","self.components_","self.n_compon","self.n_componentspca","vaild\"","x1分别乘以w1到wn，得到的k个数组成的向量，就是样本1映射到wk这个坐标系上得到的k维的向量，由于kt**(为什么是转置呢，因为我们是拿x的每一行去和w的每一行做点乘的，但是矩阵乘法规定是拿x的每一行和w的每一列做乘法)","从高维数据向地维数据的映射","如何将我们的样本x从n维转化成k维呢，回忆们之前学到的，对于一个x样本，与一个w进行点乘，其实就是讲一个样本映射到了w这个坐标轴，得到的模，如果讲这一个样本和这k个w分别做点乘，得到的就是这一个样本，在这k个方向上做映射后每一个方向上的大小，这k个元素合在一起，就代表这一个样本映射到新的k个轴所代表的坐标系上相应的这个样本的大小","对于一个数据集x来说，这个x有m行n列，代表有m个样本n个特征，通过我们前面学习的主成分分析法，假设我们已经求出了针对这个数据来说的前k个主成分，每一个主成分对应一个单位方向，用w来表示,w也是一个矩阵，他有k行，代表我们求出的前k个主成分，每一行有n列，代表每一个主成分的坐标轴应该是有n个元素的。这是因为我们的主成分分析法主要就是将数据从一个坐标系转化成了另外一个坐标系，原来这个坐标系有n个维度，现在这个坐标系也应该有n个维度，只不过对于转化的坐标系来说，我们取出来前k个，这k个方向更加重要。","我们得到新的降维后的矩阵xk以后，是可以通过和wk想乘回复回来的，但是由于我们在降维的过程中丢失了一部分信息，这时及时回复回来也和原来的矩阵不一样了，但是这个从数据角度成立的","降维的基本原理:找到另外一个坐标系，这个坐标系每一个轴依次可以表达原来的样本他们的重要程度，也就是称为所有的主成分，我们取得前k个最重要的主成分，就可以将所有的样本映射到这k个轴上，获得一个低维的数据信息","高维数据向低维数据进行映射"],"PCA/6.sklearn中的PCA.html":["#","%%time","(1347,","(1797,)","0.13735469","0.13735469])","0.14566817","0.6066666666666667","0.98","0.9866666666666667","01,","02,","03,","04,","05,","06,","07,","1.01728635e","1.05783059e","1.07377463e","1.08077650e","1.17572392e","1.17777287e","1.21","1.23186515e","1.26617002e","1.29961033e","1.37354688e","1.38003340e","1.42044841e","1.42823692e","1.45798298e","1.46","1.52769283e","1.67","1.71368535e","1.79346003e","1.80304649e","1.88195578e","1.98028746e","2)","2.20030857e","2.25417403e","2.28700570e","2.38328586e","2.42337671e","2.43","2.50362666e","2.55","2.61","2.65","2.87619649e","28","3.05407804e","3.09269839e","3.14917937e","3.26989470e","3.41105814e","3.57755036e","3.60119663e","3.63","3.72917505e","3.82793717e","33,","34,","34])","4.01354717e","4.11","4.26605279e","4.34888085e","4.89020776e","5.08255707e","5.11542945e","5.48538930e","5.60545588e","5.71688020e","5.76411920e","5.79162563e","5.86018996e","5.86686040e","6.06659094e","6.85977267e","6.sklearn中的pca","666)","7.11864860e","7.15189459e","7.44075955e","7.60516219e","7.73828332e","8.40132221e","8.49968861e","8.85220461e","9.00017642e","9.09314698e","9.55152460e","935","=","[]","[np.sum(pca.explained_variance_ratio_[:i+1])","array([0.14566817,","array([1.45668166","cpu","dataset","datasets.load_digits()","digit","digits.data","digits.target","explainedvariance_ratio","import","kneighborsclassifi","kneighborsclassifier()","knn_clf","knn_clf.fit(x_train,y_train)","knn_clf.fit(x_train_reduction,y_train)","knn_clf.score(x_test,y_test)","knn_clf.score(x_test_reduction,y_test)","matplotlib.pyplot","ms","ms,","np","numpi","pca","pca(0.95)","pca(n_components=2)","pca(n_components=64)","pca.explained_variance_ratio_","pca.fit(x)","pca.fit(x_train)","pca.n_components_","pca.transform(x)","pca.transform(x_test)","pca.transform(x_train)","pca降维到两维的意义","plt","plt.plot([i","plt.scatter(x[:,0],x[:,1],color='b',alpha=0.5)","plt.scatter(x_reduction[y==i,0],x_reduction[y==i,1],alpha=0.8)","plt.scatter(x_restore[:,0],x_restore[:,1],color='r',alpha=0.5)","range(10):","range(x_train.shape[1])])","range(x_train.shape[1])],","sklearn","sklearn.decomposit","sklearn.model_select","sklearn.neighbor","sklearn中的pca算法支持传入一个小于1的数来表示我们希望能解释多少比例的主成分","sys:","time:","times:","total:","train_test_split","train_test_split(x,y,random_st","user","wall","x","x_reduct","x_test_reduct","x_train,x_test,y_train,y_test","x_train_reduct","x_train_reduction.shap","y","y.shape","µs,","下图每个颜色代表一个数字在降维到二维空间中的分布情况","两个主成分加起来可以解释百分之27的原数据，而其他的信息丢失了","从64个维度降到两个维度以后，虽然运行速度提高了，但是识别精度大大降低了","仔细观察后可以发现，很多数字的区分还是比较明细的","代表第一个主成分可以解释14%的原数据","代表第二个主成分可以解释13%的原数据","使用64个维度的数据集，训练knn算法","使用pca进行降维，然后再训练knn算法","分割数据集","加载书写识别数据集","可以使用explainedvariance_ratio这个参数来查看每个主成分所解释的原数据，来判断要取多少个主成分","可以方便可视化展示，帮助人们理解","比如如果只是区分蓝色的数字和紫色的数字，那么使用二个维度就足够了","绘制曲线观察取前i个主成分的时候，所能解释的原数据比例","虽然训练出来的精度丢失了一些，但是效率却大大提高了","解释方差的比例","说明前28个主成分表示了百分之95的信息","这个数据可以近乎表示每个主成分轴的重要程度"],"PCA/试手MNIST数据集.html":["#","%time","(60000,","0.9688","0.9728","1.加载mnist数据集","10min","197","1min","2.54","2.55","2.使用knn","209","2s","2s,","3.pca进行降维","31.3","31.5","31.7","352","355","356","41s,","44","47","7.试手mnist数据集","784)","=","cpu","fetch_mldata","fetch_mldata(\"mnist","import","kneighborsclassifi","kneighborsclassifier()","kneighborsclassifier(algorithm='auto',","knn_clf","knn_clf.fit(x_train,y_train)","knn_clf.fit(x_train_reduction,y_train)","knn_clf.score(x_test,y_test)","knn_clf.score(x_test_reduction,y_test)","leaf_size=30,","metric='minkowski',","metric_params=none,","mnist","mnist.data,mnist.target","ms","ms,","n_jobs=1,","n_neighbors=5,","np","np.array(x[60000:],dtype=float)","np.array(x[:60000],dtype=float)","np.array(y[60000:],dtype=float)","np.array(y[:60000],dtype=float)","numpi","original\")","p=2,","pca","pca(0.9)","pca.fit(x_train)","pca.transform(x_test)","pca.transform(x_train)","s","s,","sklearn","sklearn.dataset","sklearn.decomposit","sklearn.neighbor","sys:","time:","times:","total:","user","wall","weights='uniform')","x,i","x_test","x_test_reduct","x_train","x_train.shap","x_train_reduct","y_test","y_train","从784维降到了87维，只用87维就可以解释百分之90的原数据集","使用pca进行降维后的数据集进行训练，不光时间变短了，准确度也变高了","使用可以解释百分之90原数据集的主成分","封装的kneighborsclassifier，在fit过程中如果数据集较大，会以树结构的过程进行存储，以加快knn的预测过程，但是会导致fit过程变慢","没有进行数据归一化，是因为这里的每个维度都标示的是每个像素点的亮度，他们的尺度是相同的，这个时候比较两个样本之间的距离是有意义的","这使得我们可以更加好的，更加准确的拿到我们数据集对应的特征，从而使得准确率大大提高","这是因为pca的过程中，不仅仅是进行了降维，还在降维的过程中将数据包含的噪音给消除了"],"PCA/使用PCA对数据进行降噪.html":["#","(100,","+","0.75*x[:,0]+3.+np.random.normal(0.,10.,size=100)","0.75倍的x[:,0]加上3加上一个噪音","1.回忆我们之前的例子","12","2.手写识别的例子","64)","8.使用pca对数据进行降噪","=","ax.imshow(data[i].reshape(8,8),","clim=(0,16))","cmap='binary',","compon","dataset","datasets.load_digits()","def","digit","digits.data,digits.target","enumerate(axes.flat):","example_digit","example_digits.shap","fig,ax","filtered_digit","gridspec_kw=dict(hspace=0.1,wspace=0.1))","i,ax","import","interpolation='nearest',","matplotlib.pyplot","noisy_digit","noisy_digits[y==0,:][:10]","noisy_digits[y==num,:][:10]","np","np.empty((100,2))","np.random.normal(0,4,size=x.shape)","np.random.uniform(0.,100.,size=100)","np.vstack([example_digits,x_num])","num","numpi","pca","pca(0.5)","pca(n_components=1)","pca.fit(noisy_digits)","pca.fit(x)","pca.inverse_transform(components)","pca.inverse_transform(x_reduction)","pca.n_components_","pca.transform(noisy_digits)","pca.transform(x)","plot_digits(data):","plot_digits(example_digits)","plot_digits(filtered_digits)","plt","plt.scatter(x[:,0],x[:,1],color='b',alpha=0.8)","plt.scatter(x_restore[:,0],x_restore[:,1],color='b',alpha=0.8)","plt.subplots(10,10,figsize=(10,10),","range(1,10):","sklearn","sklearn.decomposit","subplot_kw={'xticks':[],'yticks':[]},","x","x,i","x[:,0]","x[:,1]","x_num","x_reduct","x_restor","使用pca降噪","噪音，所以我们还是倾向于说从x到x_restore丢失了一些信息，不过我们丢失的信息很有可能有很大的一部分","实际情况下，应该多试一些数字，找到最合适的数字","就成为了一条直线，比较一下这两个图，我们可以说，经过这样的操作，我们将原有数据集的噪音","当然，在实际情况下，我们不好说x_restore就是一点噪音都没有，也不好说原数据的所有的抖动全都是","总结一句话就是：降低了维度，丢失了信息，同时也去除了一部分噪音","我们使用pca进行降维然后在反转回原来的维度，经过这样一个操作，可以发现此时这个数据","换句话说，这个数据集展现的是在一根直线上下进行抖动式的分布，实际上这种抖动和这根直线本身的距离是噪音","是噪音，这也解释了为什么我们在上一节降维处理以后，反而识别率提高了","是有噪音的","现在有一个问题：这个数据集展现出来这样的结果，可是是不是有这样一种情况，这个数据集就应该是一根直线呢","画出带噪音的图像","相比之前，数字清楚了很多，平滑了很多，说明使用pca进行降噪是可行的","给消除了","获得每个标记加了噪音的10个元素，一共10个标记，公100个元素","这个噪音的产生原因可能有很多，如测量人员的粗心，测量手段有问题等等原因，都会使得我们在现实世界中采集的数据"],"PCA/特征脸.html":["#","%%time","'aaron","'aj","'zumrati","'zurab","'zydruna","(13233,","(2914,","(36,","(~200mb):","...,","1.加载人脸数据库","1.高维数据向低维数据映射","1min","2.73","2.实际编程用可视化的方式观察特征脸","2.特征脸","2914","2914)","30.8","47)","54s,","57","5749","62*47","62,","9.人脸识别与特征脸","=","array(['aj","ax.imshow(data[i].reshape(62,47),cmap='bone')","cook',","cpu","data","def","download","dtype='#","eckhart',","enumerate(axes.flat):","example_fac","example_faces.shap","face","faces.data,faces.target","faces.data.shap","faces.data[random_indexs]","faces.images.shap","faces.target_nam","faces2","faces2.data,faces2.target","fetch_lfw_peopl","fetch_lfw_people()","fetch_lfw_people(min_faces_per_person=60)","fig,ax","gridspec_kw=dict(hspace=0.1,wspace=0.1))","https://ndownloader.figshare.com/files/5976006","https://ndownloader.figshare.com/files/5976009","https://ndownloader.figshare.com/files/5976012","https://ndownloader.figshare.com/files/5976015","i,ax","ilgauskas'],","images是将我们的数据集以一个二维平面可视化的角度展现出来","import","juma',","lamas',","len(faces.target_names)","lfw","matplotlib.pyplot","metadata:","np","np.random.permutation(len(faces.data))","numpi","pca","pca(svd_solver='randomized')","pca.components_.shap","pca.fit(x)","plot_digits(data):","plot_digits(example_faces)","plot_digits(pca.components_[:36])","plt","plt.subplots(6,6,figsize=(10,10),","random_index","s","s,","sklearn.dataset","sklearn.decomposit","subplot_kw={'xticks':[],'yticks':[]},","sys:","time:","times:","total:","tsereteli',","user","wall","x","x,i","x[:36,:]","一共2914个维度，所以求出了2914个主成分","一方面我们可以方便直观的看出在人脸识别的过程中我们是怎么看到每一张脸相应的特征的","使用所有的主成分绘制特征脸，","使用随机的方式来求解出pca","另一方面，也可以看出来，其实每一张脸都是这些人脸的一个线性组合，而特征脸依据重要程度顺序的排在了这里","可以看到排在前面的这些脸相应的比较笼统，排名第一的这张脸，告诉我们人脸大概就是这个位置，大概有这样一个轮廓","在人脸识别领域中，x的每一行都是人脸，而w中的每一行，相应的也可以理解为是一个人脸，就是特征脸。之所以叫特征脸就是因为，每一行都能反应原来的样本的一个重要的特征。","如果将w中的每一行都看作一个样本的话，那么我们也可以说，第一行所代表的样本是最重要的那个样本，最能反应x这个矩阵原来的那个特征的样本","对于w这个矩阵来说，每一行代表一个方向，第一行是最重要的方向，第二行是次重要的方向","每一行实际上是一个主成分，他相当于表达了一部分原来的人脸数据中对应的一个特征","每张脸对应的人名","没有指定n_componet","由于fetch_lfw_people这个库的人脸是分布不均匀的，有的人可能只有一张图片，有的人有几十张","说明一共包含5749个不同的人的脸","越往后，鼻子眼睛的信息就清晰了起来","通过求特征脸","通过这个方法我们可以取出至少有60张脸的人的数据","随机获取36张脸","，也就是说想求出所有的主成分"],"多项式回归/":["7.多项式回归"],"多项式回归/1.什么是多项式回归.html":["#","(100,","(x**2).shape","+","0.5*x**2","0.52423752])","1)","1,1)","1.9427736300237914","1.多项式回归简介","1.模拟多项式回归的数据集","2+np.random.normal(0,1,size=100)","2.使用线性回归拟合","2.编程实验多项式回归","3,3,size=100)","3.总结","3.解决方案，添加一个特征","=","[]","array([1.08043759,","import","lin_reg","lin_reg.fit(x,y)","lin_reg.predict(x)","lin_reg2","lin_reg2.coef_","lin_reg2.fit(x2,y)","lin_reg2.intercept_","lin_reg2.predict(x2)","linearregress","linearregression()","matplotlib.pyplot","np","np.hstack([x,x**2])","np.random.uniform(","numpi","plt","plt.plot(np.sort(x),y_predict2[np.argsort(x)],color='r')","plt.plot(x,y_predict,color='r')","plt.scatter(x,y)","sklearn.linear_model","x","x.reshape(","x2","y","y_predict","y_predict2","一元二次方程","与此同时需要主要，我们在上一章所讲的pca是对我们的数据进行降维处理，而我们这一章所讲的多项式回归显然在做一件相反的事情，他让我们的数据升维，在升维之后使得我们的算法可以更好的拟合高纬度的数据","从上图可以看出，当我们添加了一个特征（原来特征的平方）之后，再从x的维度来看，就形成了一条曲线，显然这个曲线对原来数据集的拟合程度是更好的","他的关键在于为原来的样本，添加新的特征。而我们得到新的特征的方式是原有特征的多项式的组合。","以上这样的方式，就是所谓的多项式回归","再将得到的数据集与原数据集进行拼接，","原来所有的数据都在x中，现在对x中每一个数据都进行平方，","在用新的数据集进行线性回归","多线性回归在机器学习算法上并没有新的地方，完全是使用线性回归的思路","如果将x2理解为一个特征，将x理解为另外一个特征,换句话说，本来我们的样本只有一个特征x，现在我们把他看成有两个特征的一个数据集。多了一个特征x2，那么从这个角度来看，这个式子依旧是一个线性回归的式子，但是从x的角度来看，他就是一个二次的方程","很明显，我们用一跟直线来拟合一根有弧度的曲线，效果是不好的","由于x是乱的，所以应该进行排序","相当于我们为样本多添加了一些特征，这些特征是原来样本的多项式项，增加了这些特征之后，我们们可以使用线性回归的思路更好的我们的数据","第一个系数是x前面的系数，第二个系数是x平方前面的系数","考虑下面的数据，虽然我们可以使用线性回归来拟合这些数据，但是这些数据更像是一条二次曲线,相应的方程是y=ax2+bx+c,这是式子虽然可以理解为二次方程，但是我们呢可以从另外一个角度来理解这个式子：","采用这样的方式，我们就可以解决一些非线性的问题"],"多项式回归/scikit-learn中的多项式回归于pipeline.html":["#","('lin_reg',linearregression())","('poly',polynomialfeatures(degree=2)),","('std_scaler',standardscaler()),","(5,","+",",","0.5*x**2","0.52423752])","1,","1,1)","1,x1,x2","1.","1.,","1.08043759,","1.3420996","1.74999096,","1.80123135]])","1.使用```polynomialfeatures```生成多项式特征的数据集","1.使用scikit","10.,","100.]])","10]])","12.,","16.],","2)","2+np.random.normal(0,1,size=100)","2.,","2.07484052,","2.5980174","2.74141103,","2.scikit","2.如果生成数据幂特别的大，那么特征直接的差距就会很大，导致我们的搜索非常慢，这时候可以进行**数据归一化**","2.调用linearregression对x2进行预测","25.,","2],","3,","3,3,size=100)","3.,","3.06246837],","3.piplin","3.关于polynomialfeatur","3.进行线性回归","30.,","36.],","4.,","4.30496317],","4.],","49.,","4],","5,","5.,","56.,","6.,","6.74969443],","64.],","6],","7,","7.,","7.51533441],","8.,","81.,","8],","9,","9.,","90.,","=","[","[]","])","array([0.","array([[","import","learn中的多项式回归于pipelin","learn中的多项式对数据进行预处理","lin_reg2","lin_reg2.coef_","lin_reg2.fit(x2,y)","lin_reg2.predict(x2)","linearregression()","np.arange(1,11).reshape(5,2)","np.random.uniform(","pipelin","pipeline([","piplin","pipline的英文名字是管道，那么","plt.plot(np.sort(x),y_predict2[np.argsort(x)],color='r')","plt.plot(np.sort(x),y_predict[np.argsort(x)],color='r')","plt.scatter(x,y)","poli","poly.fit(x)","poly.transform(x)","poly_reg","poly_reg.fit(x,y)","poly_reg.predict(x)","polynomialfeatur","polynomialfeatures()","polynomialfeatures(degree=2)","sklearn.pipelin","sklearn.preprocess","sklearn中对数据进行预处理的函数都封装在preprocessing模块下，包括之前学的归一化standardscal","standardscal","x","x.reshape(","x.shape","x13,x23,x12x2,x22x1","x2","x21,x22,x1*x2","x2[:5]","y","y_predict","y_predict2","一元二次方程","也就是说polynomialfeatures会穷举出所有的多项式组合","传入每一步的对象名和类的实例化","可以想象如果将degree设置为3，那么将产生一下10个元素","对应的是0次幂","将52的矩阵进行多项式转换后变成了56","我们如何使用管道呢，先考虑我们多项式回归的过程","测试多维的数据集","由于x是乱的，所以应该进行排序","的作用就是把上面的三个步骤合并，使得我们不用一直重复这三步","第一列是1","第一列是sklearn为我们添加的x的零次方的特征","第三列是添加的x的二次方的特征","第二列和原来的特征一样是x的一次方的特征","第二列和第三列对应的是原来的x矩阵，此时他有两列一次幂的项","第五列是原来数据的两列相乘的结果","第六列是原来数据的第二列平方的结果","第四列是原来数据的第一列平方的结果"],"多项式回归/过拟合与前拟合.html":["#","('lin_reg',linearregression())","('poly',polynomialfeatures(degree=degree)),","('std_scaler',standardscaler()),","+",",","0.5*x**2","0.5406237455773699","0.5431979125088253","0.7922037464116539","0.9394112675409493","1,","1,1)","1,10","1.000151338154146","1.336192585265726","1.什么是过拟合和欠拟合","10]","1之间）显然不符合我们的数据","2+np.random.normal(0,1,size=100)","2.6112077267395803","2.7714817137686794","2.为什么要使用训练数据集和测试数据集","3","3,","3,3,100).reshape(100,1)","3,3,size=100)","3.过拟合与前拟合","4.192433747323001e+21","=",">正合适",">过拟合）","[","[]","])","def","degree从2到10到100的过程中，虽然均方误差是越来越小的，从均方误差的角度来看是更加小的","fitting】","import","lin_reg","lin_reg.fit(x,y)","lin_reg.fit(x_train,y_train)","lin_reg.predict(x)","lin_reg.predict(x_test)","lin_reg.score(x,y)","linearregress","linearregression()","matplotlib.pyplot","mean_squared_error","mean_squared_error(y,y100_predict)","mean_squared_error(y,y10_predict)","mean_squared_error(y,y2_predict)","mean_squared_error(y,y_predict)","mean_squared_error(y_test,y100_predict)","mean_squared_error(y_test,y10_predict)","mean_squared_error(y_test,y2_predict)","mean_squared_error(y_test,y_predict)","np","np.linspace(","np.random.uniform(","numpi","pipelin","pipeline([","plt","plt.axis([","plt.plot(np.sort(x),y100_predict[np.argsort(x)],color='r')","plt.plot(np.sort(x),y10_predict[np.argsort(x)],color='r')","plt.plot(np.sort(x),y2_predict[np.argsort(x)],color='r')","plt.plot(x,y_predict,color='r')","plt.plot(x_plot,y_plot,color='r')","plt.scatter(x,y)","poly_reg","poly_reg10","poly_reg10.fit(x,y)","poly_reg10.fit(x_train,y_train)，","poly_reg10.predict(x)","poly_reg10.predict(x_test)","poly_reg100","poly_reg100.fit(x,y)","poly_reg100.fit(x_train,y_train)","poly_reg100.predict(x)","poly_reg100.predict(x_plot)","poly_reg100.predict(x_test)","poly_reg2","poly_reg2.fit(x,y)","poly_reg2.fit(x_train,y_train)","poly_reg2.predict(x)","poly_reg2.predict(x_test)","polynomialfeatur","polynomialregression(10)","polynomialregression(100)","polynomialregression(2)","polynomialregression(degree):","return","sklearn.linear_model","sklearn.metr","sklearn.model_select","sklearn.pipelin","sklearn.preprocess","standardscal","train_test_split","train_test_split(x,y)","x","x.reshape(","x_plot","x_train,x_test,y_train,y_test","y","y100_predict","y10_predict","y2_predict","y_plot","y_predict","【under","一个是对于训练数据集来说的，模型越复杂，模型准确率越高，因为模型越复杂，对训练数据集的拟合就越好，相应的模型准确率就越高","下面尝试真正还原原来的曲线（构造均匀分布的原数据集）","他为了拟合我们所有的样本点，变的太过复杂了，这种情况就是过拟合【over","但是一旦来了新的样本点，他就不能很好的预测了，在这种情况下，我们就称我们得到的这条弯弯曲曲的曲线，他的泛化能力（由此及彼的能力）非常弱","但是他真的能更好的预测我们数据的走势吗，例如我们选择2.5到3的一个x，使用上图预测出来的y的大小（0或者","使用10个维度","使用degree=10的时候得到的均方误差要大于degree=2的时候，说明当degree等于10的时候，他的模型泛化能力变弱了","使用pipeline构建多项式回归","使用上小节的过拟合结果，我们可以得知，虽然我们训练出的曲线将原来的样本点拟合的非常好，总体的误差非常的小，","使用均方误差来看拟合的结果，这是因为我们同样都是对一组数据进行拟合，所以使用不同的方法对数据进行拟合","使用多项式回归","使用更多的维度进行多项式回归","刚刚我们进行的实验实际上在实验模型的复杂度，对于多项式模型来说，我们回归的阶数越高，我们的模型会越复杂，在这种情况下对于我们的机器学习算法来说，通常是有下面一张图的。横轴是模型复杂度（对于不同的算法来说，代表的是不同的意思，比如对于多项式回归来说，是阶数越高，越复杂；对于knn来说，是k越小，模型越复杂，k越大，模型最简单，当k=n的时候，模型就简化成了看整个样本里，哪种样本最多，当k=1来说，对于每一个点，都要找到离他最近的那个点），另一个维度是模型准确率（也就是他能够多好的预测我们的曲线）","如果我们的模型面对测试数据结果很差的话，那么他的泛化能力就很弱。事实上，这是训练数据集更大的意义","对于测试数据集来说，在模型很简单的时候，模型的准确率也比较低，随着模型逐渐变复杂，对测试数据集的准确率在逐渐的提升，提升到一定程度后，如果模型继续变复杂，那么我们的模型准确率将会进行下降（欠拟合","对于现在的数据（基于二次方程构造），我们使用低于2项的拟合结果，就是欠拟合；高于2项的拟合结果，就是过拟合","得到的均方误差的指标是具有可比性的，（但是对于多项式回归来说，使用r2score进行衡量是没有问题是）","我们真正需要的是，我们得到的模型的泛化能力更高，解决这个问题的方法也就是使用训练数据集，测试数据集的分离","我们训练的模型目的是为了使得预测的数据能够尽肯能的准确，在这种情况下，我们观察训练数据集的拟合程度是没有意义的","换句话说，我们使用了一个非常高维的数据，虽然使得我们的样本点获得了更小的误差，但是这根曲线完全不是我们想要的样子","显然使用多项式回归得到的结果是更好的","模型的泛化能力","欠拟合和过拟合的标准定义","欠拟合：算法所训练的模型不能完整表述数据关系","测试数据对于我们的模型是全新的数据，如果使用训练数据获得的模型面对测试数据也能获得很好的结果，那么我们就说我们的模型泛化能力是很强的。","直接使用线性回归，显然分数太低","相反，在最开始，我们直接使用一根直线来拟合我们的数据，也没有很好的拟合我们的样本特征，当然他犯的错误不是太过复杂了，而是太过简单了","训练数据集和测试数据集的意义","训练模型使用的x_train，是预测的模型使用x_test，以计算模型的泛化能力","说明总有一条曲线，他能拟合所有的样本点，使得均方误差的值为0","过拟合：算法所训练的模型过多的表达了数据间的噪音关系","这条曲线只是原来的点对应的y的预测值连接起来的曲线，不过有很多地方可能没有那个数据点，所以连接的结果和原来的曲线不一样","这种情况，我们成为欠拟合","通常对于这样一个图，会有两根曲线："],"多项式回归/学习曲线.html":["#","'test')","'train')","(\"lin_reg\",linearregression())","(\"poly\",polynomialfeatures(degree=degree)),","(\"std_scaler\",standardscaler()),","(75,","*","+","0.5","1)","1,1)","1.","2","2.1观察线性回归的学习曲线：观察线性回归模型，随着训练数据集增加，性能的变化","2.2","2.实际编程实现学习曲线","3,3,size=100)","3.总结","4.学习曲线","=","[]","])","algo.fit(x_train[:i],y_train[:i])","algo.predict(x_test)","algo.predict(x_train[:i])","def","import","linearregress","matplotlib.pyplot","mean_squared_error","np","np.random.normal(0,1,size=100)","np.random.seed(666)","np.random.uniform(","numpi","pipelin","pipeline([","plot_learning_curve(algo,x_train,x_test,y_train,y_test):","plot_learning_curve(linearregression(),x_train,x_test,y_train,y_test)","plot_learning_curve(poly20_reg,x_train,x_test,y_train,y_test)","plot_learning_curve(poly2_reg,x_train,x_test,y_train,y_test)","plt","plt.axis([0,len(x_train)+1,0,4])","plt.legend()","plt.plot([i","plt.scatter(x,y)","poly20_reg","poly2_reg","polynomialfeatur","polynomialregression(2)","polynomialregression(20)","polynomialregression(degree):","range(1,len(x_train)+1):","range(1,len(x_train)+1)],np.sqrt(test_score),label","range(1,len(x_train)+1)],np.sqrt(train_score),label","return","sklearn.linear_model","sklearn.metr","sklearn.model_select","sklearn.pipelin","sklearn.preprocess","standardscal","test_scor","test_score.append(mean_squared_error(y_test,y_test_predict))","train_scor","train_score.append(mean_squared_error(y_train[:i],y_train_predict))","train_test_split","train_test_split(x,y,random_state=10)","x","x**2","x.reshape(","x_train,x_test,y_train,y_test","x_train.shap","y","y_test_predict","y_train_predict","什么是学习曲线","从趋势上看：","仔细观察，和线性回归曲线的不同在于，线性回归的学习曲线1.5，1.8左右；2阶多项式回归稳定在了1.0，0.9左右,2阶多项式稳定的误差比较低，说明","使用20阶多项式回归","使用pipline构建多项式回归模型","使用二阶多项式回归","使用二阶线性回归的性能是比较好的","在使用20阶多项式回归训练模型的时候可以发现，在数据量偏多的时候，我们的训练数据集拟合的是比较好的，但是测试数据集的误差相对来说增大了很多，离训练数据集比较远，通常这就是过拟合的结果，他的泛化能力是不够的","在最终，测试误差和训练误差趋于相等，不过测试误差还是高于训练误差一些，这是因为，训练数据在数据非常多的情况下，可以将数据拟合的比较好，误差小一些，但是泛化到测试数据集的时候，还是有可能多一些误差","在测试数据集上，在使用非常少的样本进行训练的时候，刚开始我们的测试误差非常的大，当训练样本大到一定程度以后，我们的测试误差就会逐渐减小，减小到一定程度后，也不会小太多，达到一种相对稳定的情况","在训练数据集上，误差是逐渐升高的。这是因为我们的训练数据越来越多，我们的数据点越难得到全部的累积，不过整体而言，在刚开始的时候误差变化的比较快，后来就几乎不变了","对于欠拟合比最佳的情况趋于稳定的那个位置要高一些，说明无论对于训练数据集还是测试数据集来说，误差都比较大。这是因为我们本身模型选的就不对，所以即使在训练数据集上，他的误差也是大的，所以才会呈现出这样的一种形态","对于过拟合的情况，在训练数据集上，他的误差不大，和最佳的情况是差不多的，甚至在极端情况，如果degree取更高的话，那么训练数据集的误差会更低，但是问题在于，测试数据集的误差相对是比较大的，并且训练数据集的误差和测试数据集的误差相差比较大（表现在图上相差比较远），这就说明了此时我们的模型的泛化能力不够好，他的泛化能力是不够的","绘制学习曲线","观察多项式回归的学习曲线","计算学习曲线数据","随着训练样本的主键增多，算法训练出的模型的表现能力","首先整体从趋势上，和线性回归的学习曲线是类似的"],"多项式回归/验证数据集与交叉验证.html":["\",best_k)","\",best_p)","\",best_score)","\"n_neighbors\":[i","\"p\":[i","\"weights\":['distance'],","#","'distance'}","'n_neighbors':","'p':","'weights':","0,0,0","0.96629213])","0.97777778,","0.980528511821975","0.9823599874006478","0.9823747680890538","0.9860917941585535","1.0min","1.两组调参得出的参数结果是不同的，通常这时候我们更愿意详细使用交叉验证的方式得出的结果。","1.交叉验证","120","2","2,","2.使用交叉验证得出的最好分数0.982是小于使用分割训练测试数据集得出的0.986，因为在交叉验证的","2.编程实现","3","3,","3.总结","3组数据集，30种组合，一共要进行120次的训练","4","4,","40","5,","5.验证数据集与交叉验证","5]}],","6,","7,","8,","9],","=","=666)",">","[","['distance'],","[1,","[2,","[parallel(n_jobs=1)]:","]","array([0.98895028,","best_k","best_knn_clf","best_knn_clf.fit(x_train,y_train)","best_knn_clf.score(x_test,y_test)","best_p","best_scor","best_score,best_k,best_p","best_score:","candidates,","cross","cross_val_scor","cross_val_score(knn_clf,x_train,y_train)","cross_val_score(knn_clf,x_train,y_train,cv=3)","cv默认为3，可以修改改参数，修改修改不同分数的数据集","dataset","datasets.load_digits()","digit","digits.data","digits.target","done","each","elapsed:","error_score='raise',","estimator=kneighborsclassifier(algorithm='auto',","finish","fit","fit_params=none,","fold","grid_search","grid_search.best_estimator_","grid_search.best_params_","grid_search.best_score_","grid_search.fit(x_train,y_train)","gridsearchcv","gridsearchcv(cv=none,","gridsearchcv(knn_clf,param_grid,verbose=1,cv=3)","gridsearchcv里的cv实际上就是交叉验证的方式","iid=true,","import","k","kneighborsclassifi","kneighborsclassifier()","kneighborsclassifier(weights='distance',n_neighbors=2,p=2)","kneighborsclassifier(weights='distance',n_neighbors=k,p=p)","knn_clf","knn_clf.fit(x_train,y_train)","knn_clf.score(x_test,y_test)","k为k近邻中的寻找k个最近元素","leaf_size=30,","metric='minkowski',","metric_params=none,","n_jobs=1,","n_neighbors=5,","np","np.mean(scores)","numpi","out","p","p=2,","param_grid","param_grid=[{'weights':","pre_dispatch='2*n_jobs',","print(\"best_k","print(\"best_p","print(\"best_scor","p为明科夫斯基距离的p","range(1,5):","range(1,6)]","range(2,10):","range(2,10)],","refit=true,","return_train_score='warn',","score","score,k,p","scoring=none,","sklearn","sklearn.model_select","sklearn.neighbor","total","train_test_split","train_test_split(x,y,test_size=0.4,random_st","valid","validation可以叫做留一法","verbose=1)","weights='uniform'),","x","x_train,x_test,y_train,y_test","y","{","{'n_neighbors':","|","}","一组超参数而已，拿到这组超参数后我们就可以训练处我们的最佳模型","交叉验证相对来说是比较正规的、比较标准的在我们调整我们的模型参数的时候看我们的性能的方式","交叉验证：在训练模型的时候，通常把数据分成k份，例如分成3份（abc）（分成k分，k属于超参数），这三份分别作为验证数据集和训练数据集。这样组合后可以分别产生三个模型，这三个模型，每个模型在测试数据集上都会产生一个性能的指标，这三个指标的平均值作为当前这个算法训练处的模型衡量的标准是怎样的。","但是使用交叉验证得到的最好参数best_score并不是真正的最好的结果，我们使用这种方式只是为了拿到","使用sklearn提供的交叉验证","使用交叉验证","使用交叉验证的方式来进行调参的过程","使用分割训练数据集和测试数据集来判断我们的机器学习性能的好坏，虽然是一个非常好的方案，但是会产生一个问题：针对特定测试数据集过拟合","和我们上面得到的best_scor","回顾网格搜素","因为使用train_test_split很有可能只是过拟合了测试数据集得出的结果","我们上面的操作，实际上在网格搜索的过程中已经进行了，只不过这个过程是sklean的网格搜索自带的一个过程","我们使用训练数据集训练好模型之后，将验证数据集送给这个模型，看看这个训练数据集训练的效果是怎么样的，如果效果不好的话，我们重新换参数，重新训练模型。直到我们的模型针对验证数据来说已经达到最优了。","我们每次使用测试数据来分析性能的好坏。一旦发现结果不好，我们就换一个参数（可能是degree也可能是其他超参数）重新进行训练。这种情况下，我们的模型在一定程度上围绕着测试数据集打转。也就是说我们在寻找一组参数，使得这组参数训练出来的模型在测试结果集上表现的最好。但是由于这组测试数据集是已知的，我们相当于在针对这组测试数据集进行调参，那么他也有可能产生过拟合的情况，也就是我们得到的模型针对测试数据集过拟合了","是吻合的","极端情况下，k","注意这个x_test,y_test在交叉验证过程中是完全没有用过的，也就是说我们这样得出的结果是可信的","用我们找到的k和p。来对x_train,y_train整体fit一下，来看他对x_test,y_test的测试结果","由于我们有一个求平均的过程，所以不会由于一份验证数据集中有比较极端的数据而导致模型有过大的偏差，这比我们只分成训练、验证、测试数据集要更加准确","的意思就是交叉验证中分割了三组数据集，而我们的参数组合为8*5=40中组合","虽然整体速度慢了，但是这个结果却是可信赖的","解决的方式其实就是：我们需要将我们的问题分为三部分，这三部分分别是训练数据集，验证数据集，测试数据集。","训练train_test_spilt","过程中，通常不会过拟合某一组的测试数据，所以平均来讲这个分数会稍微低一些","返回的是一个数组，有三个元素，说明cross_val_score方法默认将我们的数据集分成了三份","这三份数据集进行交叉验证后产生了这三个结果","这样我们的模型达到最优以后，再讲测试数据集送给模型，这样才能作为衡量模型最终的性能。换句话说，我们的测试数据集是不参与模型的创建的，而其他两个数据集都参与了训练。但是我们的测试数据集对于模型是完全不可知的，相当于我们在模型这个模型完全不知道的数据","这种方法还会有一个问题。由于我们的模型可能会针对验证数据集过拟合，而我们只有一份验证数据集，一旦我们的数据集里有比较极端的情况，那么模型的性能就会下降很多，那么为了解决这个问题，就有了交叉验证。","通过观察两组调参过程的结果可以发现","那么怎么解决这个问题呢？"],"多项式回归/偏差方差均衡.html":["1.降低模型复杂度","2.减少数据维度；降噪","3.增加样本数（模型太过复杂，模型中的参数非常多，而样本数不足以支撑计算出这么复杂的参数）","4.使用验证集","5.模型正则化","6.偏差方差均衡","偏差","偏差和方差是互相矛盾的。降低方差会提高偏差，降低偏差会提高方差","参数学习通常都是高偏差的算法。因为对数据具有极强的假设","大多数算法具有相应的参数，可以调整偏差和方差","如knn中的k，线性回归中使用多项式回归","方差","有一些算法天生是高偏差算法。如线性回归（用一条直线去拟合一条曲线，导致整体预测结果都距离真实数据查很大，偏差非常大）","有一些算法天生是高方差算法。如knn（过于依赖数据，一点选取的数据点有多数是不正确的，那么预测的结果就是错误的。导致有的很准确，有的非常不准确，方差非常大）","机器学习的主要调整来源于方差（这是站在算法的角度上，而不是问题的角度上,比如对金融市场的理解，很多人尝试用历史的数据预测未来的金融走势，这样的尝试通常都不太理想。很有可能因为历史的金融趋势不能很好的反应未来的走向，这种预测方法本身带来的非常大的偏差）换句话说，我们很容易让模型变的很复杂，从而降低模型的偏差，但是由于这样的模型的方差非常的大，最终也没有很好的性能。","模型没有完全的学到数据的中心，而学习到了很多噪音","模型误差=偏差（bias）均差(variance)+不可避免的误差","解决高方差的通常手段：","非参数学习通常都是高方差的算法。因为不对数据做任何假设"],"多项式回归/模型正则化.html":["#","(\"lin_reg\",linearregression())","(\"poly\",polynomialfeatures(degree=degree)),","(\"ridge_reg\",ridge(alpha=alpha))","(\"std_scaler\",standardscaler()),","(100,)","*","+","0.5","1,1)","1.1888759304218461","1.31964561130862","1.387437803144217","1.什么是模型正则化","167.94010860151894","2.编程实现岭回归","3","3,3,0,6])","3,3,100).reshape(100,1)","3.0,3.0,size=100)","7.模型正则化","=","])","def","import","linearregress","matplotlib.pyplot","mean_squared_error","mean_squared_error(y_test,ridge1_predict)","mean_squared_error(y_test,ridge2_predict)","mean_squared_error(y_test,ridge3_predict)","mean_squared_error(y_test,ridge4_predict)","mean_squared_error(y_test,y20_predict)","module.predict(x_plot)","np","np.linspace(","np.random.normal(0,1,size=100)","np.random.seed(42)","np.random.seed(666)","np.random.uniform(","numpi","pipelin","pipeline([","plot_module(module):","plot_module(poly20_reg)","plot_module(ridge1_reg)","plot_module(ridge2_reg)","plot_module(ridge3_reg)","plot_module(ridge4_reg)","plt","plt.axis([","plt.plot(x_plot[:,0],y_plot,color='r')","plt.scatter(x,y)","poly20_reg","poly20_reg.fit(x_train,y_train)","poly20_reg.predict(x_test)","polynomialfeatur","polynomialregression(20)","polynomialregression(degree):","regress","regular","return","ridg","ridge1_predict","ridge1_reg","ridge1_reg.fit(x_train,y_train)","ridge1_reg.predict(x_test)","ridge2_predict","ridge2_reg","ridge2_reg.fit(x_train,y_train)","ridge2_reg.predict(x_test)","ridge3_predict","ridge3_reg","ridge3_reg.fit(x_train,y_train)","ridge3_reg.predict(x_test)","ridge4_predict","ridge4_reg","ridge4_reg.fit(x_train,y_train)","ridge4_reg.predict(x_test)","ridgeregression(degree,alpha):","ridgeregression(degree=20,alpha=0.00001)","ridgeregression(degree=20,alpha=1)","ridgeregression(degree=20,alpha=100)","ridgeregression(degree=20,alpha=100000)","sklearn.linear_model","sklearn.metr","sklearn.model_select","sklearn.pipelin","sklearn.preprocess","standardscal","train_test_split","train_test_split(x,y)","x","x.reshape(","x_plot","x_train,x_test,y_train,y_test","y","y.shape","y20_predict","y_plot","α实际上是一个超参数，代表在我们模型正则化下新的损失函数中，我们要让每一个θ尽可能的小，小的程度占我们整个损失函数的多少，如果α等于0，相当于没有正则化；如果α是正无穷的话，那么我们主要的优化任务就是让每一个θ尽可能的小","θ求和的系数二分之一是一个惯例，加不加都可以，加上的原因是因为，将来对θ2>求导的时候可以抵消系数2，方便计算。不要也是可以的","一些需要注意的细节：","下图是我们之前使用多项式回归过拟合一个样本的例子，可以看到这条模型曲线非常的弯曲，而且非常的陡峭，可以想象这条曲线的一些θ系数会非常的大。","使用岭回归","分割数据集","多项式回归对样本进行训练，使用20个维度","定义多项式回归函数","定义绘图模型","对于θ的求和i是从1到n,没有将θ0加进去，因为他不是任意一项的系数，他只是一个截距，决定了整个曲线的高低，但是不决定曲线每一部分的陡峭和缓和","岭回归","当alpha非常大，我们的模型实际上相当于就是在优化θ的平方和这一项，使得其最小（因为mse的部分相对非常小）","得到的误差依然是比较小，但是比之前的1.18大了些，说明正则化做的有些过头了","所以为了限制让他们比较小，我们前面系数可以取的小一些","模型样本","模型正则化基本原理","模型正则化需要做的事情就是限制这些系数的大小","注意alpha后面的参数是所有theta的平方和，而对于多项式回归来说，岭回归之前得到的θ都非常大","的alpha值等于1，均差误差更加的缩小，并且曲线越来越趋近于一根倾斜的直线","绘制样本曲线","绘制模型曲线","而使得θ的平方和最小，就是使得每一个θ都趋近于0，这个时候曲线就趋近于一根直线了","让ridge2_reg","过拟合（非常的完全，两段有极端的情况）","通过使用岭回归，使得我们的均方误差小了非常多,曲线也缓和了非常多"],"多项式回归/LASSO.html":["#","(\"lasso_reg\",lasso(alpha=alpha))","(\"poly\",polynomialfeatures(degree=degree)),","(\"std_scatter\",standardscaler()),","1.","1.1213911351818648","1.1496080843259968","1.8408939659515595","2.","8.lasso","=","])","alpha=1的时候正则化已经过头了","def","import","lasso","lasso1_predict","lasso1_reg","lasso1_reg.fit(x_train,y_train)","lasso1_reg.predict(x_test)","lasso2_predict","lasso2_reg","lasso2_reg.fit(x_train,y_train)","lasso2_reg.predict(x_test)","lasso3_predict","lasso3_reg","lasso3_reg.fit(x_train,y_train)","lasso3_reg.predict(x_test)","lassoregression(degree,alpha):","lassoregression(degree=20,alpha=0.01)","lassoregression(degree=20,alpha=0.1)","lassoregression(degree=20,alpha=1)","lasso回归有一些选择的功能","mean_squared_error(lasso1_predict,y_test)","mean_squared_error(lasso2_predict,y_test)","mean_squared_error(lasso3_predict,y_test)","oper","operator的首字母缩写","pipeline([","plot_module(lasso1_reg)","plot_module(lasso2_reg)","plot_module(lasso3_reg)","regression认为对应的这些特征有用，所以他可以当做特征选择用","regression认为这个θ对应的特征是没有用的，剩下的那些不等于0的θ就说明lasso","return","select","sklearn.linear_model","α=100的时候，使用ridge的得到的模型曲线依旧是一根曲线，事实上，使用ridge很难得到一根倾斜的直线，他一直是弯曲的形状","但是lasso不同,在lasso的损失函数中，如果我们让α趋近于无穷，只看后面一部分的话，那么后面一部分的绝对值实际上是不可导的，我们可以使用一种sign函数刻画一下绝对值导数，如下图。那么这个时候，同样在j(θ)向0趋近的过程中，他会先走到θ等于0的y轴位置，然后再沿着y轴往下向零点的方向走","但是使用lasso的时候，当α=0.1，虽然得到的依然是一根曲线，但是他显然比radge的程度更低，更像一根直线","使用lasso的过程如果某一项θ等于0了，就说明lasso","使用|θ|代替θ2来标示θ的大小","增大alpha继续试验","实际编程（准备代码参考上一节岭回归）","当使用ridge的时候，当α趋近与无穷大，那么使用梯度下降法的j(θ)的导数如下图，j(θ)向0趋近的过程中，每个θ都是有值的","总结ridge和lasso","所以从计算准确度上来说，我们应该更加倾向于ridge，但是如果我们的维度比较多，样本非常大（比如多项式回归时degree=100）","这也说明了ridge为什么叫岭回归，因为他更像是翻山越岭一样，在梯度下降法中一点一点找坡度缓的方向前进。而lasso的路径就比较规则，会在训练的过程中碰到一些轴使得某些θ为0。","这是因为lasso趋向于使得一部分theta值为0（而不是很小的值），所以可以作为特征选择用，lasso的最后两个字母so就是select","这里穿的alpha起始值比岭回归的时候大了很多，是由于现在是绝对值","选择运算符","非常接近一根直线"],"多项式回归/L1,L2和弹性网络.html":["1.l1,l2正则","2.l0正则项","3.弹性网","9.l1,l2和弹性网络","elast","l2范数（欧拉距离|lasso","net","regression）","regression）来替代，从而达到选择去掉一些θ的过程","regression），当p=2的时候就是","ridge和lasso都是在损失函数中添加一项，来调节θ的值使其尽可能的小，使得我们的模型泛化能力更好一些","不过实际上我们是很少使用l0正则的，因为l0正则的优化是一个np难的问题，我们不能使用诸如梯度下降法甚至数学公式来找到一个最优解。","他是一个离散的值，我们需要穷举所有θ的值来找出哪些θ需要，哪些不需要。实际上可以用l1正则（lass0","回忆小批量梯度下降法也是将随机梯度下降法和批量梯度下降法结合到了一起。在机器学习领域中，经常使用这种方式来创造出一些新的方法，这些方法虽然名词非常的酷，但是他们背后的意义是非常简单的","在损失函数下，添加上一个l1正则项和一个l2正则项，并引入一个参数r来表示他们之间的比例。同时结合了岭回归和lasso回归的优势","在机器学习领域中，我们会发明不同的名词来描述不同的标准，比如用ridge和lasso来衡量正则化的这一项；mse和mae用来衡量回归结果的好坏，欧拉距离和曼哈顿距离用来衡量两点之间的距离。但是他们背后的数学思想是非常的类似的，表达出的数学含义也是一致的。只不过应用到不同的场景中产生了不同的效果","实际应用中，通常应该先尝试一下岭回归（如果计算能力足够的话）。但是如果θ数量太大的话，消耗计算资源可能非常大，而lasso由于有的时候急于把一些θ化为0，可能会导致得到的偏差比价大。这个时候需要使用弹性网","对任意一个维度x，我们都可以求这样一个值，他的每一个值x都对他的p次方进行求和，再开p次方根。通常将这个式子成为lp范数。","对明克夫斯基距离进一步泛化","当p=1的时候就是l1范数（曼哈顿距离|ridg","我们希望让θ的个数尽量小，描述的是非零θ元素的个数。我们用这样的方式来限制θ的数量尽可能的小，进而来限制我们的曲线不要太抖","模型泛化的一个举例。我们在考试前会做很多练习题。我们做练习题不是为了把全部的练习题（训练数据集）都得到满分，而是为了在最后的那一场考试（真实数据）中得到满分","注：有了l1,l2正则项，我们就可以进一步得到ln正则项，虽然实际应用中我们的n不会超过2，但是在数学推导中是有意义的"],"逻辑回归/":["8.逻辑回归"],"逻辑回归/1.什么是逻辑回归.html":["1.什么是逻辑回归","1/(1+np.exp(","10,10,100)","=","def","import","matplotlib.pyplot","np","np.linspace(","numpi","plt","plt.plot(x,y)","return","sigmoid(t):","sigmoid(x)","t))","x","y","回归问题怎么解决分类问题？","对于线性回归来说，我们得到一个函数f，将样本x输入f后，得到的值y就是要预测的值；","将样本的特征和样本发生的概率联系起来，概率是一个数。","我们可以在线性回归的结果基础上，添加一个σ函数，将结果转换成0到1之间","线性回归计算出来的值域是负无穷到正无穷，而我们使用逻辑回归得出来的p是只取0到之间的个值的。这使得我们不能直接使用线性回归的方法，单单从应用的角度来说，但是这样做不够好，因为我们的逻辑回归的值域是有限制的，使用线性回归或者多项式回归拟合出来的直线或者曲线肯定会比较差。","绘制sigmoid函数","而对于逻辑回归来说，我们要得到一个函数f，我们将样本x输入f以后，f会计算出y一个概率值p，之后我们使用这个概率值p来进行分类，如果p>=0.5,也就是有百分之50以上的概率发生的话，我们就让这个概率的值为1，否则让他为1，当然1和0在不同的场景下代表不同的意思。","逻辑回归：解决分类问题"],"逻辑回归/2.逻辑回归的损失函数.html":["2.逻辑回归的损失函数","log(0)也就是没有任何损失。","log(1","log(p^).当y=0的时候，前半部分是0，就只剩下","p^)。","中的y=1是真值；我们使用σ函数求出来的是预测值）","另一半式子反之亦然。","对于逻辑回归，定义他的损失函数比较困难。","当p=0的时候，按照我们的分类，我们应该把样本分为y^=0(注意这里是预测值)这一类。但是这个样本实际是y=1(注意这里是真值)，显然我们分错了，此时，我们对他进行惩罚，这个惩罚是正无穷的；随着p逐渐变大，我们的损失越来越小。当p=1的时候，我们会将样本分类为y=1，此时和这个样本真实的y=1是一致的，那么此时","我们用两个损失函数训练模型是不太方便的，可以将他们合成一个式子如下。当y=1的时候，后半部分是0，所以就只剩下","（注意区别if"],"逻辑回归/3.逻辑回归函数损失的梯度.html":["1.对σ(t)求导","2.对log(σ(t))进行求导","3.对j(θ)的前一部分求导","3.逻辑回归函数损失的梯度","4.对log(1","5.对j(θ)的后一部分求导","6.整合两部分的求导结果","7.整合对j(θ)所有θ的求导结果","8.对比之前的线性回归的损失函数的导数形式，找到相似点","9.向量化","σ(t))进行求导"],"逻辑回归/4.实现逻辑回归算法.html":["\"\"\"","\"\"\"初始化logist","\"\"\"根据测试数据集","\"\"\"根据训练数据集x_train,y_train,","\"linearregression()\"","\"the","#","(1","(1.+np.exp(","(θ0)","+",",","/","0","0,","0.00436497,","0.00987992,","0.04378511,","0.053248","0.07152432])","0.10411811,","0.15855562,","0.18983267,","0.5","0.7235198625270994","0.72452652,","0.88028026,","0.91446368,","0.92348443,","0.95194827,","0.98002369,","0.98183066,","0.98551291,","0.98773424,","0.99639662,","0])","1,","1.","1.0","1.逻辑回归算法的封装实现","2.测试我们的逻辑回归算法","3.02265606,","4.实现逻辑回归算法","5.07877569])","8):","=","==","\\","__init__(self):","__repr__(self):","_sigmoid(self,","accuracy_score(y_test,","array([","array([0,","array([0.02945732,","assert","class","dataset","datasets.load_iris()","def","dj(theta,","epsilon:","epsilon=1","equal","eta,","eta:学习率η","eta=0.01,","except:","fit(self,","float('inf')","gradient_descent(x_b,","i_it","import","initial_theta","initial_theta,","initial_theta:初始化的theta值","iri","iris.data","iris.target","j(theta,","len(x_b)","len(y)","linearregression()","log_reg","log_reg.coef_","log_reg.fit(x_train,y_train)","log_reg.interception_","log_reg.predict_proba(x_test)","log_reg.score(x_test,y_test)","logisticregress","logisticregression()","logisticregression:","machine_learning.logisticregress","machine_learning.module_select","matplotlib.pyplot","n_iters:","n_iters=1e4):","n_iters=n_iters,","none","np","np.sum(y*np.log(y_hat)","numpi","plt","plt.scatter(x[y==0,0],x[y==0,1],color='red')","plt.scatter(x[y==1,0],x[y==1,1],color='blue')","p越趋近于0，逻辑回归算法越愿意将数据预测为0","p越趋近于1，逻辑回归算法越愿意将数据预测为1","regress","regression模型\"\"\"","return","score(self,","self._sigmoid(x_b.dot(theta))","self._theta","self.coef_","self.interception_","self.predict(x_test)","size","sklearn","t))","t):","theta","train_test_split","train_test_split(x,y)","try:","x","x[i","x_b,","x_b.t.dot(self._sigmoid(x_b.dot(theta))","x_b:","x_test","x_test,","x_train","x_train,","x_train,x_test,y_train,y_test","x_train.shape[0]","x特征矩阵","y","y)","y)*np.log(1","y):","y,","y:","y_hat","y_hat))","y_predict","y_predict)","y_test","y_test):","y_train\"","y_train,","y_train.shape[0],","θ向量","使用我们自己的逻辑回归算法","使用梯度下降法训练logist","只取前两个特征；只取y=0，1","和","如第一个测试数据的预测概率值为0.02945732，对应的y_test为0","对于所有的测试数据，我们的logisticregression全都正确的进行了分类","截距","最大循环次数","梯度下降法封装","模型\"\"\"","确定当前模型的准确度\"\"\"","精度","系数向量（θ1,θ2,.....θn）","结果向量","观察逻辑回归预测的概率值p与y_test的关系"],"逻辑回归/5.决策边界.html":["\"\"\"","#","'linspace'","(",")","*",",","/","/anaconda3/lib/python3.6/sit","0,","0.00436497,","0.00987992,","0.04378511,","0.053248","0.07152432])","0.10411811,","0.15855562,","0.18983267,","0.7235198625270994","0.72452652,","0.88028026,","0.91446368,","0.92348443,","0.95194827,","0.98002369,","0.98183066,","0.98551291,","0.98773424,","0.99639662,","0])","1,","1,1)","1,1),","1.0","3.02265606,","5.07877569])","5.决策边界","=","[]","array([","array([0,","array([0.02945732,","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis[0])*100)).reshape(","axis[2])*100)).reshape(","contour:","custom_cmap","def","follow","import","kneighborsclassifi","kneighborsclassifier()","kneighborsclassifier(algorithm='auto',","kneighborsclassifier(n_neighbors=50)","knn_clf","knn_clf.fit(x_train,y_train)","knn_clf.score(x_test,y_test)","knn_clf_all","knn_clf_all.fit(iris.data[:,:2],iris.target)","knn的决策边界","kwarg","k越小，那么我们的模型就越复杂，在这里，我们可视化的看到了复杂的含义","leaf_size=30,","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","log_reg.coef_","log_reg.coef_[0]","log_reg.coef_[1]","log_reg.interception_","log_reg.interception_)","log_reg.predict_proba(x_test)","matplotlib.color","metric='minkowski',","metric_params=none,","model.predict(x_new)","model：模型","n_jobs=1,","n_neighbors=5,","n_neighbors=50,","np.c_[x0.ravel(),x1.ravel()]","np.linspace(4,8,1000)","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.meshgrid(","p=2,","packages/matplotlib/contour.py:967:","plot_decision_boundary(knn_clf,axis=[4,7.5,1.5,4.5])","plot_decision_boundary(knn_clf_all,axis=[4,8,1.5,4.5])","plot_decision_boundary(log_reg,axis=[4,7.5,1.5,4.5])","plot_decision_boundary(model,axis):","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.plot(x1_plot,x2_plot)","plt.scatter(iris.data[iris.target==0,0],iris.data[iris.target==0,1])","plt.scatter(iris.data[iris.target==1,0],iris.data[iris.target==1,1])","plt.scatter(iris.data[iris.target==2,0],iris.data[iris.target==2,1])","plt.scatter(x[y==0,0],x[y==0,1],color='red')","plt.scatter(x[y==1,0],x[y==1,1],color='blue')","p越趋近于0，逻辑回归算法越愿意将数据预测为0","p越趋近于1，逻辑回归算法越愿意将数据预测为1","return","s)","sklearn.neighbor","us","userwarning:","weights='uniform')","x0,x1","x1","x1_plot","x2(x1):","x2(x1_plot)","x2_plot","x_new","y_predict","y_predict.reshape(x0.shape)","y_test","zz","不规则的决策边界的绘制方法","不过无论是knn，还是逻辑回归算法，我们依然可以加入多项式项，使得他的决策边界不再是一根直线，对于这种情况，我们就不能简单的求出这根直线的方程。然后将整根直线画出来来看到这个决策的边界。这个时候我们需要一个绘制不规则的决策边界的方法","但是我们依然可以使用上面的方式绘制决策边界","使用linspace将x轴，y轴划分成无数的小点","决策边界θt·xb=0","在我们的决策平面上，分布着很多点，对于每一个点，我们都使用我们的模型来判断他分为哪一类。然后将这些颜色绘制出来得到的结果就是决策边界","如第一个测试数据的预测概率值为0.02945732，对应的y_test为0","对于knn来说，这个决策边界是没有表达式的，因为我们不是使用数学求解的方式。而是使用距离投票的方式","就是模型非常的不规整","描述决策边界","绘制knn算法的决策边界。可以看到是一根弯弯曲曲的曲线","绘制三个不同类别","表示的实际上是一条直线，如果有两个特征，那就是在二维平面上的一条直线，x1是横轴，x2是纵轴","观察逻辑回归预测的概率值p与y_test的关系","调整k为50","调整过后的样子已经比上面的决策边界规整了很多。整体分成了三大块，非常的清晰","这就是一种过拟合的表现。可以看到上面的knn_clf_all的n_neighbors（k）为5。之前的讨论曾经说过，","通过上面推导可以得出","通过绘制的结果，可以看出，绘制出的结果是非常不规则的，甚至在黄色的部分还掺杂着一些蓝色的绿色的点。","通过观察上面的图可以发现，如果新来一个数据点，落在了直线（决策边界）的上方，对应的x1θ1+x2θ2+θ>0，也就是p>0.5,那么我们就将他分类为1；反之，则分类为0。这就是决策曲线","通过这样一个例子，再次印证了对于knn算法来说k越大，模型越简单，对于决策边界的划分就是决策越规整，分块越明显"],"逻辑回归/6.在逻辑回归中使用多项式回归.html":["\"\"\"","#","'linspace'","('log_reg',","('log_reg',logisticregression())","('poly',polynomialfeatures(degree=degree)),","('std_scaler',","('std_scaler',standardscaler()),",")","+","/anaconda3/lib/python3.6/sit","0.605","0.95","1,1)","1,1),","4,4,","4,4])","6.在逻辑回归中使用多项式回归","=","])","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis[0])*100)).reshape(","axis[2])*100)).reshape(","contour:","custom_cmap","def","follow","import","include_bias=true,","interaction_only=false)),","kwarg","linearregression()","linearregression())])","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","log_reg","log_reg.fit(x,y)","log_reg.score(x,y)","logisticregress","logisticregression()","machine_learning.logisticregress","matplotlib.color","matplotlib.pyplot","model.predict(x_new)","model：模型","np","np.array(x[:,0]**2","np.c_[x0.ravel(),x1.ravel()]","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.meshgrid(","np.random.normal(0,1,size=(200,2))","np.random.seed(666)","numpi","packages/matplotlib/contour.py:967:","pipelin","pipeline([","pipeline(memory=none,","plot_decision_boundary(log_reg,axis=[","plot_decision_boundary(model,axis):","plot_decision_boundary(poly_log_reg,axis=[","plot_decision_boundary(poly_log_reg2,axis=[","plt","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.scatter(x[y==0,0],x[y==0,1])","plt.scatter(x[y==1,0],x[y==1,1])","poly_log_reg","poly_log_reg.fit(x,y)","poly_log_reg.score(x,y)","poly_log_reg2","poly_log_reg2.fit(x,y)","polynomialfeatur","polynomialfeatures(degree=2,","polynomialfeatures(degree=20,","polynomiallogisticregression(degree):","polynomiallogisticregression(degree=2)","polynomiallogisticregression(degree=20)","print(poly_log_reg.score(x,y))","r2=0。","return","s)","sklearn.pipelin","sklearn.preprocess","standardscal","standardscaler(copy=true,","steps=[('poly',","us","userwarning:","with_mean=true,","with_std=true)),","x","x0,x1","x[:,1]**2","x_new","y","y_predict","y_predict.reshape(x0.shape)","zz","。针对x12和x22依然是一个线性关系。但是对于x1，和x2就是曲线了","不过即使如此还是存在一个，就是直线太简单了，比如如下的情况","两个特征的样本x","使用degree=20得出的决策边界的外面变的很奇怪。出现这样的情况就是因为degree=20太大了。导致边界的形状非常的不规则。此时显然发生了过拟合","使用linspace将x轴，y轴划分成无数的小点","使用多项式回归后分数达到了百分之95.结果非常好","使用逻辑回归","显然有非常多的错误分类，所以导致我们的分类准确度只有0.605","模拟测试用例","直接使用逻辑回归。","线性回归添加了多项式项后。degree这个阶数越大，模型越复杂，就越容易发生过拟合","这样，我们只要引入多项式项就好了。把x12和x22分别作为一个特征项。我们学习到的他们前面的系数都是1","逻辑回归中添加多项式项","逻辑回归，实际上是在决策平面中找到一根直线，通过使用这根直线。用这条直线来分割所有样本的分类，用这样一个例子可以看到。为什么逻辑回归只能解决二分类问题，因为这根直线只能将我们的特征平面分成两部分。","，依然是在特征平面分成了两部分，但是对于这样的分布来说，是不可能使用一根直线来分割的。但是可以使用一个圆形。但是对于一个圆形，他的数学表达式是x12+x22"],"逻辑回归/7.scikit-learn中的逻辑会回归.html":["\"\"\"","#","'linspace'","('log_reg',","('log_reg',logisticregression())","('log_reg',logisticregression(c=c))","('log_reg',logisticregression(c=c,penalty=penalty))","('poly',polynomialfeatures(degree=degree)),","('std_scaler',","('std_scaler',standardscaler()),",")","+","/anaconda3/lib/python3.6/sit","0.7933333333333333","0.82","0.8266666666666667","0.8533333333333334","0.86","0.9","0.9133333333333333","0.92","0.94","1,1)","1,1),","4,4,","4,4])","7.scikit","=","])","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis[0])*100)).reshape(","axis[2])*100)).reshape(","c=0.1","c=1.0","class_weight=none,","contour:","custom_cmap","c·j(θ)","def","dual=false,","fit_intercept=true,","follow","import","include_bias=true,","interaction_only=false)),","intercept_scaling=1,","kwarg","l1","l2","learn中的逻辑会回归","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","log_reg","log_reg.fit(x_train,y_train)","logisticregress","logisticregression()","logisticregression(c=0.1,","logisticregression(c=1.0,","matplotlib.color","matplotlib.pyplot","max_iter=100,","model.predict(x_new)","model：模型","multi_class='ovr',","n_jobs=1,","np","np.array(x[:,0]**2+x[:,1]","np.c_[x0.ravel(),x1.ravel()]","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.meshgrid(","np.random.normal(0,1,size=(200,2))","np.random.seed(666)","numpi","packages/matplotlib/contour.py:967:","penalty='l1',","penalty='l2',","pipelin","pipeline([","pipeline(memory=none,","plot_decision_boundary(log_reg,axis=[","plot_decision_boundary(model,axis):","plot_decision_boundary(poly_log_reg,axis=[","plot_decision_boundary(poly_log_reg2,axis=[","plot_decision_boundary(poly_log_reg3,axis=[","plot_decision_boundary(poly_log_reg4,axis=[","plt","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.scatter(x[y==0,0],x[y==0,1])","plt.scatter(x[y==1,0],x[y==1,1])","poly_log_reg","poly_log_reg.fit(x_train,y_train)","poly_log_reg.score(x_test,y_test)","poly_log_reg.score(x_train,y_train)","poly_log_reg2","poly_log_reg2.fit(x_train,y_train)","poly_log_reg2.score(x_test,y_test)","poly_log_reg2.score(x_train,y_train)","poly_log_reg3","poly_log_reg3.fit(x_train,y_train)","poly_log_reg3.score(x_test,y_test)","poly_log_reg3.score(x_train,y_train)","poly_log_reg4","poly_log_reg4.fit(x_train,y_train)","poly_log_reg4.score(x_test,y_test)","poly_log_reg4.score(x_train,y_train)","polynomialfeatur","polynomialfeatures(degree=2,","polynomialfeatures(degree=20,","polynomiallogisticregression(degree):","polynomiallogisticregression(degree,c):","polynomiallogisticregression(degree,c,penalty='l2'):","polynomiallogisticregression(degree=2)","polynomiallogisticregression(degree=20)","polynomiallogisticregression(degree=20,c=0.1)","polynomiallogisticregression(degree=20,c=0.1,penalty='l1')","print(log_reg.score(x_test,y_test))","print(log_reg.score(x_train,y_train))","random_state=none,","return","s)","scikit","score变低是由于数据比较简单。主要看决策边界","sklearn.linear_model","sklearn.model_select","sklearn.pipelin","sklearn.preprocess","solver='liblinear',","standardscal","standardscaler(copy=true,","steps=[('poly',","tol=0.0001,","train_test_split","train_test_split(x,y,random_state=666)","us","userwarning:","verbose=0,","warm_start=false)","warm_start=false))])","with_mean=true,","with_std=true)),","x","x0,x1","x_new","x_train,x_test,y_train,y_test","y","y_predict","y_predict.reshape(x0.shape)","zz","使用linspace将x轴，y轴划分成无数的小点","使用这种正则化。如果c很小。那我们的任务就是集中精力调整l1或者l2的大小；如果c很大，那我们的任务就是集中精力调整原损失函数j(θ)的大小","使用这种正则化的好处是。我们不得不进行正则化；因为l1/l2前面的系数不能为0","决策边界是一条抛物线","另一种正则化","因为我们生成的数据是具有二次项的。但是我们这里使用的是线性逻辑回归","尝试使用l1正则项","尝试使用多项式回归","尝试增大多项式项degree的值","尝试调整正则化超参数c","已经非常接近之前的正常的决策边界了","描绘线性的决策边界","模型泛化能力并没有降低","模型返回能力变脆。因为出现了过拟合","相当于让模型正则化那一项起更大的作用","虽然边界还是比较奇怪，但是比之前degree=20要好很多","说明默认的正则化超参数c=1.0;penalty=l2,说明默认使用l2正则化"],"逻辑回归/8.OvR与OvO.html":["\"\"\"","#","'linspace'",")","/anaconda3/lib/python3.6/sit","0.6578947368421053","0.7894736842105263","0.9473684210526315","1,1)","1,1),","1.0","1.什么是ovr与ovo","2.sklearn中的ovr与ovo","8.ovr与ovo","=",">o(n)","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis[0])*100)).reshape(","axis[2])*100)).reshape(","cg","cg\")","cg',","class_weight=none,","contour:","custom_cmap","dataset","datasets.load_iris()","def","dual=false,","fit_intercept=true,","follow","import","intercept_scaling=1,","iri","iris.data","iris.data[:,:2]","iris.target","kwarg","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","log_reg","log_reg.fit(x_train,y_train)","log_reg.score(x_test,y_test)","log_reg2","log_reg2.fit(x_train,y_train)","log_reg2.score(x_test,y_test)","logisticregress","logisticregression()","logisticregression(c=1.0,","logisticregression(multi_class='multinomial',solver=\"newton","matplotlib.color","matplotlib.pyplot","max_iter=100,","model.predict(x_new)","model：模型","multi_class='multinomial',","multi_class='ovr'","multi_class='ovr',","n_jobs=1,","np","np.c_[x0.ravel(),x1.ravel()]","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.meshgrid(","numpi","on","onevsoneclassifi","onevsoneclassifier(log_reg)","onevsrestclassifi","onevsrestclassifier(log_reg)","ovo","ovo.fit(x_train,y_train)","ovo.score(x_test,y_test)","ovr","ovr.fit(x_train,y_train)","ovr.score(x_test,y_test)","packages/matplotlib/contour.py:967:","penalty='l2',","plot_decision_boundary(log_reg,axis=[4,8.5,1.5,4.5])","plot_decision_boundary(log_reg2,axis=[4,8.5,1.5,4.5])","plot_decision_boundary(model,axis):","plt","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.scatter(x[y==0,0],x[y==0,1])","plt.scatter(x[y==1,0],x[y==1,1])","plt.scatter(x[y==2,0],x[y==2,1])","random_state=none,","rest","s)","sklearn","sklearn.linear_model","sklearn.model_select","sklearn.multiclass","sklearn中计算logistic不是简单的使用梯度下降法，他是使用更快的一种方法，所以需要修改solver参数","solver='liblinear',","solver='newton","tol=0.0001,","train_test_split","train_test_split(x,y,random_state=666)","us","userwarning:","verbose=0,","vs","warm_start=false)","x","x0,x1","x_new","x_train,x_test,y_train,y_test","y","y_predict","y_predict.reshape(x0.shape)","zz","一对一的进行比较）","一针对剩余）","为了可视化，先使用两个类别","从直观的角度看，决策边界也准确了很多","以此类推，我们分别进行四次分类，哪次获得的类别得分最高，我们就任务他属于哪一个类别。对于逻辑回归来说，就是我们的概率p","使用linspace将x轴，y轴划分成无数的小点","使用ovo","使用ovo的方式预测结果达到了百分之百；","使用sklearn中的ovo","假设一共有4个类别，选取其中的某一个类别（假设红色）（one），而对于剩下的类别，把他们融合在一起，把他称之为其他的类别（rest），这样就把一个四分类问题转换成了二分类问题，转换成了使红色的概率是多少，是非红色的概率是多少","分数不好使由于我们只使用了两个维度","分类准确度比使用ovr的时候要高了很多","复杂度是o(n2),但是分类结果相比ovr是更加准确的，这是因为每次只用真实的两个类别进行比较，所以他更倾向于真实的样本属于哪个类别","复杂度由t变成了n*t","尝试使用所有的数据特征","描绘三分类的决策边界","每次就从n个类别挑出两个类别（比如这里挑出红蓝两个类别）,然后进行二分类任务，看对于这个任务来说，我们的样本点是属于哪个类别。然后依次类推进行扩展，如果我们有4个类别需要分类，那我们就能形成6个两两的对c42(排列组合公式4*3/2=6)，也就是6个二分类问题。对于这6个分类结果，判定他在哪个类别中数量最大，就判定他是哪个类别","注意：这里由于数据集比较小，耗时的差别比较小","逻辑回归只可以解决二分类问题。我们可以对逻辑回归稍加改造，让他解决多分类问题：","默认solver='liblinear',为了正确的调用ovo，缓存newton","默认支持多分类任务，而且默认使用ovr方式","（one"],"评价分类结果/":["9.评价分类结果"],"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":["9.1","confus","data），只使用分类准确度是远远不够的","matrix","准确度的陷阱和混淆矩阵","如果我们真的进行了一个机器学习算法进行训练，而准确度是99.9%的话，那么说明我们的机器学习算法是失败的，因为他比纯粹的预测一个人是监控的准确率更低。","对于二分类问题来说，混淆矩阵是一个2*2的矩阵。每一行代表的是对于预测的问题来说真实值是多少，相应的每一列是分类算法进行预测的预测值是多少。","对于极度偏斜的数据（skeke","对于这样一个系统，我们什么都没有做，就可以达到99.9%的准确率","每个维度都是按照顺序排列的。","混淆矩阵","行相当于二维数组的第一个维度，列相当于二维数组的第二个维度，相对来将，真实值（第一个维度）要在预测值（第二个维度）的前面"],"评价分类结果/9.2 精准率和召回率.html":["9.2","召回率：我们关注的那个事件，真实的发生了的数据(分母)中，我们成功的预测了多少。","在10000个癌症患者中，一共有10个癌症患者，我们成功的预测的预测出了8个","在及其有偏的数据中，我们不看准确率，而看精准率和召回率，才能分析出算法的好坏","在有偏的数据中，我们将分类1作为我们关注的对象，例如在医疗中，这个精准率就是指我们预测癌症预测的成功率","我们通过精准率和召回率，判断出了这样做的预测算法是完全没有用的。","精准率=40%：我们没做100次患病的预测，其中会有40次是对的","精准率和召回率","精准率：预测数据为1，预测对了的概率"],"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":["#","(tp","+","...,","/","0","0.8","0.9473684210526315","0.9755555555555555","1","1,","2","2,","2],","36","36]])","403","8,","8])","9","9,","9.3","=","==","[","[fn(y_true,","[tn(y_true,","])","array([0,","array([[403,","assert","confusion_matrix(y_test,y_log_predict)","confusion_matrix(y_true,","confusion_matrix,","dataset","datasets.load_digits()","def","digit","digits.data","digits.target","digits.target.copy()","except:","fn","fn(y_test,y_log_predict)","fn(y_true,","fn)","fp","fp(y_test,y_log_predict)","fp(y_true,","fp(y_true,y_predict)],","fp)","import","learn中的混淆矩阵，精准率和召回率","len(y_predict)","len(y_true)","log_reg","log_reg.fit(x_train,y_train)","log_reg.predict(x_test)","log_reg.score(x_test,y_test)","logisticregress","logisticregression()","np","np.array([","np.sum((y_true==0)&(y_predict==0))","np.sum((y_true==0)&(y_predict==1))","np.sum((y_true==1)&(y_predict==0))","np.sum((y_true==1)&(y_predict==1))","numpi","precision_score(y_test,","precision_score(y_true,","precision_score,","recall_scor","recall_score(y_test,","recall_score(y_true,","return","scikit","sklearn","sklearn.linear_model","sklearn.metr","sklearn.model_select","tn(y_test,y_log_predict)","tn(y_true,","tp","tp(y_test,y_log_predict)","tp(y_true,","tp(y_true,y_predict)]","train_test_split","train_test_split(x,y,random_state=666)","try:","x","x_train,x_test,y_train,y_test","y","y[digits.target!=9]","y[digits.target==9]","y_log_predict","y_log_predict)","y_predict)","y_predict),","y_predict):","实现混淆矩阵，精准率和召回率","手动将手写数据集变成及其偏斜的数据。不是9的y=0，是9的y=1"],"评价分类结果/F1 Score.html":["#","(precis","*","+","/","0","0.0","0.1","0.18000000000000002","0.5","0.8674698795180723","0.9","0.9243243243243242","0.95","1","1.f1","2","2.f1","9.4","=","dataset","datasets.load_digits()","def","digit","digits.data","digits.target.copy()","f1","f1_score","f1_score(precision,","f1_score(y_test,","import","log_reg","log_reg.fit(x_train,y_train)","log_reg.predict(x_test)","log_reg.score(x_test,y_test)","logisticregress","logisticregression()","np","numpi","precis","recal","recall)","recall):","return","score","score的含义","score，让我们兼顾精准率和召回率","sklearn","sklearn.linear_model","sklearn.metr","sklearn.model_select","train_test_split","train_test_split(x,y,random_state=666)","x","x_train,x_test,y_train,y_test","y","y[digits.target!=9]","y[digits.target==9]","y_log_predict","y_log_predict)","也将特别的低，只有二者都非常高，我们得到的值才会特别高","但是对于这个应用来说，很有可能我们对召回率不是特别关注。可能有很多上升周期，但是我们落掉了一些上升的周期（本来为1，我们错误的判断为0），这对我们没有太多的损失，因为我们漏掉了他，也不会投钱进去。","只有都非常大，结果才会打","召回率低意味着:本来一个病人得病了，但是我们没有把他预测出来，这就意味着这个病人的病情会继续恶化下去。所以召回率更加重要，我们希望把所有有病的患者都预测出来。但是精准率却不是特别重要，因为本来一个人没病，我们预测他有病，这时候让他去做进一步的检查，进行确诊就好了。我们犯了fp的错误，只是让他多做了一次检查而已。这个时候召回率比精准率重要。","当二者相同，得到的就是这个相同的值","我们希望这个比例越高越好。如果我们预测股票升了，我们就要购买这个股票，如果我们犯了fp的错误（实际上股票将下来了，而我们预测升上来了），那么我们就就亏钱了。","手动将手写数据集变成及其偏斜的数据。不是9的y=0，是9的y=1","显然没有精准率和召回率高，这是因为首先我们的数据是有偏的，精准率和召回率都比准确率要低一些，在这里精准率和召回率能够更好的反应我们的结果。其次使用逻辑回归进行预测，明显召回率比较低，所以f1_score","有一个非常小，整体就非常小","有的时候我们希望同时关注精准率和召回率，这个时候我们可以使用f1","有的时候我们注重召回率，如病人诊断。","有的时候我们注重精准率，如股票预测。","的实现","精准率和召回率是两个指标，有的时候精准率高一些，有的时候召回率高一些，在我们使用的时候，我们应该怎么解读这个精准率和召回率呢？","能更好的反应算法的水平","被召回率拉低了，这个时候对于这个有偏的数据来说，我们更倾向认为f1_score","调和平均值的特点：如果一个值特别高，一个值特别低，那么我们得到的f1","这个问题的答案，和机器学习大多数的取舍是一样的，应该视情况而定。"],"评价分类结果/9.5 Precision-Recall的平衡.html":["#","0","0,","0.5333333333333333","0.96","0.97831023,","0])","1.理解为什么二者是矛盾的","16.21331661,","19.7173878","19.889528301315465","1],","2.直观的理解","21,","22.05701166,","24.54003391,","24]])","25.04286766,","3.编码观察精准率和召回率的平衡","33.02939187,","44.39167886,","48.25125463,","5,","80.37908046,","85.68606536672523","9.5","=",">=","[","])","array([","array([0,","array([[404,","confusion_matrix(y_test,","dscision_scor","dtype='int')","log_reg.decision_function(x_test)","log_reg.decision_function(x_test)[:10]","log_reg.predict(x_test)[:10]","np.array(dscision_scor","precis","print(np.max(dscision_scores))","print(np.min(dscision_scores))","print(precision_score(y_test,","print(recall_score(y_test,","recall的平衡","threshold。可不可以让这个threshold作为决策边界呢","threshold。这样，相当于我们为我们的逻辑回归算法添加了一个新的超参数threshold，通过指定threshold，相对于可以平移决策边界的直线，进而影响逻辑回归的结果。","y_predict2","y_predict2)","y_predict2))","升高threshold后,精准率升高，召回率降低","即θt·xb","回忆我们上一章学习的逻辑回归，通过一系列的推导，我们得出了决策边界:θt·xb","因为对于前10个样本来说，分数值都是小于0","如果想让精准率变高的话，相应的其实就是我们只能讲那些特别有把握的数据分类为1。在这种情况下，实际上我们做的事情是让我们的算法做的事让其判断样本的概率是百分之90甚至百分之99的时候，我们才预测他为1，在这样的情况，很显然有很多真实为1的样本就被排除在了外面，所以召回率就会变低。","如果我们设置任意一个常量","如果要让召回率升高，我们就要降低算法判断的概率的threshold，这时却是召回率提高了，但是精准率下降了","由上图可知，不同的threshold对应的精准率和召回率的关系。随着threshold增大，精准率在不断的变高，而召回率在不断的降低。由此说明精准率和召回率是相互矛盾的","的，所以预测值都是0","精准率和召回率这两个指标是互相矛盾的。我们要找到的是这两个指标直接的一个平衡。","结果即为对于每一个样本来说，在逻辑回归算法中对应的score值是什么","通过构建新的y_predict，来实现修改threshold为5"],"评价分类结果/9.6 精准率-召回率曲线.html":["#","(144,)","(145,)","0","0.","1","1.","1])","9.6","=",">=","[]","axis.","correspond","dataset","datasets.load_digits()","decision_scor","decision_scores)","digit","digits.data","digits.target.copy()","dtype='int')","ensur","graph","import","last","learn中的precis","log_reg","log_reg.decision_function(x_test)","log_reg.fit(x_train,y_train)","logisticregress","logisticregression()","matplotlib","np","np.arange(np.min(decision_scores),","np.array(decision_scor","np.max(decision_scores))","numpi","plt","plt.plot(precisions,","plt.plot(thresholds,","plt.show()","precis","precision_recall_curv","precision_recall_curve(y_test,","precision_scor","precisions)","precisions,","precisions.append(precision_score(y_test,","precisions.shap","precisions[:","pyplot","recal","recall_scor","recalls)","recalls,","recalls.append(recall_score(y_test,","recalls.shap","recalls[:","respect","r曲线更靠外（也就是与x轴，y轴所包围的面积越大），那么说明这根曲线对应的模型越好，因为对应这个曲线上的每一个点，他的precision_score和recall_score都比另一个曲线要大","r曲线，如果这个p","scikit","sklearn","sklearn.linear_model","sklearn.metr","sklearn.model_select","sklearn中会根据我们顶一个的decision_scores的值来取最合适的步长","start","threshold","threshold,","threshold.","thresholds.shap","thresholds:","train_test_split","train_test_split(x,y,random_state=666)","valu","x","x_train,x_test,y_train,y_test","y","y[digits.target!=9]","y[digits.target==9]","y_predict","y_predict))","召回率曲线","我们用不同的超参数训练不同的模型，每得到一个新的模型，就可以得到一根不同的p","手动将手写数据集变成及其偏斜的数据。不是9的y=0，是9的y=1","曲线","没有从最小值开始取，在sklearn的封装中，他自动寻找了他认为最重要的数据","精准率","通过曲线再次印证了，精准率和召回率是相互制约的，这个曲线急剧下降的一个点，可能就是精准率和召回率平衡最好的一个位置"],"评价分类结果/9.7 ROC曲线.html":["#","0.1)","0.9830452674897119","1.什么是roc","9.7","=",">=","[]","characterist","curve，描述tpr","decision_scores)","fals","fpr","fprs,","fprs.append(fpr(y_test,","fpr之间的关系","fpr都是逐渐升高的，换句话说，tpr和fpr之间是存在着相一致的关系的","fpr：预测为1，但是预测错了的数量占真实值为0的百分比是多少","import","learn中的roc曲线","machine_learning.metr","np.arange(np.min(decision_scores),","np.max(decision_scores),","oper","plt.plot(fprs,","plt.plot(fprs,tprs)","posit","rate","rate，fpr","roc_auc_scor","roc_auc_score(y_test,","roc_curv","roc_curve(y_test,","roc曲线","roc：receiv","scikit","sklearn.metr","threshold","threshold,dtype='int')","thresholds:","tpr","tpr,fpr","tprs)","tprs,","tprs.append(tpr(y_test,","tpr和fpr之间的关系","tpr：预测为1，并且预测对了的数量占真实值为1的百分比是多少","true","y_predict))","y_predict=np.array(decision_scor","为了提高tpr，我们就要将threshold拉低，但是在拉低的过程中，犯fp的错误的概率也会增高","他的面积相应的也会越大，这种情况下分类算法的效果就更好","和","在fpr越低的时候（犯fp错误的次数越少的时候），相应的fpr越高的时候，这根曲线整体就会被抬的越高，","对于roc曲线来说，他对于有偏的数据并不是那么敏感，所以在有偏的数据集里，看一下精准率和召回率是非常有必要的","对于roc曲线，我们通常关注的是这根曲线下面的面积的大小，面积越大，代表分类算法效果越好。","曲线","绘制roc曲线","编程实现roc曲线","而roc的应用场景是比较两个模型的优劣","计算roc曲线的面积","随着threshold逐渐降低，tpr"],"评价分类结果/9.8 多分类问题中的混淆矩阵.html":["#",",","/","0)","0,","0.","0.00649351,","0.00657895,","0.00671141,","0.00680272,","0.00699301,","0.00704225,","0.00729927,","0.00740741,","0.00763359],","0.01298701,","0.01342282,","0.01360544,","0.01398601,","0.01408451,","0.01526718],","0.02013423,","0.02040816,","0.02097902,","0.02112676,","0.02189781,","0.03053435],","0.03496503,","0.03649635,","0.04026846,","0.04580153],","0.05109489,","0.07482993,","0.9304589707927677","0],","1,","105,","11,","116]])","130,","133,","135,","139,","141,","145,","147,","1],","2,","2],","3,","4],","5,","6,","6],","7,","9.8","=","[","[0.","[0.00675676,","],","]])","array([[0.","array([[147,","average=\"micro\")","axis=1)","cfm","cmap=plt.cm.gray)","confusion_matrix","confusion_matrix(y_test,","dataset","datasets.load_digits()","digit","digits.data","digits.target","err_matrix","import","log_reg","log_reg.fit(x_train,","log_reg.predict(x_test)","log_reg.score(x_test,","logisticregress","logisticregression()","matplotlib.pyplot","np","np.fill_diagonal(err_matrix,","np.sum(cfm,","numpi","plt","plt.matshow(cfm,","plt.matshow(err_matrix,","precision_scor","precision_score(y_test,","row_sum","sklearn","sklearn.linear_model","sklearn.metr","sklearn.model_select","test_size=0.8)","train_test_split","train_test_split(x,","x","x_test,","x_train,","y","y,","y_predict","y_predict)","y_predict,","y_test","y_test)","y_train)","y_train,","中的阈值，来相应的调整多分类问题的准确度","多分类问题中的混淆矩阵","多分类问题中越亮的地方代表的就是犯错误越多的地方，并且通过横纵坐标可以看出具体的错误","将对角线上的数字全都填成是0，剩下的其他的格子就是犯错误的百分比","我们可以看到，这个分类的结果，我们又把他规约成了一个二分类的问题，我们现在的分类的结果很容易混淆1和9以及1和8，相应的我们可以微调1和9和1和8分类问题","每一行有多少样本,在列方向上求和","的","的混淆矩阵天然支持多分类问题","真值为i而预测为j的样本数量有多少","矩阵中的每一行的数字都会除以这一行的数字和得到的一个百分比矩阵","第i行第j列的数值代表","绘制混淆矩阵，越亮的地方说明数值越大","通过传入average参数可以让precision_score处理多分类问题","通过这样一个矩阵，我们就可以很清晰的发现分类的错误，并且更重要的是，可以看出具体的错误类型，比如有很多的8我们把他规约为了1，有很多1我们规约成了8，有了这些提示，我们就可以进一步改进我们的算法了。","逻辑回归，如果我们传进来的数据集有多个分类，他讲使用ovr的方式来解决多分类的问题"],"支撑向量机SVM/":["10.支撑向量机","svm"],"支撑向量机SVM/11.1 什么是SVM.html":["11.1","hard","machin","margin","soft","support","svm","svm的决策边界的特点：理两个类别的数据点都尽可能的远，也就是这两个类别离决策边界最近的这些点离决策边界也尽肯能的远","svm这种思想对于未来的泛化能力的考量没有集中在数据预处理阶段，或者是找到了这个模型之后在进行正则化。而是将这种考量放在了算法内部。也就是要找到一条决策边界离两类样本都尽可能的远，这样的决策边界，他的泛化能力就是好的","svm进行改变得来的","svm，其是在hard","vector","之前学习的逻辑回归（通过最小化损失函数找到一个决策边界，通过决策边界来分类数据）有一个非常大的不足，就是他的模型泛化能力非常弱，因为我们通过已知的数据求出了决策边界，而并没有考虑未知的数据。","什么是svm","到现在我们所介绍的svm，都必须在样本点中能求出一根确确实实的直线，满足以上的条件，也就是线性可分的","对于那些线性不可分的情况，对应的解决办法是","支持向量机","支持向量机的思想是找到一条决策边界，让这条边界理两部分数据都尽可能的远，并且能很好的分别这两个类别的数据点。也就是说这种方式不仅要将现在的数据进行一个很好的划分，同时还考虑了未来的泛化能力。"],"支撑向量机SVM/11.2 SVM背后的最优化问题.html":["11.2","svm的最终数学推导","svm背后的最优化问题","回忆解析几何，点到直线的距离","我们只要找到d的表达式，也相应的能够求解svm的问题。","有了这个点到直线的距离，我们就可以相应的得出svm这个问题的表达式"],"支撑向量机SVM/11.3 Soft Margin SVM.html":["11.3","1之间，那么正规式子将以优化前半部分为主，如果c特别大，那么这个式子主要优化的目标就是后半部分","margin","soft","svm","svm对应的表达式为","svm就退化成了hard","在我们之前推导的svm基础上，可以适当的将条件放宽松一些，允许我们的模型犯一些错误","如果c=1，代表两部分的重要程度是一样的，如果c在0","如果c越大，对应的我们就是在逼迫着所有的℥为0，此时意味着模型的容错空间更小，soft","对于每一个样本点，都应该有一个℥。也就是说对于每一个数据点，我们都应该求出他的一个容错空间","所以完整的soft"],"支撑向量机SVM/11.4 scikit-learn中的SVM.html":["\"\"\"","#","&","'linspace'","(down_i","(up_i",")","*","+","/","/anaconda3/lib/python3.6/sit","0","1,1)","1,1),","1/w1","1/w[1]","11.4","2.49296044]])","200)","3,","3])","4.03237972,","=","=>",">=","array([0.95365304])","array([[","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis[0])*100)).reshape(","axis[1],","axis[2])","axis[2])*100)).reshape(","b","class_weight=none,","classifi","contour:","custom_cmap","c放小之后，容错空间变大，所以犯了一个错误","c越大，越偏向于是一个hard","dataset","datasets.load_iris()","def","down_i","dual=true,","fit_intercept=true,","follow","import","intercept_scaling=1,","iri","iris.data","iris.target","kwarg","learn中的svm","linearsvc","linearsvc(c=0.01)","linearsvc(c=0.01,","linearsvc(c=0.1)","linearsvc(c=0.1,","linearsvc(c=1000000000.0,","linearsvc(c=1e9)","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","loss='squared_hinge',","margin","matplotlib.color","matplotlib.pyplot","max_iter=1000,","model.coef_[0]","model.intercept_[0]","model.predict(x_new)","model：模型","multi_class='ovr',","np","np.c_[x0.ravel(),x1.ravel()]","np.linspace(axis[0],","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.meshgrid(","numpi","packages/matplotlib/contour.py:967:","penalty='l2',","plot_decision_boundary(model,axis):","plot_decision_boundary(svc,axis=[","plot_decision_boundary(svc2,axis=[","plot_svc_decision_boundary(model,axis):","plot_svc_decision_boundary(svc,axis=[","plot_svc_decision_boundary(svc2,axis=[","plot_svc_decision_boundary(svc3,axis=[","plot_x","plt","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.scatter(x[y==0,0],x[y==0,1],color='red')","plt.scatter(x[y==1,0],x[y==1,1],color='blue')","plt.scatter(x_standard[y==0,0],x_standard[y==0,1],color='red')","plt.scatter(x_standard[y==1,0],x_standard[y==1,1],color='blue')","random_state=none,","s)","scikit","sklearn","sklearn.preprocess","sklearn.svm","standardscal","standardscaler()","standardscaler.fit(x)","standardscaler.transform(x)","support","svc","svc.coef_","svc.fit(x_standard,","svc.intercept_","svc2","svc2.fit(x_standard,","svc3","svc3.fit(x_standard,","svm","svm，取值越小，相当于他的容错空间更大一些","tol=0.0001,","up_i","up_index","us","userwarning:","vector","verbose=0)","w","w0/w1","w1","w[0]/w[1]","w[1]","x","x0","x0,x1","x1","x[i","x_new","x_standard","y","y)","y_predict","y_predict.reshape(x0.shape)","zz","使用linspace将x轴，y轴划分成无数的小点","使用svm之前，和knn一样，也是要做数据标准化处理的，因为svm是涉及到距离的","使用支持向量机思想解决分类问题","决策边界上面的直线方程：x1","决策边界下面的直线方程：x1","决策边界的直线方程：w0","天然的支持多分类的问题，如果是多分类问题，在特征平面内就会有多条直线，","当c非常大，更趋近于hard","数据标准化","绘制上下两条直线","绘制决策边界","落在两根直线上的这些点即为支撑向量，这两条直线中间的距离即为margin。","过滤超出绘制范围的数据","这里是二分类，所以只有一跟直线，放在了二维数组的第一个元素中"],"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":["\"\"\"","#","'linspace'","(\"kernelsvc\",","(\"linearsvc\",","(\"poly\",","(\"std_scaler\",","('linearsvc',","('std_scaler',","(100,","(100,)",")","**kwds)","/anaconda3/lib/python3.6/importlib/_bootstrap.py:219:","/anaconda3/lib/python3.6/sit","0],x[y==0,","0],x[y==1,","1,1)","1,1),","1.0,","1.5,","1.5])","11.5","1])","2)","2.5,","88","96,","=","])","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis=[","axis[0])*100)).reshape(","axis[2])*100)).reshape(","binari","c=0.1):","changed,","class_weight=none,","contour:","custom_cmap","dataset","datasets.make_moons()","datasets.make_moons(noise=0.15,","def","dual=true,","expect","f(*args,","fit_intercept=true,","follow","import","include_bias=true,","incompatibility.","indic","interaction_only=false)),","intercept_scaling=1,","kwarg","linearsvc","linearsvc(c=0.1,","linearsvc(c=c))","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","loss='squared_hinge',","matplotlib.color","matplotlib.pyplot","max_iter=1000,","model.predict(x_new)","model：模型","multi_class='ovr',","np","np.c_[x0.ravel(),x1.ravel()]","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.meshgrid(","numpi","numpy.dtyp","packages/matplotlib/contour.py:967:","penalty='l2',","pipelin","pipeline([","pipeline(memory=none,","plot_decision_boundary(model,axis):","plot_decision_boundary(poly_kernel_svc,","plot_decision_boundary(poly_svc,","plt","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.scatter(x[y==0,","plt.scatter(x[y==1,","polnomialsvc(degree,","polnomialsvc(degree=3)","poly_kernel_svc","poly_kernel_svc.fit(x,y)","poly_svc","poly_svc.fit(x,","polynomialfeatures(degree=3,","polynomialfeatures(degree=degree)),","polynomialfeatures,","polynomialkernelsvc(3)","polynomialkernelsvc(degree,c=1.0):","random_state=666)","random_state=none,","return","runtimewarning:","s)","size","sklearn","sklearn.pipelin","sklearn.preprocess","sklearn.svm","standardscal","standardscaler()),","standardscaler(copy=true,","steps=[('poly',","svc","svc(kernel=\"poly\",degree=degree,c=c))","svm中使用多项式特征和核函数","tol=0.0001,","us","userwarning:","verbose=0))])","with_mean=true,","with_std=true)),","x,","x.shape","x0,x1","x_new","y","y)","y.shape","y_predict","y_predict.reshape(x0.shape)","zz","为数据集添加一些噪音","使用linspace将x轴，y轴划分成无数的小点","使用多项式核函数的svm","使用多项式特征的svm","在svm中，可以不使用现使用polynomialfeatures再扔给linearsvc这种方式进行多项式回归","在生成规则数据的基础上，增加标准差","生成不真实的非线性的数据集","绘制决策边界"],"支撑向量机SVM/11.6 到底什么是核函数.html":["1.什么是核函数","11.6","2.以多项式核函数为例，看核函数时怎么运作的","function），也叫核函数技巧","到底什么是核函数","在多项式求解的时候需要对xi添加多项式项形成x`i,对xj添加多项式项形成x`j。然后再将二者进行点乘。","在我们之前推导的最优化函数基础上，需要进行进一步的变形","在进行这一项变化之后，我们可以发现，在第二项中其中有一部分是xixj,也就是对于任意两个样本点的x值点乘。","如果能找到一个函数k，将xi和xj分别作为参数传入，返回的就是x`i和x`j点乘的结果，那么我们就不需要分别运算再相乘。并且也节省了空间（因为我们不需要将低维的数据变形为高维的数据进行存储，可以利用k函数直接计算出结果）。这个k就是核函数（kernel","对于d次方的多项式","核函数并不是svm特有的技巧，只要我们的算法转换成了一个最优化问题，并且在求解这个最优化问题的过程中，存在xi点乘xj这样的式子或者类似这样的式子，我们都可以使用这种技巧","首先讨论二次方的多项式项核函数"],"支撑向量机SVM/11.7 RBF核函数.html":["&","(x","*","0,","0]","0])","1","1,","1.0","1.高斯核函数","11.7","1]","2)","2))","2,","2.用一个简单的例子模拟高斯函数到底在座什么事情","3,","3.使用程序直观理解高斯函数","4,","4,5,1)","4])","=","[0]*len(x[y==0]))","[0]*len(x[y==1]))","array([","array([0,","basi","data","def","enumerate(x):","function","gamma","gaussian(data,l1)","gaussian(data,l2)","gaussian(x,","i,","import","kernel）","l)**2)","l):","l1,","l2","matplotlib.pyplot","np","np.arange(","np.array((x>=","np.empty((len(x),","np.exp(","numpi","plt","plt.scatter(x[y==0],","plt.scatter(x[y==1],","plt.scatter(x_new[y==0,0],x_new[y==0,1])","plt.scatter(x_new[y==1,0],x_new[y==1,1])","rbf核函数","return","x","x_new","x_new[i,","y","事实上正是这样。只不过高斯函数表达出的这种数据的映射是非常复杂的。","他讲原本mn","但是如果我们添加上多项式特征的话，相当于我们在座的事情就是升维。让我们的数据点不但有横轴的值，还有第二个维度的值，也就是x2.一点我们这样做了，我们原来的数据点就变成了线性可分的。这就是升维的意义","使用多项式特征为什么能处理非线性的数据问题：他的基本原理是依靠升维使得原本线性不可分的数据线性可分。","对于这样一个一维的数据，他是线性不可分的，我们很难画一跟直线，将这些样本点区分开。","对于高斯函数时将样本点映射成了无穷维空间的理解。如果样本点可以有无穷多个，那么就是将每一个样本点映射到了无穷维的空间","对于高斯函数，每一个数据点都是landmark，也就是对于每一个x，他都要尝试对于每一个样本y，进行核函数的计算，成为新的高维数据相对于的某一维的元素。","尽管如此，还是有很多的场景是比较适合使用高斯函数的，比如样本的特征非常多，但是样本点的数量可能并不多，也就是m","当我们使用高斯核函数的时候，其实这个计算开销是非常大的，也正是因为这样，在使用高斯核函数进行训练的时候，训练时间会比较长。","我们之前说，对于多项式核函数来说，他的本质就是对于没一个数据点添加了多项式项，在将这些多项式的新的数据特征进行点乘就形成了新的多项式核函数。","的数据映射成了mm的数据，如果m非常的大，那么经过高斯核函数后，就映射成了一个非常非常高维空间的数据点。","这样我们就将高斯函数从一个一维的数据映射成了二维的数据，这里，很显然我们可以通过一根直线来区分两种类别，原来在一维空间中线性不可分的空间，在二维空间中变的线性可分了","首先对高斯核函数进行一下改变，我们把y的值不取样本点，而取固定的的点，取两个固定的点分别叫l1，l2（landmark）。高斯核函数做的升维过程，就是对于每一个x的值，如果他有两个地标的话，就把他们升维成一个二维的样本点。","高斯函数是将每一个样本点映射到了无穷维的特征空间，这背后的变形是非常复杂的，但是变形之后再进行点乘的结果却是非常简单的。这再次显示了核函数这个工具的威力，他不需要我们具体的计算出来这个样本点怎么映射成新的样本点，我们只需要关注最终的点乘运算结果就可以了","高斯函数的本质也是这样的。","高斯核函数也叫rbf核（radial","高斯核函数本质也应该是将原来的数据点映射成了新的特征向量，然后是这种新的特征向量点成的结果。"],"支撑向量机SVM/11.8 RBF核函数中的gamma.html":["\"\"\"","#","'linspace'","(\"std_scaler\",standardscaler()),","(\"svc\",svc(kernel=\"rbf\",","('svc',",")","/anaconda3/lib/python3.6/sit","1,","1,1)","1,1),","1.0,1.5])","1.5,2.5,","11.8","=","])","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis[0])*100)).reshape(","axis[2])*100)).reshape(","cache_size=200,","class_weight=none,","coef0=0.0,","contour:","custom_cmap","dataset","datasets.make_moons(noise=0.15,","decision_function_shape='ovr',","def","degree=3,","follow","gamma=1.0,","gamma=100,","gamma=gamma))","gamma越低，模型复杂度越低，欠拟合程度越低","gamma越高，模型复杂度越高，过拟合程度越高","import","kernel='rbf',","kwarg","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","matplotlib.color","matplotlib.pyplot","max_iter=","model.predict(x_new)","model：模型","np","np.c_[x0.ravel(),x1.ravel()]","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.meshgrid(","numpi","packages/matplotlib/contour.py:967:","pipelin","pipeline([","pipeline(memory=none,","plot_decision_boundary(model,axis):","plot_decision_boundary(svc,axis=[","plot_decision_boundary(svc_gamma01,axis=[","plot_decision_boundary(svc_gamma05,axis=[","plot_decision_boundary(svc_gamma10,axis=[","plot_decision_boundary(svc_gamma100,axis=[","plt","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.scatter(x[y==0,0],x[y==0,1])","plt.scatter(x[y==1,0],x[y==1,1])","probability=false,","random_state=666)","random_state=none,","rbfkernalsvc()","rbfkernalsvc(gamma=0.1)","rbfkernalsvc(gamma=0.5)","rbfkernalsvc(gamma=1.0):","rbfkernalsvc(gamma=10)","rbfkernalsvc(gamma=100)","rbf核函数中的gamma","return","s)","shrinking=true,","sklearn","sklearn.pipelin","sklearn.preprocess","sklearn.svm","standardscal","standardscaler(copy=true,","steps=[('std_scaler',","svc","svc(c=1.0,","svc.fit(x,y)","svc_gamma01","svc_gamma01.fit(x,","svc_gamma05","svc_gamma05.fit(x,","svc_gamma10","svc_gamma10.fit(x,","svc_gamma100","svc_gamma100.fit(x,","tol=0.001,","us","userwarning:","verbose=false))])","with_mean=true,","with_std=true)),","x,i","x0,x1","x_new","y)","y_predict","y_predict.reshape(x0.shape)","zz","γ越大，正态分布对应的中型图案越窄。在这里rbf的kernal对应的γ变大了以后，这个决策边界针对其中的某一类，对于这一类他的每一个样本点的周围都形成了一个中型的图案，我们可以把这个图看成我们是在俯视这个中型图案，每个蓝色的点就是图案的尖。由于我们的γ值比较大，所以中型图案比较窄。在每一个蓝色的点周围都围绕了一定的区域，只有在这个区域内，我们才判断成蓝色的点，否则判断为红色的点。这也是高斯核函数的几何意义。","使用linspace将x轴，y轴划分成无数的小点","在我们调小gamma以后，可以想象成每一个蓝色点周围的中型图案变宽了一些，由于这些蓝色的点离的比较近，所以图案连在了一起","当gamma越来越小，决策边界越来越像一条直线，开始变的欠拟合。","当我们使用svm算法，我们的kernal选用高斯kernal，我们的gamma值相当于在调整模型的复杂度。","当然这显然是过拟合了","这时候回过头来看gamma=1的时候，其实就是蓝色的点周围的中型图案变的更宽了"],"支撑向量机SVM/11.9 SVM思想解决回归问题.html":["#","(\"linear_svr\",","(\"std_scaler\",","('linear_svr',","0.1):","0.6360330630824074","11.9","666)","=","])","boston","boston.data","boston.target","dataset","datasets.load_boston()","def","dual=true,","epsilon))","epsilon=0.1,","fit_intercept=true,","import","intercept_scaling=1.0,","linearsvr","linearsvr(c=1.0,","linearsvr(epsilon=","loss='epsilon_insensitive',","matplotlib.pyplot","max_iter=1000,","np","numpi","pipelin","pipeline([","pipeline(memory=none,","plt","random_state=none,","regress","return","sklearn","sklearn.model_select","sklearn.pipelin","sklearn.preprocess","sklearn.svm","standardlinearsvr()","standardlinearsvr(epsilon","standardscal","standardscaler()),","standardscaler(copy=true,","steps=[('std_scaler',","support","svm思想解决回归问题","svr","svr.fit(x_train,","svr.score(x_test,","tol=0.0001,","train_test_split","train_test_split(x,y,random_st","vector","verbose=0))])","with_mean=true,","with_std=true)),","x","x_test,","x_train,","y","y_test","y_test)","y_train)","y_train,","回归算法的本质就是找到一根直线或者一个曲线能够最大程度上的拟合我们的数据点，如何定义拟合，就是不同的回归算法的关键。比如线性回归的算法定义拟合的方式就是让数据点到直线的mse的值最小。","在具体训练svm回归问题的结果的时候，我们对margin是有一个指定的，所以在这里引入了一个超参数ɛ，代表margin范围的两根直线的任意一跟到中间直线的距离。","在这种情况下，svm干的事，和解决分类算法是相反的过程。我们期望的是在margin范围里，包围的点越多越好。","对于svm的算法来说。对于拟合的定义是，我们要定义一个mergin值，在margin范围内，我们能够包含的样本点最多。也就代表我们这个范围能够比较好的来表达我们的样本数据点，在这种情况下，我们取中间的直线作为我们真正的回归的结果，用他来预测其他点，位置点的值。","这里分数不是很高。但是使用svr有很多超参数可以调节，比如c。如果我们使用svr，还可以对不同的kernal还有不同的参数调节。对于高斯kernal来说就需要对γ进行调节，对于polynomial来说有degree和c进行调节。"],"11jue-ce-shu.html":["11.决策树"],"121-shi-yao-shi-jue-ce-shu.html":["\"\"\"","#","'linspace'",")","/anaconda3/lib/python3.6/sit","0,","1,1)","1,1),","12.1","3])","7.5,","=","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis=[0.5,","axis[0])*100)).reshape(","axis[2])*100)).reshape(","contour:","criterion=\"entropy\")","criterion='entropy',","custom_cmap","dataset","datasets.load_iris()","decisiontreeclassifi","decisiontreeclassifier(class_weight=none,","decisiontreeclassifier(max_depth=2,","def","dt_cfl","dt_cfl.fit(x,","follow","import","iri","iris.data[:,2:]","iris.target","kwarg","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","matplotlib.color","matplotlib.pyplot","max_depth=2,","max_features=none,","max_leaf_nodes=none,","min_impurity_decrease=0.0,","min_impurity_split=none,","min_samples_leaf=1,","min_samples_split=2,","min_weight_fraction_leaf=0.0,","model.predict(x_new)","model：模型","np","np.c_[x0.ravel(),x1.ravel()]","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.meshgrid(","numpi","packages/matplotlib/contour.py:967:","plot_decision_boundary(dt_cfl,","plot_decision_boundary(model,axis):","plt","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.scatter(x[y==0,0],x[y==0,1])","plt.scatter(x[y==1,0],x[y==1,1])","plt.scatter(x[y==2,0],x[y==2,1])","presort=false,","random_state=none,","s)","sklearn","sklearn.tre","splitter='best')","us","userwarning:","x","x0,x1","x_new","y","y)","y_predict","y_predict.reshape(x0.shape)","zz","什么是决策树","使用linspace将x轴，y轴划分成无数的小点","使用sklearn中的决策树直观的感受","决策树可以解决分类问题","决策树是一个非参数学习的算法","可以解决回归问题（将最终预测数据点落在叶子节点所有数据点的平均值作为预测值）","在每一个结点上，他首先选择一个维度以及这个维度上的一个阈值，分成两支，循环往复，来进行分类","天然的可以解决多分类问题","对于这样的决策树来说，他具有数据结构利用树的所有性质。包括根结点，叶子结点，深度。在这里我们称决策树的深度为3","引入决策树","总结","构建决策树的问题","某个维度在哪个值上做划分","每个结点在哪个维度做划分","这就是决策树在面对属性是数值特征的时候是怎么处理的。","这样的一个招聘过程形成了一个树结构，这颗树的所有叶子结点其实就是我们最终的决策。也相对于是对与输入（录用者信息）的分类（录用/考察）。这样的一个过程，就是决策树。","非常好的可解释性"],"122-xin-xi-shang.html":["(1","*","0.99,","12.2","200)","=","def","entropy(p):","entropy(x))","import","matplotlib.pyplot","np","np.linspace(0.01,","np.log(1","np.log(p)","numpi","p","p)","plt","plt.scatter(x,","return","x","了解了信息熵的概念，解决上面的两个问题就好说了。在每一个结点上，都希望在某一个维度上基于某一个阈值进行划分，在划分以后要做的事情就是要让我们的数据划分成两部分之后，相应我们的系统整体的信息熵降低（也就是让我们的系统变的更加确定）","二分类的信息熵函数","信息熵","当x=0.5","当系统每一个类别都是等概率的时候，其实是他最不确定的时候，此时他的信息熵是最高的。如果系统偏向于某一类，相当于有了约定向，信息熵逐渐降低，知道有一个类别占到了百分之百，此时信息熵达到最低值0","接下来的任务，就是找每个节点上游一个维度，在这个维度上有一个取值，根据这个取值进行划分，划分后是所有其他划分方式的信息熵中的最小值。我们就成当前的划分方式就是一个最好的划分。找到这个划分的方法就是对所有的可能性进行搜索。","熵在信息论中代表随机变量中不确定度的度量。","熵越大，数据的不确定性越高","熵越小，数据的不确定性越低","的时候，信息熵达到了最大值（当两类样本各占一半的时候），也就是说这个时候的样本是最不稳定的"],"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":["!=","\"\"\"","#","'linspace'","(x[:,d]","(x[sorted_index[i",")","+","+=",",best_d,","/","/anaconda3/lib/python3.6/sit","0","0,","0.0","0.10473243910508653","0.30849545083110386","0.4132278899361904","0.6931471805599453","1","1,","1,1)","1,1),","1.75","12.3","1],d]","2","2.45","3])","7.5,","=","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis=[0.5,","axis[0])*100)).reshape(","axis[2])*100)).reshape(","best_d,","best_d2,","best_entropi","best_entropy,","best_entropy2,","best_v","best_v)","best_v2","collect","contour:","counter","counter(y)","counter.values():","criterion=\"entropy\")","criterion='entropy',","custom_cmap","d","d,","dataset","datasets.load_iris()","decisiontreeclassifi","decisiontreeclassifier(class_weight=none,","decisiontreeclassifier(max_depth=2,","def","dt_cfl","dt_cfl.fit(x,","e","entropy(y):","entropy(y1_l)","entropy(y1_r)","entropy(y2_l)","entropy(y2_r)","entropy(y_l)","entropy(y_r)","float('inf')","follow","import","index_a","iri","iris.data[:,2:]","iris.target","kwarg","len(y)","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","log","math","matplotlib.color","matplotlib.pyplot","max_depth=2,","max_features=none,","max_leaf_nodes=none,","min_impurity_decrease=0.0,","min_impurity_split=none,","min_samples_leaf=1,","min_samples_split=2,","min_weight_fraction_leaf=0.0,","model.predict(x_new)","model：模型","np","np.argsort(x[:,d])","np.c_[x0.ravel(),x1.ravel()]","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.meshgrid(","num","numpi","p","p*log(p)","packages/matplotlib/contour.py:967:","plot_decision_boundary(dt_cfl,","plot_decision_boundary(model,axis):","plt","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.scatter(x[y==0,0],x[y==0,1])","plt.scatter(x[y==1,0],x[y==1,1])","plt.scatter(x[y==2,0],x[y==2,1])","presort=false,","print(best_d)","print(best_d2)","print(best_entropy)","print(best_entropy2)","print(best_v)","print(best_v2)","random_state=none,","range(1,len(x)):","range(x.shape[1]):","re","return","s)","sklearn","sklearn.tre","sorted_index","spilt(x,","spilt(x1_r,y1_r,best_d2,best_v2)","splitter='best')","try_spilt(x,","try_spilt(x1_r,","us","userwarning:","v","v)","value)","value):","x","x0,x1","x1_l,","x1_r,","x2_l,","x2_r,","x[index_a],","x[index_b],","x[sorted_index[i","x[sorted_index[i],d])","x[sorted_index[i],d]:","x_l,","x_new","x_r,","y","y)","y):","y,","y1_l,","y1_r","y1_r)","y2_l,","y2_r","y[index_a],","y[index_b]","y_l,","y_predict","y_predict.reshape(x0.shape)","y_r","zz","使用linspace将x轴，y轴划分成无数的小点","使用信息熵寻找最优划分","分别计算两部分的信息熵然后相加","在第一个维度0.75的位置进行划分，划分的结果的信息熵为0.41左右","对比sklearn的划分结果，就是在横轴上（第0个维度），大约2.45的位置进行了划分","将两个数据点的平均值作为切分点","引入决策树","模拟使用信息熵进行划分","计算每两个值之间的信息熵，所以从1开始","遍历每一个维度"],"124-ji-ni-xi-shu.html":["!=","\"\"\"","#","#在每一个特征维度上排序","'linspace'","(x[:,d]","(x[sorted_index[i",")","+",",best_d,","/","/anaconda3/lib/python3.6/sit","0","0,","0.0","0.04253308128544431","0.1680384087791495","0.2105714900645938","0.5","1","1,","1,1)","1,1),","1.0","1.75","12.4","1],d]","2","2.45","3])","7.5,","=","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis=[0.5,","axis[0])*100)).reshape(","axis[2])*100)).reshape(","best_d,","best_d2,","best_g","best_g,","best_g2,","best_v","best_v)","best_v2","collect","contour:","counter","counter(y)","counter.values():","criterion=\"gini\")","criterion='gini',","custom_cmap","d","d,","dataset","datasets.load_iris()","decisiontreeclassifi","decisiontreeclassifier(class_weight=none,","decisiontreeclassifier(max_depth=2,","def","dt_cfl","dt_cfl.fit(x,","e","float('inf')","follow","gini(y):","gini(y1_l)","gini(y1_r)","gini(y2_l)","gini(y2_r)","gini(y_l)","gini(y_r)","import","index_a","iri","iris.data[:,2:]","iris.target","kwarg","learn中默认使用基尼系数","len(y)","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","log","math","matplotlib.color","matplotlib.pyplot","max_depth=2,","max_features=none,","max_leaf_nodes=none,","min_impurity_decrease=0.0,","min_impurity_split=none,","min_samples_leaf=1,","min_samples_split=2,","min_weight_fraction_leaf=0.0,","model.predict(x_new)","model：模型","np","np.argsort(x[:,d])","np.c_[x0.ravel(),x1.ravel()]","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.meshgrid(","num","numpi","p","p**2","packages/matplotlib/contour.py:967:","plot_decision_boundary(dt_cfl,","plot_decision_boundary(model,axis):","plt","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.scatter(x[y==0,0],x[y==0,1])","plt.scatter(x[y==1,0],x[y==1,1])","plt.scatter(x[y==2,0],x[y==2,1])","presort=false,","print(best_d)","print(best_d2)","print(best_g)","print(best_g2)","print(best_v)","print(best_v2)","random_state=none,","range(1,len(x)):","range(x.shape[1]):","re","return","s)","scikit","sklearn","sklearn.tre","sorted_index","spilt(x,","spilt(x1_r,y1_r,best_d2,best_v2)","splitter='best')","try_spilt(x,","try_spilt(x1_r,","us","userwarning:","v","v)","value)","value):","x","x0,x1","x1_l,","x1_r,","x2_l,","x2_r,","x[index_a],","x[index_b],","x[sorted_index[i","x[sorted_index[i],d])","x[sorted_index[i],d]:","x_l,","x_new","x_r,","y","y)","y):","y,","y1_l,","y1_r","y1_r)","y2_l,","y2_r","y[index_a],","y[index_b]","y_l,","y_predict","y_predict.reshape(x0.shape)","y_r","zz","二分类基尼系数函数","使用linspace将x轴，y轴划分成无数的小点","分别计算两部分的基尼系数然后相加","只需要把信息熵的函数改成基尼系数","基尼系数","基尼系数的意义和信息熵相同","大多数时候二者没有特别的效果优劣","将两个数据点的平均值作为切分点","引入决策树","总结","模拟使用基尼系数进行划分","熵信息的计算比基尼系数稍慢","计算每两个值之间的信息熵，所以从1开始","遍历每一个维度"],"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":["\"\"\"","#","'linspace'",")","/anaconda3/lib/python3.6/sit","1,1)","1,1),","1.0,","1.5,","1.5])","12.5","2.5,","=","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis=[","axis[0])*100)).reshape(","axis[2])*100)).reshape(","classif","contour:","crat","crat：","criterion='gini',","custom_cmap","dataset","datasets.make_moons(noise=0.25,","decisiontreeclassifi","decisiontreeclassifier()","decisiontreeclassifier(class_weight=none,","decisiontreeclassifier(max_depth=2)","decisiontreeclassifier(max_leaf_nodes=4)","decisiontreeclassifier(min_samples_leaf=6)","decisiontreeclassifier(min_samples_split=10)","def","dt_clf","dt_clf.fit(x,","dt_clf2","dt_clf2.fit(x,","dt_clf3","dt_clf3.fit(x,","dt_clf4","dt_clf4.fit(x,","dt_clf5","dt_clf5.fit(x,","follow","id3，c4.5，c5.0等是使用其他的方式实现决策树","import","kwarg","learn的决策实现：crat","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","matplotlib.color","matplotlib.pyplot","max_depth=none,","max_features=none,","max_leaf_nodes=none,","min_impurity_decrease=0.0,","min_impurity_split=none,","min_samples_leaf=1,","min_samples_split=2,","min_weight_fraction_leaf=0.0,","model.predict(x_new)","model：模型","np","np.c_[x0.ravel(),x1.ravel()]","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.meshgrid(","numpi","packages/matplotlib/contour.py:967:","plot_decision_boundary(dt_clf,","plot_decision_boundary(dt_clf2,","plot_decision_boundary(dt_clf3,","plot_decision_boundary(dt_clf4,","plot_decision_boundary(dt_clf5,","plot_decision_boundary(model,axis):","plt","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.scatter(x[y==0,0],x[y==0,1])","plt.scatter(x[y==1,0],x[y==1,1])","presort=false,","random_state=666)","random_state=none,","regress","s)","scikit","sklearn","sklearn.tre","splitter='best')","tree","us","userwarning:","x,","x0,x1","x_new","y","y)","y_predict","y_predict.reshape(x0.shape)","zz","不传max_depth会一直划分直到基尼系数为0为止","与决策树中的超参数","使用linspace将x轴，y轴划分成无数的小点","决策树最多的叶子节点","决策边界非常不规则，产生了过拟合","对于一个结点来说，至少要有多少个样本数据，我们才对他继续进行拆分下去","对于叶子节点来说，至少要有几个样本","很显然，现在这个样子相比上面的形状不在有过拟合，有了非常清晰的边界（不会针对某几个特别的样本点进行特殊的变化）","根据某一个维度d和某一个阈值v进行二分","这种情况下，很有可能存在欠拟合，所以我们要对这些参数进行比较精细的调整，让他既不过拟合也不欠拟合","默认使用基尼系数划分数据"],"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":["#","0.5782292347434448","1.0","12.6","666)","=","boston","boston.data","boston.target","dataset","datasets.load_boston()","decis","decisiontreeregressor","decisiontreeregressor()","decisiontreeregressor(criterion='mse',","dt_reg","dt_reg.fit(x_train,","dt_reg.score(x_test,","dt_reg.score(x_train,","import","learn封装的决策树解决回归问题","matplotlib.pyplot","max_depth=none,","max_features=none,","max_leaf_nodes=none,","min_impurity_decrease=0.0,","min_impurity_split=none,","min_samples_leaf=1,","min_samples_split=2,","min_weight_fraction_leaf=0.0,","np","numpi","plt","presort=false,","random_state=none,","regress","scikit","sklearn","sklearn.model_select","sklearn.tre","splitter='best')","train_test_split","train_test_split(x,y,random_st","tree","x","x_test,","x_train,","y","y_test","y_test)","y_train)","y_train,","决策树解决回归问题","在决策树建立之后，每个叶子结点都包含了一些数据。","在训练数据集上预测的准确率是百分之百的","如果我们的数据输出的是数据的话，那就是回归问题所解决的问题，一个预测的数据来临之后，通过决策树到达了某一个叶子节点，我们就可以将这些叶子节点包含数据的平均值作为我们的预测值","如果我们的数据输出的是类别的话，我们就让这些叶子结点包含的数据进行投票，票数最多的即为我们输出的分类","过拟合"],"127-jue-ce-shu-de-ju-xian-xing.html":["12.7","决策树的局限性","决策边界必须是横平竖直的","对数据敏感","案例"],"13ji-cheng-xue-xi-he-sui-ji-sen-lin.html":["13.集成学习和随机森林"],"131-shi-yao-shi-ji-cheng-xue-xi.html":["#","(\"dt_clf\",","(\"log_clf\",","(\"svm_clf\",","/anaconda3/lib/python3.6/sit","0,","0.824","0.84","0.88","0.896","0],x[y==0,","0],x[y==1,","0`","1,","13.1","1])","2,","=",">",">=","],","`array.s","accuracy_scor","accuracy_score(y_test,","ambiguous.","array","array([1,","check","classifi","dataset","datasets.make_moons(n_samples=500,","decisiontreeclassifi","decisiontreeclassifier()","decisiontreeclassifier())","deprecationwarning:","diff:","dt_clf","dt_clf.fit(x_train,","dt_clf.predict(x_test)","dt_clf.score(x_test,","dtype='int')","empti","empty.","error.","false,","futur","hard","import","log_clf","log_clf.fit(x_train,","log_clf.predict(x_test)","log_clf.score(x_test,","logisticregress","logisticregression()","logisticregression()),","matplotlib.pyplot","noise=0.3,","np","np.array((y_predict1+y_predict2+y_predict3)","numpi","packages/sklearn/preprocessing/label.py:151:","plt","plt.scatter(x[y==0,","plt.scatter(x[y==1,","random_state=42)","random_state=666)","result","return","sklearn","sklearn.ensembl","sklearn.linear_model","sklearn.metr","sklearn.model_select","sklearn.svm","sklearn.tre","svc","svc()","svc()),","svm_clf","svm_clf.fit(x_train,","svm_clf.predict(x_test)","svm_clf.score(x_test,","train_test_split","train_test_split(x,","truth","us","valu","voting=\"hard\")","voting_clf","voting_clf.fit(x_train,","voting_clf.score(x_test,","votingclassifi","votingclassifier(estimators=[","x,","x_test,","x_train,","y","y,","y_predict","y_predict)","y_predict1","y_predict2","y_predict3","y_predict[:10]","y_test","y_test)","y_train)","y_train,","什么是集成学习","使用vote","只有至少两个算法预测结果为1，三个相加才会大于2","少数服从多数","手动完成集成学习的过程"],"132-softvoting-classifier.html":["(\"dt_clf\",","(\"log_clf\",","(\"svm_clf\",","0.921","13.2","=",">","],","boolean,","classifi","decisiontreeclassifier(random_state=666))","hard","http://scikit","knn可以估算规律，结果占离他最近的k个点","learn.org/stable/modules/generated/sklearn.svm.svc.html","logisticregression()),","optional(default=false)","soft","softvot","svc(probability=true)),","svm算法：probability:","true","vote","voting=\"soft\")","voting_clf2","voting_clf2.fit(x_train,","voting_clf2.score(x_test,","votingclassifier(estimators=[","y_test)","y_train)","决策树，叶子结点中占比例最大的类别数据占整个叶子结点量的比值","更合理的投票，应该有权值","要求集合的每一个模型都能估算概率","逻辑回归可以估算概率"],"133-bagging-and-pasting.html":["#","0.904","0.92","13.3","13.4","=","bag","bagging_clf","bagging_clf.fit(x_train,","bagging_clf.score(x_test,","bagging_clf2","bagging_clf2.fit(x_train,","bagging_clf2.score(x_test,","baggingclassifi","baggingclassifier(decisiontreeclassifier(),","bagging更常用","bootstrap","bootstrap=true)","decisiontreeclassifi","import","max_samples=100,","max_samples每个子模型看几个样本数据","n_estim","n_estimators=500,","n_estimators=5000,","past","sklearn.ensembl","sklearn.tre","y_test)","y_train)","但是从投票的角度看，还是不够多","例如:","共有500个样本数据;每个子模型只看100个样本数据，每个子模型不需要太高的准确率","创建更多的子模型！集成更多的子模型的意见","取样：放回取样（bagging），不放回取样（pasting）","只看样本的一部分","如何创建差异性？","如果每个子模型只有51%的准确率","如果每个子模型有60%的准确率","子模型之间不能一直！模型之间要有差异","是否放回取样","统计学中，放回取样：bootstrap","虽然有很多机器学习方法","集成几个模型"],"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":["#","%%time","(out","0.82","0.864","0.92","1)","1,","13.4","166","220","4.64","480","54","575","578","580","=","bag)","bag.","bagging_clf","bagging_clf.fit(x,","bagging_clf.oob_score_","baggingclassifier(decisiontreeclassifier(),","bagging的思路极易并行化处理","bootstrap=true,","bootstrap_featur","bootstrap_features=true)","cpu","learn中对于并行处理的算法，可以传入n_jobs来调整使用几个核来处理","learn：oob_score_","max_featur","max_features=1,","max_samples=100,","max_samples=500,","ms","ms,","n_estimators=500,","n_jobs=","oob","oob_score=true)","oob_score=true,","patch","random","random_patches_clf","random_patches_clf.fit(x,","random_patches_clf.oob_score_","random_subspaces_clf","random_subspaces_clf.fit(x,","random_subspaces_clf.oob_score_","sikit","subspac","sys:","time:","times:","total:","user","wall","y)","不使用测试数据集，而使用这部分没有取到的样本做测试/验证。","使模型产生差异化","使用n_job","使用oob","使用决策树方式进行集成学习的方式也叫随机森林","和关于bagging的更多讨论","对特征进行随机取样的方式","对特征随机取样","平均大约有37%的样本没有取到。这些样本就叫out","放回取样导致一部分样本很有可能没有取到","既针对样本，又针对特征进行随机采样","针对特征进行随机采样"],"135-sui-ji-sen-lin.html":["#","0.892","0.896","0.912","1)","1,","13.5","=","bag","base","bootstrap=true,","class_weight=none,","classfif","criterion='gini',","decis","estimator:","et_clf","et_clf.fit(x,","et_clf.oob_score_","extra","extratreesclassifi","extratreesclassifier(bootstrap=true,","extratreesclassifier(n_estimators=500,","import","max_depth=none,","max_features='auto',","max_leaf_nodes=10,","max_leaf_nodes=none,","min_impurity_decrease=0.0,","min_impurity_split=none,","min_samples_leaf=1,","min_samples_split=2,","min_weight_fraction_leaf=0.0,","n_estimators=500,","n_jobs=","n_jobs=1,","oob_score=true,","random_state=666)","random_state=666,","randomforestclassifi","randomforestclassifier(bootstrap=true,","randomforestclassifier(n_estimators=500,","rf_clf","rf_clf.fit(x,","rf_clf.oob_score_","rf_clf2","rf_clf2.fit(x,","rf_clf2.oob_score_","sklearn.ensembl","tree","verbose=0,","warm_start=false)","y)","决策树在节点划分上，使用随机的特征和随机的阈值(理论基础：根据bag","决策树在节点划分上，在随机的特征子集上寻找最优划分特征","提供额外的随机性，抑制过拟合，但增大了bia","更快的训练速度","的原理，只要大多数的决策树的决策能力比扔硬币的能力好一点就够了)","随机森林","随机森林包含决策树的参数"],"136-ada-boosting-he-gradient-boosting.html":["+","0.864","0.912","13.6","=","ada","ada_clf","ada_clf.fit(x_train,","ada_clf.score(x_test,","adaboostclassifi","adaboostclassifier(algorithm='samme.r',","adaboostclassifier(base_estimator=decisiontreeclassifier(max_depth=2),","base_estimator=decisiontreeclassifier(class_weight=none,","boost","criterion='gini',","decisiontreeclassifi","grad_clf","grad_clf.fit(x_train,","grad_clf.score(x_test,","gradient","gradientboostingclassifi","gradientboostingclassifier(criterion='friedman_mse',","gradientboostingclassifier(max_depth=2,","import","init=none,","learning_rate=0.1,","learning_rate=1.0,","loss='deviance',","m2","m3","max_depth=2,","max_features=none,","max_leaf_nodes=none,","min_impurity_decrease=0.0,","min_impurity_split=none,","min_samples_leaf=1,","min_samples_split=2,","min_weight_fraction_leaf=0.0,","n_estimators=30,","n_estimators=500,","presort='auto',","presort=false,","random_state=666)","random_state=666,","random_state=none,","sklearn.ensembl","sklearn.tre","splitter='best'),","subsample=1.0,","verbose=0,","warm_start=false)","y_test)","y_train)","使用每次增强训练出来的子模型进行投票得出最终的结果","和","最终预测结果是:m1","每个模型都在尝试增强(boosting)整体的效果","训练一个模型m1,产生错误e1","針対e1訓繚第二个模型m2,产生錯俣e2","针对e2训练第三个模型m3,产生错误e3..","集成多个模型"],"137-stacking.html":["13.7","stack","层次继续增多，就越来越像是一个神经网络"]},"length":95},"tokenStore":{"root":{"0":{"1":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.00872093023255814},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669}}},"]":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}},"2":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.014534883720930232},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.05177993527508091}}}},"3":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.08737864077669903}}}},"4":{"docs":{},",":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.016181229773462782}}}},"5":{"docs":{},",":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.012944983818770227}}}},"6":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}},",":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669}}}},"7":{"docs":{},",":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114}}}},"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.016666666666666666},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.016},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},".":{"0":{"0":{"1":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}},"4":{"3":{"6":{"4":{"9":{"7":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"1":{"9":{"9":{"6":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"4":{"9":{"3":{"5":{"1":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"7":{"8":{"9":{"5":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"1":{"1":{"4":{"1":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"2":{"7":{"2":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"9":{"3":{"0":{"1":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"0":{"4":{"2":{"2":{"5":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"9":{"9":{"2":{"7":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"7":{"4":{"1":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"3":{"3":{"5":{"9":{"docs":{},"]":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"8":{"7":{"9":{"9":{"2":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"2":{"9":{"8":{"7":{"0":{"1":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"4":{"2":{"2":{"8":{"2":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"0":{"5":{"4":{"4":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"8":{"6":{"0":{"1":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"8":{"4":{"5":{"1":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"2":{"6":{"7":{"1":{"8":{"docs":{},"]":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}},",":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}},"2":{"0":{"1":{"3":{"4":{"2":{"3":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"0":{"2":{"0":{"2":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"8":{"1":{"6":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"7":{"9":{"0":{"2":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"1":{"2":{"6":{"7":{"6":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"9":{"7":{"8":{"1":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"0":{"5":{"3":{"4":{"3":{"5":{"docs":{},"]":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"9":{"6":{"5":{"0":{"3":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"4":{"9":{"6":{"3":{"5":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"7":{"0":{"3":{"3":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.008843036109064112}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"2":{"6":{"8":{"4":{"6":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"7":{"6":{"8":{"4":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0036845983787767134}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"1":{"7":{"0":{"9":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.005158437730287398}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"3":{"3":{"0":{"8":{"1":{"2":{"8":{"5":{"4":{"4":{"4":{"3":{"1":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"7":{"8":{"5":{"1":{"1":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"8":{"0":{"1":{"5":{"3":{"docs":{},"]":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"1":{"0":{"9":{"4":{"8":{"9":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"2":{"3":{"4":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"2":{"4":{"8":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"0":{"6":{"0":{"6":{"0":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"9":{"2":{"2":{"6":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"0":{"7":{"0":{"7":{"0":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"5":{"2":{"4":{"3":{"2":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"8":{"2":{"9":{"9":{"3":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"8":{"0":{"8":{"0":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"2":{"5":{"6":{"1":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"0":{"9":{"0":{"9":{"0":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"6":{"9":{"6":{"7":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},"1":{"0":{"1":{"0":{"1":{"0":{"1":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"1":{"1":{"8":{"1":{"1":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"3":{"2":{"4":{"3":{"9":{"1":{"0":{"5":{"0":{"8":{"6":{"5":{"3":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"2":{"4":{"6":{"4":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"8":{"6":{"4":{"5":{"7":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"1":{"2":{"1":{"2":{"1":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"7":{"3":{"5":{"4":{"6":{"9":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}},"]":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"5":{"6":{"6":{"8":{"1":{"7":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"1":{"5":{"1":{"5":{"1":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"5":{"5":{"5":{"6":{"2":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"1":{"6":{"1":{"6":{"1":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"9":{"6":{"9":{"5":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0036845983787767134}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"5":{"9":{"7":{"9":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0058953574060427415}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"3":{"8":{"4":{"0":{"8":{"7":{"7":{"9":{"1":{"4":{"9":{"5":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"3":{"4":{"5":{"0":{"3":{"8":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.009579955784819455}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"0":{"0":{"0":{"0":{"0":{"0":{"0":{"0":{"0":{"0":{"0":{"0":{"0":{"2":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"8":{"1":{"8":{"1":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"1":{"7":{"2":{"9":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.01915991156963891}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"8":{"3":{"2":{"6":{"7":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"1":{"9":{"1":{"9":{"1":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}},")":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}},":":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}},"2":{"0":{"2":{"0":{"2":{"0":{"2":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"6":{"8":{"3":{"2":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"0":{"5":{"7":{"1":{"4":{"9":{"0":{"0":{"6":{"4":{"5":{"9":{"3":{"8":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"1":{"2":{"1":{"2":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"1":{"6":{"9":{"2":{"5":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"2":{"7":{"0":{"2":{"4":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.005158437730287398}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"2":{"0":{"9":{"1":{"8":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"6":{"5":{"7":{"9":{"7":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.009579955784819455}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"2":{"7":{"2":{"7":{"2":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"4":{"1":{"5":{"6":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"5":{"2":{"7":{"7":{"2":{"8":{"6":{"6":{"5":{"7":{"2":{"7":{"4":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"8":{"2":{"8":{"2":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"0":{"2":{"5":{"0":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},"3":{"0":{"6":{"0":{"2":{"3":{"9":{"2":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0058953574060427415}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"3":{"1":{"9":{"9":{"9":{"2":{"9":{"7":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"4":{"9":{"5":{"4":{"5":{"0":{"8":{"3":{"1":{"1":{"0":{"3":{"8":{"6":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"3":{"1":{"3":{"1":{"3":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"6":{"5":{"5":{"4":{"5":{"0":{"5":{"0":{"3":{"0":{"8":{"0":{"7":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"3":{"2":{"3":{"2":{"3":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"1":{"1":{"0":{"3":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"3":{"3":{"3":{"3":{"3":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"1":{"3":{"8":{"6":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"3":{"4":{"3":{"4":{"3":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"3":{"5":{"3":{"5":{"3":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"3":{"7":{"3":{"7":{"3":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"1":{"8":{"6":{"1":{"7":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"9":{"3":{"9":{"3":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"9":{"9":{"9":{"9":{"9":{"9":{"9":{"9":{"9":{"9":{"9":{"9":{"4":{"7":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"2":{"2":{"6":{"0":{"9":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"4":{"0":{"4":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"2":{"4":{"5":{"2":{"1":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.005158437730287398}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"2":{"2":{"7":{"8":{"8":{"9":{"9":{"3":{"6":{"1":{"9":{"0":{"4":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"4":{"9":{"3":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"4":{"2":{"4":{"2":{"4":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"4":{"3":{"4":{"3":{"4":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"5":{"9":{"7":{"4":{"6":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.008843036109064112}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"7":{"6":{"8":{"7":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"4":{"4":{"4":{"4":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"5":{"8":{"4":{"7":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0058953574060427415}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"1":{"5":{"3":{"7":{"3":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"5":{"4":{"5":{"4":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"4":{"6":{"4":{"6":{"4":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"4":{"8":{"4":{"8":{"4":{"8":{"4":{"8":{"4":{"8":{"4":{"8":{"4":{"5":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"4":{"7":{"4":{"7":{"4":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"4":{"8":{"4":{"8":{"4":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"1":{"7":{"1":{"7":{"1":{"7":{"1":{"7":{"1":{"7":{"1":{"7":{"1":{"7":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"4":{"5":{"6":{"0":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"9":{"4":{"9":{"4":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"0":{"5":{"3":{"0":{"7":{"8":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"4":{"2":{"3":{"7":{"5":{"2":{"docs":{},"]":{"docs":{},")":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"2":{"5":{"2":{"5":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"3":{"5":{"3":{"5":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"7":{"9":{"2":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"6":{"2":{"3":{"7":{"4":{"5":{"5":{"7":{"7":{"3":{"6":{"9":{"9":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"1":{"9":{"7":{"9":{"1":{"2":{"5":{"0":{"8":{"8":{"2":{"5":{"3":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"4":{"5":{"4":{"5":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"5":{"5":{"5":{"5":{"5":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}},"]":{"docs":{},")":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"2":{"0":{"3":{"0":{"8":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0036845983787767134}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"6":{"5":{"6":{"5":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"1":{"1":{"7":{"1":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}},"docs":{}},"docs":{}},"docs":{}},"4":{"9":{"6":{"4":{"4":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.006632277081798084}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"7":{"5":{"7":{"5":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"2":{"2":{"9":{"2":{"3":{"4":{"7":{"4":{"3":{"4":{"4":{"4":{"8":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"5":{"8":{"5":{"8":{"5":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"5":{"9":{"5":{"9":{"6":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.024},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541}},"*":{"docs":{},"x":{"docs":{},"*":{"docs":{},"*":{"2":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"docs":{}}}}}},"6":{"0":{"2":{"6":{"7":{"4":{"5":{"0":{"5":{"0":{"8":{"0":{"9":{"5":{"3":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}},"6":{"0":{"3":{"2":{"7":{"9":{"9":{"1":{"7":{"3":{"5":{"7":{"4":{"1":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"0":{"6":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"7":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"8":{"7":{"5":{"3":{"8":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"6":{"2":{"6":{"2":{"6":{"3":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"6":{"0":{"3":{"3":{"0":{"6":{"3":{"0":{"8":{"2":{"4":{"0":{"7":{"4":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"6":{"3":{"6":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"9":{"2":{"5":{"6":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.010316875460574797}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"4":{"6":{"4":{"6":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"0":{"6":{"8":{"9":{"2":{"7":{"docs":{},",":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}},"docs":{}},"7":{"2":{"docs":{},",":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"1":{"6":{"docs":{},"]":{"docs":{},")":{"docs":{},",":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}},"7":{"docs":{},"]":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}},"docs":{}},"2":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"8":{"9":{"4":{"7":{"3":{"6":{"8":{"4":{"2":{"1":{"0":{"5":{"3":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"0":{"6":{"2":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"6":{"6":{"6":{"6":{"6":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"5":{"4":{"7":{"6":{"9":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0036845983787767134}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"7":{"6":{"7":{"6":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"6":{"8":{"6":{"8":{"6":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"1":{"4":{"7":{"1":{"8":{"0":{"5":{"5":{"9":{"9":{"4":{"5":{"3":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"9":{"6":{"9":{"7":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"3":{"9":{"1":{"5":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.005158437730287398}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"0":{"3":{"7":{"4":{"4":{"5":{"4":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"0":{"7":{"0":{"7":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"0":{"3":{"3":{"2":{"3":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.008843036109064112}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"5":{"1":{"9":{"8":{"6":{"2":{"5":{"2":{"7":{"0":{"9":{"9":{"4":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"5":{"2":{"6":{"5":{"2":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.008771929824561403},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"2":{"7":{"2":{"7":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"2":{"1":{"9":{"9":{"9":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"4":{"2":{"4":{"4":{"9":{"0":{"6":{"0":{"9":{"2":{"7":{"7":{"1":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"3":{"7":{"3":{"7":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"7":{"4":{"7":{"4":{"7":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"4":{"2":{"9":{"8":{"3":{"3":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"6":{"4":{"4":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"4":{"3":{"7":{"2":{"docs":{},"]":{"docs":{},")":{"docs":{},"]":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}},"docs":{}},"docs":{}},"4":{"1":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},"*":{"docs":{},"x":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},"+":{"3":{"docs":{},".":{"docs":{},"+":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},".":{"docs":{},"n":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"l":{"docs":{},"(":{"0":{"docs":{},".":{"docs":{},",":{"1":{"0":{"docs":{},".":{"docs":{},",":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"1":{"0":{"0":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}},"docs":{}},"docs":{}}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"docs":{}}}},"docs":{}}}}}},"倍":{"docs":{},"的":{"docs":{},"x":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},"加":{"docs":{},"上":{"3":{"docs":{},"加":{"docs":{},"上":{"docs":{},"一":{"docs":{},"个":{"docs":{},"噪":{"docs":{},"音":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}},"docs":{}}}}},"docs":{}}}}}}}},"6":{"7":{"6":{"7":{"6":{"7":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"7":{"7":{"7":{"7":{"7":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"2":{"7":{"3":{"3":{"3":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"9":{"2":{"3":{"0":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0058953574060427415}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"4":{"7":{"3":{"6":{"8":{"4":{"2":{"1":{"0":{"5":{"2":{"6":{"3":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"2":{"2":{"0":{"3":{"7":{"4":{"6":{"4":{"1":{"1":{"6":{"5":{"3":{"9":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"0":{"8":{"9":{"1":{"6":{"1":{"9":{"9":{"5":{"1":{"9":{"0":{"7":{"7":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"8":{"0":{"8":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"8":{"1":{"8":{"1":{"8":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"1":{"8":{"1":{"8":{"5":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0036845983787767134}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"7":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"2":{"8":{"2":{"8":{"3":{"docs":{},"]":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}},"3":{"6":{"3":{"1":{"8":{"0":{"8":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.008843036109064112}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"3":{"8":{"3":{"8":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"5":{"6":{"4":{"6":{"0":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"4":{"8":{"4":{"8":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"5":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"4":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"5":{"8":{"5":{"8":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"4":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}},"7":{"4":{"6":{"9":{"8":{"7":{"9":{"5":{"1":{"8":{"0":{"7":{"2":{"3":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.016}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"2":{"0":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.006632277081798084}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"6":{"8":{"6":{"8":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}},"7":{"8":{"7":{"8":{"7":{"8":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"2":{"8":{"0":{"2":{"6":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"1":{"3":{"7":{"9":{"4":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.013513513513513514}}},"9":{"2":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}},"6":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}},"8":{"9":{"8":{"9":{"9":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}}},"9":{"0":{"2":{"3":{"6":{"9":{"1":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}},"6":{"1":{"6":{"0":{"4":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.006632277081798084}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"2":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}},"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"4":{"6":{"3":{"6":{"8":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"1":{"9":{"1":{"9":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"1":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}},"3":{"4":{"8":{"4":{"4":{"3":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"3":{"2":{"4":{"3":{"2":{"4":{"3":{"2":{"4":{"3":{"2":{"4":{"2":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"2":{"9":{"2":{"9":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.005714285714285714},"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}},"3":{"0":{"4":{"5":{"8":{"9":{"7":{"0":{"7":{"9":{"2":{"7":{"6":{"7":{"7":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"9":{"3":{"9":{"4":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"1":{"1":{"2":{"6":{"7":{"5":{"4":{"0":{"9":{"4":{"9":{"3":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"5":{"2":{"4":{"5":{"6":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"5":{"2":{"3":{"3":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"2":{"0":{"8":{"7":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"6":{"8":{"4":{"2":{"1":{"0":{"5":{"2":{"6":{"3":{"1":{"5":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"4":{"9":{"4":{"9":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.005714285714285714}}},"5":{"1":{"9":{"4":{"8":{"2":{"7":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"0":{"9":{"2":{"1":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"9":{"5":{"9":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}},"6":{"6":{"2":{"9":{"2":{"1":{"3":{"docs":{},"]":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"8":{"9":{"1":{"6":{"2":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"docs":{}},"9":{"6":{"9":{"6":{"9":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}},"7":{"2":{"8":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"docs":{}},"5":{"5":{"5":{"5":{"5":{"5":{"5":{"5":{"5":{"5":{"5":{"5":{"5":{"5":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"7":{"7":{"7":{"7":{"8":{"docs":{},",":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"3":{"1":{"0":{"2":{"3":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"7":{"9":{"7":{"9":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"0":{"2":{"3":{"6":{"9":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"2":{"8":{"5":{"1":{"1":{"8":{"2":{"1":{"9":{"7":{"5":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"8":{"3":{"0":{"6":{"6":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"3":{"5":{"9":{"9":{"8":{"7":{"4":{"0":{"0":{"6":{"4":{"7":{"8":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"4":{"7":{"6":{"8":{"0":{"8":{"9":{"0":{"5":{"3":{"8":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"0":{"4":{"5":{"2":{"6":{"7":{"4":{"8":{"9":{"7":{"1":{"1":{"9":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"3":{"8":{"6":{"2":{"2":{"1":{"2":{"9":{"4":{"3":{"6":{"3":{"3":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"1":{"2":{"9":{"1":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"0":{"9":{"1":{"7":{"9":{"4":{"1":{"5":{"8":{"5":{"5":{"3":{"5":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"6":{"7":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"7":{"3":{"4":{"2":{"4":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"9":{"6":{"3":{"9":{"6":{"6":{"2":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},",":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}},"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.024}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.152}}},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.008}},"x":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.019704433497536946},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}},"1":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.019704433497536946},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}},"docs":{}}}}}}}},",":{"0":{"docs":{},",":{"0":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}},"docs":{}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.06991525423728813},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.03070175438596491},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.03271028037383177},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.1038961038961039},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.136},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.03},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.033783783783783786}}},")":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}},"`":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}},"1":{"0":{"0":{"0":{"0":{"0":{"0":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}},".":{"docs":{},"]":{"docs":{},"]":{"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}},"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"docs":{},",":{"1":{"0":{"docs":{},",":{"1":{"0":{"0":{"docs":{},")":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"]":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}},"}":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"]":{"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},")":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}},".":{"9":{"9":{"7":{"5":{"8":{"4":{"8":{"4":{"docs":{},"]":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"支":{"docs":{},"撑":{"docs":{},"向":{"docs":{},"量":{"docs":{},"机":{"docs":{"支撑向量机SVM/":{"ref":"支撑向量机SVM/","tf":5.5}}}}}}}},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.01948051948051948}}}}}},"1":{"0":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},"]":{"docs":{},"]":{"docs":{},")":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}},"，":{"docs":{},"分":{"docs":{},"别":{"docs":{},"拿":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"k":{"docs":{},"去":{"docs":{},"调":{"docs":{},"用":{"docs":{},"算":{"docs":{},"法":{"docs":{},"，":{"docs":{},"得":{"docs":{},"出":{"docs":{},"分":{"docs":{},"数":{"docs":{},"，":{"docs":{},"取":{"docs":{},"得":{"docs":{},"分":{"docs":{},"最":{"docs":{},"高":{"docs":{},"的":{"docs":{},"那":{"docs":{},"个":{"docs":{},"k":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},".":{"1":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":5.05}}},"2":{"docs":{"支撑向量机SVM/11.2 SVM背后的最优化问题.html":{"ref":"支撑向量机SVM/11.2 SVM背后的最优化问题.html","tf":5.166666666666667}}},"3":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":2.5625}}},"4":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":3.3361344537815123}}},"5":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":5.004926108374384}}},"6":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":5.083333333333333}}},"7":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":5.01}}},"8":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":5.004716981132075}}},"9":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":5.011363636363637}}},"docs":{},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{"11jue-ce-shu.html":{"ref":"11jue-ce-shu.html","tf":10}}}}}}},"2":{"0":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.014234875444839857}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"1":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":5.008928571428571}}},"2":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":5.0285714285714285}}},"3":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":5.00418410041841}}},"4":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":5.004098360655738}}},"5":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}},"6":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":5.014705882352941}}},"7":{"docs":{"127-jue-ce-shu-de-ju-xian-xing.html":{"ref":"127-jue-ce-shu-de-ju-xian-xing.html","tf":5.2}}},"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"3":{"0":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"9":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},".":{"1":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":5.006756756756757}}},"2":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":3.3619047619047615}}},"3":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":3.333333333333333}}},"4":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":2.007751937984496}}},"5":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":5.010989010989011}}},"6":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":1.6782945736434107}}},"7":{"docs":{"137-stacking.html":{"ref":"137-stacking.html","tf":5.333333333333333}}},"docs":{},"集":{"docs":{},"成":{"docs":{},"学":{"docs":{},"习":{"docs":{},"和":{"docs":{},"随":{"docs":{},"机":{"docs":{},"森":{"docs":{},"林":{"docs":{"13ji-cheng-xue-xi-he-sui-ji-sen-lin.html":{"ref":"13ji-cheng-xue-xi-he-sui-ji-sen-lin.html","tf":10}}}}}}}}}}}}},"4":{"0":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"6":{"6":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}},"7":{"docs":{},".":{"9":{"4":{"0":{"1":{"0":{"8":{"6":{"0":{"1":{"5":{"1":{"8":{"9":{"4":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"2":{"1":{"3":{"3":{"1":{"6":{"6":{"1":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},"]":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}},"7":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"8":{"0":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.00872093023255814}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"9":{"7":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"7":{"1":{"7":{"3":{"8":{"7":{"8":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"8":{"9":{"5":{"2":{"8":{"3":{"0":{"1":{"3":{"1":{"5":{"4":{"6":{"5":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.015873015873015872},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.015151515151515152},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541}},".":{"0":{"0":{"0":{"1":{"5":{"1":{"3":{"3":{"8":{"1":{"5":{"4":{"1":{"4":{"6":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"5":{"8":{"1":{"5":{"2":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"2":{"8":{"6":{"3":{"5":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"7":{"1":{"1":{"4":{"8":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.005158437730287398}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"6":{"2":{"1":{"4":{"4":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"5":{"8":{"7":{"5":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.007369196757553427}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"7":{"8":{"3":{"0":{"5":{"9":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"2":{"7":{"0":{"9":{"6":{"0":{"docs":{},"e":{"docs":{},"+":{"0":{"0":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"6":{"7":{"2":{"7":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0058953574060427415}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"2":{"5":{"3":{"8":{"2":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"7":{"7":{"4":{"6":{"3":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"4":{"3":{"7":{"5":{"9":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"7":{"6":{"5":{"0":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"4":{"6":{"7":{"8":{"3":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0036845983787767134}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"4":{"0":{"0":{"3":{"6":{"docs":{},"e":{"docs":{},"+":{"0":{"1":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}},",":{"1":{"docs":{},".":{"5":{"docs":{},"]":{"docs":{},")":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02358490566037736}}}}},"docs":{}}},"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}}},"1":{"0":{"1":{"4":{"6":{"5":{"1":{"6":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"1":{"3":{"9":{"1":{"1":{"3":{"5":{"1":{"8":{"1":{"8":{"6":{"4":{"8":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"2":{"6":{"1":{"3":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"9":{"6":{"6":{"0":{"5":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"2":{"3":{"5":{"7":{"3":{"9":{"docs":{},"e":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"6":{"0":{"8":{"0":{"8":{"4":{"3":{"2":{"5":{"9":{"9":{"6":{"8":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"2":{"2":{"8":{"5":{"0":{"2":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.005158437730287398}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"0":{"1":{"4":{"5":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"9":{"1":{"6":{"6":{"6":{"7":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"4":{"0":{"8":{"5":{"0":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"7":{"2":{"3":{"9":{"2":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"7":{"7":{"2":{"8":{"7":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"5":{"9":{"8":{"4":{"3":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"8":{"7":{"5":{"9":{"3":{"0":{"4":{"2":{"1":{"8":{"4":{"6":{"1":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"2":{"0":{"9":{"9":{"8":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"3":{"8":{"3":{"5":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.005158437730287398}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.015151515151515152},"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}},"2":{"1":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"3":{"1":{"7":{"9":{"7":{"3":{"8":{"docs":{},"e":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"6":{"5":{"1":{"5":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"3":{"8":{"7":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"2":{"7":{"0":{"7":{"4":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"3":{"1":{"0":{"6":{"6":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.010316875460574797}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"6":{"1":{"7":{"0":{"0":{"2":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"6":{"4":{"4":{"1":{"6":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0036845983787767134}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"4":{"8":{"5":{"8":{"5":{"6":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.020633750921149593}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"9":{"4":{"3":{"0":{"4":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"1":{"0":{"3":{"3":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}},"3":{"0":{"9":{"8":{"2":{"9":{"6":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.008843036109064112}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"5":{"5":{"2":{"6":{"8":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"6":{"4":{"5":{"6":{"1":{"1":{"3":{"0":{"8":{"6":{"2":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"1":{"5":{"5":{"4":{"7":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0058953574060427415}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"6":{"1":{"9":{"2":{"5":{"8":{"5":{"2":{"6":{"5":{"7":{"2":{"6":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"2":{"0":{"9":{"9":{"6":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"6":{"1":{"5":{"3":{"4":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"6":{"5":{"5":{"2":{"7":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.005158437730287398}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"1":{"2":{"2":{"4":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"3":{"5":{"4":{"6":{"8":{"8":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"1":{"1":{"3":{"2":{"6":{"7":{"5":{"1":{"4":{"4":{"6":{"5":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"0":{"3":{"3":{"4":{"0":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"4":{"3":{"7":{"8":{"0":{"3":{"1":{"4":{"4":{"2":{"1":{"7":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"9":{"8":{"6":{"8":{"7":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}},"4":{"0":{"0":{"9":{"6":{"1":{"4":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"7":{"8":{"0":{"0":{"5":{"docs":{},"e":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"7":{"4":{"3":{"2":{"1":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"0":{"4":{"4":{"8":{"4":{"1":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"2":{"7":{"5":{"7":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"2":{"3":{"6":{"9":{"2":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"8":{"9":{"5":{"3":{"9":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"7":{"9":{"8":{"2":{"9":{"8":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"9":{"5":{"9":{"9":{"5":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"7":{"9":{"9":{"9":{"8":{"8":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"0":{"0":{"1":{"1":{"4":{"0":{"2":{"4":{"3":{"2":{"9":{"5":{"2":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"1":{"8":{"5":{"7":{"8":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.005158437730287398}}},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}},"5":{"2":{"3":{"2":{"9":{"5":{"7":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"3":{"2":{"2":{"6":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"6":{"9":{"2":{"8":{"3":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"6":{"7":{"2":{"1":{"8":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"8":{"4":{"3":{"1":{"0":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"2":{"3":{"8":{"1":{"0":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}},",":{"2":{"docs":{},".":{"5":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02358490566037736}}}},"docs":{}}},"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}},"]":{"docs":{},")":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}}}},"6":{"2":{"7":{"8":{"3":{"7":{"7":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.004421518054532056}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"1":{"7":{"5":{"9":{"3":{"2":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"7":{"6":{"8":{"7":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"6":{"7":{"2":{"2":{"8":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114}}},"8":{"5":{"8":{"0":{"8":{"1":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"6":{"4":{"9":{"1":{"7":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"1":{"3":{"6":{"8":{"5":{"3":{"5":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"9":{"9":{"9":{"0":{"9":{"6":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},"6":{"2":{"9":{"5":{"1":{"8":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"3":{"3":{"2":{"8":{"6":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"0":{"1":{"4":{"9":{"9":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"3":{"0":{"7":{"3":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"3":{"5":{"6":{"8":{"3":{"9":{"4":{"0":{"0":{"2":{"5":{"0":{"5":{"docs":{},"e":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"4":{"6":{"0":{"0":{"3":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"1":{"2":{"3":{"1":{"3":{"5":{"docs":{},"]":{"docs":{},"]":{"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"0":{"4":{"6":{"4":{"9":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"2":{"3":{"5":{"1":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"7":{"8":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"0":{"8":{"9":{"3":{"9":{"6":{"5":{"9":{"5":{"1":{"5":{"5":{"9":{"5":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"4":{"7":{"1":{"4":{"1":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"1":{"9":{"5":{"5":{"7":{"8":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"5":{"7":{"7":{"0":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"2":{"6":{"6":{"2":{"2":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"2":{"7":{"7":{"3":{"6":{"3":{"0":{"0":{"2":{"3":{"7":{"9":{"1":{"4":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"2":{"8":{"7":{"4":{"6":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"2":{"7":{"1":{"2":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028},"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.025252525252525252},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"基":{"docs":{},"本":{"docs":{},"概":{"docs":{},"念":{"docs":{"ji-qi-xue-xi-ji-ben-gai-nian.html":{"ref":"ji-qi-xue-xi-ji-ben-gai-nian.html","tf":10}}}}}}}}}},"%":{"docs":{},"r":{"docs":{},"u":{"docs":{},"n":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}},"加":{"docs":{},"载":{"docs":{},"n":{"docs":{},"u":{"docs":{},"m":{"docs":{},"p":{"docs":{},"y":{"docs":{},"与":{"docs":{},"查":{"docs":{},"看":{"docs":{},"版":{"docs":{},"本":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":0.1}}}}}}}}}}}},"m":{"docs":{},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}},"人":{"docs":{},"脸":{"docs":{},"数":{"docs":{},"据":{"docs":{},"库":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}},"创":{"docs":{},"建":{"docs":{},"“":{"0":{"docs":{},"”":{"docs":{},"数":{"docs":{},"组":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}}}},"docs":{}}}},"基":{"docs":{},"本":{"docs":{},"属":{"docs":{},"性":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html","tf":0.2}}}}}},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"原":{"docs":{},"理":{"docs":{},"介":{"docs":{},"绍":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":10}}}}}}}}}}}},"将":{"docs":{},"这":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"0":{"docs":{},"~":{"docs":{},"x":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}},"docs":{}}}}}}}},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"分":{"docs":{},"割":{"docs":{},"成":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"合":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}},"成":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"合":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}}},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"简":{"docs":{},"介":{"docs":{"线性回归算法/1.线性回归算法简介.html":{"ref":"线性回归算法/1.线性回归算法简介.html","tf":10}}}}}}}}}},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"简":{"docs":{},"介":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":10}}}}}}}}},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"简":{"docs":{},"介":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":10.027777777777779}}}}}}},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"分":{"docs":{},"析":{"docs":{},"法":{"docs":{},"的":{"docs":{},"两":{"docs":{},"个":{"docs":{},"轴":{"docs":{},"都":{"docs":{},"是":{"docs":{},"特":{"docs":{},"征":{"docs":{},"，":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"y":{"docs":{},"轴":{"docs":{},"是":{"docs":{},"目":{"docs":{},"标":{"docs":{},"结":{"docs":{},"果":{"docs":{},"值":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"注":{"docs":{},"意":{"docs":{},"上":{"docs":{},"面":{"docs":{},"式":{"docs":{},"子":{"docs":{},"里":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"(":{"docs":{},"x":{"1":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"·":{"docs":{},"w":{"1":{"docs":{},"+":{"docs":{},"x":{"2":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"·":{"docs":{},"w":{"2":{"docs":{},"+":{"docs":{},".":{"docs":{},".":{"docs":{},".":{"docs":{},".":{"docs":{},".":{"docs":{},".":{"docs":{},"x":{"docs":{},"n":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"·":{"docs":{},"w":{"docs":{},"n":{"docs":{},")":{"docs":{},"都":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"x":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"和":{"docs":{},"w":{"docs":{},"的":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"式":{"docs":{},"子":{"docs":{},"可":{"docs":{},"以":{"docs":{},"进":{"docs":{},"一":{"docs":{},"步":{"docs":{},"化":{"docs":{},"解":{"docs":{},"，":{"docs":{"PCA/2.使用梯度上升法解决PCA问题.html":{"ref":"PCA/2.使用梯度上升法解决PCA问题.html","tf":0.16666666666666666}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}},"docs":{}}}},"docs":{}}}}}}},"docs":{}}}}}}}}}}}}}},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"改":{"docs":{},"变":{"docs":{},"，":{"docs":{},"将":{"docs":{},"数":{"docs":{},"据":{"docs":{},"在":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"上":{"docs":{},"的":{"docs":{},"分":{"docs":{},"量":{"docs":{},"去":{"docs":{},"掉":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}}}}}}},"回":{"docs":{},"忆":{"docs":{},"我":{"docs":{},"们":{"docs":{},"之":{"docs":{},"前":{"docs":{},"的":{"docs":{},"例":{"docs":{},"子":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}},"高":{"docs":{},"维":{"docs":{},"数":{"docs":{},"据":{"docs":{},"向":{"docs":{},"低":{"docs":{},"维":{"docs":{},"数":{"docs":{},"据":{"docs":{},"映":{"docs":{},"射":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}},"斯":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"简":{"docs":{},"介":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":10.012345679012345}}}}}}}}},"模":{"docs":{},"拟":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.03535353535353535}}},"使":{"docs":{},"用":{"docs":{},"`":{"docs":{},"`":{"docs":{},"`":{"docs":{},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"f":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"`":{"docs":{},"`":{"docs":{},"`":{"docs":{},"生":{"docs":{},"成":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}},"什":{"docs":{},"么":{"docs":{},"是":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"和":{"docs":{},"欠":{"docs":{},"拟":{"docs":{},"合":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}},"模":{"docs":{},"型":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":10.035714285714286}}}}}},"o":{"docs":{},"v":{"docs":{},"r":{"docs":{},"与":{"docs":{},"o":{"docs":{},"v":{"docs":{},"o":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":0.08333333333333333}}}}}}}},"两":{"docs":{},"组":{"docs":{},"调":{"docs":{},"参":{"docs":{},"得":{"docs":{},"出":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"结":{"docs":{},"果":{"docs":{},"是":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"，":{"docs":{},"通":{"docs":{},"常":{"docs":{},"这":{"docs":{},"时":{"docs":{},"候":{"docs":{},"我":{"docs":{},"们":{"docs":{},"更":{"docs":{},"愿":{"docs":{},"意":{"docs":{},"详":{"docs":{},"细":{"docs":{},"使":{"docs":{},"用":{"docs":{},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"得":{"docs":{},"出":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}},"降":{"docs":{},"低":{"docs":{},"模":{"docs":{},"型":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"度":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}},"l":{"1":{"docs":{},",":{"docs":{},"l":{"2":{"docs":{},"正":{"docs":{},"则":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}},"docs":{}}}},"docs":{}},"对":{"docs":{},"σ":{"docs":{},"(":{"docs":{},"t":{"docs":{},")":{"docs":{},"求":{"docs":{},"导":{"docs":{"逻辑回归/3.逻辑回归函数损失的梯度.html":{"ref":"逻辑回归/3.逻辑回归函数损失的梯度.html","tf":0.09090909090909091}}}}}}}}},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"封":{"docs":{},"装":{"docs":{},"实":{"docs":{},"现":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}},"f":{"1":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}},"docs":{}},"理":{"docs":{},"解":{"docs":{},"为":{"docs":{},"什":{"docs":{},"么":{"docs":{},"二":{"docs":{},"者":{"docs":{},"是":{"docs":{},"矛":{"docs":{},"盾":{"docs":{},"的":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}},",":{"1":{"0":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}},"*":{"docs":{},"w":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}},",":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}},"6":{"docs":{},",":{"1":{"4":{"1":{"docs":{},")":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.031413612565445025},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.1038135593220339},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.04824561403508772},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.0514018691588785},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.034666666666666665},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.08},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.02027027027027027},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"b":{"docs":{},"o":{"docs":{},"s":{"docs":{},"e":{"docs":{},"=":{"1":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"2":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}},"docs":{}}}}}}}}},"\\":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}},"x":{"1":{"docs":{},",":{"docs":{},"x":{"2":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"docs":{}}}},"docs":{}}},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"随":{"docs":{},"机":{"docs":{},"排":{"docs":{},"列":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}},"为":{"docs":{},"使":{"docs":{},"用":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"核":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}},"之":{"docs":{},"间":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}},"）":{"docs":{},"显":{"docs":{},"然":{"docs":{},"不":{"docs":{},"符":{"docs":{},"合":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"正":{"docs":{},"规":{"docs":{},"式":{"docs":{},"子":{"docs":{},"将":{"docs":{},"以":{"docs":{},"优":{"docs":{},"化":{"docs":{},"前":{"docs":{},"半":{"docs":{},"部":{"docs":{},"分":{"docs":{},"为":{"docs":{},"主":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"c":{"docs":{},"特":{"docs":{},"别":{"docs":{},"大":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"这":{"docs":{},"个":{"docs":{},"式":{"docs":{},"子":{"docs":{},"主":{"docs":{},"要":{"docs":{},"优":{"docs":{},"化":{"docs":{},"的":{"docs":{},"目":{"docs":{},"标":{"docs":{},"就":{"docs":{},"是":{"docs":{},"后":{"docs":{},"半":{"docs":{},"部":{"docs":{},"分":{"docs":{"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":0.0625}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},")":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198}},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}},"]":{"docs":{},":":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}},"]":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}},"d":{"docs":{},"]":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541}}}}},")":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.014388489208633094},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.03940886699507389},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.02027027027027027}}}},"e":{"4":{"docs":{},",":{"docs":{},"e":{"docs":{},"p":{"docs":{},"s":{"docs":{},"i":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}},"=":{"1":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}},"docs":{}}}}}}}}}},")":{"docs":{},":":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}},"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.01948051948051948},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.013605442176870748}}}}},"/":{"docs":{},"(":{"1":{"docs":{},"+":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"e":{"docs":{},"x":{"docs":{},"p":{"docs":{},"(":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}}}}}}}},"docs":{}},"w":{"1":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435}}},"docs":{},"[":{"1":{"docs":{},"]":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435}}}},"docs":{}}}}},"2":{"0":{"0":{"docs":{},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}},"9":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"1":{"4":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"2":{"0":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}},"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},".":{"0":{"5":{"7":{"0":{"1":{"1":{"6":{"6":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},".":{"5":{"4":{"0":{"0":{"3":{"3":{"9":{"1":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"]":{"docs":{},"]":{"docs":{},")":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},".":{"0":{"4":{"2":{"8":{"6":{"7":{"6":{"6":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"6":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}},"docs":{}}},"8":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"9":{"1":{"4":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}},")":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.02040816326530612}}}},"docs":{}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},".":{"0":{"0":{"2":{"1":{"8":{"3":{"7":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"4":{"8":{"4":{"0":{"5":{"2":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"1":{"5":{"9":{"0":{"4":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},"1":{"7":{"9":{"5":{"1":{"6":{"4":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}},"观":{"docs":{},"察":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"学":{"docs":{},"习":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"：":{"docs":{},"观":{"docs":{},"察":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"随":{"docs":{},"着":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"增":{"docs":{},"加":{"docs":{},"，":{"docs":{},"性":{"docs":{},"能":{"docs":{},"的":{"docs":{},"变":{"docs":{},"化":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"2":{"0":{"0":{"3":{"0":{"8":{"5":{"7":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"4":{"1":{"7":{"4":{"0":{"3":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"7":{"0":{"0":{"5":{"7":{"0":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}},"3":{"0":{"2":{"9":{"4":{"3":{"4":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"4":{"6":{"6":{"7":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"4":{"5":{"7":{"4":{"8":{"9":{"7":{"4":{"3":{"1":{"5":{"1":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"3":{"2":{"8":{"5":{"8":{"6":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"2":{"3":{"3":{"7":{"6":{"7":{"1":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"5":{"3":{"0":{"7":{"5":{"1":{"6":{"docs":{},"e":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"9":{"3":{"6":{"4":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},"9":{"2":{"9":{"6":{"0":{"4":{"4":{"docs":{},"]":{"docs":{},"]":{"docs":{},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"0":{"3":{"6":{"2":{"6":{"6":{"6":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"5":{"3":{"4":{"3":{"1":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"9":{"7":{"9":{"7":{"6":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"5":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"7":{"8":{"6":{"8":{"4":{"0":{"9":{"5":{"7":{"4":{"7":{"8":{"8":{"8":{"7":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"8":{"0":{"1":{"7":{"4":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},")":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}},"*":{"docs":{},"*":{"2":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.015151515151515152}}},"docs":{}}}},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}}},"6":{"1":{"1":{"2":{"0":{"7":{"7":{"2":{"6":{"7":{"3":{"9":{"5":{"8":{"0":{"3":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"5":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114}}},"docs":{}},"7":{"3":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},"4":{"1":{"4":{"1":{"1":{"0":{"3":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"1":{"4":{"8":{"1":{"7":{"1":{"3":{"7":{"6":{"8":{"6":{"7":{"9":{"4":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"7":{"6":{"1":{"9":{"6":{"4":{"9":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"8":{"9":{"0":{"9":{"5":{"8":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html","tf":0.2},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993},"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}},"j":{"docs":{},"u":{"docs":{},"p":{"docs":{},"y":{"docs":{},"t":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong.html","tf":5}}}}}}},"%":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{},"i":{"docs":{},"t":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}},"p":{"docs":{},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"n":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":0.1}}}}}}}},"创":{"docs":{},"建":{"docs":{},"全":{"docs":{},"\"":{"1":{"docs":{},"\"":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"和":{"docs":{},"创":{"docs":{},"建":{"docs":{},"全":{"docs":{},"\"":{"docs":{},"n":{"docs":{},"\"":{"docs":{},"矩":{"docs":{},"阵":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}}}}}}}}}}}}},"docs":{}}}}},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"简":{"docs":{},"单":{"docs":{},"实":{"docs":{},"现":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":10}}}}}}}}}}}}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"然":{"docs":{},"后":{"docs":{},"对":{"docs":{},"于":{"docs":{},"每":{"docs":{},"个":{"docs":{},"x":{"docs":{},"相":{"docs":{},"比":{"docs":{},"于":{"docs":{},"整":{"docs":{},"个":{"docs":{},"范":{"docs":{},"围":{"docs":{},"所":{"docs":{},"占":{"docs":{},"的":{"docs":{},"比":{"docs":{},"例":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}}}}}}}},"将":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"进":{"docs":{},"行":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{},"处":{"docs":{},"理":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"进":{"docs":{},"行":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}},"简":{"docs":{},"单":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":10}}}}}}}}}}},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"模":{"docs":{},"拟":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":10}}}}}}}}},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"分":{"docs":{},"析":{"docs":{},"法":{"docs":{},"的":{"docs":{},"点":{"docs":{},"是":{"docs":{},"垂":{"docs":{},"直":{"docs":{},"于":{"docs":{},"方":{"docs":{},"差":{"docs":{},"轴":{"docs":{},"直":{"docs":{},"线":{"docs":{},"的":{"docs":{},"，":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"点":{"docs":{},"事":{"docs":{},"垂":{"docs":{},"直":{"docs":{},"于":{"docs":{},"x":{"docs":{},"轴":{"docs":{},"的":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"上":{"docs":{},"升":{"docs":{},"法":{"docs":{},"解":{"docs":{},"决":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"问":{"docs":{},"题":{"docs":{"PCA/2.使用梯度上升法解决PCA问题.html":{"ref":"PCA/2.使用梯度上升法解决PCA问题.html","tf":10.166666666666666}}}}}}}}}}}}}},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"拟":{"docs":{},"合":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{},"得":{"docs":{},"出":{"docs":{},"的":{"docs":{},"最":{"docs":{},"好":{"docs":{},"分":{"docs":{},"数":{"0":{"docs":{},".":{"9":{"8":{"2":{"docs":{},"是":{"docs":{},"小":{"docs":{},"于":{"docs":{},"使":{"docs":{},"用":{"docs":{},"分":{"docs":{},"割":{"docs":{},"训":{"docs":{},"练":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"得":{"docs":{},"出":{"docs":{},"的":{"0":{"docs":{},".":{"9":{"8":{"6":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"在":{"docs":{},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{},"的":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}}},"化":{"docs":{},"简":{"docs":{},"过":{"docs":{},"后":{"docs":{},"可":{"docs":{},"以":{"docs":{},"进":{"docs":{},"行":{"docs":{},"向":{"docs":{},"量":{"docs":{},"化":{"docs":{},"，":{"docs":{},"即":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"∑":{"docs":{},"(":{"docs":{},"x":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"·":{"docs":{},"w":{"1":{"docs":{},")":{"docs":{},"·":{"docs":{},"x":{"1":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{"PCA/2.使用梯度上升法解决PCA问题.html":{"ref":"PCA/2.使用梯度上升法解决PCA问题.html","tf":0.16666666666666666}}}}}},"docs":{}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}},"在":{"docs":{},"新":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"上":{"docs":{},"求":{"docs":{},"第":{"docs":{},"一":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}},"手":{"docs":{},"写":{"docs":{},"识":{"docs":{},"别":{"docs":{},"的":{"docs":{},"例":{"docs":{},"子":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}},"实":{"docs":{},"际":{"docs":{},"编":{"docs":{},"程":{"docs":{},"用":{"docs":{},"可":{"docs":{},"视":{"docs":{},"化":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"观":{"docs":{},"察":{"docs":{},"特":{"docs":{},"征":{"docs":{},"脸":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}},"实":{"docs":{},"现":{"docs":{},"学":{"docs":{},"习":{"docs":{},"曲":{"docs":{},"线":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}},"特":{"docs":{},"征":{"docs":{},"脸":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}},"编":{"docs":{},"程":{"docs":{},"实":{"docs":{},"验":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}},"现":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},"岭":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.010101010101010102}}},"s":{"docs":{},"c":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":5.005050505050505}}}}}}},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"中":{"docs":{},"的":{"docs":{},"o":{"docs":{},"v":{"docs":{},"r":{"docs":{},"与":{"docs":{},"o":{"docs":{},"v":{"docs":{},"o":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},"如":{"docs":{},"果":{"docs":{},"生":{"docs":{},"成":{"docs":{},"数":{"docs":{},"据":{"docs":{},"幂":{"docs":{},"特":{"docs":{},"别":{"docs":{},"的":{"docs":{},"大":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"特":{"docs":{},"征":{"docs":{},"直":{"docs":{},"接":{"docs":{},"的":{"docs":{},"差":{"docs":{},"距":{"docs":{},"就":{"docs":{},"会":{"docs":{},"很":{"docs":{},"大":{"docs":{},"，":{"docs":{},"导":{"docs":{},"致":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"非":{"docs":{},"常":{"docs":{},"慢":{"docs":{},"，":{"docs":{},"这":{"docs":{},"时":{"docs":{},"候":{"docs":{},"可":{"docs":{},"以":{"docs":{},"进":{"docs":{},"行":{"docs":{},"*":{"docs":{},"*":{"docs":{},"数":{"docs":{},"据":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{},"*":{"docs":{},"*":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"调":{"docs":{},"用":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"对":{"docs":{},"x":{"2":{"docs":{},"进":{"docs":{},"行":{"docs":{},"预":{"docs":{},"测":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"为":{"docs":{},"什":{"docs":{},"么":{"docs":{},"要":{"docs":{},"使":{"docs":{},"用":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"和":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}},"减":{"docs":{},"少":{"docs":{},"数":{"docs":{},"据":{"docs":{},"维":{"docs":{},"度":{"docs":{},"；":{"docs":{},"降":{"docs":{},"噪":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}},"l":{"0":{"docs":{},"正":{"docs":{},"则":{"docs":{},"项":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}},"docs":{}},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{"逻辑回归/2.逻辑回归的损失函数.html":{"ref":"逻辑回归/2.逻辑回归的损失函数.html","tf":10.090909090909092}}}}}}}}}}},"对":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"docs":{},"σ":{"docs":{},"(":{"docs":{},"t":{"docs":{},")":{"docs":{},")":{"docs":{},"进":{"docs":{},"行":{"docs":{},"求":{"docs":{},"导":{"docs":{"逻辑回归/3.逻辑回归函数损失的梯度.html":{"ref":"逻辑回归/3.逻辑回归函数损失的梯度.html","tf":0.09090909090909091}}}}}}}}}}}}}}}},"测":{"docs":{},"试":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}},"f":{"1":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}},"docs":{}},"直":{"docs":{},"观":{"docs":{},"的":{"docs":{},"理":{"docs":{},"解":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}},"以":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"为":{"docs":{},"例":{"docs":{},"，":{"docs":{},"看":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"时":{"docs":{},"怎":{"docs":{},"么":{"docs":{},"运":{"docs":{},"作":{"docs":{},"的":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}},"用":{"docs":{},"一":{"docs":{},"个":{"docs":{},"简":{"docs":{},"单":{"docs":{},"的":{"docs":{},"例":{"docs":{},"子":{"docs":{},"模":{"docs":{},"拟":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"函":{"docs":{},"数":{"docs":{},"到":{"docs":{},"底":{"docs":{},"在":{"docs":{},"座":{"docs":{},"什":{"docs":{},"么":{"docs":{},"事":{"docs":{},"情":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.13771186440677965},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.00872093023255814},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.010676156583629894},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.016},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.02},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00847457627118644}}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}},"*":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},".":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},"(":{"docs":{},"s":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}},"/":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},"_":{"docs":{},"k":{"docs":{},")":{"docs":{},".":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}},")":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}},"s":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988}},",":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}},"+":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},".":{"docs":{},"n":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"l":{"docs":{},"(":{"0":{"docs":{},",":{"1":{"docs":{},",":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"1":{"0":{"0":{"docs":{},")":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}},"3":{"0":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},".":{"8":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"1":{"8":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},".":{"3":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"5":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"7":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"2":{"3":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}},".":{"6":{"4":{"5":{"6":{"6":{"0":{"8":{"3":{"9":{"6":{"5":{"2":{"2":{"4":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"3":{"7":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},".":{"0":{"2":{"9":{"3":{"9":{"1":{"8":{"7":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114}}},"]":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}},"5":{"2":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"5":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"6":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"6":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},".":{"docs":{},"]":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"]":{"docs":{},"]":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}}}}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/3.多元线性回归中的梯度下降法.html":{"ref":"梯度下降法/3.多元线性回归中的梯度下降法.html","tf":0.2},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.010676156583629894},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}},".":{"0":{"2":{"2":{"6":{"5":{"6":{"0":{"6":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"4":{"0":{"7":{"8":{"0":{"4":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"2":{"4":{"6":{"8":{"3":{"7":{"docs":{},"]":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"8":{"2":{"0":{"6":{"5":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"2":{"5":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"docs":{}},"docs":{}},"9":{"2":{"6":{"9":{"8":{"3":{"9":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}},",":{"3":{"docs":{},".":{"0":{"docs":{},",":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"1":{"0":{"0":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}},"docs":{}}},"docs":{}}},"1":{"0":{"8":{"6":{"2":{"4":{"4":{"6":{"8":{"9":{"5":{"0":{"4":{"3":{"8":{"6":{"docs":{},"e":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"7":{"8":{"3":{"1":{"6":{"3":{"docs":{},"e":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"9":{"1":{"7":{"9":{"3":{"7":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}},"2":{"6":{"9":{"8":{"9":{"4":{"7":{"0":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}},"3":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}},"4":{"1":{"1":{"0":{"5":{"8":{"1":{"4":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"1":{"5":{"5":{"7":{"2":{"7":{"docs":{},"e":{"docs":{},"+":{"0":{"0":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"7":{"7":{"5":{"5":{"0":{"3":{"6":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"0":{"1":{"1":{"9":{"6":{"6":{"3":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"docs":{}},"7":{"0":{"9":{"1":{"6":{"6":{"6":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"9":{"1":{"7":{"5":{"0":{"5":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"2":{"7":{"9":{"3":{"7":{"1":{"7":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"1":{"1":{"8":{"6":{"2":{"3":{"docs":{},"]":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"8":{"3":{"5":{"8":{"6":{"3":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"6":{"6":{"7":{"7":{"2":{"7":{"docs":{},"e":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}},"%":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}},"n":{"docs":{},"u":{"docs":{},"m":{"docs":{},"p":{"docs":{},"y":{"docs":{},".":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":0.1}}}}}}}}}}}}},"生":{"docs":{},"成":{"docs":{},"等":{"docs":{},"步":{"docs":{},"长":{"docs":{},"数":{"docs":{},"组":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html","tf":0.2}}}}}}}},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"学":{"docs":{},"习":{"docs":{},"与":{"docs":{},"使":{"docs":{},"用":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong.html","tf":10}}}}}}}}}}}}},"判":{"docs":{},"断":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":10}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"均":{"docs":{},"值":{"docs":{},"和":{"docs":{},"方":{"docs":{},"差":{"docs":{},"将":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}}}}}},"程":{"docs":{},"序":{"docs":{},"直":{"docs":{},"观":{"docs":{},"理":{"docs":{},"解":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"函":{"docs":{},"数":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}},"创":{"docs":{},"建":{"docs":{},"一":{"docs":{},"个":{"docs":{},"k":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}}}}}}}},"衡":{"docs":{},"量":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"指":{"docs":{},"标":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":10.014705882352942}}}}}}}}}}}}},"多":{"docs":{},"元":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"中":{"docs":{},"的":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{"梯度下降法/3.多元线性回归中的梯度下降法.html":{"ref":"梯度下降法/3.多元线性回归中的梯度下降法.html","tf":10}}}}}}}}}}}}}}},"最":{"docs":{},"后":{"docs":{},"根":{"docs":{},"据":{"docs":{},"转":{"docs":{},"置":{"docs":{},"法":{"docs":{},"则":{"docs":{"PCA/2.使用梯度上升法解决PCA问题.html":{"ref":"PCA/2.使用梯度上升法解决PCA问题.html","tf":0.16666666666666666}}}}}}}}}},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":10.008}}}}}}}}}}},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"进":{"docs":{},"行":{"docs":{},"降":{"docs":{},"维":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}},"i":{"docs":{},"p":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}},"总":{"docs":{},"结":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}},"解":{"docs":{},"决":{"docs":{},"方":{"docs":{},"案":{"docs":{},"，":{"docs":{},"添":{"docs":{},"加":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"关":{"docs":{},"于":{"docs":{},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"f":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}}},"进":{"docs":{},"行":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"与":{"docs":{},"前":{"docs":{},"拟":{"docs":{},"合":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":10.004672897196262}}}}}}}}},"增":{"docs":{},"加":{"docs":{},"样":{"docs":{},"本":{"docs":{},"数":{"docs":{},"（":{"docs":{},"模":{"docs":{},"型":{"docs":{},"太":{"docs":{},"过":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"，":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"非":{"docs":{},"常":{"docs":{},"多":{"docs":{},"，":{"docs":{},"而":{"docs":{},"样":{"docs":{},"本":{"docs":{},"数":{"docs":{},"不":{"docs":{},"足":{"docs":{},"以":{"docs":{},"支":{"docs":{},"撑":{"docs":{},"计":{"docs":{},"算":{"docs":{},"出":{"docs":{},"这":{"docs":{},"么":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"）":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"弹":{"docs":{},"性":{"docs":{},"网":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}},"对":{"docs":{},"j":{"docs":{},"(":{"docs":{},"θ":{"docs":{},")":{"docs":{},"的":{"docs":{},"前":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"求":{"docs":{},"导":{"docs":{"逻辑回归/3.逻辑回归函数损失的梯度.html":{"ref":"逻辑回归/3.逻辑回归函数损失的梯度.html","tf":0.09090909090909091}}}}}}}}}}}}}},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"函":{"docs":{},"数":{"docs":{},"损":{"docs":{},"失":{"docs":{},"的":{"docs":{},"梯":{"docs":{},"度":{"docs":{"逻辑回归/3.逻辑回归函数损失的梯度.html":{"ref":"逻辑回归/3.逻辑回归函数损失的梯度.html","tf":10.090909090909092}}}}}}}}}}}}},"编":{"docs":{},"码":{"docs":{},"观":{"docs":{},"察":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"的":{"docs":{},"平":{"docs":{},"衡":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}},"：":{"0":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333}}},"docs":{}},",":{"3":{"docs":{},",":{"0":{"docs":{},",":{"6":{"docs":{},"]":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}},"docs":{}}},"1":{"0":{"0":{"docs":{},")":{"docs":{},".":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"(":{"1":{"0":{"0":{"docs":{},",":{"1":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"1":{"0":{"0":{"docs":{},")":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.01984126984126984},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.00872093023255814},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.016},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.04201680672268908},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.02}}},"组":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"3":{"0":{"docs":{},"种":{"docs":{},"组":{"docs":{},"合":{"docs":{},"，":{"docs":{},"一":{"docs":{},"共":{"docs":{},"要":{"docs":{},"进":{"docs":{},"行":{"1":{"2":{"0":{"docs":{},"次":{"docs":{},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}},"docs":{}},"docs":{}}}}}},"]":{"docs":{},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}},"4":{"0":{"3":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}},"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},"s":{"docs":{},",":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}},"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"4":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"3":{"9":{"1":{"6":{"7":{"8":{"8":{"6":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"7":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},")":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}},"8":{"0":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"2":{"5":{"1":{"2":{"5":{"4":{"6":{"3":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},".":{"0":{"1":{"3":{"5":{"4":{"7":{"1":{"7":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"1":{"4":{"5":{"7":{"8":{"5":{"8":{"2":{"0":{"4":{"8":{"5":{"9":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"2":{"3":{"7":{"9":{"7":{"2":{"docs":{},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"1":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"9":{"2":{"4":{"3":{"3":{"7":{"4":{"7":{"3":{"2":{"3":{"0":{"0":{"1":{"docs":{},"e":{"docs":{},"+":{"2":{"1":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}},".":{"1":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}},"2":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}},"docs":{}}},"2":{"6":{"6":{"0":{"5":{"2":{"7":{"9":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}},"3":{"0":{"4":{"9":{"6":{"3":{"1":{"7":{"docs":{},"]":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"2":{"6":{"2":{"8":{"1":{"docs":{},"e":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"8":{"8":{"8":{"0":{"8":{"5":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}},"4":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}},"6":{"4":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}},"9":{"8":{"6":{"2":{"6":{"6":{"1":{"4":{"4":{"1":{"1":{"0":{"6":{"9":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"9":{"0":{"2":{"0":{"7":{"7":{"6":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"7":{"1":{"3":{"9":{"9":{"3":{"2":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":5},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}},"其":{"docs":{},"他":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}}}}}},"使":{"docs":{},"用":{"docs":{},"k":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}}}},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"集":{"docs":{},"训":{"docs":{},"练":{"docs":{},"处":{"docs":{},"模":{"docs":{},"型":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}},"验":{"docs":{},"证":{"docs":{},"集":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{"线性回归算法/":{"ref":"线性回归算法/","tf":10}}}},"中":{"docs":{},"的":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":10}}}}}}}}}}}}}}}},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"衡":{"docs":{},"量":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"法":{"docs":{},"的":{"docs":{},"指":{"docs":{},"标":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":3.352201257861635}}}}}}}}}}}}}}},"求":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"前":{"docs":{},"n":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":10.006993006993007}}}}}}}}}}}},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"]":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}},"学":{"docs":{},"习":{"docs":{},"曲":{"docs":{},"线":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":10.008}}}}}},"对":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"1":{"docs":{"逻辑回归/3.逻辑回归函数损失的梯度.html":{"ref":"逻辑回归/3.逻辑回归函数损失的梯度.html","tf":0.09090909090909091}}},"docs":{}}}}}},"实":{"docs":{},"现":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":10.00438596491228}}}}}}}}}}},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.01059322033898305}}},",":{"4":{"docs":{},",":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.017142857142857144},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285}}},"]":{"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.017142857142857144},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285}}}}},"5":{"docs":{},",":{"1":{"docs":{},")":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}},"docs":{}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.00872093023255814},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}},"]":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}},")":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}},"5":{"0":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"3":{"1":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"4":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}},"s":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"7":{"4":{"9":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},"docs":{}},"5":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}},"8":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}},"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"8":{"0":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224}},".":{"0":{"7":{"8":{"7":{"7":{"5":{"6":{"9":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"2":{"5":{"5":{"7":{"0":{"7":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"1":{"5":{"4":{"2":{"9":{"4":{"5":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}},"2":{"2":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}},"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}},"3":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}},"4":{"8":{"5":{"3":{"8":{"9":{"3":{"0":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}},"6":{"0":{"5":{"4":{"5":{"5":{"8":{"8":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"1":{"6":{"8":{"8":{"0":{"2":{"0":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"4":{"1":{"1":{"9":{"2":{"0":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"1":{"6":{"2":{"5":{"6":{"3":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}},"8":{"3":{"4":{"6":{"0":{"0":{"1":{"4":{"5":{"5":{"6":{"8":{"5":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"0":{"1":{"8":{"9":{"9":{"6":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"8":{"6":{"0":{"4":{"0":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"8":{"5":{"9":{"0":{"7":{"7":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}},"数":{"docs":{},"据":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":10}}}}}}},"使":{"docs":{},"用":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{},"后":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"测":{"docs":{},"试":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{},"（":{"docs":{},"a":{"docs":{},"c":{"docs":{},"c":{"docs":{},"u":{"docs":{},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"y":{"docs":{},"）":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"得":{"docs":{},"出":{"docs":{},"分":{"docs":{},"类":{"docs":{},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}},"多":{"docs":{},"元":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":10.002906976744185}}}}}}}},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{"梯度下降法/":{"ref":"梯度下降法/","tf":10}}}}}}},"随":{"docs":{},"机":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":10}}}}}}}}},"高":{"docs":{},"维":{"docs":{},"数":{"docs":{},"据":{"docs":{},"向":{"docs":{},"低":{"docs":{},"维":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"映":{"docs":{},"射":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":10}}}}}}}}}}}}}}},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"验":{"docs":{},"证":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"与":{"docs":{},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":10.00355871886121}}}}}}}}}}}},"模":{"docs":{},"型":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}},"对":{"docs":{},"j":{"docs":{},"(":{"docs":{},"θ":{"docs":{},")":{"docs":{},"的":{"docs":{},"后":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"求":{"docs":{},"导":{"docs":{"逻辑回归/3.逻辑回归函数损失的梯度.html":{"ref":"逻辑回归/3.逻辑回归函数损失的梯度.html","tf":0.09090909090909091}}}}}}}}}}}}}},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":10.004672897196262}}}}}}},")":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}},"}":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}},"]":{"docs":{},"}":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}},"6":{"0":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"1":{"3":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},".":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"*":{"4":{"7":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},"docs":{}},"docs":{}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}},".":{"docs":{},"]":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"6":{"6":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"7":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}},".":{"0":{"6":{"6":{"5":{"9":{"0":{"9":{"4":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"8":{"9":{"6":{"9":{"6":{"3":{"6":{"2":{"0":{"6":{"6":{"0":{"9":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}},"7":{"4":{"9":{"6":{"9":{"4":{"4":{"3":{"docs":{},"]":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"9":{"8":{"9":{"9":{"9":{"1":{"6":{"0":{"0":{"6":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"5":{"9":{"7":{"7":{"2":{"6":{"7":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},"docs":{}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"s":{"docs":{},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":5}},"中":{"docs":{},"的":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":10.003236245954692}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"预":{"docs":{},"测":{"docs":{},"新":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}},"网":{"docs":{},"格":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"寻":{"docs":{},"找":{"docs":{},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"回":{"docs":{},"到":{"1":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}},"docs":{}}}}}}}}}}}}}}}}}}}},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"可":{"docs":{},"解":{"docs":{},"性":{"docs":{},"和":{"docs":{},"更":{"docs":{},"多":{"docs":{},"思":{"docs":{},"考":{"docs":{"线性回归法/6线性回归的可解性和更多思考.html":{"ref":"线性回归法/6线性回归的可解性和更多思考.html","tf":11}}}}}}}}}}}}}}},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"的":{"docs":{},"调":{"docs":{},"试":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":10}}}}}}}}}},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"分":{"docs":{},"析":{"docs":{},"法":{"docs":{"PCA/":{"ref":"PCA/","tf":5}}}}}}}},"偏":{"docs":{},"差":{"docs":{},"方":{"docs":{},"差":{"docs":{},"均":{"docs":{},"衡":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":10.052631578947368}}}}}}}},"整":{"docs":{},"合":{"docs":{},"两":{"docs":{},"部":{"docs":{},"分":{"docs":{},"的":{"docs":{},"求":{"docs":{},"导":{"docs":{},"结":{"docs":{},"果":{"docs":{"逻辑回归/3.逻辑回归函数损失的梯度.html":{"ref":"逻辑回归/3.逻辑回归函数损失的梯度.html","tf":0.09090909090909091}}}}}}}}}}}},"在":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"中":{"docs":{},"使":{"docs":{},"用":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":10.005714285714285}}}}}}}}}}}}}}}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.00872093023255814},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}},"]":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}}}},"7":{"0":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"4":{"5":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"7":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}},"docs":{}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"8":{"4":{"docs":{},")":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988}}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}},".":{"0":{"1":{"0":{"7":{"7":{"3":{"9":{"2":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"1":{"8":{"6":{"4":{"8":{"6":{"0":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"1":{"8":{"9":{"4":{"5":{"9":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"2":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"3":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"4":{"4":{"0":{"7":{"5":{"9":{"5":{"5":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"1":{"5":{"3":{"3":{"4":{"4":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"6":{"0":{"5":{"1":{"6":{"2":{"1":{"9":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"3":{"8":{"2":{"8":{"3":{"3":{"2":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"9":{"2":{"5":{"0":{"4":{"1":{"4":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},"探":{"docs":{},"索":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"的":{"docs":{},"总":{"docs":{},"结":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":10.008}}}}}}}}}},"试":{"docs":{},"手":{"docs":{},"m":{"docs":{},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":10.006493506493506}}}}}}}}}}}},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/":{"ref":"多项式回归/","tf":11}}}}}}},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"模":{"docs":{},"型":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":10.005714285714285}}}}}}},"整":{"docs":{},"合":{"docs":{},"对":{"docs":{},"j":{"docs":{},"(":{"docs":{},"θ":{"docs":{},")":{"docs":{},"所":{"docs":{},"有":{"docs":{},"θ":{"docs":{},"的":{"docs":{},"求":{"docs":{},"导":{"docs":{},"结":{"docs":{},"果":{"docs":{"逻辑回归/3.逻辑回归函数损失的梯度.html":{"ref":"逻辑回归/3.逻辑回归函数损失的梯度.html","tf":0.09090909090909091}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":5.002857142857143}}}}}}}}}},"8":{"0":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}},".":{"3":{"7":{"9":{"0":{"8":{"0":{"4":{"6":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"2":{"6":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},".":{"docs":{},"]":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}},"]":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"6":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},"docs":{}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}},".":{"6":{"8":{"6":{"0":{"6":{"5":{"3":{"6":{"6":{"7":{"2":{"5":{"2":{"3":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"8":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}},".":{"4":{"0":{"1":{"3":{"2":{"2":{"2":{"1":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"2":{"4":{"3":{"5":{"4":{"4":{"docs":{},"e":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"9":{"6":{"8":{"8":{"6":{"1":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"0":{"6":{"1":{"8":{"3":{"2":{"0":{"docs":{},"e":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"2":{"2":{"0":{"4":{"6":{"1":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"9":{"1":{"5":{"1":{"3":{"8":{"3":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},"使":{"docs":{},"用":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"对":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"降":{"docs":{},"噪":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":10.00909090909091}}}}}}}}}}}}}},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":10.01388888888889}}}}}}},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{"逻辑回归/":{"ref":"逻辑回归/","tf":11}}}}}},"对":{"docs":{},"比":{"docs":{},"之":{"docs":{},"前":{"docs":{},"的":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"的":{"docs":{},"导":{"docs":{},"数":{"docs":{},"形":{"docs":{},"式":{"docs":{},"，":{"docs":{},"找":{"docs":{},"到":{"docs":{},"相":{"docs":{},"似":{"docs":{},"点":{"docs":{"逻辑回归/3.逻辑回归函数损失的梯度.html":{"ref":"逻辑回归/3.逻辑回归函数损失的梯度.html","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"v":{"docs":{},"r":{"docs":{},"与":{"docs":{},"o":{"docs":{},"v":{"docs":{},"o":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":10.004566210045661}}}}}}}}}},")":{"docs":{},":":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.013986013986013986},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}},"]":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}},"9":{"0":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},".":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"3":{"5":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},".":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"6":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}},"docs":{}}},"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}},"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.016666666666666666}}},".":{"0":{"0":{"0":{"1":{"7":{"6":{"4":{"2":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"1":{"4":{"6":{"9":{"8":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"6":{"4":{"2":{"5":{"5":{"3":{"1":{"docs":{},"e":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":5.083333333333333}}},"2":{"docs":{"评价分类结果/9.2 精准率和召回率.html":{"ref":"评价分类结果/9.2 精准率和召回率.html","tf":5.111111111111111}}},"3":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":5.0055555555555555}}},"4":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":3.341333333333333}}},"5":{"5":{"1":{"5":{"2":{"4":{"6":{"0":{"docs":{},"e":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":3.346320346320346}}},"6":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":3.3405275779376495}}},"7":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":5.0125}}},"8":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":5.002666666666666}}},"9":{"7":{"5":{"2":{"5":{"8":{"1":{"1":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},"人":{"docs":{},"脸":{"docs":{},"识":{"docs":{},"别":{"docs":{},"与":{"docs":{},"特":{"docs":{},"征":{"docs":{},"脸":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":10.006802721088436}}}}}}}}}},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.010101010101010102}}},"l":{"1":{"docs":{},",":{"docs":{},"l":{"2":{"docs":{},"和":{"docs":{},"弹":{"docs":{},"性":{"docs":{},"网":{"docs":{},"络":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":10.043478260869565}}}}}}}},"docs":{}}}},"docs":{}},"向":{"docs":{},"量":{"docs":{},"化":{"docs":{"逻辑回归/3.逻辑回归函数损失的梯度.html":{"ref":"逻辑回归/3.逻辑回归函数损失的梯度.html","tf":0.09090909090909091}}}}},"评":{"docs":{},"价":{"docs":{},"分":{"docs":{},"类":{"docs":{},"结":{"docs":{},"果":{"docs":{"评价分类结果/":{"ref":"评价分类结果/","tf":11}}}}}}}}},"]":{"docs":{},",":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}},"docs":{},"#":{"docs":{"./":{"ref":"./","tf":0.15384615384615385},"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.05235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.012711864406779662},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.047619047619047616},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0036845983787767134},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.08247422680412371},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.04296875},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.03488372093023256},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.05303030303030303},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.054945054945054944},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.024096385542168676},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.04891304347826087},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.08},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.07692307692307693},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.016181229773462782},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.01948051948051948},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.045454545454545456},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.047619047619047616},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.037037037037037035},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.04040404040404041},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.028037383177570093},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.04},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0498220640569395},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.08},"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.06944444444444445},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.021929824561403508},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.018691588785046728},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.03428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.03142857142857143},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0502283105022831},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.032},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.05194805194805195},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.03597122302158273},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.025},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.024},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.04481792717086835},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.024630541871921183},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.017857142857142856},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.03347280334728033},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.02459016393442623},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.039603960396039604},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.02027027027027027},"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.057692307692307696},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.031007751937984496},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}},"#":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}},"在":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"维":{"docs":{},"度":{"docs":{},"上":{"docs":{},"排":{"docs":{},"序":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{},"g":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"[":{"docs":{},":":{"docs":{},"i":{"docs":{},"]":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"[":{"docs":{},":":{"docs":{},"i":{"docs":{},"]":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"[":{"docs":{},":":{"docs":{},"i":{"docs":{},"]":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"=":{"1":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"已":{"docs":{},"经":{"docs":{},"过":{"docs":{},"头":{"docs":{},"了":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}},"docs":{}}}}}},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"g":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}}},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.013513513513513514}},"(":{"docs":{},"[":{"0":{"docs":{},".":{"0":{"2":{"9":{"4":{"5":{"7":{"3":{"2":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"4":{"5":{"6":{"6":{"8":{"1":{"7":{"docs":{},",":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"5":{"9":{"3":{"4":{"0":{"7":{"3":{"docs":{},",":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016}}}},"7":{"docs":{},",":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"1":{"0":{"1":{"9":{"5":{"0":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"5":{"3":{"6":{"5":{"3":{"0":{"4":{"docs":{},"]":{"docs":{},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"8":{"8":{"9":{"5":{"0":{"2":{"8":{"docs":{},",":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}},"1":{"3":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"docs":{}},"docs":{},".":{"0":{"8":{"0":{"4":{"3":{"7":{"5":{"9":{"docs":{},",":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"5":{"6":{"6":{"8":{"1":{"6":{"6":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}},"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00847457627118644}}}},"3":{"docs":{},".":{"0":{"0":{"7":{"0":{"6":{"2":{"7":{"7":{"docs":{},"]":{"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"1":{"8":{"2":{"2":{"6":{"9":{"docs":{},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"5":{"docs":{},".":{"2":{"docs":{},"]":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}},"8":{"3":{"4":{"1":{"6":{"6":{"6":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"9":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.013986013986013986},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}},"[":{"0":{"docs":{},".":{"5":{"2":{"5":{"2":{"5":{"2":{"5":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"1":{"4":{"7":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}},"docs":{}},"4":{"0":{"3":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}}}},"4":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}},"5":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.015151515151515152},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}},"'":{"docs":{},"a":{"docs":{},"j":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}},">":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}},"s":{"docs":{},"s":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.015706806282722512},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0029476787030213707},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.015625},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.00872093023255814},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.016},"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.07142857142857142},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.022222222222222223}}}}}}},",":{"docs":{},"b":{"docs":{},"公":{"docs":{},"式":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"与":{"docs":{},"b":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"列":{"docs":{},"相":{"docs":{},"乘":{"docs":{},"再":{"docs":{},"相":{"docs":{},"加":{"docs":{},"，":{"docs":{},"等":{"docs":{},"到":{"docs":{},"结":{"docs":{},"果":{"docs":{},"是":{"docs":{},"m":{"docs":{},"行":{"docs":{},"n":{"docs":{},"列":{"docs":{},"的":{"docs":{},"）":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"y":{"docs":{},"s":{"docs":{},"i":{"docs":{},"s":{"docs":{},"）":{"docs":{},"：":{"docs":{},"也":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"分":{"docs":{},"析":{"docs":{},"的":{"docs":{},"应":{"docs":{},"用":{"docs":{},"，":{"docs":{},"不":{"docs":{},"仅":{"docs":{},"是":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"，":{"docs":{},"也":{"docs":{},"是":{"docs":{},"统":{"docs":{},"计":{"docs":{},"学":{"docs":{},"的":{"docs":{},"经":{"docs":{},"典":{"docs":{},"算":{"docs":{},"法":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"x":{"docs":{},".":{"docs":{},"i":{"docs":{},"m":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"w":{"docs":{},"(":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},"[":{"docs":{},"i":{"docs":{},"]":{"docs":{},".":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"(":{"6":{"2":{"docs":{},",":{"4":{"7":{"docs":{},")":{"docs":{},",":{"docs":{},"c":{"docs":{},"m":{"docs":{},"a":{"docs":{},"p":{"docs":{},"=":{"docs":{},"'":{"docs":{},"b":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"'":{"docs":{},")":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}},"docs":{}},"8":{"docs":{},",":{"8":{"docs":{},")":{"docs":{},",":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"s":{"docs":{},":":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"轴":{"docs":{},"的":{"docs":{},"范":{"docs":{},"围":{"docs":{},"；":{"0":{"1":{"2":{"3":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"就":{"docs":{},"是":{"docs":{},"x":{"docs":{},"轴":{"docs":{},"和":{"docs":{},"y":{"docs":{},"轴":{"docs":{},"的":{"docs":{},"范":{"docs":{},"围":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}},"[":{"0":{"docs":{},"]":{"docs":{},")":{"docs":{},"*":{"1":{"0":{"0":{"docs":{},")":{"docs":{},")":{"docs":{},".":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"(":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}},"1":{"docs":{},"]":{"docs":{},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}},"2":{"docs":{},"]":{"docs":{},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435}},"*":{"1":{"0":{"0":{"docs":{},")":{"docs":{},")":{"docs":{},".":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"(":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}},"docs":{}},".":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}},"=":{"1":{"docs":{},")":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{},"[":{"0":{"docs":{},".":{"5":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"docs":{}}},"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}}}}}},"c":{"docs":{},"c":{"docs":{},"u":{"docs":{},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"y":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"e":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}}}}}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"a":{"docs":{},"g":{"docs":{},"e":{"docs":{},"=":{"docs":{},"\"":{"docs":{},"m":{"docs":{},"i":{"docs":{},"c":{"docs":{},"r":{"docs":{},"o":{"docs":{},"\"":{"docs":{},")":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}},"m":{"docs":{},"b":{"docs":{},"i":{"docs":{},"g":{"docs":{},"u":{"docs":{},"o":{"docs":{},"u":{"docs":{},"s":{"docs":{},".":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}},"d":{"docs":{},"a":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":1.689922480620155}},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}}}}}}}},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"s":{"docs":{},"t":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"a":{"docs":{},"l":{"docs":{},"g":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"m":{"docs":{},"=":{"docs":{},"'":{"docs":{},"s":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"e":{"docs":{},".":{"docs":{},"r":{"docs":{},"'":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}}}}}}}}}},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"_":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"d":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"h":{"docs":{},"=":{"2":{"docs":{},")":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}},"i":{"docs":{},"f":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}},"i":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":3.3619047619047615}}}}},"_":{"docs":{},"w":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"=":{"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198}}}}}}}}}}}}}}},"f":{"docs":{},"i":{"docs":{},"f":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}},"i":{"docs":{},"m":{"docs":{},"=":{"docs":{},"(":{"0":{"docs":{},",":{"1":{"6":{"docs":{},")":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}},"docs":{}},"docs":{}}},"docs":{}}}}}},"o":{"docs":{},"n":{"docs":{},"f":{"docs":{},"u":{"docs":{},"s":{"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":0.08333333333333333}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}},",":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}},"y":{"docs":{},"_":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},":":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.018691588785046728},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.017142857142857144},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02358490566037736},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}}}}}}},"l":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}},"l":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},"的":{"docs":{},"c":{"docs":{},"o":{"docs":{},"u":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"方":{"docs":{},"法":{"docs":{},"可":{"docs":{},"以":{"docs":{},"求":{"docs":{},"出":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"组":{"docs":{},"的":{"docs":{},"相":{"docs":{},"同":{"docs":{},"元":{"docs":{},"素":{"docs":{},"的":{"docs":{},"个":{"docs":{},"数":{"docs":{},"，":{"docs":{},"返":{"docs":{},"回":{"docs":{},"一":{"docs":{},"个":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"【":{"docs":{},"k":{"docs":{},"e":{"docs":{},"y":{"docs":{},"=":{"docs":{},"元":{"docs":{},"素":{"docs":{},"名":{"docs":{},"，":{"docs":{},"v":{"docs":{},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"=":{"docs":{},"元":{"docs":{},"素":{"docs":{},"个":{"docs":{},"数":{"docs":{},"】":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541}},"(":{"docs":{},"t":{"docs":{},"o":{"docs":{},"p":{"docs":{},"k":{"docs":{},"_":{"docs":{},"y":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}}}}}}},"{":{"0":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}},"docs":{}},"y":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}},".":{"docs":{},"v":{"docs":{},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"s":{"docs":{},"(":{"docs":{},")":{"docs":{},":":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}}}}},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}},"o":{"docs":{},"k":{"docs":{},"'":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"d":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}},"e":{"docs":{},"f":{"0":{"docs":{},"=":{"0":{"docs":{},".":{"0":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}},"docs":{}}},"docs":{}}},"docs":{}}}},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"i":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}}}}}}}}}},"c":{"docs":{},"h":{"docs":{},"e":{"docs":{},"_":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"2":{"0":{"0":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}},"p":{"docs":{},"u":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.025974025974025976},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}}},"u":{"docs":{},"r":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.013986013986013986}}}}},"v":{"docs":{},"e":{"docs":{},"，":{"docs":{},"描":{"docs":{},"述":{"docs":{},"t":{"docs":{},"p":{"docs":{},"r":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"m":{"docs":{},"_":{"docs":{},"c":{"docs":{},"m":{"docs":{},"a":{"docs":{},"p":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"p":{"docs":{},"=":{"docs":{},"'":{"docs":{},"b":{"docs":{},"i":{"docs":{},"n":{"docs":{},"a":{"docs":{},"r":{"docs":{},"y":{"docs":{},"'":{"docs":{},",":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}},"p":{"docs":{},"l":{"docs":{},"t":{"docs":{},".":{"docs":{},"c":{"docs":{},"m":{"docs":{},".":{"docs":{},"g":{"docs":{},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{},")":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}}}}}}}}}}}}}}}}},"r":{"docs":{},"o":{"docs":{},"s":{"docs":{},"s":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}},"_":{"docs":{},"v":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},"e":{"docs":{},"(":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}},",":{"docs":{},"c":{"docs":{},"v":{"docs":{},"=":{"3":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"=":{"docs":{},"\"":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{},"y":{"docs":{},"\"":{"docs":{},")":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}}}}}}}},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"\"":{"docs":{},")":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}},"'":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{},"y":{"docs":{},"'":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}}}}}}}},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"'":{"docs":{},",":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}}}}},"a":{"docs":{},"t":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":5.0049504950495045}},"：":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}},"v":{"docs":{},"默":{"docs":{},"认":{"docs":{},"为":{"3":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"修":{"docs":{},"改":{"docs":{},"改":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"修":{"docs":{},"改":{"docs":{},"修":{"docs":{},"改":{"docs":{},"不":{"docs":{},"同":{"docs":{},"分":{"docs":{},"数":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}},"=":{"0":{"docs":{},".":{"1":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.005714285714285714}},")":{"docs":{},":":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}},"docs":{}}},"1":{"docs":{},".":{"0":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}},"docs":{}}},"docs":{}},"·":{"docs":{},"j":{"docs":{},"(":{"docs":{},"θ":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.005714285714285714}}}}}}},"g":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}},"\"":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}}}},"'":{"docs":{},",":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}},"h":{"docs":{},"a":{"docs":{},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}},"n":{"docs":{},"g":{"docs":{},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}},"e":{"docs":{},"c":{"docs":{},"k":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}},"f":{"docs":{},"m":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}}},"放":{"docs":{},"小":{"docs":{},"之":{"docs":{},"后":{"docs":{},"，":{"docs":{},"容":{"docs":{},"错":{"docs":{},"空":{"docs":{},"间":{"docs":{},"变":{"docs":{},"大":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"犯":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"错":{"docs":{},"误":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}}}}}}}}},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"越":{"docs":{},"偏":{"docs":{},"向":{"docs":{},"于":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"h":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}}}},"d":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.01171875},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}},"s":{"docs":{},"e":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"s":{"docs":{},".":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{},"_":{"docs":{},"b":{"docs":{},"o":{"docs":{},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},")":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}}}}}},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"s":{"docs":{},"(":{"docs":{},")":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}},"i":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},"(":{"docs":{},")":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"k":{"docs":{},"e":{"docs":{},"_":{"docs":{},"m":{"docs":{},"o":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},"(":{"docs":{},")":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}},"n":{"docs":{},"o":{"docs":{},"i":{"docs":{},"s":{"docs":{},"e":{"docs":{},"=":{"0":{"docs":{},".":{"1":{"5":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}},"docs":{}},"2":{"5":{"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"docs":{}},"docs":{}}},"docs":{}}}}}},"_":{"docs":{},"s":{"docs":{},"a":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"=":{"5":{"0":{"0":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"可":{"docs":{},"以":{"docs":{},"用":{"docs":{},"来":{"docs":{},"加":{"docs":{},"载":{"docs":{},"真":{"docs":{},"实":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"模":{"docs":{},"型":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}}}}},"\"":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}},"）":{"docs":{},"，":{"docs":{},"只":{"docs":{},"使":{"docs":{},"用":{"docs":{},"分":{"docs":{},"类":{"docs":{},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{},"是":{"docs":{},"远":{"docs":{},"远":{"docs":{},"不":{"docs":{},"够":{"docs":{},"的":{"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"提":{"docs":{},"供":{"docs":{},"了":{"docs":{},"降":{"docs":{},"维":{"docs":{},"相":{"docs":{},"关":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"s":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}},"e":{"docs":{},"s":{"docs":{},")":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.025}}}}}}}}},"f":{"docs":{},"u":{"docs":{},"n":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"'":{"docs":{},"o":{"docs":{},"v":{"docs":{},"r":{"docs":{},"'":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"_":{"docs":{},"w":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"=":{"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"h":{"docs":{},"=":{"2":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},")":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"docs":{}}}}}}},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"f":{"docs":{},"_":{"docs":{},"n":{"docs":{},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"s":{"docs":{},"=":{"4":{"docs":{},")":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"docs":{}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"a":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"_":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"f":{"docs":{},"=":{"6":{"docs":{},")":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"docs":{}}}}}},"s":{"docs":{},"p":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"=":{"1":{"0":{"docs":{},")":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}},")":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},")":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},"_":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"=":{"6":{"6":{"6":{"docs":{},")":{"docs":{},")":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}},"(":{"docs":{},")":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}},"c":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"=":{"docs":{},"'":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"'":{"docs":{},",":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"f":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.01953125},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.04411764705882353},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.03773584905660377},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.014534883720930232},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.022727272727272728},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.027472527472527472},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.028112449799196786},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.021739130434782608},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.032},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.048},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.03496503496503497},"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.07142857142857142},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.016},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.017142857142857144},"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888},"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.03508771929824561},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.03888888888888889},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.014778325123152709},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.016736401673640166},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.01639344262295082},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}},"v":{"docs":{},".":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}}},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}},"_":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"从":{"2":{"docs":{},"到":{"1":{"0":{"docs":{},"到":{"1":{"0":{"0":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"虽":{"docs":{},"然":{"docs":{},"均":{"docs":{},"方":{"docs":{},"误":{"docs":{},"差":{"docs":{},"是":{"docs":{},"越":{"docs":{},"来":{"docs":{},"越":{"docs":{},"小":{"docs":{},"的":{"docs":{},"，":{"docs":{},"从":{"docs":{},"均":{"docs":{},"方":{"docs":{},"误":{"docs":{},"差":{"docs":{},"的":{"docs":{},"角":{"docs":{},"度":{"docs":{},"来":{"docs":{},"看":{"docs":{},"是":{"docs":{},"更":{"docs":{},"加":{"docs":{},"小":{"docs":{},"的":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}}},"docs":{}},"=":{"3":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}},"docs":{}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},":":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.020942408376963352}},"e":{"docs":{},"s":{"docs":{},".":{"docs":{},"a":{"docs":{},"p":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"(":{"docs":{},"d":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"s":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}}}},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"w":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}},"w":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}},"s":{"docs":{},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}},",":{"docs":{},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"s":{"docs":{},".":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"p":{"docs":{},"y":{"docs":{},"(":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}}}}}}},"f":{"docs":{},"f":{"docs":{},":":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}},"t":{"docs":{},"(":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}},"w":{"docs":{},"n":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.027210884353741496}}}}}},"_":{"docs":{},"i":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}},"j":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}},"_":{"docs":{},"s":{"docs":{},"g":{"docs":{},"d":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},"[":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"_":{"docs":{},"i":{"docs":{},"]":{"docs":{},",":{"docs":{},"y":{"docs":{},"[":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"_":{"docs":{},"i":{"docs":{},"]":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}}}}}}}}},"_":{"docs":{},"i":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"i":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}}}}}}}}}}}}},"_":{"docs":{},"j":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"b":{"docs":{},"u":{"docs":{},"g":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},",":{"docs":{},"y":{"docs":{},",":{"docs":{},"e":{"docs":{},"p":{"docs":{},"s":{"docs":{},"i":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{},"=":{"0":{"docs":{},".":{"0":{"1":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"通":{"docs":{},"用":{"docs":{},"的":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"放":{"docs":{},"在":{"docs":{},"任":{"docs":{},"何":{"docs":{},"求":{"docs":{},"导":{"docs":{},"的":{"docs":{},"d":{"docs":{},"e":{"docs":{},"b":{"docs":{},"u":{"docs":{},"g":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"可":{"docs":{},"以":{"docs":{},"作":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"工":{"docs":{},"具":{"docs":{},"箱":{"docs":{},"来":{"docs":{},"使":{"docs":{},"用":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}}}}}}}}},"f":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"b":{"docs":{},"u":{"docs":{},"g":{"docs":{},"(":{"docs":{},"w":{"docs":{},",":{"docs":{},"x":{"docs":{},",":{"docs":{},"e":{"docs":{},"p":{"docs":{},"s":{"docs":{},"i":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{},"=":{"0":{"docs":{},".":{"0":{"0":{"0":{"1":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},"(":{"docs":{},"w":{"docs":{},",":{"docs":{},"x":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}}}}},"(":{"docs":{},"w":{"docs":{},",":{"docs":{},"x":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}},"t":{"docs":{},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"'":{"docs":{},"#":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"'":{"docs":{},")":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}},"_":{"docs":{},"c":{"docs":{},"f":{"docs":{},"l":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}},"l":{"docs":{},"f":{"2":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}},"3":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}},"4":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}},"5":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}},"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"a":{"docs":{},"l":{"docs":{},"=":{"docs":{},"f":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},"e":{"docs":{},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}}}}}}}},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541}}}},"f":{"1":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":3.341333333333333}},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}},"(":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.04}}}}}}}}}}}},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}}}}},"docs":{},"e":{"docs":{},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{},"_":{"docs":{},"l":{"docs":{},"f":{"docs":{},"w":{"docs":{},"_":{"docs":{},"p":{"docs":{},"e":{"docs":{},"o":{"docs":{},"p":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}},"e":{"docs":{},"用":{"docs":{},"于":{"docs":{},"加":{"docs":{},"载":{"docs":{},"人":{"docs":{},"脸":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}},"(":{"docs":{},")":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"f":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"_":{"docs":{},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"_":{"docs":{},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"o":{"docs":{},"n":{"docs":{},"=":{"6":{"0":{"docs":{},")":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"l":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}},"用":{"docs":{},"于":{"docs":{},"加":{"docs":{},"载":{"docs":{},"m":{"docs":{},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}},"(":{"docs":{},"\"":{"docs":{},"m":{"docs":{},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}},"i":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.00872093023255814},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.014234875444839857}},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}},"x":{"docs":{},",":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"=":{"0":{"docs":{},".":{"0":{"1":{"docs":{},",":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"=":{"1":{"docs":{},"e":{"4":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}},"docs":{}}},"docs":{}}}}}}}}}}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}},"_":{"docs":{},"p":{"docs":{},"a":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{},"s":{"docs":{},"=":{"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"c":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}},"n":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"l":{"docs":{},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}},"g":{"docs":{},"d":{"docs":{},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}}}}},"s":{"docs":{},"g":{"docs":{},"d":{"docs":{},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"_":{"docs":{},"s":{"docs":{},"g":{"docs":{},"d":{"docs":{},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}}},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"】":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}}}}}}}},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"h":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"t":{"docs":{},"(":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}},"x":{"2":{"docs":{},",":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"w":{"docs":{},",":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}},"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},",":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"w":{"docs":{},",":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},",":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"w":{"docs":{},",":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"t":{"docs":{},"(":{"2":{"docs":{},",":{"docs":{},"x":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}},"docs":{},"n":{"docs":{},",":{"docs":{},"x":{"docs":{},",":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}}}}}},"g":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}},"l":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}},"o":{"docs":{},"r":{"docs":{},"循":{"docs":{},"环":{"docs":{},"方":{"docs":{},"式":{"docs":{},"实":{"docs":{},"现":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}},"l":{"docs":{},"d":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.010676156583629894}}},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.018691588785046728},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.017142857142857144},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02358490566037736},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}}}}}},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{},"(":{"docs":{},"'":{"docs":{},"i":{"docs":{},"n":{"docs":{},"f":{"docs":{},"'":{"docs":{},")":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}},"(":{"docs":{},"w":{"docs":{},",":{"docs":{},"x":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}},"_":{"2":{"docs":{},",":{"docs":{},"x":{"docs":{},")":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}},"docs":{}}},"*":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"s":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}},"s":{"2":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{},"f":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"2":{"docs":{},".":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}},"docs":{}}}}}}}}}}}}},"docs":{},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{},"f":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},".":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}},"[":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},"s":{"docs":{},"]":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"g":{"docs":{},"e":{"docs":{},"s":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},"_":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}},"l":{"docs":{},"s":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}},"e":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}},"n":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}}}}}}}}}},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}},"p":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{},"]":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}}}}}}}}}}}}},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}},"r":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}},"s":{"docs":{},",":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}},".":{"docs":{},"a":{"docs":{},"p":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"(":{"docs":{},"f":{"docs":{},"p":{"docs":{},"r":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}}},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"关":{"docs":{},"系":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}},"都":{"docs":{},"是":{"docs":{},"逐":{"docs":{},"渐":{"docs":{},"升":{"docs":{},"高":{"docs":{},"的":{"docs":{},"，":{"docs":{},"换":{"docs":{},"句":{"docs":{},"话":{"docs":{},"说":{"docs":{},"，":{"docs":{},"t":{"docs":{},"p":{"docs":{},"r":{"docs":{},"和":{"docs":{},"f":{"docs":{},"p":{"docs":{},"r":{"docs":{},"之":{"docs":{},"间":{"docs":{},"是":{"docs":{},"存":{"docs":{},"在":{"docs":{},"着":{"docs":{},"相":{"docs":{},"一":{"docs":{},"致":{"docs":{},"的":{"docs":{},"关":{"docs":{},"系":{"docs":{},"的":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"：":{"docs":{},"预":{"docs":{},"测":{"docs":{},"为":{"1":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"预":{"docs":{},"测":{"docs":{},"错":{"docs":{},"了":{"docs":{},"的":{"docs":{},"数":{"docs":{},"量":{"docs":{},"占":{"docs":{},"真":{"docs":{},"实":{"docs":{},"值":{"docs":{},"为":{"0":{"docs":{},"的":{"docs":{},"百":{"docs":{},"分":{"docs":{},"比":{"docs":{},"是":{"docs":{},"多":{"docs":{},"少":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}}}},"u":{"docs":{},"n":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}},"）":{"docs":{},"，":{"docs":{},"也":{"docs":{},"叫":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"技":{"docs":{},"巧":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}},"g":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}},"s":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{},"c":{"docs":{},"v":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},"用":{"docs":{},"于":{"docs":{},"进":{"docs":{},"行":{"docs":{},"参":{"docs":{},"数":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"，":{"docs":{},"寻":{"docs":{},"找":{"docs":{},"合":{"docs":{},"适":{"docs":{},"的":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}},"(":{"docs":{},"c":{"docs":{},"v":{"docs":{},"=":{"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{},"p":{"docs":{},"a":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{},"_":{"docs":{},"g":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}},",":{"docs":{},"n":{"docs":{},"_":{"docs":{},"j":{"docs":{},"o":{"docs":{},"b":{"docs":{},"s":{"docs":{},"=":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"b":{"docs":{},"o":{"docs":{},"s":{"docs":{},"e":{"docs":{},"=":{"1":{"docs":{},",":{"docs":{},"c":{"docs":{},"v":{"docs":{},"=":{"3":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}},"docs":{}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},",":{"docs":{},"p":{"docs":{},"a":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{},"_":{"docs":{},"g":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},",":{"docs":{},"n":{"docs":{},"_":{"docs":{},"j":{"docs":{},"o":{"docs":{},"b":{"docs":{},"s":{"docs":{},"=":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"里":{"docs":{},"的":{"docs":{},"c":{"docs":{},"v":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"就":{"docs":{},"是":{"docs":{},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"_":{"docs":{},"k":{"docs":{},"w":{"docs":{},"=":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"h":{"docs":{},"s":{"docs":{},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"=":{"0":{"docs":{},".":{"1":{"docs":{},",":{"docs":{},"w":{"docs":{},"s":{"docs":{},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"=":{"0":{"docs":{},".":{"1":{"docs":{},")":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}},"docs":{}}},"docs":{}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"_":{"docs":{},"s":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},".":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"_":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},".":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"a":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{},"s":{"docs":{},"_":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"_":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"d":{"docs":{},"i":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.01606425702811245},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":1.689922480620155}},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"s":{"docs":{},"c":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"(":{"0":{"docs":{},".":{"docs":{},",":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.03787878787878788}}},",":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}}}},"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}}}}}}}}}}}}}}},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}},"d":{"docs":{},"_":{"docs":{},"j":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},",":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"b":{"docs":{},"u":{"docs":{},"g":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},",":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},",":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"s":{"docs":{},"c":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"(":{"docs":{},"d":{"docs":{},"f":{"docs":{},",":{"docs":{},"x":{"docs":{},",":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"w":{"docs":{},",":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"b":{"docs":{},"u":{"docs":{},"g":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},",":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"w":{"docs":{},",":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},",":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"w":{"docs":{},",":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"c":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"=":{"docs":{},"'":{"docs":{},"f":{"docs":{},"r":{"docs":{},"i":{"docs":{},"e":{"docs":{},"d":{"docs":{},"m":{"docs":{},"a":{"docs":{},"n":{"docs":{},"_":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"'":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"h":{"docs":{},"=":{"2":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"h":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}},"u":{"docs":{},"i":{"docs":{},"d":{"docs":{},"e":{"docs":{},"是":{"docs":{},"对":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"概":{"docs":{},"述":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"。":{"docs":{},"a":{"docs":{},"p":{"docs":{},"i":{"docs":{},"是":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"使":{"docs":{},"用":{"docs":{},"文":{"docs":{},"档":{"docs":{},"（":{"docs":{},"也":{"docs":{},"可":{"docs":{},"以":{"docs":{},"在":{"docs":{},"首":{"docs":{},"页":{"docs":{},"大":{"docs":{},"搜":{"docs":{},"中":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"）":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.02}},"=":{"1":{"0":{"0":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}},"docs":{}},"docs":{},".":{"0":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}},"docs":{}}},"docs":{},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},")":{"docs":{},")":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}},"越":{"docs":{},"低":{"docs":{},"，":{"docs":{},"模":{"docs":{},"型":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"度":{"docs":{},"越":{"docs":{},"低":{"docs":{},"，":{"docs":{},"欠":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"程":{"docs":{},"度":{"docs":{},"越":{"docs":{},"低":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}}}}}}}}}}},"高":{"docs":{},"，":{"docs":{},"模":{"docs":{},"型":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"度":{"docs":{},"越":{"docs":{},"高":{"docs":{},"，":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"程":{"docs":{},"度":{"docs":{},"越":{"docs":{},"高":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"a":{"docs":{},"n":{"docs":{},"(":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{},"l":{"1":{"docs":{},")":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}},"2":{"docs":{},")":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}},"docs":{}}}}}}},"x":{"docs":{},",":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"(":{"docs":{},"y":{"1":{"docs":{},"_":{"docs":{},"l":{"docs":{},")":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"r":{"docs":{},")":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}},"2":{"docs":{},"_":{"docs":{},"l":{"docs":{},")":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"r":{"docs":{},")":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}},"docs":{},")":{"docs":{},":":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"_":{"docs":{},"l":{"docs":{},")":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"r":{"docs":{},")":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}},"i":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.14285714285714285},"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571},"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.03664921465968586},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.019067796610169493},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.006632277081798084},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.041237113402061855},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.01171875},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.03773584905660377},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.02616279069767442},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.015151515151515152},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.016483516483516484},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.022653721682847898},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.025974025974025976},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.03636363636363636},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.027210884353741496},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.037037037037037035},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.020202020202020204},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.037383177570093455},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.064},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.021352313167259787},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.05142857142857143},"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888},"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.07142857142857142},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.021929824561403508},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.04},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.022857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0365296803652968},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.027777777777777776},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.04},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.05755395683453238},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0375},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.018666666666666668},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0196078431372549},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.03940886699507389},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.02},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0330188679245283},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.09090909090909091},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.044642857142857144},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.05714285714285714},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.029288702928870293},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.028688524590163935},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.07352941176470588},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.060810810810810814},"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.038461538461538464},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.03488372093023256}}}}}},"a":{"docs":{},"g":{"docs":{},"e":{"docs":{},"s":{"docs":{},"是":{"docs":{},"将":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"以":{"docs":{},"一":{"docs":{},"个":{"docs":{},"二":{"docs":{},"维":{"docs":{},"平":{"docs":{},"面":{"docs":{},"可":{"docs":{},"视":{"docs":{},"化":{"docs":{},"的":{"docs":{},"角":{"docs":{},"度":{"docs":{},"展":{"docs":{},"现":{"docs":{},"出":{"docs":{},"来":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"t":{"docs":{},"(":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}},"e":{"docs":{},"r":{"docs":{},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"=":{"docs":{},"'":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"'":{"docs":{},",":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"o":{"docs":{},"n":{"docs":{},"l":{"docs":{},"y":{"docs":{},"=":{"docs":{},"f":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},"e":{"docs":{},")":{"docs":{},")":{"docs":{},",":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}}}}}}}}}},"c":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"=":{"1":{"docs":{},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}},".":{"0":{"docs":{},",":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.01606425702811245},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}},":":{"docs":{},"初":{"docs":{},"始":{"docs":{},"化":{"docs":{},"的":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"值":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.012048192771084338},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.016304347826086956},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}},"w":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"w":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}},"=":{"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}},"_":{"docs":{},"a":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}},"i":{"docs":{},"c":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}},"c":{"docs":{},"l":{"docs":{},"u":{"docs":{},"d":{"docs":{},"e":{"docs":{},"_":{"docs":{},"b":{"docs":{},"i":{"docs":{},"a":{"docs":{},"s":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}}}}},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},".":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}}}}},"r":{"docs":{},"i":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},"s":{"docs":{},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}},"[":{"docs":{},":":{"docs":{},",":{"2":{"docs":{},":":{"docs":{},"]":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}},"docs":{},":":{"2":{"docs":{},"]":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}},"docs":{}}}}}}}}},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}},"i":{"docs":{},"d":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.008771929824561403}},"e":{"docs":{},"r":{"docs":{},"s":{"2":{"docs":{},".":{"2":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}},"docs":{}}},"docs":{}}}}}}},"]":{"docs":{},")":{"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099}}}}},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}},",":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}},"a":{"docs":{},"x":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}},"l":{"docs":{},"g":{"docs":{},"a":{"docs":{},"u":{"docs":{},"s":{"docs":{},"k":{"docs":{},"a":{"docs":{},"s":{"docs":{},"'":{"docs":{},"]":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}},"d":{"3":{"docs":{},"，":{"docs":{},"c":{"4":{"docs":{},".":{"5":{"docs":{},"，":{"docs":{},"c":{"5":{"docs":{},".":{"0":{"docs":{},"等":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"其":{"docs":{},"他":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"实":{"docs":{},"现":{"docs":{},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}},"docs":{}}},"docs":{}}}},"docs":{}}},"k":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.015706806282722512},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.023809523809523808},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}},"e":{"docs":{},"r":{"docs":{},"是":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"算":{"docs":{},"法":{"docs":{},"解":{"docs":{},"决":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}},"(":{"docs":{},"a":{"docs":{},"l":{"docs":{},"g":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"m":{"docs":{},"=":{"docs":{},"'":{"docs":{},"a":{"docs":{},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{},"'":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"_":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"=":{"3":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"5":{"0":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"6":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}},"docs":{},"k":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}},",":{"docs":{},"w":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"s":{"docs":{},"=":{"docs":{},"'":{"docs":{},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"'":{"docs":{},",":{"docs":{},"p":{"docs":{},"=":{"docs":{},"p":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}}}},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"d":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}},"w":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"s":{"docs":{},"=":{"docs":{},"'":{"docs":{},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"'":{"docs":{},",":{"docs":{},"n":{"docs":{},"_":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"=":{"2":{"docs":{},",":{"docs":{},"p":{"docs":{},"=":{"2":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}},"docs":{}}}}},"docs":{},"k":{"docs":{},",":{"docs":{},"p":{"docs":{},"=":{"docs":{},"p":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},"(":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}},"e":{"docs":{},"r":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}},"(":{"docs":{},"k":{"docs":{},"=":{"6":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"docs":{}}}}}}}}}}}}}},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"封":{"docs":{},"装":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.01984126984126984},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.017793594306049824},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.015873015873015872},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"_":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"i":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"docs":{},":":{"2":{"docs":{},"]":{"docs":{},",":{"docs":{},"i":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},".":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"另":{"docs":{},"外":{"docs":{},"一":{"docs":{},"个":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"：":{"docs":{},"距":{"docs":{},"离":{"docs":{},"的":{"docs":{},"权":{"docs":{},"重":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}},"可":{"docs":{},"以":{"docs":{},"估":{"docs":{},"算":{"docs":{},"规":{"docs":{},"律":{"docs":{},"，":{"docs":{},"结":{"docs":{},"果":{"docs":{},"占":{"docs":{},"离":{"docs":{},"他":{"docs":{},"最":{"docs":{},"近":{"docs":{},"的":{"docs":{},"k":{"docs":{},"个":{"docs":{},"点":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}}}}}},"近":{"docs":{},"邻":{"docs":{},"算":{"docs":{},"法":{"docs":{},"是":{"docs":{},"非":{"docs":{},"常":{"docs":{},"特":{"docs":{},"殊":{"docs":{},"的":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"被":{"docs":{},"认":{"docs":{},"为":{"docs":{},"是":{"docs":{},"没":{"docs":{},"有":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}}}}}}}}}}}}},":":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"为":{"docs":{},"k":{"docs":{},"近":{"docs":{},"邻":{"docs":{},"中":{"docs":{},"的":{"docs":{},"寻":{"docs":{},"找":{"docs":{},"k":{"docs":{},"个":{"docs":{},"最":{"docs":{},"近":{"docs":{},"元":{"docs":{},"素":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}}}}}}}}}}}}}}},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.018691588785046728},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.017142857142857144},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02358490566037736},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}}}}},"越":{"docs":{},"小":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"就":{"docs":{},"越":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"，":{"docs":{},"在":{"docs":{},"这":{"docs":{},"里":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"视":{"docs":{},"化":{"docs":{},"的":{"docs":{},"看":{"docs":{},"到":{"docs":{},"了":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"的":{"docs":{},"含":{"docs":{},"义":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"e":{"docs":{},"l":{"docs":{},"）":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"b":{"docs":{},"f":{"docs":{},"'":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}}}}}}}}}}}}},"l":{"1":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}},",":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}},"2":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}},"范":{"docs":{},"数":{"docs":{},"（":{"docs":{},"欧":{"docs":{},"拉":{"docs":{},"距":{"docs":{},"离":{"docs":{},"|":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}},"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{},"/":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{},"e":{"docs":{},"/":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}},"d":{"docs":{},"o":{"docs":{},"c":{"docs":{},"u":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{},"h":{"docs":{},"t":{"docs":{},"m":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"/":{"docs":{},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"d":{"docs":{},"/":{"docs":{},"s":{"docs":{},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},".":{"docs":{},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{},".":{"docs":{},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},".":{"docs":{},"h":{"docs":{},"t":{"docs":{},"m":{"docs":{},"l":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"中":{"docs":{},"提":{"docs":{},"供":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{"./":{"ref":"./","tf":0.01098901098901099}}}}}}},"的":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"曲":{"docs":{},"线":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}},"混":{"docs":{},"淆":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"，":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}},",":{"docs":{},"f":{"1":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}},"docs":{}}}}}}}}}}}}}}},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"于":{"docs":{},"p":{"docs":{},"i":{"docs":{},"p":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":5.005050505050505}}}}}}}}}}}},"对":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"预":{"docs":{},"处":{"docs":{},"理":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}},"逻":{"docs":{},"辑":{"docs":{},"会":{"docs":{},"回":{"docs":{},"归":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":5.005714285714285}}}}}}},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":3.3361344537815123}}}}}},"使":{"docs":{},"用":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}},"默":{"docs":{},"认":{"docs":{},"使":{"docs":{},"用":{"docs":{},"基":{"docs":{},"尼":{"docs":{},"系":{"docs":{},"数":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}},"对":{"docs":{},"于":{"docs":{},"并":{"docs":{},"行":{"docs":{},"处":{"docs":{},"理":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"传":{"docs":{},"入":{"docs":{},"n":{"docs":{},"_":{"docs":{},"j":{"docs":{},"o":{"docs":{},"b":{"docs":{},"s":{"docs":{},"来":{"docs":{},"调":{"docs":{},"整":{"docs":{},"使":{"docs":{},"用":{"docs":{},"几":{"docs":{},"个":{"docs":{},"核":{"docs":{},"来":{"docs":{},"处":{"docs":{},"理":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"和":{"docs":{},"n":{"docs":{},"u":{"docs":{},"m":{"docs":{},"p":{"docs":{},"y":{"docs":{},"为":{"docs":{},"技":{"docs":{},"术":{"docs":{},"栈":{"docs":{},"，":{"docs":{},"学":{"docs":{},"习":{"docs":{},"了":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"入":{"docs":{},"门":{"docs":{},"的":{"docs":{},"基":{"docs":{},"本":{"docs":{},"算":{"docs":{},"法":{"docs":{},"，":{"docs":{},"并":{"docs":{},"自":{"docs":{},"己":{"docs":{},"实":{"docs":{},"现":{"docs":{},"了":{"docs":{},"部":{"docs":{},"分":{"docs":{},"s":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.01098901098901099}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"官":{"docs":{},"网":{"docs":{},"：":{"docs":{},"h":{"docs":{},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"s":{"docs":{},"c":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}},"算":{"docs":{},"法":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"_":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"(":{"docs":{},"c":{"docs":{},"u":{"docs":{},"r":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}},"e":{"docs":{},"r":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}},"t":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}},"=":{"0":{"docs":{},".":{"1":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}},"docs":{}}},"1":{"docs":{},".":{"0":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}},"docs":{}}},"docs":{}}}}}}}}}},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"实":{"docs":{},"现":{"docs":{},"：":{"docs":{},"c":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}},"封":{"docs":{},"装":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"解":{"docs":{},"决":{"docs":{},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}}}}}}}}}},"：":{"docs":{},"o":{"docs":{},"o":{"docs":{},"b":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"_":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}}}}}}},"f":{"docs":{},"_":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"3":{"0":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}}},"docs":{}},"docs":{}}}}}}}}},"n":{"docs":{},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},".":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"_":{"docs":{},")":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}},"c":{"docs":{},"o":{"docs":{},"e":{"docs":{},"f":{"docs":{},"_":{"docs":{},")":{"docs":{},",":{"docs":{},"\\":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}},"b":{"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.02197802197802198},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{},",":{"docs":{},"\\":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}},"u":{"docs":{},"e":{"docs":{},")":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.058823529411764705},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.022222222222222223}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.022222222222222223}},",":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353}}}}}}}}}}}},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099}}}}}}}}}},"f":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},".":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},"_":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"s":{"docs":{},")":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.015625}},"_":{"docs":{},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{},"提":{"docs":{},"供":{"docs":{},"了":{"docs":{},"线":{"docs":{},"性":{"docs":{},"模":{"docs":{},"型":{"docs":{},"相":{"docs":{},"关":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"是":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}},"(":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.024691358024691357},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}},")":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429}}}}}},"c":{"docs":{},"o":{"docs":{},"p":{"docs":{},"y":{"docs":{},"_":{"docs":{},"x":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}},":":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}},"(":{"docs":{},"c":{"docs":{},"=":{"0":{"docs":{},".":{"0":{"1":{"docs":{},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}},"docs":{}},"1":{"docs":{},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}},"docs":{}}},"1":{"0":{"0":{"0":{"0":{"0":{"0":{"0":{"0":{"0":{"docs":{},".":{"0":{"docs":{},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},"e":{"9":{"docs":{},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}},"docs":{}}},"docs":{},"c":{"docs":{},")":{"docs":{},")":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}},"(":{"docs":{},"c":{"docs":{},"=":{"1":{"docs":{},".":{"0":{"docs":{},",":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}},"docs":{}}},"docs":{}}},"e":{"docs":{},"p":{"docs":{},"s":{"docs":{},"i":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{},"=":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}},"s":{"docs":{},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}}}},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"2":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"e":{"docs":{},"f":{"docs":{},"_":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"2":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}},"docs":{}}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"c":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"_":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"2":{"docs":{},")":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}},"docs":{}}}}}}}}}}}},"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"e":{"docs":{},"f":{"docs":{},"_":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}},",":{"docs":{},"y":{"docs":{},")":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}},"_":{"docs":{},"g":{"docs":{},"d":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"c":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"_":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}},",":{"docs":{},"y":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}}},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":0.1}},"的":{"docs":{},"元":{"docs":{},"素":{"docs":{},"可":{"docs":{},"以":{"docs":{},"存":{"docs":{},"任":{"docs":{},"何":{"docs":{},"类":{"docs":{},"型":{"docs":{},"，":{"docs":{},"在":{"docs":{},"灵":{"docs":{},"活":{"docs":{},"度":{"docs":{},"提":{"docs":{},"升":{"docs":{},"的":{"docs":{},"同":{"docs":{},"时":{"docs":{},"，":{"docs":{},"也":{"docs":{},"导":{"docs":{},"致":{"docs":{},"性":{"docs":{},"能":{"docs":{},"下":{"docs":{},"降":{"docs":{},"了":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":0.1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"d":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"p":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}},"(":{"docs":{},"[":{"docs":{},"'":{"docs":{},"#":{"docs":{},"e":{"docs":{},"f":{"9":{"docs":{},"a":{"9":{"docs":{},"a":{"docs":{},"'":{"docs":{},",":{"docs":{},"'":{"docs":{},"#":{"docs":{},"f":{"docs":{},"f":{"docs":{},"f":{"5":{"9":{"docs":{},"d":{"docs":{},"'":{"docs":{},",":{"docs":{},"'":{"docs":{},"#":{"9":{"0":{"docs":{},"c":{"docs":{},"a":{"docs":{},"f":{"9":{"docs":{},"'":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}},"docs":{}}}}},"docs":{}},"docs":{}}}}}}},"docs":{}},"docs":{}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"g":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"是":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{},"，":{"docs":{},"默":{"docs":{},"认":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"l":{"2":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}},"docs":{}}}}}}}}}}}}}}}},"(":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},")":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}},"c":{"docs":{},"=":{"0":{"docs":{},".":{"1":{"docs":{},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.005714285714285714}}}},"docs":{}}},"1":{"docs":{},".":{"0":{"docs":{},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.008571428571428572},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}}}},"docs":{}}},"docs":{}}},"m":{"docs":{},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"i":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"=":{"docs":{},"'":{"docs":{},"m":{"docs":{},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"'":{"docs":{},",":{"docs":{},"s":{"docs":{},"o":{"docs":{},"l":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"=":{"docs":{},"\"":{"docs":{},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{},"t":{"docs":{},"o":{"docs":{},"n":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},":":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}}}}},"(":{"0":{"docs":{},")":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"没":{"docs":{},"有":{"docs":{},"任":{"docs":{},"何":{"docs":{},"损":{"docs":{},"失":{"docs":{},"。":{"docs":{"逻辑回归/2.逻辑回归的损失函数.html":{"ref":"逻辑回归/2.逻辑回归的损失函数.html","tf":0.09090909090909091}}}}}}}}}}}}}},"1":{"docs":{"逻辑回归/2.逻辑回归的损失函数.html":{"ref":"逻辑回归/2.逻辑回归的损失函数.html","tf":0.09090909090909091}}},"docs":{},"p":{"docs":{},"^":{"docs":{},")":{"docs":{},".":{"docs":{},"当":{"docs":{},"y":{"docs":{},"=":{"0":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"前":{"docs":{},"半":{"docs":{},"部":{"docs":{},"分":{"docs":{},"是":{"0":{"docs":{},"，":{"docs":{},"就":{"docs":{},"只":{"docs":{},"剩":{"docs":{},"下":{"docs":{"逻辑回归/2.逻辑回归的损失函数.html":{"ref":"逻辑回归/2.逻辑回归的损失函数.html","tf":0.09090909090909091}}}}}}}},"docs":{}}}}}}}}}}},"docs":{}}}}}}}}},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"2":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}}}}}}}}}}}}}}}}}}}}}}}},"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"e":{"docs":{},"f":{"docs":{},"_":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}},"[":{"0":{"docs":{},"]":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"1":{"docs":{},"]":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}}}}}}},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}}}},",":{"docs":{},"y":{"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"c":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"b":{"docs":{},"a":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}},"[":{"docs":{},":":{"1":{"0":{"docs":{},"]":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}}},",":{"docs":{},"y":{"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"f":{"docs":{},"u":{"docs":{},"n":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}},"[":{"docs":{},":":{"1":{"0":{"docs":{},"]":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"p":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.015625}}}},"s":{"docs":{},"s":{"docs":{},"=":{"docs":{},"'":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"h":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"e":{"docs":{},"'":{"docs":{},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}}}}},"e":{"docs":{},"p":{"docs":{},"s":{"docs":{},"i":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},"'":{"docs":{},",":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"v":{"docs":{},"i":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"'":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}}}}},"a":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"'":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}},"s":{"docs":{},"s":{"docs":{},"o":{"1":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}},"2":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}},"3":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}},"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},",":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},")":{"docs":{},":":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}},"=":{"2":{"0":{"docs":{},",":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"=":{"0":{"docs":{},".":{"0":{"1":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}},"docs":{}},"1":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}},"docs":{}}},"1":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}},"docs":{}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}},"回":{"docs":{},"归":{"docs":{},"有":{"docs":{},"一":{"docs":{},"些":{"docs":{},"选":{"docs":{},"择":{"docs":{},"的":{"docs":{},"功":{"docs":{},"能":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}},"t":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}},"f":{"docs":{},"w":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.027210884353741496}}}},")":{"docs":{},"*":{"docs":{},"*":{"2":{"docs":{},")":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}},"docs":{}}},":":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}},"m":{"2":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}},"3":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{},"i":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05}},"e":{"docs":{},"_":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"l":{"docs":{},"e":{"docs":{},"_":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}},"e":{"docs":{},"t":{"docs":{},"r":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}},"s":{"docs":{},"i":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"1":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}}}}}}}}},"o":{"docs":{},"g":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}},"局":{"docs":{},"限":{"docs":{},"性":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}}}},"t":{"docs":{},"h":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},"l":{"docs":{},"i":{"docs":{},"b":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}},".":{"docs":{},"p":{"docs":{},"y":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":0.08333333333333333}}}}}},"x":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"=":{"1":{"0":{"0":{"0":{"docs":{},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}},"docs":{},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}}}},"docs":{}},"docs":{}},"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}}}}},"d":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"h":{"docs":{},"=":{"2":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}}}},"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198}}}}}}}}}}}}},"f":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}},"e":{"docs":{},"s":{"docs":{},"=":{"1":{"docs":{},",":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}}},"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}}}}}}},"'":{"docs":{},"a":{"docs":{},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{},"'":{"docs":{},",":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198}}}}}}}}}}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"f":{"docs":{},"_":{"docs":{},"n":{"docs":{},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"s":{"docs":{},"=":{"1":{"0":{"docs":{},",":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}},"docs":{}},"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}}}}}}}}}}}}}}}}}},"s":{"docs":{},"a":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"=":{"1":{"0":{"0":{"docs":{},",":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.038461538461538464},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.031007751937984496}}}},"docs":{}},"docs":{}},"5":{"0":{"0":{"docs":{},",":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}},"docs":{}},"docs":{}},"docs":{}},"每":{"docs":{},"个":{"docs":{},"子":{"docs":{},"模":{"docs":{},"型":{"docs":{},"看":{"docs":{},"几":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"数":{"docs":{},"据":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.15},"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":2.75},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435}}}}}}},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"_":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}},"a":{"docs":{},"b":{"docs":{},"s":{"docs":{},"o":{"docs":{},"l":{"docs":{},"u":{"docs":{},"t":{"docs":{},"e":{"docs":{},"_":{"docs":{},"e":{"docs":{},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"e":{"docs":{},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"1":{"0":{"0":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}},"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}},"docs":{}},"2":{"0":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}},"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}},"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"1":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}},"2":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}},"3":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}},"4":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}},"docs":{}}}}}}}}}},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}}}}},",":{"docs":{},"y":{"1":{"0":{"0":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}},"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}},"docs":{}},"2":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}},"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"1":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}},"2":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}},"3":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"c":{"docs":{},"s":{"docs":{},"模":{"docs":{},"块":{"docs":{},"提":{"docs":{},"供":{"docs":{},"了":{"docs":{},"数":{"docs":{},"据":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"度":{"docs":{},"量":{"docs":{},"相":{"docs":{},"关":{"docs":{},"运":{"docs":{},"算":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}},"=":{"docs":{},"'":{"docs":{},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{},"k":{"docs":{},"o":{"docs":{},"w":{"docs":{},"s":{"docs":{},"k":{"docs":{},"i":{"docs":{},"'":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}}}}}}}}}}}}}},"_":{"docs":{},"p":{"docs":{},"a":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{},"s":{"docs":{},"=":{"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}}}}}}}}}}}}}}}}},"h":{"docs":{},"o":{"docs":{},"d":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936}}}}},"a":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},":":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.02040816326530612}}}}}}}}}},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{},"_":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"模":{"docs":{},"块":{"docs":{},"提":{"docs":{},"供":{"docs":{},"了":{"docs":{},"模":{"docs":{},"型":{"docs":{},"选":{"docs":{},"择":{"docs":{},"的":{"docs":{},"相":{"docs":{},"关":{"docs":{},"操":{"docs":{},"作":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}}}}}},"产":{"docs":{},"生":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{},"会":{"docs":{},"很":{"docs":{},"大":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"预":{"docs":{},"测":{"docs":{},"产":{"docs":{},"生":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{},"会":{"docs":{},"相":{"docs":{},"对":{"docs":{},"少":{"docs":{},"些":{"docs":{},"（":{"docs":{},"因":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"充":{"docs":{},"分":{"docs":{},"的":{"docs":{},"考":{"docs":{},"虑":{"docs":{},"了":{"docs":{},"y":{"docs":{},"和":{"docs":{},"x":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"关":{"docs":{},"系":{"docs":{},"）":{"docs":{},"，":{"docs":{},"用":{"docs":{},"这":{"docs":{},"两":{"docs":{},"者":{"docs":{},"相":{"docs":{},"减":{"docs":{},"，":{"docs":{},"结":{"docs":{},"果":{"docs":{},"就":{"docs":{},"是":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"了":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{},"指":{"docs":{},"标":{"docs":{},"，":{"docs":{},"用":{"1":{"docs":{},"减":{"docs":{},"去":{"docs":{},"这":{"docs":{},"个":{"docs":{},"商":{"docs":{},"结":{"docs":{},"果":{"docs":{},"就":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"没":{"docs":{},"有":{"docs":{},"产":{"docs":{},"生":{"docs":{},"错":{"docs":{},"误":{"docs":{},"的":{"docs":{},"指":{"docs":{},"标":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},".":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}},"c":{"docs":{},"o":{"docs":{},"e":{"docs":{},"f":{"docs":{},"_":{"docs":{},"[":{"0":{"docs":{},"]":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}},"docs":{}}}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"c":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"_":{"docs":{},"[":{"0":{"docs":{},"]":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}},"docs":{}}}}}}}}}}}}},"：":{"docs":{},"模":{"docs":{},"型":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}},"u":{"docs":{},"l":{"docs":{},"e":{"docs":{},".":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"m":{"docs":{},"o":{"docs":{},"n":{"docs":{},"方":{"docs":{},"法":{"docs":{},"求":{"docs":{},"出":{"docs":{},"最":{"docs":{},"多":{"docs":{},"的":{"docs":{},"元":{"docs":{},"素":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"那":{"docs":{},"个":{"docs":{},"键":{"docs":{},"值":{"docs":{},"对":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.01171875},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.021739130434782608},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.019417475728155338},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.031007751937984496}},"e":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}},",":{"docs":{},"r":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},",":{"docs":{},"m":{"docs":{},"a":{"docs":{},"e":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}}}}}}}}}}}}},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.021739130434782608},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.016181229773462782},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.025974025974025976},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.031007751937984496}}}},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"模":{"docs":{},"块":{"docs":{},"提":{"docs":{},"供":{"docs":{},"了":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"相":{"docs":{},"关":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"=":{"docs":{},"'":{"docs":{},"o":{"docs":{},"v":{"docs":{},"r":{"docs":{},"'":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}},"m":{"docs":{},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"'":{"docs":{},",":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}},"y":{"docs":{},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"l":{"docs":{},"e":{"docs":{},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{},"l":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}}}}}}},"_":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{},"m":{"docs":{},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},".":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"m":{"docs":{},"p":{"docs":{},"u":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"c":{"docs":{},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"=":{"0":{"docs":{},".":{"0":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}}}},"docs":{}}},"docs":{}}}}}}}}}},"s":{"docs":{},"p":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"=":{"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"a":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"_":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"f":{"docs":{},"=":{"1":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}}}},"docs":{}}}}}},"s":{"docs":{},"p":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"=":{"2":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}}}},"docs":{}}}}}}}}}}}}}}},"w":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"_":{"docs":{},"f":{"docs":{},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"f":{"docs":{},"=":{"0":{"docs":{},".":{"0":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"模":{"docs":{},"块":{"docs":{},"提":{"docs":{},"供":{"docs":{},"了":{"docs":{},"近":{"docs":{},"邻":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"r":{"docs":{},"s":{"docs":{},"e":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}},"[":{"docs":{},":":{"docs":{},"k":{"docs":{},"]":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}},"t":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}},"o":{"docs":{},"t":{"docs":{},"e":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{},"与":{"docs":{},"n":{"docs":{},"u":{"docs":{},"m":{"docs":{},"p":{"docs":{},"y":{"docs":{},"的":{"docs":{},"使":{"docs":{},"用":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong.html","tf":5},"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":5}}}}}}}}}}},"自":{"docs":{},"动":{"docs":{},"决":{"docs":{},"定":{"docs":{},"的":{"docs":{},"）":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}}}}},"n":{"docs":{},"e":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.01171875},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.011627906976744186},"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.013157894736842105}},",":{"docs":{},"\\":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"l":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}},"i":{"docs":{},"z":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"：":{"docs":{},"把":{"docs":{},"所":{"docs":{},"有":{"docs":{},"数":{"docs":{},"据":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"0":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"docs":{}}}}}}}}}}}}}}},"e":{"docs":{},"=":{"docs":{},"f":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},"e":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}},"i":{"docs":{},"s":{"docs":{},"y":{"docs":{},"_":{"docs":{},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}},"s":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"docs":{},":":{"docs":{},"]":{"docs":{},"[":{"docs":{},":":{"1":{"0":{"docs":{},"]":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}},"docs":{}},"docs":{}}}}}}},"docs":{},"n":{"docs":{},"u":{"docs":{},"m":{"docs":{},",":{"docs":{},":":{"docs":{},"]":{"docs":{},"[":{"docs":{},":":{"1":{"0":{"docs":{},"]":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"=":{"0":{"docs":{},".":{"3":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}},"docs":{}}},"docs":{}}}}}},"u":{"docs":{},"m":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.01171875},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541}},"p":{"docs":{},"i":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":5},"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"y":{"docs":{},".":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html","tf":5}}}}}}},"d":{"docs":{},"t":{"docs":{},"y":{"docs":{},"p":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},"）":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}},"/":{"docs":{},"d":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}},"p":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},".":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"(":{"docs":{},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}},"x":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"docs":{},"d":{"docs":{},"]":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{},"(":{"docs":{},"[":{"1":{"docs":{},".":{"docs":{},",":{"2":{"docs":{},".":{"docs":{},",":{"3":{"docs":{},".":{"docs":{},",":{"4":{"docs":{},".":{"docs":{},",":{"5":{"docs":{},".":{"docs":{},"]":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}},"docs":{}}}},"docs":{}}}},"docs":{}}}},"3":{"docs":{},".":{"docs":{},",":{"2":{"docs":{},".":{"docs":{},",":{"3":{"docs":{},".":{"docs":{},",":{"5":{"docs":{},".":{"docs":{},"]":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}},"docs":{}}}},"docs":{}}}},"docs":{}}}},"docs":{}}}},"8":{"docs":{},".":{"0":{"9":{"3":{"6":{"0":{"7":{"3":{"1":{"8":{"docs":{},",":{"3":{"docs":{},".":{"3":{"6":{"5":{"7":{"3":{"1":{"5":{"1":{"4":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"[":{"docs":{},":":{"docs":{},"i":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"d":{"docs":{},"(":{"docs":{},"x":{"docs":{},"[":{"docs":{},":":{"docs":{},"i":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},".":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"w":{"docs":{},"_":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},"_":{"docs":{},"x":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}},"y":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}},"x":{"2":{"docs":{},",":{"docs":{},"d":{"docs":{},"t":{"docs":{},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}}},"docs":{},",":{"docs":{},"d":{"docs":{},"t":{"docs":{},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}},"[":{"6":{"0":{"0":{"0":{"0":{"docs":{},":":{"docs":{},"]":{"docs":{},",":{"docs":{},"d":{"docs":{},"t":{"docs":{},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{},")":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},":":{"6":{"0":{"0":{"0":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"d":{"docs":{},"t":{"docs":{},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{},")":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},",":{"0":{"docs":{},"]":{"docs":{},"*":{"docs":{},"*":{"2":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}},"+":{"docs":{},"x":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}},"docs":{}}}}}}},"docs":{}}}}},"docs":{}}}}},"y":{"docs":{},"[":{"6":{"0":{"0":{"0":{"0":{"docs":{},":":{"docs":{},"]":{"docs":{},",":{"docs":{},"d":{"docs":{},"t":{"docs":{},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{},")":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},":":{"6":{"0":{"0":{"0":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"d":{"docs":{},"t":{"docs":{},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{},")":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}},"d":{"docs":{},"s":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}},"(":{"docs":{},"x":{"docs":{},">":{"docs":{},"=":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"1":{"docs":{},"+":{"docs":{},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"2":{"docs":{},"+":{"docs":{},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"3":{"docs":{},")":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}},"docs":{}}}}}}}}}}}},"docs":{}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}},"a":{"docs":{},"n":{"docs":{},"g":{"docs":{},"e":{"docs":{},"(":{"1":{"docs":{},",":{"1":{"1":{"docs":{},")":{"docs":{},".":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"(":{"5":{"docs":{},",":{"2":{"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}},"docs":{}}},"docs":{}}}}}}}}}}}},"2":{"docs":{},",":{"docs":{},"d":{"docs":{},"t":{"docs":{},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}},"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},")":{"docs":{},",":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},".":{"docs":{},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"m":{"docs":{},"u":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}},"f":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{},")":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}},"m":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"(":{"0":{"docs":{},",":{"1":{"0":{"0":{"docs":{},",":{"docs":{},"(":{"5":{"0":{"docs":{},",":{"2":{"docs":{},")":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}}},"docs":{}}},"docs":{}},"docs":{}},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"1":{"0":{"0":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},")":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}}},"o":{"docs":{},"m":{"docs":{},"(":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"docs":{},"m":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}},"x":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"[":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}},"docs":{}}}}}}}},"_":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"[":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}},"docs":{}}}}}}}}}}}}}}}},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"[":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}},"docs":{}}}}}}}}}}}}}}}}},"n":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"l":{"docs":{},"(":{"0":{"docs":{},",":{"1":{"docs":{},",":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"1":{"0":{"0":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}},"docs":{}},"docs":{}},"docs":{},"(":{"2":{"0":{"0":{"docs":{},",":{"2":{"docs":{},")":{"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}},"4":{"docs":{},",":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"docs":{},"x":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"1":{"0":{"0":{"0":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}}},"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}},"docs":{}},"docs":{}},"docs":{},"m":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}},"(":{"1":{"0":{"0":{"0":{"docs":{},",":{"1":{"0":{"docs":{},")":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}},"s":{"docs":{},"e":{"docs":{},"e":{"docs":{},"d":{"docs":{},"(":{"4":{"2":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}},"docs":{}},"6":{"6":{"6":{"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}},"docs":{}},"docs":{}},"docs":{}}}}}},"u":{"docs":{},"n":{"docs":{},"i":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"(":{"0":{"docs":{},".":{"docs":{},",":{"1":{"0":{"0":{"docs":{},".":{"docs":{},",":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"1":{"0":{"0":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}},"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}},"e":{"docs":{},"m":{"docs":{},"p":{"docs":{},"t":{"docs":{},"y":{"docs":{},"(":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"x":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},",":{"docs":{},"d":{"docs":{},"t":{"docs":{},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}},"w":{"docs":{},")":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}},"(":{"1":{"0":{"0":{"docs":{},",":{"2":{"docs":{},")":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{},",":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}},"x":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}},"x":{"docs":{},"p":{"docs":{},"(":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"2":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}},")":{"docs":{},"/":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"s":{"docs":{},"t":{"docs":{},"d":{"docs":{},"(":{"docs":{},"x":{"2":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}}}}},"docs":{}}}}}}}}}}}}}},"1":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}},")":{"docs":{},"/":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"s":{"docs":{},"t":{"docs":{},"d":{"docs":{},"(":{"docs":{},"x":{"2":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}}}}},"docs":{}}}}}}}}}}}}}},"docs":{}}}}},"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"1":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}}}},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"0":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}},"docs":{}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}},"s":{"docs":{},"h":{"docs":{},"g":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"(":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{},")":{"docs":{},"/":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},")":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}},"/":{"docs":{},"(":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"(":{"docs":{},"x":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}}}}}}}}}}}}}}}}}},"1":{"docs":{},"]":{"docs":{},")":{"docs":{},")":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"/":{"docs":{},"(":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"(":{"docs":{},"x":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}},"a":{"docs":{},"x":{"docs":{},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},")":{"docs":{},")":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}},",":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"d":{"docs":{},"(":{"docs":{},"x":{"2":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"1":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}}}}},"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"1":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{}}}}}}}},"u":{"docs":{},"m":{"docs":{},"(":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}},"e":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},")":{"docs":{},"&":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},")":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}},"1":{"docs":{},")":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}},"docs":{}}}}}}}}}}}}}}}},"1":{"docs":{},")":{"docs":{},"&":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},")":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}},"1":{"docs":{},")":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}},"docs":{}}}}}}}}}}}}}}}},"docs":{}}}}}}}}},"i":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099}}}}}}}}}}}}},"_":{"docs":{},"k":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"w":{"docs":{},")":{"docs":{},"*":{"docs":{},"*":{"2":{"docs":{},")":{"docs":{},")":{"docs":{},"/":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}},"docs":{}}}}}}}}}}}},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"a":{"docs":{},"b":{"docs":{},"s":{"docs":{},"o":{"docs":{},"l":{"docs":{},"u":{"docs":{},"t":{"docs":{},"e":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099}}}}}}}}}}}}}}}},"y":{"docs":{},"*":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"h":{"docs":{},"a":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}}}},"c":{"docs":{},"f":{"docs":{},"m":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}},"h":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"c":{"docs":{},"k":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},"(":{"docs":{},"[":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"s":{"docs":{},"(":{"docs":{},"(":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}},")":{"docs":{},",":{"1":{"docs":{},")":{"docs":{},")":{"docs":{},",":{"docs":{},"x":{"docs":{},"]":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}},"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}}}},"x":{"docs":{},",":{"docs":{},"x":{"docs":{},"*":{"docs":{},"*":{"2":{"docs":{},"]":{"docs":{},")":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}},"docs":{}}}}}}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"g":{"docs":{},".":{"docs":{},"i":{"docs":{},"n":{"docs":{},"v":{"docs":{},"(":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},".":{"docs":{},"t":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},")":{"docs":{},")":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},".":{"docs":{},"t":{"docs":{},")":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"(":{"docs":{},"w":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}},"s":{"docs":{},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"(":{"0":{"docs":{},".":{"0":{"1":{"docs":{},",":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}},"docs":{}},"docs":{}}},"4":{"docs":{},",":{"8":{"docs":{},",":{"1":{"0":{"0":{"0":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"[":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"[":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"(":{"docs":{},"(":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"[":{"1":{"docs":{},"]":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"docs":{}}}}}}}}}}}}}},"docs":{}}}}}}}}},"2":{"docs":{},"]":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"[":{"3":{"docs":{},"]":{"docs":{},",":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"(":{"docs":{},"(":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"[":{"3":{"docs":{},"]":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"docs":{}}}}}}}}}}}}}},"docs":{}}}}}}}}},"docs":{}}}}}}}}}}}}}},"o":{"docs":{},"g":{"docs":{},"(":{"1":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}},"docs":{},"p":{"docs":{},")":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}}}}}},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"s":{"docs":{},"(":{"docs":{},"(":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}}}}},"z":{"docs":{},"e":{"docs":{},"r":{"docs":{},"o":{"docs":{},"s":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"[":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}},"docs":{}}}}}}}}}}}}}}}}},"v":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"c":{"docs":{},"k":{"docs":{},"(":{"docs":{},"[":{"docs":{},"e":{"docs":{},"x":{"docs":{},"a":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"_":{"docs":{},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"s":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"n":{"docs":{},"u":{"docs":{},"m":{"docs":{},"]":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"c":{"docs":{},"_":{"docs":{},"[":{"docs":{},"x":{"0":{"docs":{},".":{"docs":{},"r":{"docs":{},"a":{"docs":{},"v":{"docs":{},"e":{"docs":{},"l":{"docs":{},"(":{"docs":{},")":{"docs":{},",":{"docs":{},"x":{"1":{"docs":{},".":{"docs":{},"r":{"docs":{},"a":{"docs":{},"v":{"docs":{},"e":{"docs":{},"l":{"docs":{},"(":{"docs":{},")":{"docs":{},"]":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}},"docs":{}}}}}}}}}}}},"docs":{}}}}},"f":{"docs":{},"i":{"docs":{},"l":{"docs":{},"l":{"docs":{},"_":{"docs":{},"d":{"docs":{},"i":{"docs":{},"a":{"docs":{},"g":{"docs":{},"o":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"(":{"docs":{},"e":{"docs":{},"r":{"docs":{},"r":{"docs":{},"_":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"_":{"docs":{},"j":{"docs":{},"o":{"docs":{},"b":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}},"s":{"docs":{},"=":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}},"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.023255813953488372},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.03296703296703297}}}}}}},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"=":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"5":{"0":{"docs":{},",":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"docs":{}}}}}}}}}}},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}},"s":{"docs":{},":":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},"=":{"1":{"docs":{},"e":{"4":{"docs":{},",":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}},")":{"docs":{},":":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}},"docs":{}}},"5":{"docs":{},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}},"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},")":{"docs":{},"/":{"docs":{},"/":{"3":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}},"docs":{}}}}}}}}}}}},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},">":{"docs":{},"=":{"1":{"docs":{},",":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}},"docs":{}}}}}}}}}}}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}},"a":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"=":{"3":{"0":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}}}},"docs":{}},"5":{"0":{"0":{"0":{"docs":{},",":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}},"docs":{},",":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.03875968992248062},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}},"a":{"docs":{},"n":{"docs":{},"（":{"docs":{},"n":{"docs":{},"o":{"docs":{},"t":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}},"o":{"docs":{},"n":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}},"e":{"docs":{},"v":{"docs":{},"s":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}},"e":{"docs":{},"r":{"docs":{},"是":{"docs":{},"o":{"docs":{},"v":{"docs":{},"o":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}},"(":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}},"e":{"docs":{},"r":{"docs":{},"是":{"docs":{},"o":{"docs":{},"v":{"docs":{},"r":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}},"(":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"t":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}},"r":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"\"":{"docs":{},")":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}},"a":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"的":{"docs":{},"首":{"docs":{},"字":{"docs":{},"母":{"docs":{},"缩":{"docs":{},"写":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"f":{"docs":{},"a":{"docs":{},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"=":{"docs":{},"f":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},"e":{"docs":{},")":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}}}}}}}},"v":{"docs":{},"o":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0136986301369863}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"b":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":2.007751937984496}},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},")":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}},",":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.023255813953488372},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.054945054945054944}}}}}}}}}}}}}}}}},"p":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},"c":{"docs":{},"a":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"PCA/":{"ref":"PCA/","tf":5.5},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.016181229773462782},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.02727272727272727},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.013605442176870748}},"给":{"docs":{},"出":{"docs":{},"了":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"分":{"docs":{},"析":{"docs":{},"法":{"docs":{},"的":{"docs":{},"相":{"docs":{},"关":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}},"（":{"docs":{},"p":{"docs":{},"r":{"docs":{},"i":{"docs":{},"n":{"docs":{},"c":{"docs":{},"i":{"docs":{},"p":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}},":":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}},"(":{"0":{"docs":{},".":{"5":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}},"9":{"5":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{},")":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}},"docs":{}}},"docs":{},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},"=":{"1":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}},"2":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114}}}},"6":{"4":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}}}}}}}}}}}}}},"s":{"docs":{},"v":{"docs":{},"d":{"docs":{},"_":{"docs":{},"s":{"docs":{},"o":{"docs":{},"l":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"d":{"docs":{},"'":{"docs":{},")":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}},".":{"docs":{},"e":{"docs":{},"x":{"docs":{},"p":{"docs":{},"l":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"v":{"docs":{},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"_":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"_":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114}}}}}}}}}}}}}}}}}}}}}}}}}}},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}},"n":{"docs":{},"o":{"docs":{},"i":{"docs":{},"s":{"docs":{},"y":{"docs":{},"_":{"docs":{},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"s":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},"_":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}},"n":{"docs":{},"o":{"docs":{},"i":{"docs":{},"s":{"docs":{},"y":{"docs":{},"_":{"docs":{},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"s":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"e":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"(":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}},"x":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},"_":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}},"降":{"docs":{},"维":{"docs":{},"到":{"docs":{},"两":{"docs":{},"维":{"docs":{},"的":{"docs":{},"意":{"docs":{},"义":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}},"o":{"docs":{},"l":{"docs":{},"y":{"2":{"0":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}},"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}},"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"f":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.010101010101010102},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}},"e":{"docs":{},"s":{"docs":{},"进":{"docs":{},"行":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"曾":{"docs":{},"维":{"docs":{},"处":{"docs":{},"理":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"解":{"docs":{},"决":{"docs":{},"非":{"docs":{},"线":{"docs":{},"性":{"docs":{},"问":{"docs":{},"题":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}}}}}}}},"(":{"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"=":{"2":{"0":{"docs":{},",":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.008571428571428572}}}},"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},",":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}},"3":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}},"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},")":{"docs":{},")":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}}}}}}},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"1":{"0":{"0":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}}}},"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}}}},"docs":{}},"2":{"0":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}},"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}},"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},")":{"docs":{},":":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},")":{"docs":{},":":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}},"=":{"2":{"0":{"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}},",":{"docs":{},"c":{"docs":{},"=":{"0":{"docs":{},".":{"1":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}},",":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"t":{"docs":{},"y":{"docs":{},"=":{"docs":{},"'":{"docs":{},"l":{"1":{"docs":{},"'":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}},"docs":{}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}},"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}},"docs":{}},",":{"docs":{},"c":{"docs":{},")":{"docs":{},":":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}},",":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"t":{"docs":{},"y":{"docs":{},"=":{"docs":{},"'":{"docs":{},"l":{"2":{"docs":{},"'":{"docs":{},")":{"docs":{},":":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"e":{"docs":{},"l":{"docs":{},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},"(":{"3":{"docs":{},")":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}},"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},",":{"docs":{},"c":{"docs":{},"=":{"1":{"docs":{},".":{"0":{"docs":{},")":{"docs":{},":":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.010101010101010102}}}}}}}},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.010101010101010102}}}}}}}}}}}}}}},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"1":{"0":{"0":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"_":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}},"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{},"，":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}},"docs":{}},"2":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}},"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"2":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}},"3":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}},"4":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"e":{"docs":{},"l":{"docs":{},"_":{"docs":{},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}},"i":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.010101010101010102}}},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}},"=":{"3":{"docs":{},")":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}},"docs":{}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.025}}}}}},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.032},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":3.346320346320346},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.02158273381294964}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"_":{"docs":{},"c":{"docs":{},"u":{"docs":{},"r":{"docs":{},"v":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}},"e":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}},"e":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}}}}}}},"s":{"docs":{},")":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}},",":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}},".":{"docs":{},"a":{"docs":{},"p":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"(":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}},"[":{"docs":{},":":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"模":{"docs":{},"块":{"docs":{},"提":{"docs":{},"供":{"docs":{},"了":{"docs":{},"数":{"docs":{},"据":{"docs":{},"预":{"docs":{},"处":{"docs":{},"理":{"docs":{},"的":{"docs":{},"相":{"docs":{},"关":{"docs":{},"操":{"docs":{},"作":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}}}}}}},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"_":{"docs":{},"i":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"!":{"docs":{},"\"":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}},"\"":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}},"_":{"docs":{},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"p":{"docs":{},"a":{"docs":{},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{},"=":{"docs":{},"'":{"2":{"docs":{},"*":{"docs":{},"n":{"docs":{},"_":{"docs":{},"j":{"docs":{},"o":{"docs":{},"b":{"docs":{},"s":{"docs":{},"'":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}},"docs":{}}}}}}}}}}}},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"=":{"docs":{},"f":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},"e":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}},"'":{"docs":{},"a":{"docs":{},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{},"'":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322}}}}}}}}}}}}}}}},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322}}}}}}}}}}}}}}}},"\"":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"k":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}},"=":{"docs":{},"\"":{"docs":{},",":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"k":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904}}}}}}}}}}}}},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"d":{"docs":{},"=":{"docs":{},"\"":{"docs":{},",":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"d":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}},"=":{"docs":{},"\"":{"docs":{},",":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"p":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}},"e":{"docs":{},"=":{"0":{"docs":{},".":{"0":{"docs":{},".":{"docs":{},"\"":{"docs":{},",":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{},"\"":{"docs":{},",":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936}}}}}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"_":{"docs":{},"h":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"y":{"docs":{},")":{"docs":{},")":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},".":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}}}}}}},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"_":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},".":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"(":{"docs":{},"d":{"docs":{},"s":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},")":{"docs":{},")":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"(":{"docs":{},"d":{"docs":{},"s":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},")":{"docs":{},")":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"d":{"2":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{},"y":{"2":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}},"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}}}}}}},"v":{"2":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"g":{"2":{"docs":{},")":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"docs":{},")":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}},"o":{"docs":{},"b":{"docs":{},"a":{"docs":{},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},"=":{"docs":{},"f":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},"e":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}}}}}}}}}}}}}}}}},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"n":{"3":{"docs":{},"入":{"docs":{},"门":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}},"docs":{}}}}},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}},"l":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},".":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"t":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"x":{"2":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"2":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}},"docs":{}}}}},"docs":{}}}}},"docs":{}}}}},"docs":{},"[":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"[":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"b":{"docs":{},"'":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}},"docs":{}}}}}},"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"b":{"docs":{},"'":{"docs":{},",":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"=":{"0":{"docs":{},".":{"5":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"8":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}},"y":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"'":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.014018691588785047},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.022857142857142857},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02830188679245283},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.017857142857142856},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0297029702970297}}}}},"docs":{}}},"docs":{}}}}}}}}},"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.019704433497536946},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"]":{"docs":{},",":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}},"1":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"1":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"b":{"docs":{},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"'":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.014018691588785047},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}}},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.022857142857142857},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02830188679245283},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.017857142857142856},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0297029702970297}}}}},"docs":{}}},"docs":{}}}}}}}}},"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.019704433497536946},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"]":{"docs":{},",":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}},"2":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"2":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.017857142857142856},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}},"docs":{}}},"docs":{}}}}}}}}},"docs":{}}},"docs":{}}}}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"[":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"[":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"g":{"docs":{},"'":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"1":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"[":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"=":{"docs":{},"=":{"1":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016}}}}},"docs":{}}}}}}}}}}}}}}},"docs":{}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"docs":{},"i":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"docs":{},"i":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"=":{"0":{"docs":{},".":{"8":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}}},"docs":{}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},",":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"=":{"0":{"docs":{},".":{"5":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}}},"docs":{}}}}}}}}}},"b":{"docs":{},"'":{"docs":{},",":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"=":{"0":{"docs":{},".":{"8":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"'":{"docs":{},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}},"docs":{}}},"1":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"1":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"b":{"docs":{},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"'":{"docs":{},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}},"docs":{}}},"1":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"1":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}},",":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}},"y":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.037037037037037035},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.010101010101010102},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.02336448598130841},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.011428571428571429}}}}}},"i":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},"[":{"docs":{},"i":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},".":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"i":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},"[":{"docs":{},"i":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},".":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"1":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"i":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},"[":{"docs":{},"i":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},".":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},"=":{"docs":{},"=":{"1":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"2":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"i":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},"[":{"docs":{},"i":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},".":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},"=":{"docs":{},"=":{"2":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"b":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},"s":{"docs":{},"(":{"1":{"0":{"docs":{},",":{"1":{"0":{"docs":{},",":{"docs":{},"f":{"docs":{},"i":{"docs":{},"g":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"docs":{},"(":{"1":{"0":{"docs":{},",":{"1":{"0":{"docs":{},")":{"docs":{},",":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}}}}}}}}}}}},"docs":{}},"docs":{}}},"docs":{}},"6":{"docs":{},",":{"6":{"docs":{},",":{"docs":{},"f":{"docs":{},"i":{"docs":{},"g":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"docs":{},"(":{"1":{"0":{"docs":{},",":{"1":{"0":{"docs":{},")":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}},"h":{"docs":{},"o":{"docs":{},"w":{"docs":{},"(":{"docs":{},")":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.014388489208633094}}}}}}}},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"(":{"docs":{},"[":{"0":{"docs":{},",":{"6":{"docs":{},",":{"0":{"docs":{},",":{"6":{"docs":{},"]":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}}}},"docs":{}}},"docs":{}}},"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{},"+":{"1":{"docs":{},",":{"0":{"docs":{},",":{"4":{"docs":{},"]":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}},"docs":{}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}},"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"1":{"docs":{},"_":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},",":{"docs":{},"x":{"2":{"docs":{},"_":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}},"docs":{}}}}}}}}},"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"h":{"docs":{},"a":{"docs":{},"t":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},")":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}},")":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}},"_":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},"_":{"docs":{},"x":{"docs":{},",":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},"_":{"docs":{},"y":{"docs":{},")":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.014388489208633094}}}}}}}}}}}}},"[":{"0":{"docs":{},",":{"docs":{},"w":{"docs":{},"[":{"0":{"docs":{},"]":{"docs":{},"*":{"3":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"[":{"0":{"docs":{},",":{"docs":{},"w":{"docs":{},"[":{"1":{"docs":{},"]":{"docs":{},"*":{"3":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}},"docs":{}}}}},"docs":{}}}}},"docs":{}},"docs":{}}}},"docs":{}}}}},"docs":{},"i":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.016}}}},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{},",":{"docs":{},"y":{"1":{"0":{"0":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"[":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"[":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"2":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"[":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},")":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"2":{"docs":{},"[":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},")":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{},"[":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{},"]":{"docs":{},",":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"'":{"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"s":{"docs":{},",":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.02877697841726619}}}}}}}}}}}}},"f":{"docs":{},"p":{"docs":{},"r":{"docs":{},"s":{"docs":{},",":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}},"t":{"docs":{},"p":{"docs":{},"r":{"docs":{},"s":{"docs":{},")":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"(":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"t":{"docs":{},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},"f":{"docs":{},"(":{"docs":{},"x":{"0":{"docs":{},",":{"docs":{},"x":{"1":{"docs":{},",":{"docs":{},"z":{"docs":{},"z":{"docs":{},",":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"s":{"docs":{},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"=":{"5":{"docs":{},",":{"docs":{},"c":{"docs":{},"m":{"docs":{},"a":{"docs":{},"p":{"docs":{},"=":{"docs":{},"c":{"docs":{},"u":{"docs":{},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"m":{"docs":{},"_":{"docs":{},"c":{"docs":{},"m":{"docs":{},"a":{"docs":{},"p":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}},"docs":{}}}},"docs":{}}}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"w":{"docs":{},"(":{"docs":{},"c":{"docs":{},"f":{"docs":{},"m":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}},"e":{"docs":{},"r":{"docs":{},"r":{"docs":{},"_":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"t":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},"_":{"docs":{},"i":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"_":{"docs":{},"h":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"y":{"docs":{},"(":{"docs":{},")":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.045454545454545456}}}}}}}}}}}}}}}}},"x":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815}}},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"s":{"docs":{},"(":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}},"e":{"docs":{},"x":{"docs":{},"a":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"_":{"docs":{},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"s":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}},"f":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},")":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}},"f":{"docs":{},"i":{"docs":{},"l":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"s":{"docs":{},")":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},"_":{"docs":{},"[":{"docs":{},":":{"3":{"6":{"docs":{},"]":{"docs":{},")":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"b":{"docs":{},"o":{"docs":{},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"y":{"docs":{},"(":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"4":{"docs":{},",":{"7":{"docs":{},".":{"5":{"docs":{},",":{"1":{"docs":{},".":{"5":{"docs":{},",":{"4":{"docs":{},".":{"5":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}}}}}}},"_":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"4":{"docs":{},",":{"8":{"docs":{},",":{"1":{"docs":{},".":{"5":{"docs":{},",":{"4":{"docs":{},".":{"5":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"2":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"4":{"docs":{},",":{"8":{"docs":{},".":{"5":{"docs":{},",":{"1":{"docs":{},".":{"5":{"docs":{},",":{"4":{"docs":{},".":{"5":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}}}}}}}},"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"4":{"docs":{},",":{"7":{"docs":{},".":{"5":{"docs":{},",":{"1":{"docs":{},".":{"5":{"docs":{},",":{"4":{"docs":{},".":{"5":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"8":{"docs":{},".":{"5":{"docs":{},",":{"1":{"docs":{},".":{"5":{"docs":{},",":{"4":{"docs":{},".":{"5":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},")":{"docs":{},":":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"_":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"2":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}},"3":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}},"4":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}},"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"e":{"docs":{},"l":{"docs":{},"_":{"docs":{},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}},"s":{"docs":{},"v":{"docs":{},"c":{"2":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}},"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}},"_":{"docs":{},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"0":{"1":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}},"5":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}},"docs":{}},"1":{"0":{"0":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}},"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}},"d":{"docs":{},"t":{"docs":{},"_":{"docs":{},"c":{"docs":{},"f":{"docs":{},"l":{"docs":{},",":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}},"l":{"docs":{},"f":{"2":{"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"3":{"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"4":{"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"5":{"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"docs":{},",":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"_":{"docs":{},"c":{"docs":{},"u":{"docs":{},"r":{"docs":{},"v":{"docs":{},"e":{"docs":{},"(":{"docs":{},"a":{"docs":{},"l":{"docs":{},"g":{"docs":{},"o":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{},":":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},")":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"2":{"0":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"l":{"docs":{},"e":{"docs":{},"(":{"docs":{},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"l":{"docs":{},"e":{"docs":{},")":{"docs":{},":":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"2":{"0":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}},"docs":{}},"docs":{}}}}},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"1":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}},"2":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}},"3":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}},"4":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}},"docs":{}}}}}},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"1":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}},"2":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}},"3":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}},"docs":{}}}}}}}}}}}}},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"b":{"docs":{},"o":{"docs":{},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"y":{"docs":{},"(":{"docs":{},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},")":{"docs":{},":":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}},"s":{"docs":{},"v":{"docs":{},"c":{"2":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}},"3":{"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}},"docs":{},",":{"docs":{},"a":{"docs":{},"x":{"docs":{},"i":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"=":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}},"docs":{}},"e":{"docs":{},"r":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}},"m":{"docs":{},"u":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},"n":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}}},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"t":{"docs":{},"y":{"docs":{},"=":{"docs":{},"'":{"docs":{},"l":{"1":{"docs":{},"'":{"docs":{},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}},"2":{"docs":{},"'":{"docs":{},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}},"docs":{}}}}}}}}}},"a":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{},"_":{"docs":{},"g":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},"=":{"docs":{},"[":{"docs":{},"{":{"docs":{},"'":{"docs":{},"w":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"s":{"docs":{},"'":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}},"c":{"docs":{},"k":{"docs":{},"a":{"docs":{},"g":{"docs":{},"e":{"docs":{},"s":{"docs":{},"/":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},"l":{"docs":{},"i":{"docs":{},"b":{"docs":{},"/":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"t":{"docs":{},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},".":{"docs":{},"p":{"docs":{},"y":{"docs":{},":":{"9":{"6":{"7":{"docs":{},":":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.018691588785046728},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.017142857142857144},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02358490566037736},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"/":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"/":{"docs":{},"l":{"docs":{},"a":{"docs":{},"b":{"docs":{},"e":{"docs":{},"l":{"docs":{},".":{"docs":{},"p":{"docs":{},"y":{"docs":{},":":{"1":{"5":{"1":{"docs":{},":":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":3.352564102564102}}}},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}},"i":{"docs":{},"p":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}},"e":{"docs":{},"(":{"docs":{},"[":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.011428571428571429},"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.008571428571428572},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}},"m":{"docs":{},"e":{"docs":{},"m":{"docs":{},"o":{"docs":{},"r":{"docs":{},"y":{"docs":{},"=":{"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}},"e":{"docs":{},"的":{"docs":{},"英":{"docs":{},"文":{"docs":{},"名":{"docs":{},"字":{"docs":{},"是":{"docs":{},"管":{"docs":{},"道":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}},"为":{"docs":{},"明":{"docs":{},"科":{"docs":{},"夫":{"docs":{},"斯":{"docs":{},"基":{"docs":{},"距":{"docs":{},"离":{"docs":{},"的":{"docs":{},"p":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}}}}}}}}}}},"^":{"docs":{},")":{"docs":{},"。":{"docs":{"逻辑回归/2.逻辑回归的损失函数.html":{"ref":"逻辑回归/2.逻辑回归的损失函数.html","tf":0.09090909090909091}}}}},"越":{"docs":{},"趋":{"docs":{},"近":{"docs":{},"于":{"0":{"docs":{},"，":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"越":{"docs":{},"愿":{"docs":{},"意":{"docs":{},"将":{"docs":{},"数":{"docs":{},"据":{"docs":{},"预":{"docs":{},"测":{"docs":{},"为":{"0":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}},"docs":{}}}}}}}}}}}}}}}}}},"1":{"docs":{},"，":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"越":{"docs":{},"愿":{"docs":{},"意":{"docs":{},"将":{"docs":{},"数":{"docs":{},"据":{"docs":{},"预":{"docs":{},"测":{"docs":{},"为":{"1":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}},"docs":{}}}}}}}}}}}}}}}}}},"docs":{}}}}},")":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.05714285714285714}}},"*":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"docs":{},"p":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}}}}},"*":{"2":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},"docs":{}}}},"r":{"2":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.03773584905660377},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}}}}}}}}}}}}},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}}}}}}}}}}}}},"=":{"0":{"docs":{},"。":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}},"docs":{}}},"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":3.389937106918239}},"e":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.02197802197802198},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.013986013986013986},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.012552301255230125},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.012295081967213115}},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.032},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.02877697841726619}},"l":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}},"e":{"docs":{},",":{"docs":{},"f":{"1":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}},"docs":{}}},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}}}}}},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}}}}}}},")":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.048}},":":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}},"的":{"docs":{},"平":{"docs":{},"衡":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":3.346320346320346}}}}},"s":{"docs":{},")":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.02158273381294964}}},",":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}},".":{"docs":{},"a":{"docs":{},"p":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"(":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}},"[":{"docs":{},":":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}},"g":{"1":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}},".":{"docs":{},"a":{"docs":{},"_":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}},"b":{"docs":{},"_":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"b":{"docs":{},"i":{"docs":{},"g":{"docs":{},"_":{"docs":{},"x":{"docs":{},",":{"docs":{},"b":{"docs":{},"i":{"docs":{},"g":{"docs":{},"_":{"docs":{},"y":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{},"(":{"docs":{},"[":{"6":{"docs":{},".":{"docs":{},"]":{"docs":{},")":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}},"docs":{}}}}}}}}}}},"x":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}},"2":{"docs":{},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"b":{"docs":{},"i":{"docs":{},"g":{"docs":{},"_":{"docs":{},"x":{"docs":{},",":{"docs":{},"b":{"docs":{},"i":{"docs":{},"g":{"docs":{},"_":{"docs":{},"y":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}},"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.015625},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"模":{"docs":{},"型":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}},"认":{"docs":{},"为":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"这":{"docs":{},"些":{"docs":{},"特":{"docs":{},"征":{"docs":{},"有":{"docs":{},"用":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"他":{"docs":{},"可":{"docs":{},"以":{"docs":{},"当":{"docs":{},"做":{"docs":{},"特":{"docs":{},"征":{"docs":{},"选":{"docs":{},"择":{"docs":{},"用":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}}},"这":{"docs":{},"个":{"docs":{},"θ":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"是":{"docs":{},"没":{"docs":{},"有":{"docs":{},"用":{"docs":{},"的":{"docs":{},"，":{"docs":{},"剩":{"docs":{},"下":{"docs":{},"的":{"docs":{},"那":{"docs":{},"些":{"docs":{},"不":{"docs":{},"等":{"docs":{},"于":{"0":{"docs":{},"的":{"docs":{},"θ":{"docs":{},"就":{"docs":{},"说":{"docs":{},"明":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}},"）":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}},"来":{"docs":{},"替":{"docs":{},"代":{"docs":{},"，":{"docs":{},"从":{"docs":{},"而":{"docs":{},"达":{"docs":{},"到":{"docs":{},"选":{"docs":{},"择":{"docs":{},"去":{"docs":{},"掉":{"docs":{},"一":{"docs":{},"些":{"docs":{},"θ":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"当":{"docs":{},"p":{"docs":{},"=":{"2":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"就":{"docs":{},"是":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}},"docs":{}}}}}}}}},"o":{"docs":{},"r":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}},"u":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.015625},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.04411764705882353},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.03773584905660377},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.011627906976744186},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.022727272727272728},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.027472527472527472},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.028112449799196786},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.021739130434782608},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.016},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.04},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.027972027972027972},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.011428571428571429},"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888},"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.02631578947368421},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.008571428571428572},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.05},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.014778325123152709},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"=":{"docs":{},"'":{"docs":{},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"'":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"x":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}},"[":{"docs":{},":":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}},"[":{"0":{"docs":{},"]":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099}}}},"docs":{},"i":{"docs":{},"]":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}},".":{"docs":{},"a":{"docs":{},"p":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"(":{"docs":{},"w":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}},"t":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.011428571428571429}},"e":{"1":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}},"2":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}},"3":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}},"4":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}},"docs":{},"是":{"docs":{},"岭":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},",":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},")":{"docs":{},":":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}},"=":{"2":{"0":{"docs":{},",":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"=":{"0":{"docs":{},".":{"0":{"0":{"0":{"0":{"1":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"1":{"0":{"0":{"0":{"0":{"0":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}},"docs":{}},"docs":{}},"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}},"docs":{}},"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}},"docs":{}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}},"和":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"都":{"docs":{},"是":{"docs":{},"在":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"中":{"docs":{},"添":{"docs":{},"加":{"docs":{},"一":{"docs":{},"项":{"docs":{},"，":{"docs":{},"来":{"docs":{},"调":{"docs":{},"节":{"docs":{},"θ":{"docs":{},"的":{"docs":{},"值":{"docs":{},"使":{"docs":{},"其":{"docs":{},"尽":{"docs":{},"可":{"docs":{},"能":{"docs":{},"的":{"docs":{},"小":{"docs":{},"，":{"docs":{},"使":{"docs":{},"得":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"更":{"docs":{},"好":{"docs":{},"一":{"docs":{},"些":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"c":{"docs":{},"_":{"docs":{},"c":{"docs":{},"u":{"docs":{},"r":{"docs":{},"v":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}},"e":{"docs":{},",":{"docs":{},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"_":{"docs":{},"a":{"docs":{},"u":{"docs":{},"c":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}},"a":{"docs":{},"u":{"docs":{},"c":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}},"e":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}},"曲":{"docs":{},"线":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":5.025}}}},"：":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"e":{"docs":{},"i":{"docs":{},"v":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}},"o":{"docs":{},"t":{"docs":{},"_":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"e":{"docs":{},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"r":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"w":{"docs":{},"_":{"docs":{},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}}}}}}},"a":{"docs":{},"w":{"docs":{},"_":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},"_":{"docs":{},"i":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}},"x":{"docs":{},"=":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}},"n":{"docs":{},"g":{"docs":{},"e":{"docs":{},"(":{"1":{"0":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}},"docs":{},",":{"1":{"0":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}},"1":{"docs":{},")":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904}}},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}},"docs":{}},"5":{"docs":{},")":{"docs":{},":":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}}}},"6":{"docs":{},")":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}},"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099}},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{},"+":{"1":{"docs":{},")":{"docs":{},":":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}},"]":{"docs":{},",":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"s":{"docs":{},"q":{"docs":{},"r":{"docs":{},"t":{"docs":{},"(":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},")":{"docs":{},",":{"docs":{},"l":{"docs":{},"a":{"docs":{},"b":{"docs":{},"e":{"docs":{},"l":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},")":{"docs":{},",":{"docs":{},"l":{"docs":{},"a":{"docs":{},"b":{"docs":{},"e":{"docs":{},"l":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}},")":{"docs":{},")":{"docs":{},":":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}},"2":{"docs":{},",":{"1":{"0":{"docs":{},")":{"docs":{},":":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}},"]":{"docs":{},",":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}},"docs":{}},"docs":{}}},"docs":{},"x":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"(":{"1":{"docs":{},")":{"docs":{},")":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}}}}},"docs":{}},"[":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}},"docs":{}}}}}}}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"[":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{},"]":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}},",":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}},"docs":{}}}}}}}}}}}}}}},"m":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}},")":{"docs":{},":":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}},"w":{"docs":{},")":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}},"x":{"docs":{},")":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}},"d":{"docs":{},"_":{"docs":{},"i":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}},"o":{"docs":{},"m":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"=":{"4":{"2":{"docs":{},")":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}},"docs":{}},"6":{"6":{"6":{"docs":{},")":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.03488372093023256}}},",":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.04395604395604396},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}},"docs":{}},"docs":{}},"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}},"u":{"docs":{},"b":{"docs":{},"s":{"docs":{},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}},"o":{"docs":{},"o":{"docs":{},"b":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"_":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"a":{"docs":{},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{},"e":{"docs":{},"s":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}},"o":{"docs":{},"o":{"docs":{},"b":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"_":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}}}}}}}}}}}}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"t":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"p":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}}}}}}}}}},"n":{"docs":{},"_":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"=":{"5":{"0":{"0":{"docs":{},",":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"e":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}},"，":{"docs":{},"f":{"docs":{},"p":{"docs":{},"r":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}},"u":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}}},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},":":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}}}},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}},"曲":{"docs":{},"线":{"docs":{},"更":{"docs":{},"靠":{"docs":{},"外":{"docs":{},"（":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"与":{"docs":{},"x":{"docs":{},"轴":{"docs":{},"，":{"docs":{},"y":{"docs":{},"轴":{"docs":{},"所":{"docs":{},"包":{"docs":{},"围":{"docs":{},"的":{"docs":{},"面":{"docs":{},"积":{"docs":{},"越":{"docs":{},"大":{"docs":{},"）":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"说":{"docs":{},"明":{"docs":{},"这":{"docs":{},"根":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"越":{"docs":{},"好":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"对":{"docs":{},"应":{"docs":{},"这":{"docs":{},"个":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"上":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"点":{"docs":{},"，":{"docs":{},"他":{"docs":{},"的":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"和":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"都":{"docs":{},"比":{"docs":{},"另":{"docs":{},"一":{"docs":{},"个":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"要":{"docs":{},"大":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"这":{"docs":{},"个":{"docs":{},"p":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}},"b":{"docs":{},"f":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":5.01}},"中":{"docs":{},"的":{"docs":{},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":5.004716981132075}}}}}}}}}}}},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},"(":{"docs":{},")":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"=":{"0":{"docs":{},".":{"1":{"docs":{},")":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}},"5":{"docs":{},")":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}},"docs":{}}},"1":{"0":{"0":{"docs":{},")":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}},"docs":{},")":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}},"docs":{},".":{"0":{"docs":{},")":{"docs":{},":":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}},"f":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"2":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}},"o":{"docs":{},"o":{"docs":{},"b":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"_":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}}}}}}},"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}},"o":{"docs":{},"o":{"docs":{},"b":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"_":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}}}}}}}}}}}},"s":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}},"c":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.02197802197802198},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":3.3361344537815123},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.03571428571428571},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.017793594306049824},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":3.357333333333333}},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}},",":{"docs":{},"k":{"docs":{},",":{"docs":{},"p":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}}}}},"变":{"docs":{},"低":{"docs":{},"是":{"docs":{},"由":{"docs":{},"于":{"docs":{},"数":{"docs":{},"据":{"docs":{},"比":{"docs":{},"较":{"docs":{},"简":{"docs":{},"单":{"docs":{},"。":{"docs":{},"主":{"docs":{},"要":{"docs":{},"看":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"含":{"docs":{},"义":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}},"，":{"docs":{},"让":{"docs":{},"我":{"docs":{},"们":{"docs":{},"兼":{"docs":{},"顾":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"=":{"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}},"g":{"docs":{},"d":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}},"是":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.012048192771084338},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}},"y":{"docs":{},",":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.008},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.016483516483516484},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}},"e":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"_":{"docs":{},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.016483516483516484},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.011428571428571429},"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}},"r":{"docs":{},"_":{"docs":{},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.03296703296703297},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.02158273381294964},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.025},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{},"_":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.01098901098901099},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.020618556701030927},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.010676156583629894},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.01098901098901099},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}}}}}}}}}}}},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.01098901098901099},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.01098901098901099},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.015151515151515152},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.016},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.011428571428571429},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.005714285714285714},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}},"i":{"docs":{},"p":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.02197802197802198},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.022727272727272728},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}},"t":{"docs":{},"r":{"docs":{},"e":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}},"e":{"docs":{},"n":{"docs":{},"s":{"docs":{},"e":{"docs":{},"m":{"docs":{},"b":{"docs":{},"l":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}}}}}}}}}},"_":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"中":{"docs":{},"的":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"算":{"docs":{},"法":{"docs":{},"支":{"docs":{},"持":{"docs":{},"传":{"docs":{},"入":{"docs":{},"一":{"docs":{},"个":{"docs":{},"小":{"docs":{},"于":{"1":{"docs":{},"的":{"docs":{},"数":{"docs":{},"来":{"docs":{},"表":{"docs":{},"示":{"docs":{},"我":{"docs":{},"们":{"docs":{},"希":{"docs":{},"望":{"docs":{},"能":{"docs":{},"解":{"docs":{},"释":{"docs":{},"多":{"docs":{},"少":{"docs":{},"比":{"docs":{},"例":{"docs":{},"的":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}},"对":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"预":{"docs":{},"处":{"docs":{},"理":{"docs":{},"的":{"docs":{},"函":{"docs":{},"数":{"docs":{},"都":{"docs":{},"封":{"docs":{},"装":{"docs":{},"在":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"模":{"docs":{},"块":{"docs":{},"下":{"docs":{},"，":{"docs":{},"包":{"docs":{},"括":{"docs":{},"之":{"docs":{},"前":{"docs":{},"学":{"docs":{},"的":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"计":{"docs":{},"算":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"不":{"docs":{},"是":{"docs":{},"简":{"docs":{},"单":{"docs":{},"的":{"docs":{},"使":{"docs":{},"用":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"，":{"docs":{},"他":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"更":{"docs":{},"快":{"docs":{},"的":{"docs":{},"一":{"docs":{},"种":{"docs":{},"方":{"docs":{},"法":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"需":{"docs":{},"要":{"docs":{},"修":{"docs":{},"改":{"docs":{},"s":{"docs":{},"o":{"docs":{},"l":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"参":{"docs":{},"数":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"会":{"docs":{},"根":{"docs":{},"据":{"docs":{},"我":{"docs":{},"们":{"docs":{},"顶":{"docs":{},"一":{"docs":{},"个":{"docs":{},"的":{"docs":{},"d":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"的":{"docs":{},"值":{"docs":{},"来":{"docs":{},"取":{"docs":{},"最":{"docs":{},"合":{"docs":{},"适":{"docs":{},"的":{"docs":{},"步":{"docs":{},"长":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.020618556701030927},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}},"e":{"docs":{},"r":{"docs":{},"提":{"docs":{},"供":{"docs":{},"数":{"docs":{},"据":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{},"运":{"docs":{},"算":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}},"(":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}},")":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}},"c":{"docs":{},"o":{"docs":{},"p":{"docs":{},"y":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"_":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{},"_":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}},")":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"s":{"docs":{},"v":{"docs":{},"r":{"docs":{},"(":{"docs":{},")":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}},"e":{"docs":{},"p":{"docs":{},"s":{"docs":{},"i":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"t":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}},"c":{"docs":{},"k":{"docs":{"137-stacking.html":{"ref":"137-stacking.html","tf":5.333333333333333}}}}},"d":{"docs":{},"_":{"docs":{},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},".":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}}},"e":{"docs":{},"p":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{},"(":{"docs":{},"'":{"docs":{},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"'":{"docs":{},",":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}},"s":{"docs":{},"t":{"docs":{},"d":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{},"r":{"docs":{},"'":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"p":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.01098901098901099},"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}},"m":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"=":{"docs":{},"=":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{},"/":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"b":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},"_":{"docs":{},"k":{"docs":{},"w":{"docs":{},"=":{"docs":{},"{":{"docs":{},"'":{"docs":{},"x":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"k":{"docs":{},"s":{"docs":{},"'":{"docs":{},":":{"docs":{},"[":{"docs":{},"]":{"docs":{},",":{"docs":{},"'":{"docs":{},"y":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"k":{"docs":{},"s":{"docs":{},"'":{"docs":{},":":{"docs":{},"[":{"docs":{},"]":{"docs":{},"}":{"docs":{},",":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}},"a":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"=":{"1":{"docs":{},".":{"0":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}},"docs":{}}},"docs":{}}}}}}}}}},"v":{"docs":{},"c":{"2":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}}}}}},"3":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}}}}}},"docs":{"./":{"ref":"./","tf":0.01098901098901099},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"e":{"docs":{},"f":{"docs":{},"_":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},",":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}},",":{"docs":{},"y":{"docs":{},")":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"c":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"_":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}},"(":{"docs":{},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"e":{"docs":{},"l":{"docs":{},"=":{"docs":{},"\"":{"docs":{},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"\"":{"docs":{},",":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"=":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},",":{"docs":{},"c":{"docs":{},"=":{"docs":{},"c":{"docs":{},")":{"docs":{},")":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"c":{"docs":{},"=":{"1":{"docs":{},".":{"0":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}},"docs":{}}},"docs":{}}},")":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},")":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"b":{"docs":{},"a":{"docs":{},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},")":{"docs":{},")":{"docs":{},",":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}}}}}},"_":{"docs":{},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"0":{"1":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}},"5":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}},"docs":{}},"1":{"0":{"0":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}},"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}},"docs":{}},"docs":{}}}}}}}},"m":{"docs":{"支撑向量机SVM/":{"ref":"支撑向量机SVM/","tf":5.5},"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05},"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":2.625},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}},"提":{"docs":{},"供":{"docs":{},"了":{"docs":{},"支":{"docs":{},"持":{"docs":{},"向":{"docs":{},"量":{"docs":{},"机":{"docs":{},"相":{"docs":{},"关":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"的":{"docs":{},"特":{"docs":{},"点":{"docs":{},"：":{"docs":{},"理":{"docs":{},"两":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"都":{"docs":{},"尽":{"docs":{},"可":{"docs":{},"能":{"docs":{},"的":{"docs":{},"远":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"这":{"docs":{},"两":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"离":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"最":{"docs":{},"近":{"docs":{},"的":{"docs":{},"这":{"docs":{},"些":{"docs":{},"点":{"docs":{},"离":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"也":{"docs":{},"尽":{"docs":{},"肯":{"docs":{},"能":{"docs":{},"的":{"docs":{},"远":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"最":{"docs":{},"终":{"docs":{},"数":{"docs":{},"学":{"docs":{},"推":{"docs":{},"导":{"docs":{"支撑向量机SVM/11.2 SVM背后的最优化问题.html":{"ref":"支撑向量机SVM/11.2 SVM背后的最优化问题.html","tf":0.16666666666666666}}}}}}}}},"这":{"docs":{},"种":{"docs":{},"思":{"docs":{},"想":{"docs":{},"对":{"docs":{},"于":{"docs":{},"未":{"docs":{},"来":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"的":{"docs":{},"考":{"docs":{},"量":{"docs":{},"没":{"docs":{},"有":{"docs":{},"集":{"docs":{},"中":{"docs":{},"在":{"docs":{},"数":{"docs":{},"据":{"docs":{},"预":{"docs":{},"处":{"docs":{},"理":{"docs":{},"阶":{"docs":{},"段":{"docs":{},"，":{"docs":{},"或":{"docs":{},"者":{"docs":{},"是":{"docs":{},"找":{"docs":{},"到":{"docs":{},"了":{"docs":{},"这":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"之":{"docs":{},"后":{"docs":{},"在":{"docs":{},"进":{"docs":{},"行":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"。":{"docs":{},"而":{"docs":{},"是":{"docs":{},"将":{"docs":{},"这":{"docs":{},"种":{"docs":{},"考":{"docs":{},"量":{"docs":{},"放":{"docs":{},"在":{"docs":{},"了":{"docs":{},"算":{"docs":{},"法":{"docs":{},"内":{"docs":{},"部":{"docs":{},"。":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"要":{"docs":{},"找":{"docs":{},"到":{"docs":{},"一":{"docs":{},"条":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"离":{"docs":{},"两":{"docs":{},"类":{"docs":{},"样":{"docs":{},"本":{"docs":{},"都":{"docs":{},"尽":{"docs":{},"可":{"docs":{},"能":{"docs":{},"的":{"docs":{},"远":{"docs":{},"，":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"，":{"docs":{},"他":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"就":{"docs":{},"是":{"docs":{},"好":{"docs":{},"的":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"进":{"docs":{},"行":{"docs":{},"改":{"docs":{},"变":{"docs":{},"得":{"docs":{},"来":{"docs":{},"的":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05}}}}}}}}},"，":{"docs":{},"其":{"docs":{},"是":{"docs":{},"在":{"docs":{},"h":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05}}}}}}}}},"取":{"docs":{},"值":{"docs":{},"越":{"docs":{},"小":{"docs":{},"，":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"他":{"docs":{},"的":{"docs":{},"容":{"docs":{},"错":{"docs":{},"空":{"docs":{},"间":{"docs":{},"更":{"docs":{},"大":{"docs":{},"一":{"docs":{},"些":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}}}}}}}},"背":{"docs":{},"后":{"docs":{},"的":{"docs":{},"最":{"docs":{},"优":{"docs":{},"化":{"docs":{},"问":{"docs":{},"题":{"docs":{"支撑向量机SVM/11.2 SVM背后的最优化问题.html":{"ref":"支撑向量机SVM/11.2 SVM背后的最优化问题.html","tf":5.166666666666667}}}}}}}}}},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"表":{"docs":{},"达":{"docs":{},"式":{"docs":{},"为":{"docs":{"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":0.0625}}}}}}}}},"就":{"docs":{},"退":{"docs":{},"化":{"docs":{},"成":{"docs":{},"了":{"docs":{},"h":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":0.0625}}}}}}}}}}},"中":{"docs":{},"使":{"docs":{},"用":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"特":{"docs":{},"征":{"docs":{},"和":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":5.004926108374384}}}}}}}}}}}}}},"思":{"docs":{},"想":{"docs":{},"解":{"docs":{},"决":{"docs":{},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":5.011363636363637}}}}}}}}}},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}}}}}}}},"算":{"docs":{},"法":{"docs":{},"：":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"b":{"docs":{},"a":{"docs":{},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},":":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.022727272727272728}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}}},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}},".":{"docs":{},"_":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}},"[":{"0":{"docs":{},"]":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}},"1":{"docs":{},":":{"docs":{},"]":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}},"docs":{}}}}}}},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"m":{"docs":{},"o":{"docs":{},"i":{"docs":{},"d":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}}}}}}}}}}}}},"k":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"_":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}},"[":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"]":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{},"_":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}},"[":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}},"a":{"docs":{},"_":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.015625}},"*":{"docs":{},"x":{"docs":{},"_":{"docs":{},"s":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"l":{"docs":{},"e":{"docs":{},"+":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},".":{"docs":{},"b":{"docs":{},"_":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}},"b":{"docs":{},"_":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.01171875}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}}}},"c":{"docs":{},"o":{"docs":{},"e":{"docs":{},"f":{"docs":{},"_":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.00872093023255814},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},"_":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"c":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.00872093023255814},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}},"e":{"docs":{},"d":{"docs":{},"=":{"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},")":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.058823529411764705},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.016},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.008771929824561403},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"1":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}},"(":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}}},":":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}}}},"g":{"docs":{},"m":{"docs":{},"o":{"docs":{},"i":{"docs":{},"d":{"docs":{},"(":{"docs":{},"t":{"docs":{},")":{"docs":{},":":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}},"x":{"docs":{},")":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}}}}}}}},"q":{"docs":{},"r":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}},"(":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{},"(":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}}}},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"e":{"docs":{},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"r":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"a":{"docs":{},"r":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":3.371069182389937}},"e":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}},"d":{"docs":{},"简":{"docs":{},"介":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}}}}}}},"h":{"docs":{},"u":{"docs":{},"f":{"docs":{},"f":{"docs":{},"l":{"docs":{},"e":{"docs":{},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322}},"e":{"docs":{},"s":{"docs":{},"[":{"docs":{},":":{"docs":{},"t":{"docs":{},"e":{"docs":{},"t":{"docs":{},"s":{"docs":{},"_":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}},"t":{"docs":{},"e":{"docs":{},"t":{"docs":{},"s":{"docs":{},"_":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},":":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"n":{"docs":{},"k":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}}}}}}}}}}}}}},"p":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322}},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"'":{"docs":{},")":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}}}}},"i":{"docs":{},"l":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"1":{"docs":{},"_":{"docs":{},"r":{"docs":{},",":{"docs":{},"y":{"1":{"docs":{},"_":{"docs":{},"r":{"docs":{},",":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"d":{"2":{"docs":{},",":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"v":{"2":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"docs":{}}}}}}}}},"docs":{}}}}}}}}}}},"docs":{}}}}}},"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.012552301255230125},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.012295081967213115}}}}}}}}},"o":{"docs":{},"l":{"docs":{},"v":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}},"e":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"l":{"docs":{},"i":{"docs":{},"b":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"'":{"docs":{},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{},"t":{"docs":{},"o":{"docs":{},"n":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}},"f":{"docs":{},"t":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05},"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":2.5625},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}},"v":{"docs":{},"o":{"docs":{},"t":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":3.3619047619047615}}}}}}},"r":{"docs":{},"t":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}},"y":{"docs":{},"s":{"docs":{},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.025974025974025976},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}}}},",":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.018691588785046728},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.017142857142857144},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02358490566037736},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}}},"t":{"0":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.012048192771084338},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"=":{"5":{"docs":{},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.016}}}},"docs":{}}},"1":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.012048192771084338},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"=":{"5":{"0":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}},",":{"docs":{},"k":{"docs":{},"=":{"1":{"0":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}},"docs":{}},"docs":{}}}}},"docs":{}},"docs":{}}},"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"s":{"docs":{},"p":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"用":{"docs":{},"于":{"docs":{},"分":{"docs":{},"割":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"和":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"y":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},",":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"r":{"docs":{},"a":{"docs":{},"d":{"docs":{},"i":{"docs":{},"o":{"docs":{},"=":{"0":{"docs":{},".":{"2":{"5":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"docs":{}},"docs":{}}},"docs":{}}}}}}},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"0":{"docs":{},".":{"2":{"docs":{},",":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},"_":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"=":{"6":{"6":{"6":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}},"4":{"docs":{},",":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},"_":{"docs":{},"s":{"docs":{},"t":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},"_":{"docs":{},"s":{"docs":{},"t":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"=":{"1":{"0":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}},"docs":{}},"6":{"6":{"6":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"l":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}},"e":{"docs":{},".":{"docs":{},"a":{"docs":{},"p":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"(":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"e":{"docs":{},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"r":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"[":{"docs":{},":":{"docs":{},"i":{"docs":{},"]":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"!":{"docs":{},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}},"y":{"docs":{},":":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}}},"_":{"docs":{},"s":{"docs":{},"p":{"docs":{},"i":{"docs":{},"l":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"1":{"docs":{},"_":{"docs":{},"r":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}},"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541}}}}}}}}}}}},"u":{"docs":{},"e":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}},"_":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}},"t":{"docs":{},"h":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}},"e":{"docs":{},"e":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198}}}}},"o":{"docs":{},"p":{"docs":{},"k":{"docs":{},"_":{"docs":{},"i":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}}}},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.025974025974025976},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}}}}},"l":{"docs":{},"=":{"0":{"docs":{},".":{"0":{"0":{"0":{"1":{"docs":{},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}},"docs":{}},"1":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322}},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}},"r":{"docs":{},"a":{"docs":{},"d":{"docs":{},"i":{"docs":{},"o":{"docs":{},"=":{"0":{"docs":{},".":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"docs":{}}},"docs":{}}}}},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}},"e":{"docs":{},".":{"docs":{},"a":{"docs":{},"p":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"(":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"e":{"docs":{},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"r":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},"=":{"0":{"docs":{},".":{"8":{"docs":{},")":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}},"docs":{}}},"docs":{}}}}}}}}},"t":{"docs":{},"s":{"docs":{},"_":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322}}}}}}}}},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.0321285140562249},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.016304347826086956},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}},"_":{"1":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}},"[":{"docs":{},"i":{"docs":{},"]":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}},"2":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}},"[":{"docs":{},"i":{"docs":{},"]":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}},"docs":{},"h":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.05303030303030303}}},"y":{"docs":{},".":{"docs":{},"a":{"docs":{},"p":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"(":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}}}}}}}}}}}}}}},"[":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"p":{"docs":{},"y":{"docs":{},"(":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.02158273381294964},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0375}},"。":{"docs":{},"可":{"docs":{},"不":{"docs":{},"可":{"docs":{},"以":{"docs":{},"让":{"docs":{},"这":{"docs":{},"个":{"docs":{},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"作":{"docs":{},"为":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"呢":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}}}}},"这":{"docs":{},"样":{"docs":{},"，":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"添":{"docs":{},"加":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"新":{"docs":{},"的":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"指":{"docs":{},"定":{"docs":{},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"，":{"docs":{},"相":{"docs":{},"对":{"docs":{},"于":{"docs":{},"可":{"docs":{},"以":{"docs":{},"平":{"docs":{},"移":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"的":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"进":{"docs":{},"而":{"docs":{},"影":{"docs":{},"响":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},",":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}},"d":{"docs":{},"t":{"docs":{},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{},"=":{"docs":{},"'":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"'":{"docs":{},")":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}},".":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}},"s":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}},":":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.025974025974025976},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}},"s":{"docs":{},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.025974025974025976},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}}}}}},"s":{"docs":{},"e":{"docs":{},"r":{"docs":{},"e":{"docs":{},"t":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"'":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}},")":{"docs":{},")":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},":":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}},"n":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}},"p":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.022222222222222223}},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.016666666666666666}},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{},"]":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.025}},",":{"docs":{},"f":{"docs":{},"p":{"docs":{},"r":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}},"s":{"docs":{},")":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}},",":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}},".":{"docs":{},"a":{"docs":{},"p":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"(":{"docs":{},"t":{"docs":{},"p":{"docs":{},"r":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}}},"和":{"docs":{},"f":{"docs":{},"p":{"docs":{},"r":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"关":{"docs":{},"系":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}},"：":{"docs":{},"预":{"docs":{},"测":{"docs":{},"为":{"1":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"预":{"docs":{},"测":{"docs":{},"对":{"docs":{},"了":{"docs":{},"的":{"docs":{},"数":{"docs":{},"量":{"docs":{},"占":{"docs":{},"真":{"docs":{},"实":{"docs":{},"值":{"docs":{},"为":{"1":{"docs":{},"的":{"docs":{},"百":{"docs":{},"分":{"docs":{},"比":{"docs":{},"是":{"docs":{},"多":{"docs":{},"少":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}}}}},"v":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.01098901098901099},"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}},"r":{"docs":{},"b":{"docs":{},"o":{"docs":{},"s":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}},"e":{"docs":{},"=":{"0":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815}},")":{"docs":{},"]":{"docs":{},")":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}},",":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}},"1":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}},"docs":{},"f":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},"e":{"docs":{},")":{"docs":{},")":{"docs":{},"]":{"docs":{},")":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}}}}}}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"d":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"可":{"docs":{},"以":{"docs":{},"叫":{"docs":{},"做":{"docs":{},"留":{"docs":{},"一":{"docs":{},"法":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}},"u":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"e":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},":":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}},"i":{"docs":{},"l":{"docs":{},"d":{"docs":{},"\"":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}},"o":{"docs":{},"t":{"docs":{},"e":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.05714285714285714}},"s":{"docs":{},".":{"docs":{},"m":{"docs":{},"o":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"m":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"1":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}},"[":{"0":{"docs":{},"]":{"docs":{},"[":{"0":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"docs":{}}}},"docs":{}}}},"docs":{}}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"=":{"docs":{},"\"":{"docs":{},"h":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"\"":{"docs":{},")":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}},"s":{"docs":{},"o":{"docs":{},"f":{"docs":{},"t":{"docs":{},"\"":{"docs":{},")":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"2":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}},"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}}}}}}}}},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"=":{"docs":{},"[":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}}},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"中":{"docs":{},"的":{"docs":{},"u":{"docs":{},"s":{"docs":{},"e":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{},"r":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}},"随":{"docs":{},"机":{"docs":{},"一":{"docs":{},"个":{"docs":{},"元":{"docs":{},"素":{"docs":{},"进":{"docs":{},"行":{"docs":{},"导":{"docs":{},"数":{"docs":{},"公":{"docs":{},"式":{"docs":{},"的":{"docs":{},"计":{"docs":{},"算":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}}},"选":{"docs":{},"择":{"docs":{},"k":{"docs":{},"个":{"docs":{},"元":{"docs":{},"素":{"docs":{},"进":{"docs":{},"行":{"docs":{},"导":{"docs":{},"数":{"docs":{},"公":{"docs":{},"式":{"docs":{},"的":{"docs":{},"计":{"docs":{},"算":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}}}}}},"y":{"docs":{},"=":{"1":{"docs":{},"是":{"docs":{},"真":{"docs":{},"值":{"docs":{},"；":{"docs":{},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"σ":{"docs":{},"函":{"docs":{},"数":{"docs":{},"求":{"docs":{},"出":{"docs":{},"来":{"docs":{},"的":{"docs":{},"是":{"docs":{},"预":{"docs":{},"测":{"docs":{},"值":{"docs":{},"）":{"docs":{"逻辑回归/2.逻辑回归的损失函数.html":{"ref":"逻辑回归/2.逻辑回归的损失函数.html","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"阈":{"docs":{},"值":{"docs":{},"，":{"docs":{},"来":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"调":{"docs":{},"整":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"总":{"docs":{},"结":{"docs":{},"整":{"docs":{},"理":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":5}}}}}}}}}}}}}}},"主":{"docs":{},"要":{"docs":{},"以":{"docs":{},"s":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.01098901098901099}}}}}}}}},"成":{"docs":{},"分":{"docs":{},"分":{"docs":{},"析":{"docs":{},"法":{"docs":{"PCA/":{"ref":"PCA/","tf":0.5}}}}}}}},"以":{"docs":{},"下":{"docs":{},"列":{"docs":{},"出":{"docs":{},"本":{"docs":{},"笔":{"docs":{},"记":{"docs":{},"（":{"docs":{},"课":{"docs":{},"程":{"docs":{},"）":{"docs":{},"学":{"docs":{},"习":{"docs":{},"使":{"docs":{},"用":{"docs":{},"到":{"docs":{},"的":{"docs":{},"s":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"定":{"docs":{},"义":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"参":{"docs":{},"数":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"j":{"docs":{},"的":{"docs":{},"值":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"示":{"docs":{},"例":{"docs":{},"图":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"需":{"docs":{},"要":{"docs":{},"找":{"docs":{},"到":{"docs":{},"使":{"docs":{},"得":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"值":{"docs":{},"j":{"docs":{},"取":{"docs":{},"得":{"docs":{},"最":{"docs":{},"小":{"docs":{},"值":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"（":{"docs":{},"这":{"docs":{},"里":{"docs":{},"是":{"docs":{},"二":{"docs":{},"维":{"docs":{},"平":{"docs":{},"面":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"只":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"）":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"上":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"所":{"docs":{},"谓":{"docs":{},"的":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}},"此":{"docs":{},"类":{"docs":{},"推":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"分":{"docs":{},"别":{"docs":{},"进":{"docs":{},"行":{"docs":{},"四":{"docs":{},"次":{"docs":{},"分":{"docs":{},"类":{"docs":{},"，":{"docs":{},"哪":{"docs":{},"次":{"docs":{},"获":{"docs":{},"得":{"docs":{},"的":{"docs":{},"类":{"docs":{},"别":{"docs":{},"得":{"docs":{},"分":{"docs":{},"最":{"docs":{},"高":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"任":{"docs":{},"务":{"docs":{},"他":{"docs":{},"属":{"docs":{},"于":{"docs":{},"哪":{"docs":{},"一":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"。":{"docs":{},"对":{"docs":{},"于":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"p":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"1":{"0":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}},"docs":{}},"2":{"0":{"docs":{},"阶":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}},"docs":{}},"6":{"4":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"训":{"docs":{},"练":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"算":{"docs":{},"法":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{},"支":{"docs":{},"持":{"docs":{},"向":{"docs":{},"量":{"docs":{},"机":{"docs":{},"思":{"docs":{},"想":{"docs":{},"解":{"docs":{},"决":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"的":{"docs":{},"支":{"docs":{},"撑":{"docs":{},"向":{"docs":{},"量":{"docs":{},"机":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}},"解":{"docs":{},"决":{"docs":{},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"支":{"docs":{},"撑":{"docs":{},"向":{"docs":{},"量":{"docs":{},"机":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}},"%":{"docs":{},"l":{"docs":{},"s":{"docs":{},"m":{"docs":{},"a":{"docs":{},"g":{"docs":{},"i":{"docs":{},"c":{"docs":{},"查":{"docs":{},"看":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"魔":{"docs":{},"法":{"docs":{},"命":{"docs":{},"令":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{},"i":{"docs":{},"t":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}},"让":{"docs":{},"测":{"docs":{},"试":{"docs":{},"只":{"docs":{},"执":{"docs":{},"行":{"docs":{},"一":{"docs":{},"次":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}}}}}},"n":{"docs":{},"u":{"docs":{},"m":{"docs":{},"p":{"docs":{},"y":{"docs":{},"创":{"docs":{},"建":{"docs":{},"数":{"docs":{},"组":{"docs":{},"(":{"docs":{},"和":{"docs":{},"p":{"docs":{},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"n":{"docs":{},"的":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{},"中":{"docs":{},"几":{"docs":{},"乎":{"docs":{},"一":{"docs":{},"样":{"docs":{},")":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":0.1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"_":{"docs":{},"j":{"docs":{},"o":{"docs":{},"b":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}},"我":{"docs":{},"们":{"docs":{},"自":{"docs":{},"己":{"docs":{},"封":{"docs":{},"装":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{},"分":{"docs":{},"割":{"docs":{},"函":{"docs":{},"数":{"docs":{},"分":{"docs":{},"割":{"docs":{},"训":{"docs":{},"练":{"docs":{},"集":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}}}}},"的":{"docs":{},"s":{"docs":{},"i":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"1":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}},"向":{"docs":{},"量":{"docs":{},"化":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"计":{"docs":{},"算":{"docs":{},"分":{"docs":{},"子":{"docs":{},"和":{"docs":{},"分":{"docs":{},"母":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}}}}}},"不":{"docs":{},"同":{"docs":{},"η":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"测":{"docs":{},"试":{"docs":{},"并":{"docs":{},"观":{"docs":{},"察":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}}}}}}}}}}}}}}},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"训":{"docs":{},"练":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}},"o":{"docs":{},"g":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}},"上":{"docs":{},"升":{"docs":{},"法":{"docs":{},"求":{"docs":{},"解":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}}}},"真":{"docs":{},"实":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"测":{"docs":{},"试":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}},"，":{"docs":{},"调":{"docs":{},"整":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"和":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"，":{"docs":{},"要":{"docs":{},"么":{"docs":{},"由":{"docs":{},"于":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"太":{"docs":{},"小":{"docs":{},"导":{"docs":{},"致":{"docs":{},"无":{"docs":{},"法":{"docs":{},"得":{"docs":{},"出":{"docs":{},"真":{"docs":{},"实":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"要":{"docs":{},"么":{"docs":{},"由":{"docs":{},"于":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"太":{"docs":{},"大":{"docs":{},"导":{"docs":{},"致":{"docs":{},"训":{"docs":{},"练":{"docs":{},"时":{"docs":{},"间":{"docs":{},"加":{"docs":{},"长":{"docs":{},"，":{"docs":{},"这":{"docs":{},"是":{"docs":{},"由":{"docs":{},"于":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"规":{"docs":{},"模":{"docs":{},"在":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"上":{"docs":{},"不":{"docs":{},"同":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"我":{"docs":{},"们":{"docs":{},"需":{"docs":{},"要":{"docs":{},"对":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"中":{"docs":{},"的":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}},"o":{"docs":{},"v":{"docs":{},"o":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"直":{"docs":{},"观":{"docs":{},"的":{"docs":{},"感":{"docs":{},"受":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}}}},"提":{"docs":{},"供":{"docs":{},"的":{"docs":{},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}},"v":{"docs":{},"m":{"docs":{},"之":{"docs":{},"前":{"docs":{},"，":{"docs":{},"和":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"一":{"docs":{},"样":{"docs":{},"，":{"docs":{},"也":{"docs":{},"是":{"docs":{},"要":{"docs":{},"做":{"docs":{},"数":{"docs":{},"据":{"docs":{},"标":{"docs":{},"准":{"docs":{},"化":{"docs":{},"处":{"docs":{},"理":{"docs":{},"的":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{},"是":{"docs":{},"涉":{"docs":{},"及":{"docs":{},"到":{"docs":{},"距":{"docs":{},"离":{"docs":{},"的":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"随":{"docs":{},"机":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"训":{"docs":{},"练":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"求":{"docs":{},"解":{"docs":{},"出":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}},"d":{"docs":{},"_":{"docs":{},"j":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"b":{"docs":{},"u":{"docs":{},"g":{"docs":{},"调":{"docs":{},"试":{"docs":{},"模":{"docs":{},"式":{"docs":{},"求":{"docs":{},"出":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"b":{"docs":{},"u":{"docs":{},"g":{"docs":{},"模":{"docs":{},"式":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"=":{"1":{"0":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"均":{"docs":{},"方":{"docs":{},"误":{"docs":{},"差":{"docs":{},"要":{"docs":{},"大":{"docs":{},"于":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"=":{"2":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"说":{"docs":{},"明":{"docs":{},"当":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"等":{"docs":{},"于":{"1":{"0":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"他":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"变":{"docs":{},"弱":{"docs":{},"了":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"2":{"0":{"docs":{},"得":{"docs":{},"出":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"的":{"docs":{},"外":{"docs":{},"面":{"docs":{},"变":{"docs":{},"的":{"docs":{},"很":{"docs":{},"奇":{"docs":{},"怪":{"docs":{},"。":{"docs":{},"出":{"docs":{},"现":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"就":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"=":{"2":{"0":{"docs":{},"太":{"docs":{},"大":{"docs":{},"了":{"docs":{},"。":{"docs":{},"导":{"docs":{},"致":{"docs":{},"边":{"docs":{},"界":{"docs":{},"的":{"docs":{},"形":{"docs":{},"状":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"不":{"docs":{},"规":{"docs":{},"则":{"docs":{},"。":{"docs":{},"此":{"docs":{},"时":{"docs":{},"显":{"docs":{},"然":{"docs":{},"发":{"docs":{},"生":{"docs":{},"了":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}},"数":{"docs":{},"学":{"docs":{},"解":{"docs":{},"求":{"docs":{},"出":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},"数":{"docs":{},"学":{"docs":{},"解":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"进":{"docs":{},"行":{"docs":{},"降":{"docs":{},"维":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"再":{"docs":{},"训":{"docs":{},"练":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"算":{"docs":{},"法":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}},"后":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"进":{"docs":{},"行":{"docs":{},"训":{"docs":{},"练":{"docs":{},"，":{"docs":{},"不":{"docs":{},"光":{"docs":{},"时":{"docs":{},"间":{"docs":{},"变":{"docs":{},"短":{"docs":{},"了":{"docs":{},"，":{"docs":{},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{},"也":{"docs":{},"变":{"docs":{},"高":{"docs":{},"了":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"降":{"docs":{},"噪":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}},"i":{"docs":{},"p":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"构":{"docs":{},"建":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"构":{"docs":{},"建":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"模":{"docs":{},"型":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}},"可":{"docs":{},"以":{"docs":{},"解":{"docs":{},"释":{"docs":{},"百":{"docs":{},"分":{"docs":{},"之":{"9":{"0":{"docs":{},"原":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"绘":{"docs":{},"制":{"docs":{},"特":{"docs":{},"征":{"docs":{},"脸":{"docs":{},"，":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}},"上":{"docs":{},"小":{"docs":{},"节":{"docs":{},"的":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"得":{"docs":{},"知":{"docs":{},"，":{"docs":{},"虽":{"docs":{},"然":{"docs":{},"我":{"docs":{},"们":{"docs":{},"训":{"docs":{},"练":{"docs":{},"出":{"docs":{},"的":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"将":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"的":{"docs":{},"非":{"docs":{},"常":{"docs":{},"好":{"docs":{},"，":{"docs":{},"总":{"docs":{},"体":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"小":{"docs":{},"，":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"均":{"docs":{},"方":{"docs":{},"误":{"docs":{},"差":{"docs":{},"来":{"docs":{},"看":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"这":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"同":{"docs":{},"样":{"docs":{},"都":{"docs":{},"是":{"docs":{},"对":{"docs":{},"一":{"docs":{},"组":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"使":{"docs":{},"用":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"对":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"拟":{"docs":{},"合":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}},"后":{"docs":{},"分":{"docs":{},"数":{"docs":{},"达":{"docs":{},"到":{"docs":{},"了":{"docs":{},"百":{"docs":{},"分":{"docs":{},"之":{"9":{"5":{"docs":{},".":{"docs":{},"结":{"docs":{},"果":{"docs":{},"非":{"docs":{},"常":{"docs":{},"好":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"的":{"docs":{},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}},"为":{"docs":{},"什":{"docs":{},"么":{"docs":{},"能":{"docs":{},"处":{"docs":{},"理":{"docs":{},"非":{"docs":{},"线":{"docs":{},"性":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"问":{"docs":{},"题":{"docs":{},"：":{"docs":{},"他":{"docs":{},"的":{"docs":{},"基":{"docs":{},"本":{"docs":{},"原":{"docs":{},"理":{"docs":{},"是":{"docs":{},"依":{"docs":{},"靠":{"docs":{},"升":{"docs":{},"维":{"docs":{},"使":{"docs":{},"得":{"docs":{},"原":{"docs":{},"本":{"docs":{},"线":{"docs":{},"性":{"docs":{},"不":{"docs":{},"可":{"docs":{},"分":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"线":{"docs":{},"性":{"docs":{},"可":{"docs":{},"分":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"更":{"docs":{},"多":{"docs":{},"的":{"docs":{},"维":{"docs":{},"度":{"docs":{},"进":{"docs":{},"行":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}},"二":{"docs":{},"阶":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"是":{"docs":{},"比":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"调":{"docs":{},"参":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}},"分":{"docs":{},"割":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"和":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"来":{"docs":{},"判":{"docs":{},"断":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"性":{"docs":{},"能":{"docs":{},"的":{"docs":{},"好":{"docs":{},"坏":{"docs":{},"，":{"docs":{},"虽":{"docs":{},"然":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"非":{"docs":{},"常":{"docs":{},"好":{"docs":{},"的":{"docs":{},"方":{"docs":{},"案":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"会":{"docs":{},"产":{"docs":{},"生":{"docs":{},"一":{"docs":{},"个":{"docs":{},"问":{"docs":{},"题":{"docs":{},"：":{"docs":{},"针":{"docs":{},"对":{"docs":{},"特":{"docs":{},"定":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"岭":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"如":{"docs":{},"果":{"docs":{},"某":{"docs":{},"一":{"docs":{},"项":{"docs":{},"θ":{"docs":{},"等":{"docs":{},"于":{"0":{"docs":{},"了":{"docs":{},"，":{"docs":{},"就":{"docs":{},"说":{"docs":{},"明":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"s":{"docs":{},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"将":{"docs":{},"x":{"docs":{},"轴":{"docs":{},"，":{"docs":{},"y":{"docs":{},"轴":{"docs":{},"划":{"docs":{},"分":{"docs":{},"成":{"docs":{},"无":{"docs":{},"数":{"docs":{},"的":{"docs":{},"小":{"docs":{},"点":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}}}}}}}}}},"|":{"docs":{},"θ":{"docs":{},"|":{"docs":{},"代":{"docs":{},"替":{"docs":{},"θ":{"2":{"docs":{},"来":{"docs":{},"标":{"docs":{},"示":{"docs":{},"θ":{"docs":{},"的":{"docs":{},"大":{"docs":{},"小":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}},"docs":{}}}}}}},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}},"这":{"docs":{},"种":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"。":{"docs":{},"如":{"docs":{},"果":{"docs":{},"c":{"docs":{},"很":{"docs":{},"小":{"docs":{},"。":{"docs":{},"那":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"就":{"docs":{},"是":{"docs":{},"集":{"docs":{},"中":{"docs":{},"精":{"docs":{},"力":{"docs":{},"调":{"docs":{},"整":{"docs":{},"l":{"1":{"docs":{},"或":{"docs":{},"者":{"docs":{},"l":{"2":{"docs":{},"的":{"docs":{},"大":{"docs":{},"小":{"docs":{},"；":{"docs":{},"如":{"docs":{},"果":{"docs":{},"c":{"docs":{},"很":{"docs":{},"大":{"docs":{},"，":{"docs":{},"那":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"就":{"docs":{},"是":{"docs":{},"集":{"docs":{},"中":{"docs":{},"精":{"docs":{},"力":{"docs":{},"调":{"docs":{},"整":{"docs":{},"原":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"j":{"docs":{},"(":{"docs":{},"θ":{"docs":{},")":{"docs":{},"的":{"docs":{},"大":{"docs":{},"小":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"好":{"docs":{},"处":{"docs":{},"是":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"不":{"docs":{},"得":{"docs":{},"不":{"docs":{},"进":{"docs":{},"行":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"；":{"docs":{},"因":{"docs":{},"为":{"docs":{},"l":{"1":{"docs":{},"/":{"docs":{},"l":{"2":{"docs":{},"前":{"docs":{},"面":{"docs":{},"的":{"docs":{},"系":{"docs":{},"数":{"docs":{},"不":{"docs":{},"能":{"docs":{},"为":{"0":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}},"docs":{}}}}}}}}}},"docs":{}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"v":{"docs":{},"o":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"预":{"docs":{},"测":{"docs":{},"结":{"docs":{},"果":{"docs":{},"达":{"docs":{},"到":{"docs":{},"了":{"docs":{},"百":{"docs":{},"分":{"docs":{},"之":{"docs":{},"百":{"docs":{},"；":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"b":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"寻":{"docs":{},"找":{"docs":{},"最":{"docs":{},"优":{"docs":{},"划":{"docs":{},"分":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":5.00418410041841}}}}}}}}}}},"v":{"docs":{},"o":{"docs":{},"t":{"docs":{},"e":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"方":{"docs":{},"式":{"docs":{},"进":{"docs":{},"行":{"docs":{},"集":{"docs":{},"成":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"也":{"docs":{},"叫":{"docs":{},"随":{"docs":{},"机":{"docs":{},"森":{"docs":{},"林":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}}}}}}}}}}}}}},"每":{"docs":{},"次":{"docs":{},"增":{"docs":{},"强":{"docs":{},"训":{"docs":{},"练":{"docs":{},"出":{"docs":{},"来":{"docs":{},"的":{"docs":{},"子":{"docs":{},"模":{"docs":{},"型":{"docs":{},"进":{"docs":{},"行":{"docs":{},"投":{"docs":{},"票":{"docs":{},"得":{"docs":{},"出":{"docs":{},"最":{"docs":{},"终":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}}}}}}}}}}}}}},"模":{"docs":{},"型":{"docs":{},"产":{"docs":{},"生":{"docs":{},"差":{"docs":{},"异":{"docs":{},"化":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}},"官":{"docs":{},"方":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{},"代":{"docs":{},"码":{"docs":{},"：":{"docs":{},"h":{"docs":{},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},"s":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},"l":{"docs":{},"i":{"docs":{},"u":{"docs":{},"y":{"docs":{},"u":{"docs":{},"b":{"docs":{},"o":{"docs":{},"b":{"docs":{},"o":{"docs":{},"b":{"docs":{},"o":{"docs":{},"/":{"docs":{},"p":{"docs":{},"l":{"docs":{},"a":{"docs":{},"y":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"曲":{"docs":{},"线":{"docs":{"./":{"ref":"./","tf":0.005494505494505495},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.014388489208633094},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}},"本":{"docs":{},"笔":{"docs":{},"记":{"docs":{},"记":{"docs":{},"录":{"docs":{},"笔":{"docs":{},"者":{"docs":{},"观":{"docs":{},"看":{"docs":{},"慕":{"docs":{},"课":{"docs":{},"网":{"docs":{},"入":{"docs":{},"门":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"课":{"docs":{},"程":{"docs":{},"的":{"docs":{},"笔":{"docs":{},"记":{"docs":{},"过":{"docs":{},"程":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}}}}}},"次":{"docs":{},"测":{"docs":{},"试":{"docs":{},"时":{"docs":{},"间":{"docs":{},"比":{"docs":{},"上":{"docs":{},"面":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{},"时":{"docs":{},"间":{"docs":{},"会":{"docs":{},"多":{"docs":{},"，":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"只":{"docs":{},"测":{"docs":{},"试":{"docs":{},"了":{"docs":{},"一":{"docs":{},"次":{"docs":{},"。":{"docs":{},"可":{"docs":{},"能":{"docs":{},"不":{"docs":{},"够":{"docs":{},"准":{"docs":{},"确":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"质":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333}}}},"用":{"docs":{},"于":{"docs":{},"加":{"docs":{},"载":{"docs":{},"手":{"docs":{},"写":{"docs":{},"识":{"docs":{},"别":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}},"波":{"docs":{},"士":{"docs":{},"顿":{"docs":{},"房":{"docs":{},"价":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}},"鸢":{"docs":{},"尾":{"docs":{},"花":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}},"我":{"docs":{},"们":{"docs":{},"找":{"docs":{},"到":{"docs":{},"的":{"docs":{},"k":{"docs":{},"和":{"docs":{},"p":{"docs":{},"。":{"docs":{},"来":{"docs":{},"对":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"整":{"docs":{},"体":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"一":{"docs":{},"下":{"docs":{},"，":{"docs":{},"来":{"docs":{},"看":{"docs":{},"他":{"docs":{},"对":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{},"结":{"docs":{},"果":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"笔":{"docs":{},"记":{"docs":{},"整":{"docs":{},"理":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}},"简":{"docs":{},"介":{"docs":{"./":{"ref":"./","tf":10}}},"单":{"docs":{},"自":{"docs":{},"定":{"docs":{},"义":{"docs":{},"一":{"docs":{},"个":{"docs":{},"训":{"docs":{},"练":{"docs":{},"集":{"docs":{},"并":{"docs":{},"描":{"docs":{},"绘":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}},"模":{"docs":{},"拟":{"docs":{},"一":{"docs":{},"个":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}}},"重":{"docs":{},"点":{"docs":{},"看":{"docs":{},"h":{"docs":{},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"s":{"docs":{},"c":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.005494505494505495}}}}}}}}}}}}}}}}}},"有":{"docs":{},"关":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"一":{"docs":{},"些":{"docs":{},"术":{"docs":{},"语":{"docs":{"chapter1/you-guan-shu-ju-de-yi-xie-zhu-yu.html":{"ref":"chapter1/you-guan-shu-ju-de-yi-xie-zhu-yu.html","tf":10}}}}}}}}}},"一":{"docs":{},"些":{"docs":{},"算":{"docs":{},"法":{"docs":{},"只":{"docs":{},"能":{"docs":{},"解":{"docs":{},"决":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}},"天":{"docs":{},"然":{"docs":{},"可":{"docs":{},"以":{"docs":{},"完":{"docs":{},"成":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}},"生":{"docs":{},"是":{"docs":{},"高":{"docs":{},"偏":{"docs":{},"差":{"docs":{},"算":{"docs":{},"法":{"docs":{},"。":{"docs":{},"如":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"（":{"docs":{},"用":{"docs":{},"一":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{},"去":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"一":{"docs":{},"条":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"，":{"docs":{},"导":{"docs":{},"致":{"docs":{},"整":{"docs":{},"体":{"docs":{},"预":{"docs":{},"测":{"docs":{},"结":{"docs":{},"果":{"docs":{},"都":{"docs":{},"距":{"docs":{},"离":{"docs":{},"真":{"docs":{},"实":{"docs":{},"数":{"docs":{},"据":{"docs":{},"查":{"docs":{},"很":{"docs":{},"大":{"docs":{},"，":{"docs":{},"偏":{"docs":{},"差":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{},"）":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"方":{"docs":{},"差":{"docs":{},"算":{"docs":{},"法":{"docs":{},"。":{"docs":{},"如":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"（":{"docs":{},"过":{"docs":{},"于":{"docs":{},"依":{"docs":{},"赖":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"一":{"docs":{},"点":{"docs":{},"选":{"docs":{},"取":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"有":{"docs":{},"多":{"docs":{},"数":{"docs":{},"是":{"docs":{},"不":{"docs":{},"正":{"docs":{},"确":{"docs":{},"的":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"就":{"docs":{},"是":{"docs":{},"错":{"docs":{},"误":{"docs":{},"的":{"docs":{},"。":{"docs":{},"导":{"docs":{},"致":{"docs":{},"有":{"docs":{},"的":{"docs":{},"很":{"docs":{},"准":{"docs":{},"确":{"docs":{},"，":{"docs":{},"有":{"docs":{},"的":{"docs":{},"非":{"docs":{},"常":{"docs":{},"不":{"docs":{},"准":{"docs":{},"确":{"docs":{},"，":{"docs":{},"方":{"docs":{},"差":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{},"）":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"既":{"docs":{},"能":{"docs":{},"解":{"docs":{},"决":{"docs":{},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"也":{"docs":{},"能":{"docs":{},"解":{"docs":{},"决":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}}}}},"个":{"docs":{},"非":{"docs":{},"常":{"docs":{},"小":{"docs":{},"，":{"docs":{},"整":{"docs":{},"体":{"docs":{},"就":{"docs":{},"非":{"docs":{},"常":{"docs":{},"小":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"我":{"docs":{},"们":{"docs":{},"希":{"docs":{},"望":{"docs":{},"同":{"docs":{},"时":{"docs":{},"关":{"docs":{},"注":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"时":{"docs":{},"候":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"使":{"docs":{},"用":{"docs":{},"f":{"1":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}},"注":{"docs":{},"重":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"，":{"docs":{},"如":{"docs":{},"病":{"docs":{},"人":{"docs":{},"诊":{"docs":{},"断":{"docs":{},"。":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"，":{"docs":{},"如":{"docs":{},"股":{"docs":{},"票":{"docs":{},"预":{"docs":{},"测":{"docs":{},"。":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}}}}}}},"了":{"docs":{},"这":{"docs":{},"个":{"docs":{},"点":{"docs":{},"到":{"docs":{},"直":{"docs":{},"线":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"得":{"docs":{},"出":{"docs":{},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{},"这":{"docs":{},"个":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"表":{"docs":{},"达":{"docs":{},"式":{"docs":{"支撑向量机SVM/11.2 SVM背后的最优化问题.html":{"ref":"支撑向量机SVM/11.2 SVM背后的最优化问题.html","tf":0.16666666666666666}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"一":{"docs":{},"些":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"回":{"docs":{},"归":{"docs":{},"任":{"docs":{},"务":{"docs":{},"可":{"docs":{},"以":{"docs":{},"简":{"docs":{},"化":{"docs":{},"成":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"比":{"docs":{},"如":{"docs":{},"学":{"docs":{},"生":{"docs":{},"的":{"docs":{},"具":{"docs":{},"体":{"docs":{},"成":{"docs":{},"绩":{"docs":{},"预":{"docs":{},"测":{"docs":{},"转":{"docs":{},"换":{"docs":{},"成":{"docs":{},"评":{"docs":{},"级":{"docs":{},"，":{"docs":{},"无":{"docs":{},"人":{"docs":{},"车":{"docs":{},"驾":{"docs":{},"驶":{"docs":{},"，":{"docs":{},"转":{"docs":{},"换":{"docs":{},"成":{"docs":{},"油":{"docs":{},"门":{"docs":{},"，":{"docs":{},"刹":{"docs":{},"车":{"docs":{},"，":{"docs":{},"方":{"docs":{},"向":{"docs":{},"盘":{"docs":{},"的":{"docs":{},"程":{"docs":{},"度":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"算":{"docs":{},"法":{"docs":{},"只":{"docs":{},"支":{"docs":{},"持":{"docs":{},"完":{"docs":{},"成":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}},"需":{"docs":{},"要":{"docs":{},"注":{"docs":{},"意":{"docs":{},"的":{"docs":{},"细":{"docs":{},"节":{"docs":{},"：":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}},"部":{"docs":{},"分":{"docs":{},"数":{"docs":{},"据":{"docs":{},"有":{"docs":{},"“":{"docs":{},"标":{"docs":{},"记":{"docs":{},"”":{"docs":{},"或":{"docs":{},"者":{"docs":{},"“":{"docs":{},"答":{"docs":{},"案":{"docs":{},"”":{"docs":{},"，":{"docs":{},"另":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"数":{"docs":{},"据":{"docs":{},"没":{"docs":{},"有":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}}}}}}}}},"个":{"docs":{},"陷":{"docs":{},"阱":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}},"三":{"docs":{},"维":{"docs":{},"空":{"docs":{},"间":{"docs":{},"中":{"docs":{},"的":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"（":{"docs":{},"x":{"docs":{},",":{"docs":{},"y":{"docs":{},"为":{"docs":{},"系":{"docs":{},"数":{"docs":{},"，":{"docs":{},"z":{"docs":{},"为":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"）":{"docs":{"梯度下降法/3.多元线性回归中的梯度下降法.html":{"ref":"梯度下降法/3.多元线性回归中的梯度下降法.html","tf":0.2}}}}}}}}}}}}}}}}}}}}}}}}}}}},"矩":{"docs":{},"阵":{"docs":{},"可":{"docs":{},"以":{"docs":{},"把":{"docs":{},"一":{"docs":{},"个":{"docs":{},"向":{"docs":{},"量":{"docs":{},"拉":{"docs":{},"伸":{"docs":{},"或":{"docs":{},"者":{"docs":{},"缩":{"docs":{},"短":{"docs":{},"λ":{"docs":{},"倍":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"向":{"docs":{},"量":{"docs":{},"就":{"docs":{},"是":{"docs":{},"特":{"docs":{},"征":{"docs":{},"向":{"docs":{},"量":{"docs":{},"，":{"docs":{},"λ":{"docs":{},"是":{"docs":{},"特":{"docs":{},"征":{"docs":{},"值":{"docs":{},"；":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"对":{"docs":{},"于":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"来":{"docs":{},"说":{"docs":{},"的":{"docs":{},"，":{"docs":{},"模":{"docs":{},"型":{"docs":{},"越":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"，":{"docs":{},"模":{"docs":{},"型":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"越":{"docs":{},"高":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"模":{"docs":{},"型":{"docs":{},"越":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"，":{"docs":{},"对":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"就":{"docs":{},"越":{"docs":{},"好":{"docs":{},"，":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"就":{"docs":{},"越":{"docs":{},"高":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"直":{"docs":{},"替":{"docs":{},"代":{"docs":{},"方":{"docs":{},"法":{"docs":{},"是":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{},"可":{"docs":{},"以":{"docs":{},"在":{"docs":{},"构":{"docs":{},"造":{"docs":{},"数":{"docs":{},"组":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"限":{"docs":{},"定":{"docs":{},"类":{"docs":{},"型":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"由":{"docs":{},"于":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{},"只":{"docs":{},"是":{"docs":{},"把":{"docs":{},"元":{"docs":{},"素":{"docs":{},"当":{"docs":{},"成":{"docs":{},"一":{"docs":{},"个":{"docs":{},"一":{"docs":{},"维":{"docs":{},"或":{"docs":{},"者":{"docs":{},"多":{"docs":{},"维":{"docs":{},"数":{"docs":{},"组":{"docs":{},"，":{"docs":{},"而":{"docs":{},"并":{"docs":{},"没":{"docs":{},"有":{"docs":{},"当":{"docs":{},"做":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"，":{"docs":{},"向":{"docs":{},"量":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"也":{"docs":{},"没":{"docs":{},"有":{"docs":{},"提":{"docs":{},"供":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"函":{"docs":{},"数":{"docs":{},"，":{"docs":{},"使":{"docs":{},"得":{"docs":{},"在":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"中":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"不":{"docs":{},"方":{"docs":{},"便":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":0.1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"般":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"使":{"docs":{},"用":{"docs":{},"距":{"docs":{},"离":{"docs":{},"的":{"docs":{},"导":{"docs":{},"数":{"docs":{},"作":{"docs":{},"为":{"docs":{},"权":{"docs":{},"证":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}}}},"组":{"docs":{},"线":{"docs":{},"性":{"docs":{},"不":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"向":{"docs":{},"量":{"docs":{},"可":{"docs":{},"以":{"docs":{},"组":{"docs":{},"成":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"向":{"docs":{},"量":{"docs":{},"空":{"docs":{},"间":{"docs":{},"；":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"而":{"docs":{},"已":{"docs":{},"，":{"docs":{},"拿":{"docs":{},"到":{"docs":{},"这":{"docs":{},"组":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"后":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"训":{"docs":{},"练":{"docs":{},"处":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"最":{"docs":{},"佳":{"docs":{},"模":{"docs":{},"型":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"共":{"2":{"9":{"1":{"4":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"求":{"docs":{},"出":{"docs":{},"了":{"2":{"9":{"1":{"4":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"方":{"docs":{},"面":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"方":{"docs":{},"便":{"docs":{},"直":{"docs":{},"观":{"docs":{},"的":{"docs":{},"看":{"docs":{},"出":{"docs":{},"在":{"docs":{},"人":{"docs":{},"脸":{"docs":{},"识":{"docs":{},"别":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"我":{"docs":{},"们":{"docs":{},"是":{"docs":{},"怎":{"docs":{},"么":{"docs":{},"看":{"docs":{},"到":{"docs":{},"每":{"docs":{},"一":{"docs":{},"张":{"docs":{},"脸":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"元":{"docs":{},"二":{"docs":{},"次":{"docs":{},"方":{"docs":{},"程":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}},"对":{"docs":{},"一":{"docs":{},"的":{"docs":{},"进":{"docs":{},"行":{"docs":{},"比":{"docs":{},"较":{"docs":{},"）":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}},"针":{"docs":{},"对":{"docs":{},"剩":{"docs":{},"余":{"docs":{},"）":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"函":{"docs":{},"数":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}}}}}},"基":{"docs":{},"尼":{"docs":{},"系":{"docs":{},"数":{"docs":{},"函":{"docs":{},"数":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}},"但":{"docs":{},"是":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"可":{"docs":{},"以":{"docs":{},"转":{"docs":{},"换":{"docs":{},"成":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}},"一":{"docs":{},"旦":{"docs":{},"来":{"docs":{},"了":{"docs":{},"新":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"，":{"docs":{},"他":{"docs":{},"就":{"docs":{},"不":{"docs":{},"能":{"docs":{},"很":{"docs":{},"好":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"了":{"docs":{},"，":{"docs":{},"在":{"docs":{},"这":{"docs":{},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"称":{"docs":{},"我":{"docs":{},"们":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"这":{"docs":{},"条":{"docs":{},"弯":{"docs":{},"弯":{"docs":{},"曲":{"docs":{},"曲":{"docs":{},"的":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"，":{"docs":{},"他":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"（":{"docs":{},"由":{"docs":{},"此":{"docs":{},"及":{"docs":{},"彼":{"docs":{},"的":{"docs":{},"能":{"docs":{},"力":{"docs":{},"）":{"docs":{},"非":{"docs":{},"常":{"docs":{},"弱":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"他":{"docs":{},"真":{"docs":{},"的":{"docs":{},"能":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"我":{"docs":{},"们":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"走":{"docs":{},"势":{"docs":{},"吗":{"docs":{},"，":{"docs":{},"例":{"docs":{},"如":{"docs":{},"我":{"docs":{},"们":{"docs":{},"选":{"docs":{},"择":{"2":{"docs":{},".":{"5":{"docs":{},"到":{"3":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"x":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"上":{"docs":{},"图":{"docs":{},"预":{"docs":{},"测":{"docs":{},"出":{"docs":{},"来":{"docs":{},"的":{"docs":{},"y":{"docs":{},"的":{"docs":{},"大":{"docs":{},"小":{"docs":{},"（":{"0":{"docs":{},"或":{"docs":{},"者":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"最":{"docs":{},"好":{"docs":{},"参":{"docs":{},"数":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"并":{"docs":{},"不":{"docs":{},"是":{"docs":{},"真":{"docs":{},"正":{"docs":{},"的":{"docs":{},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"这":{"docs":{},"种":{"docs":{},"方":{"docs":{},"式":{"docs":{},"只":{"docs":{},"是":{"docs":{},"为":{"docs":{},"了":{"docs":{},"拿":{"docs":{},"到":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"当":{"docs":{},"α":{"docs":{},"=":{"0":{"docs":{},".":{"1":{"docs":{},"，":{"docs":{},"虽":{"docs":{},"然":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"依":{"docs":{},"然":{"docs":{},"是":{"docs":{},"一":{"docs":{},"根":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"他":{"docs":{},"显":{"docs":{},"然":{"docs":{},"比":{"docs":{},"r":{"docs":{},"a":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"docs":{},"的":{"docs":{},"程":{"docs":{},"度":{"docs":{},"更":{"docs":{},"低":{"docs":{},"，":{"docs":{},"更":{"docs":{},"像":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"不":{"docs":{},"同":{"docs":{},",":{"docs":{},"在":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"的":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"中":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"我":{"docs":{},"们":{"docs":{},"让":{"docs":{},"α":{"docs":{},"趋":{"docs":{},"近":{"docs":{},"于":{"docs":{},"无":{"docs":{},"穷":{"docs":{},"，":{"docs":{},"只":{"docs":{},"看":{"docs":{},"后":{"docs":{},"面":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"后":{"docs":{},"面":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"的":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"值":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"是":{"docs":{},"不":{"docs":{},"可":{"docs":{},"导":{"docs":{},"的":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"使":{"docs":{},"用":{"docs":{},"一":{"docs":{},"种":{"docs":{},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"n":{"docs":{},"函":{"docs":{},"数":{"docs":{},"刻":{"docs":{},"画":{"docs":{},"一":{"docs":{},"下":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"值":{"docs":{},"导":{"docs":{},"数":{"docs":{},"，":{"docs":{},"如":{"docs":{},"下":{"docs":{},"图":{"docs":{},"。":{"docs":{},"那":{"docs":{},"么":{"docs":{},"这":{"docs":{},"个":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"同":{"docs":{},"样":{"docs":{},"在":{"docs":{},"j":{"docs":{},"(":{"docs":{},"θ":{"docs":{},")":{"docs":{},"向":{"0":{"docs":{},"趋":{"docs":{},"近":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"他":{"docs":{},"会":{"docs":{},"先":{"docs":{},"走":{"docs":{},"到":{"docs":{},"θ":{"docs":{},"等":{"docs":{},"于":{"0":{"docs":{},"的":{"docs":{},"y":{"docs":{},"轴":{"docs":{},"位":{"docs":{},"置":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"再":{"docs":{},"沿":{"docs":{},"着":{"docs":{},"y":{"docs":{},"轴":{"docs":{},"往":{"docs":{},"下":{"docs":{},"向":{"docs":{},"零":{"docs":{},"点":{"docs":{},"的":{"docs":{},"方":{"docs":{},"向":{"docs":{},"走":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"我":{"docs":{},"们":{"docs":{},"依":{"docs":{},"然":{"docs":{},"可":{"docs":{},"以":{"docs":{},"使":{"docs":{},"用":{"docs":{},"上":{"docs":{},"面":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"绘":{"docs":{},"制":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}},"对":{"docs":{},"于":{"docs":{},"这":{"docs":{},"个":{"docs":{},"应":{"docs":{},"用":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"很":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"我":{"docs":{},"们":{"docs":{},"对":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"不":{"docs":{},"是":{"docs":{},"特":{"docs":{},"别":{"docs":{},"关":{"docs":{},"注":{"docs":{},"。":{"docs":{},"可":{"docs":{},"能":{"docs":{},"有":{"docs":{},"很":{"docs":{},"多":{"docs":{},"上":{"docs":{},"升":{"docs":{},"周":{"docs":{},"期":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"落":{"docs":{},"掉":{"docs":{},"了":{"docs":{},"一":{"docs":{},"些":{"docs":{},"上":{"docs":{},"升":{"docs":{},"的":{"docs":{},"周":{"docs":{},"期":{"docs":{},"（":{"docs":{},"本":{"docs":{},"来":{"docs":{},"为":{"1":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"错":{"docs":{},"误":{"docs":{},"的":{"docs":{},"判":{"docs":{},"断":{"docs":{},"为":{"0":{"docs":{},"）":{"docs":{},"，":{"docs":{},"这":{"docs":{},"对":{"docs":{},"我":{"docs":{},"们":{"docs":{},"没":{"docs":{},"有":{"docs":{},"太":{"docs":{},"多":{"docs":{},"的":{"docs":{},"损":{"docs":{},"失":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"漏":{"docs":{},"掉":{"docs":{},"了":{"docs":{},"他":{"docs":{},"，":{"docs":{},"也":{"docs":{},"不":{"docs":{},"会":{"docs":{},"投":{"docs":{},"钱":{"docs":{},"进":{"docs":{},"去":{"docs":{},"。":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"如":{"docs":{},"果":{"docs":{},"我":{"docs":{},"们":{"docs":{},"添":{"docs":{},"加":{"docs":{},"上":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"在":{"docs":{},"座":{"docs":{},"的":{"docs":{},"事":{"docs":{},"情":{"docs":{},"就":{"docs":{},"是":{"docs":{},"升":{"docs":{},"维":{"docs":{},"。":{"docs":{},"让":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"不":{"docs":{},"但":{"docs":{},"有":{"docs":{},"横":{"docs":{},"轴":{"docs":{},"的":{"docs":{},"值":{"docs":{},"，":{"docs":{},"还":{"docs":{},"有":{"docs":{},"第":{"docs":{},"二":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"的":{"docs":{},"值":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"x":{"2":{"docs":{},".":{"docs":{},"一":{"docs":{},"点":{"docs":{},"我":{"docs":{},"们":{"docs":{},"这":{"docs":{},"样":{"docs":{},"做":{"docs":{},"了":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"就":{"docs":{},"变":{"docs":{},"成":{"docs":{},"了":{"docs":{},"线":{"docs":{},"性":{"docs":{},"可":{"docs":{},"分":{"docs":{},"的":{"docs":{},"。":{"docs":{},"这":{"docs":{},"就":{"docs":{},"是":{"docs":{},"升":{"docs":{},"维":{"docs":{},"的":{"docs":{},"意":{"docs":{},"义":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"从":{"docs":{},"投":{"docs":{},"票":{"docs":{},"的":{"docs":{},"角":{"docs":{},"度":{"docs":{},"看":{"docs":{},"，":{"docs":{},"还":{"docs":{},"是":{"docs":{},"不":{"docs":{},"够":{"docs":{},"多":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}}}}}}}}},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{},"比":{"docs":{},"使":{"docs":{},"用":{"docs":{},"o":{"docs":{},"v":{"docs":{},"r":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"要":{"docs":{},"高":{"docs":{},"了":{"docs":{},"很":{"docs":{},"多":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}},"子":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},"母":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},"割":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}},"数":{"docs":{},"不":{"docs":{},"好":{"docs":{},"使":{"docs":{},"由":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"只":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"两":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},"别":{"docs":{},"计":{"docs":{},"算":{"docs":{},"两":{"docs":{},"部":{"docs":{},"分":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"然":{"docs":{},"后":{"docs":{},"相":{"docs":{},"加":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}}}}}},"基":{"docs":{},"尼":{"docs":{},"系":{"docs":{},"数":{"docs":{},"然":{"docs":{},"后":{"docs":{},"相":{"docs":{},"加":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}}}}}},"判":{"docs":{},"断":{"docs":{},"发":{"docs":{},"放":{"docs":{},"给":{"docs":{},"客":{"docs":{},"户":{"docs":{},"信":{"docs":{},"用":{"docs":{},"卡":{"docs":{},"有":{"docs":{},"风":{"docs":{},"险":{"docs":{},"；":{"docs":{},"没":{"docs":{},"有":{"docs":{},"风":{"docs":{},"险":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}},"的":{"docs":{},"风":{"docs":{},"险":{"docs":{},"评":{"docs":{},"级":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}},"某":{"docs":{},"支":{"docs":{},"股":{"docs":{},"票":{"docs":{},"涨":{"docs":{},"；":{"docs":{},"跌":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}},"病":{"docs":{},"患":{"docs":{},"良":{"docs":{},"性":{"docs":{},"肿":{"docs":{},"瘤":{"docs":{},"；":{"docs":{},"恶":{"docs":{},"性":{"docs":{},"肿":{"docs":{},"瘤":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}},"邮":{"docs":{},"件":{"docs":{},"是":{"docs":{},"垃":{"docs":{},"圾":{"docs":{},"邮":{"docs":{},"件":{"docs":{},"；":{"docs":{},"不":{"docs":{},"是":{"docs":{},"垃":{"docs":{},"圾":{"docs":{},"邮":{"docs":{},"件":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}},"医":{"docs":{},"院":{"docs":{},"已":{"docs":{},"经":{"docs":{},"积":{"docs":{},"累":{"docs":{},"了":{"docs":{},"一":{"docs":{},"定":{"docs":{},"的":{"docs":{},"病":{"docs":{},"人":{"docs":{},"信":{"docs":{},"息":{"docs":{},"和":{"docs":{},"他":{"docs":{},"们":{"docs":{},"最":{"docs":{},"终":{"docs":{},"确":{"docs":{},"诊":{"docs":{},"是":{"docs":{},"否":{"docs":{},"患":{"docs":{},"病":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"半":{"docs":{},"监":{"docs":{},"督":{"docs":{},"学":{"docs":{},"习":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}},"回":{"docs":{},"归":{"docs":{},"任":{"docs":{},"务":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}},"问":{"docs":{},"题":{"docs":{},"怎":{"docs":{},"么":{"docs":{},"解":{"docs":{},"决":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"？":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}}}}}}}}}},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"本":{"docs":{},"质":{"docs":{},"就":{"docs":{},"是":{"docs":{},"找":{"docs":{},"到":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"或":{"docs":{},"者":{"docs":{},"一":{"docs":{},"个":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"能":{"docs":{},"够":{"docs":{},"最":{"docs":{},"大":{"docs":{},"程":{"docs":{},"度":{"docs":{},"上":{"docs":{},"的":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"，":{"docs":{},"如":{"docs":{},"何":{"docs":{},"定":{"docs":{},"义":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"关":{"docs":{},"键":{"docs":{},"。":{"docs":{},"比":{"docs":{},"如":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"定":{"docs":{},"义":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"就":{"docs":{},"是":{"docs":{},"让":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"到":{"docs":{},"直":{"docs":{},"线":{"docs":{},"的":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"的":{"docs":{},"值":{"docs":{},"最":{"docs":{},"小":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"顾":{"docs":{},"网":{"docs":{},"格":{"docs":{},"搜":{"docs":{},"素":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}},"忆":{"docs":{},"小":{"docs":{},"批":{"docs":{},"量":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"也":{"docs":{},"是":{"docs":{},"将":{"docs":{},"随":{"docs":{},"机":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"和":{"docs":{},"批":{"docs":{},"量":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"结":{"docs":{},"合":{"docs":{},"到":{"docs":{},"了":{"docs":{},"一":{"docs":{},"起":{"docs":{},"。":{"docs":{},"在":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"领":{"docs":{},"域":{"docs":{},"中":{"docs":{},"，":{"docs":{},"经":{"docs":{},"常":{"docs":{},"使":{"docs":{},"用":{"docs":{},"这":{"docs":{},"种":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"创":{"docs":{},"造":{"docs":{},"出":{"docs":{},"一":{"docs":{},"些":{"docs":{},"新":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"方":{"docs":{},"法":{"docs":{},"虽":{"docs":{},"然":{"docs":{},"名":{"docs":{},"词":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"酷":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"他":{"docs":{},"们":{"docs":{},"背":{"docs":{},"后":{"docs":{},"的":{"docs":{},"意":{"docs":{},"义":{"docs":{},"是":{"docs":{},"非":{"docs":{},"常":{"docs":{},"简":{"docs":{},"单":{"docs":{},"的":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"我":{"docs":{},"们":{"docs":{},"上":{"docs":{},"一":{"docs":{},"章":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"一":{"docs":{},"系":{"docs":{},"列":{"docs":{},"的":{"docs":{},"推":{"docs":{},"导":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"得":{"docs":{},"出":{"docs":{},"了":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},":":{"docs":{},"θ":{"docs":{},"t":{"docs":{},"·":{"docs":{},"x":{"docs":{},"b":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"解":{"docs":{},"析":{"docs":{},"几":{"docs":{},"何":{"docs":{},"，":{"docs":{},"点":{"docs":{},"到":{"docs":{},"直":{"docs":{},"线":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{"支撑向量机SVM/11.2 SVM背后的最优化问题.html":{"ref":"支撑向量机SVM/11.2 SVM背后的最优化问题.html","tf":0.16666666666666666}}}}}}}}}}}}}}}},"图":{"docs":{},"像":{"docs":{},"已":{"docs":{},"经":{"docs":{},"拥":{"docs":{},"有":{"docs":{},"了":{"docs":{},"标":{"docs":{},"记":{"docs":{},"信":{"docs":{},"息":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}},"识":{"docs":{},"别":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}},"增":{"docs":{},"强":{"docs":{},"学":{"docs":{},"习":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}},"大":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"继":{"docs":{},"续":{"docs":{},"试":{"docs":{},"验":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.027777777777777776}}}}}}}}}}}}},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}},"问":{"docs":{},"题":{"docs":{},"中":{"docs":{},"的":{"docs":{},"混":{"docs":{},"淆":{"docs":{},"矩":{"docs":{},"阵":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":5.002666666666666}}}}}}},"越":{"docs":{},"亮":{"docs":{},"的":{"docs":{},"地":{"docs":{},"方":{"docs":{},"代":{"docs":{},"表":{"docs":{},"的":{"docs":{},"就":{"docs":{},"是":{"docs":{},"犯":{"docs":{},"错":{"docs":{},"误":{"docs":{},"越":{"docs":{},"多":{"docs":{},"的":{"docs":{},"地":{"docs":{},"方":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"通":{"docs":{},"过":{"docs":{},"横":{"docs":{},"纵":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"出":{"docs":{},"具":{"docs":{},"体":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"线":{"docs":{},"程":{"docs":{},"并":{"docs":{},"行":{"docs":{},"处":{"docs":{},"理":{"docs":{},"，":{"docs":{},"占":{"docs":{},"用":{"docs":{},"几":{"docs":{},"个":{"docs":{},"核":{"docs":{},"，":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"在":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"算":{"docs":{},"法":{"docs":{},"上":{"docs":{},"并":{"docs":{},"没":{"docs":{},"有":{"docs":{},"新":{"docs":{},"的":{"docs":{},"地":{"docs":{},"方":{"docs":{},"，":{"docs":{},"完":{"docs":{},"全":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"思":{"docs":{},"路":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"元":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"实":{"docs":{},"现":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"简":{"docs":{},"介":{"docs":{},"和":{"docs":{},"正":{"docs":{},"规":{"docs":{},"方":{"docs":{},"程":{"docs":{},"解":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}},"中":{"docs":{},"的":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{"梯度下降法/3.多元线性回归中的梯度下降法.html":{"ref":"梯度下降法/3.多元线性回归中的梯度下降法.html","tf":0.2}}}}}}}}}}}}}},"次":{"docs":{},"运":{"docs":{},"行":{"docs":{},"，":{"docs":{},"随":{"docs":{},"机":{"docs":{},"化":{"docs":{},"初":{"docs":{},"始":{"docs":{},"点":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}}}}}}}}}},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"对":{"docs":{},"样":{"docs":{},"本":{"docs":{},"进":{"docs":{},"行":{"docs":{},"训":{"docs":{},"练":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"2":{"0":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}},"学":{"docs":{},"生":{"docs":{},"成":{"docs":{},"绩":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}},"对":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"降":{"docs":{},"维":{"docs":{},"处":{"docs":{},"理":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}},"敏":{"docs":{},"感":{"docs":{"127-jue-ce-shu-de-ju-xian-xing.html":{"ref":"127-jue-ce-shu-de-ju-xian-xing.html","tf":0.2}}}}}},"没":{"docs":{},"有":{"docs":{},"“":{"docs":{},"标":{"docs":{},"记":{"docs":{},"”":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"分":{"docs":{},"类":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"如":{"docs":{},"何":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{},"？":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}},"象":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}},"进":{"docs":{},"行":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"创":{"docs":{},"建":{"docs":{},"出":{"docs":{},"模":{"docs":{},"型":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},"进":{"docs":{},"行":{"docs":{},"一":{"docs":{},"个":{"docs":{},"乱":{"docs":{},"序":{"docs":{},"的":{"docs":{},"排":{"docs":{},"序":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}},"整":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"看":{"docs":{},"一":{"docs":{},"遍":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}},"于":{"docs":{},"小":{"docs":{},"批":{"docs":{},"量":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"，":{"docs":{},"由":{"docs":{},"多":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}}}}}},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"x":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"x":{"docs":{},"有":{"docs":{},"m":{"docs":{},"行":{"docs":{},"n":{"docs":{},"列":{"docs":{},"，":{"docs":{},"代":{"docs":{},"表":{"docs":{},"有":{"docs":{},"m":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"n":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"我":{"docs":{},"们":{"docs":{},"前":{"docs":{},"面":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"分":{"docs":{},"析":{"docs":{},"法":{"docs":{},"，":{"docs":{},"假":{"docs":{},"设":{"docs":{},"我":{"docs":{},"们":{"docs":{},"已":{"docs":{},"经":{"docs":{},"求":{"docs":{},"出":{"docs":{},"了":{"docs":{},"针":{"docs":{},"对":{"docs":{},"这":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"来":{"docs":{},"说":{"docs":{},"的":{"docs":{},"前":{"docs":{},"k":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"，":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"对":{"docs":{},"应":{"docs":{},"一":{"docs":{},"个":{"docs":{},"单":{"docs":{},"位":{"docs":{},"方":{"docs":{},"向":{"docs":{},"，":{"docs":{},"用":{"docs":{},"w":{"docs":{},"来":{"docs":{},"表":{"docs":{},"示":{"docs":{},",":{"docs":{},"w":{"docs":{},"也":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"，":{"docs":{},"他":{"docs":{},"有":{"docs":{},"k":{"docs":{},"行":{"docs":{},"，":{"docs":{},"代":{"docs":{},"表":{"docs":{},"我":{"docs":{},"们":{"docs":{},"求":{"docs":{},"出":{"docs":{},"的":{"docs":{},"前":{"docs":{},"k":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"，":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"有":{"docs":{},"n":{"docs":{},"列":{"docs":{},"，":{"docs":{},"代":{"docs":{},"表":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"的":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"轴":{"docs":{},"应":{"docs":{},"该":{"docs":{},"是":{"docs":{},"有":{"docs":{},"n":{"docs":{},"个":{"docs":{},"元":{"docs":{},"素":{"docs":{},"的":{"docs":{},"。":{"docs":{},"这":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"分":{"docs":{},"析":{"docs":{},"法":{"docs":{},"主":{"docs":{},"要":{"docs":{},"就":{"docs":{},"是":{"docs":{},"将":{"docs":{},"数":{"docs":{},"据":{"docs":{},"从":{"docs":{},"一":{"docs":{},"个":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"转":{"docs":{},"化":{"docs":{},"成":{"docs":{},"了":{"docs":{},"另":{"docs":{},"外":{"docs":{},"一":{"docs":{},"个":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"，":{"docs":{},"原":{"docs":{},"来":{"docs":{},"这":{"docs":{},"个":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"有":{"docs":{},"n":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"，":{"docs":{},"现":{"docs":{},"在":{"docs":{},"这":{"docs":{},"个":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"也":{"docs":{},"应":{"docs":{},"该":{"docs":{},"有":{"docs":{},"n":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"，":{"docs":{},"只":{"docs":{},"不":{"docs":{},"过":{"docs":{},"对":{"docs":{},"于":{"docs":{},"转":{"docs":{},"化":{"docs":{},"的":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"取":{"docs":{},"出":{"docs":{},"来":{"docs":{},"前":{"docs":{},"k":{"docs":{},"个":{"docs":{},"，":{"docs":{},"这":{"docs":{},"k":{"docs":{},"个":{"docs":{},"方":{"docs":{},"向":{"docs":{},"更":{"docs":{},"加":{"docs":{},"重":{"docs":{},"要":{"docs":{},"。":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"结":{"docs":{},"点":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"至":{"docs":{},"少":{"docs":{},"要":{"docs":{},"有":{"docs":{},"多":{"docs":{},"少":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"才":{"docs":{},"对":{"docs":{},"他":{"docs":{},"继":{"docs":{},"续":{"docs":{},"进":{"docs":{},"行":{"docs":{},"拆":{"docs":{},"分":{"docs":{},"下":{"docs":{},"去":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"w":{"docs":{},"这":{"docs":{},"个":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"代":{"docs":{},"表":{"docs":{},"一":{"docs":{},"个":{"docs":{},"方":{"docs":{},"向":{"docs":{},"，":{"docs":{},"第":{"docs":{},"一":{"docs":{},"行":{"docs":{},"是":{"docs":{},"最":{"docs":{},"重":{"docs":{},"要":{"docs":{},"的":{"docs":{},"方":{"docs":{},"向":{"docs":{},"，":{"docs":{},"第":{"docs":{},"二":{"docs":{},"行":{"docs":{},"是":{"docs":{},"次":{"docs":{},"重":{"docs":{},"要":{"docs":{},"的":{"docs":{},"方":{"docs":{},"向":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"在":{"docs":{},"模":{"docs":{},"型":{"docs":{},"很":{"docs":{},"简":{"docs":{},"单":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"也":{"docs":{},"比":{"docs":{},"较":{"docs":{},"低":{"docs":{},"，":{"docs":{},"随":{"docs":{},"着":{"docs":{},"模":{"docs":{},"型":{"docs":{},"逐":{"docs":{},"渐":{"docs":{},"变":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"，":{"docs":{},"对":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"在":{"docs":{},"逐":{"docs":{},"渐":{"docs":{},"的":{"docs":{},"提":{"docs":{},"升":{"docs":{},"，":{"docs":{},"提":{"docs":{},"升":{"docs":{},"到":{"docs":{},"一":{"docs":{},"定":{"docs":{},"程":{"docs":{},"度":{"docs":{},"后":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"模":{"docs":{},"型":{"docs":{},"继":{"docs":{},"续":{"docs":{},"变":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"将":{"docs":{},"会":{"docs":{},"进":{"docs":{},"行":{"docs":{},"下":{"docs":{},"降":{"docs":{},"（":{"docs":{},"欠":{"docs":{},"拟":{"docs":{},"合":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"现":{"docs":{},"在":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"（":{"docs":{},"基":{"docs":{},"于":{"docs":{},"二":{"docs":{},"次":{"docs":{},"方":{"docs":{},"程":{"docs":{},"构":{"docs":{},"造":{"docs":{},"）":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"低":{"docs":{},"于":{"2":{"docs":{},"项":{"docs":{},"的":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"欠":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"；":{"docs":{},"高":{"docs":{},"于":{"2":{"docs":{},"项":{"docs":{},"的":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"欠":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"比":{"docs":{},"最":{"docs":{},"佳":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"趋":{"docs":{},"于":{"docs":{},"稳":{"docs":{},"定":{"docs":{},"的":{"docs":{},"那":{"docs":{},"个":{"docs":{},"位":{"docs":{},"置":{"docs":{},"要":{"docs":{},"高":{"docs":{},"一":{"docs":{},"些":{"docs":{},"，":{"docs":{},"说":{"docs":{},"明":{"docs":{},"无":{"docs":{},"论":{"docs":{},"对":{"docs":{},"于":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"还":{"docs":{},"是":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"误":{"docs":{},"差":{"docs":{},"都":{"docs":{},"比":{"docs":{},"较":{"docs":{},"大":{"docs":{},"。":{"docs":{},"这":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"本":{"docs":{},"身":{"docs":{},"模":{"docs":{},"型":{"docs":{},"选":{"docs":{},"的":{"docs":{},"就":{"docs":{},"不":{"docs":{},"对":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"即":{"docs":{},"使":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"上":{"docs":{},"，":{"docs":{},"他":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"也":{"docs":{},"是":{"docs":{},"大":{"docs":{},"的":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"才":{"docs":{},"会":{"docs":{},"呈":{"docs":{},"现":{"docs":{},"出":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"一":{"docs":{},"种":{"docs":{},"形":{"docs":{},"态":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"，":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"上":{"docs":{},"，":{"docs":{},"他":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"不":{"docs":{},"大":{"docs":{},"，":{"docs":{},"和":{"docs":{},"最":{"docs":{},"佳":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"是":{"docs":{},"差":{"docs":{},"不":{"docs":{},"多":{"docs":{},"的":{"docs":{},"，":{"docs":{},"甚":{"docs":{},"至":{"docs":{},"在":{"docs":{},"极":{"docs":{},"端":{"docs":{},"情":{"docs":{},"况":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"取":{"docs":{},"更":{"docs":{},"高":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"会":{"docs":{},"更":{"docs":{},"低":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"问":{"docs":{},"题":{"docs":{},"在":{"docs":{},"于":{"docs":{},"，":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"相":{"docs":{},"对":{"docs":{},"是":{"docs":{},"比":{"docs":{},"较":{"docs":{},"大":{"docs":{},"的":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"和":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"相":{"docs":{},"差":{"docs":{},"比":{"docs":{},"较":{"docs":{},"大":{"docs":{},"（":{"docs":{},"表":{"docs":{},"现":{"docs":{},"在":{"docs":{},"图":{"docs":{},"上":{"docs":{},"相":{"docs":{},"差":{"docs":{},"比":{"docs":{},"较":{"docs":{},"远":{"docs":{},"）":{"docs":{},"，":{"docs":{},"这":{"docs":{},"就":{"docs":{},"说":{"docs":{},"明":{"docs":{},"了":{"docs":{},"此":{"docs":{},"时":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"不":{"docs":{},"够":{"docs":{},"好":{"docs":{},"，":{"docs":{},"他":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"是":{"docs":{},"不":{"docs":{},"够":{"docs":{},"的":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"θ":{"docs":{},"的":{"docs":{},"求":{"docs":{},"和":{"docs":{},"i":{"docs":{},"是":{"docs":{},"从":{"1":{"docs":{},"到":{"docs":{},"n":{"docs":{},",":{"docs":{},"没":{"docs":{},"有":{"docs":{},"将":{"docs":{},"θ":{"0":{"docs":{},"加":{"docs":{},"进":{"docs":{},"去":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"他":{"docs":{},"不":{"docs":{},"是":{"docs":{},"任":{"docs":{},"意":{"docs":{},"一":{"docs":{},"项":{"docs":{},"的":{"docs":{},"系":{"docs":{},"数":{"docs":{},"，":{"docs":{},"他":{"docs":{},"只":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"截":{"docs":{},"距":{"docs":{},"，":{"docs":{},"决":{"docs":{},"定":{"docs":{},"了":{"docs":{},"整":{"docs":{},"个":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"的":{"docs":{},"高":{"docs":{},"低":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"不":{"docs":{},"决":{"docs":{},"定":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"每":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"的":{"docs":{},"陡":{"docs":{},"峭":{"docs":{},"和":{"docs":{},"缓":{"docs":{},"和":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}},"docs":{}}}}}}}},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"得":{"docs":{},"到":{"docs":{},"一":{"docs":{},"个":{"docs":{},"函":{"docs":{},"数":{"docs":{},"f":{"docs":{},"，":{"docs":{},"将":{"docs":{},"样":{"docs":{},"本":{"docs":{},"x":{"docs":{},"输":{"docs":{},"入":{"docs":{},"f":{"docs":{},"后":{"docs":{},"，":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"值":{"docs":{},"y":{"docs":{},"就":{"docs":{},"是":{"docs":{},"要":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"值":{"docs":{},"；":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"，":{"docs":{},"定":{"docs":{},"义":{"docs":{},"他":{"docs":{},"的":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"比":{"docs":{},"较":{"docs":{},"困":{"docs":{},"难":{"docs":{},"。":{"docs":{"逻辑回归/2.逻辑回归的损失函数.html":{"ref":"逻辑回归/2.逻辑回归的损失函数.html","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"全":{"docs":{},"都":{"docs":{},"正":{"docs":{},"确":{"docs":{},"的":{"docs":{},"进":{"docs":{},"行":{"docs":{},"了":{"docs":{},"分":{"docs":{},"类":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"是":{"docs":{},"没":{"docs":{},"有":{"docs":{},"表":{"docs":{},"达":{"docs":{},"式":{"docs":{},"的":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"不":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"数":{"docs":{},"学":{"docs":{},"求":{"docs":{},"解":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"。":{"docs":{},"而":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"距":{"docs":{},"离":{"docs":{},"投":{"docs":{},"票":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"混":{"docs":{},"淆":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"2":{"docs":{},"*":{"2":{"docs":{},"的":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"。":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"代":{"docs":{},"表":{"docs":{},"的":{"docs":{},"是":{"docs":{},"对":{"docs":{},"于":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"来":{"docs":{},"说":{"docs":{},"真":{"docs":{},"实":{"docs":{},"值":{"docs":{},"是":{"docs":{},"多":{"docs":{},"少":{"docs":{},"，":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"列":{"docs":{},"是":{"docs":{},"分":{"docs":{},"类":{"docs":{},"算":{"docs":{},"法":{"docs":{},"进":{"docs":{},"行":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"值":{"docs":{},"是":{"docs":{},"多":{"docs":{},"少":{"docs":{},"。":{"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}},"极":{"docs":{},"度":{"docs":{},"偏":{"docs":{},"斜":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"（":{"docs":{},"s":{"docs":{},"k":{"docs":{},"e":{"docs":{},"k":{"docs":{},"e":{"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":0.08333333333333333}}}}}}}}}}}}}}},"这":{"docs":{},"样":{"docs":{},"一":{"docs":{},"个":{"docs":{},"系":{"docs":{},"统":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"什":{"docs":{},"么":{"docs":{},"都":{"docs":{},"没":{"docs":{},"有":{"docs":{},"做":{"docs":{},"，":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"达":{"docs":{},"到":{"9":{"9":{"docs":{},".":{"9":{"docs":{},"%":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":0.08333333333333333}}}}}}}},"docs":{}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}},"一":{"docs":{},"维":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"他":{"docs":{},"是":{"docs":{},"线":{"docs":{},"性":{"docs":{},"不":{"docs":{},"可":{"docs":{},"分":{"docs":{},"的":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"很":{"docs":{},"难":{"docs":{},"画":{"docs":{},"一":{"docs":{},"跟":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"将":{"docs":{},"这":{"docs":{},"些":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"区":{"docs":{},"分":{"docs":{},"开":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"他":{"docs":{},"具":{"docs":{},"有":{"docs":{},"数":{"docs":{},"据":{"docs":{},"结":{"docs":{},"构":{"docs":{},"利":{"docs":{},"用":{"docs":{},"树":{"docs":{},"的":{"docs":{},"所":{"docs":{},"有":{"docs":{},"性":{"docs":{},"质":{"docs":{},"。":{"docs":{},"包":{"docs":{},"括":{"docs":{},"根":{"docs":{},"结":{"docs":{},"点":{"docs":{},"，":{"docs":{},"叶":{"docs":{},"子":{"docs":{},"结":{"docs":{},"点":{"docs":{},"，":{"docs":{},"深":{"docs":{},"度":{"docs":{},"。":{"docs":{},"在":{"docs":{},"这":{"docs":{},"里":{"docs":{},"我":{"docs":{},"们":{"docs":{},"称":{"docs":{},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"的":{"docs":{},"深":{"docs":{},"度":{"docs":{},"为":{"3":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"他":{"docs":{},"对":{"docs":{},"于":{"docs":{},"有":{"docs":{},"偏":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"并":{"docs":{},"不":{"docs":{},"是":{"docs":{},"那":{"docs":{},"么":{"docs":{},"敏":{"docs":{},"感":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"在":{"docs":{},"有":{"docs":{},"偏":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"里":{"docs":{},"，":{"docs":{},"看":{"docs":{},"一":{"docs":{},"下":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"是":{"docs":{},"非":{"docs":{},"常":{"docs":{},"有":{"docs":{},"必":{"docs":{},"要":{"docs":{},"的":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"通":{"docs":{},"常":{"docs":{},"关":{"docs":{},"注":{"docs":{},"的":{"docs":{},"是":{"docs":{},"这":{"docs":{},"根":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"下":{"docs":{},"面":{"docs":{},"的":{"docs":{},"面":{"docs":{},"积":{"docs":{},"的":{"docs":{},"大":{"docs":{},"小":{"docs":{},"，":{"docs":{},"面":{"docs":{},"积":{"docs":{},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"代":{"docs":{},"表":{"docs":{},"分":{"docs":{},"类":{"docs":{},"算":{"docs":{},"法":{"docs":{},"效":{"docs":{},"果":{"docs":{},"越":{"docs":{},"好":{"docs":{},"。":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"那":{"docs":{},"些":{"docs":{},"线":{"docs":{},"性":{"docs":{},"不":{"docs":{},"可":{"docs":{},"分":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"，":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"解":{"docs":{},"决":{"docs":{},"办":{"docs":{},"法":{"docs":{},"是":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05}}}}}}}}}}}}}}}}}}}}},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"，":{"docs":{},"都":{"docs":{},"应":{"docs":{},"该":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"℥":{"docs":{},"。":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"说":{"docs":{},"对":{"docs":{},"于":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"都":{"docs":{},"应":{"docs":{},"该":{"docs":{},"求":{"docs":{},"出":{"docs":{},"他":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"容":{"docs":{},"错":{"docs":{},"空":{"docs":{},"间":{"docs":{"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":0.0625}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"d":{"docs":{},"次":{"docs":{},"方":{"docs":{},"的":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":0.08333333333333333}}}}}}}}},"高":{"docs":{},"斯":{"docs":{},"函":{"docs":{},"数":{"docs":{},"时":{"docs":{},"将":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"映":{"docs":{},"射":{"docs":{},"成":{"docs":{},"了":{"docs":{},"无":{"docs":{},"穷":{"docs":{},"维":{"docs":{},"空":{"docs":{},"间":{"docs":{},"的":{"docs":{},"理":{"docs":{},"解":{"docs":{},"。":{"docs":{},"如":{"docs":{},"果":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"可":{"docs":{},"以":{"docs":{},"有":{"docs":{},"无":{"docs":{},"穷":{"docs":{},"多":{"docs":{},"个":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"就":{"docs":{},"是":{"docs":{},"将":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"了":{"docs":{},"无":{"docs":{},"穷":{"docs":{},"维":{"docs":{},"的":{"docs":{},"空":{"docs":{},"间":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"都":{"docs":{},"是":{"docs":{},"l":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"k":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"对":{"docs":{},"于":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"x":{"docs":{},"，":{"docs":{},"他":{"docs":{},"都":{"docs":{},"要":{"docs":{},"尝":{"docs":{},"试":{"docs":{},"对":{"docs":{},"于":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"y":{"docs":{},"，":{"docs":{},"进":{"docs":{},"行":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"的":{"docs":{},"计":{"docs":{},"算":{"docs":{},"，":{"docs":{},"成":{"docs":{},"为":{"docs":{},"新":{"docs":{},"的":{"docs":{},"高":{"docs":{},"维":{"docs":{},"数":{"docs":{},"据":{"docs":{},"相":{"docs":{},"对":{"docs":{},"于":{"docs":{},"的":{"docs":{},"某":{"docs":{},"一":{"docs":{},"维":{"docs":{},"的":{"docs":{},"元":{"docs":{},"素":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"来":{"docs":{},"说":{"docs":{},"。":{"docs":{},"对":{"docs":{},"于":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"的":{"docs":{},"定":{"docs":{},"义":{"docs":{},"是":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"要":{"docs":{},"定":{"docs":{},"义":{"docs":{},"一":{"docs":{},"个":{"docs":{},"m":{"docs":{},"e":{"docs":{},"r":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"值":{"docs":{},"，":{"docs":{},"在":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"范":{"docs":{},"围":{"docs":{},"内":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"能":{"docs":{},"够":{"docs":{},"包":{"docs":{},"含":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"最":{"docs":{},"多":{"docs":{},"。":{"docs":{},"也":{"docs":{},"就":{"docs":{},"代":{"docs":{},"表":{"docs":{},"我":{"docs":{},"们":{"docs":{},"这":{"docs":{},"个":{"docs":{},"范":{"docs":{},"围":{"docs":{},"能":{"docs":{},"够":{"docs":{},"比":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"来":{"docs":{},"表":{"docs":{},"达":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"，":{"docs":{},"在":{"docs":{},"这":{"docs":{},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"取":{"docs":{},"中":{"docs":{},"间":{"docs":{},"的":{"docs":{},"直":{"docs":{},"线":{"docs":{},"作":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"真":{"docs":{},"正":{"docs":{},"的":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"用":{"docs":{},"他":{"docs":{},"来":{"docs":{},"预":{"docs":{},"测":{"docs":{},"其":{"docs":{},"他":{"docs":{},"点":{"docs":{},"，":{"docs":{},"位":{"docs":{},"置":{"docs":{},"点":{"docs":{},"的":{"docs":{},"值":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"叶":{"docs":{},"子":{"docs":{},"节":{"docs":{},"点":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"至":{"docs":{},"少":{"docs":{},"要":{"docs":{},"有":{"docs":{},"几":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}}}},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"和":{"docs":{},"线":{"docs":{},"性":{"docs":{},"代":{"docs":{},"数":{"docs":{},"中":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"向":{"docs":{},"量":{"docs":{},"的":{"docs":{},"对":{"docs":{},"比":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}},"应":{"docs":{},"的":{"docs":{},"是":{"0":{"docs":{},"次":{"docs":{},"幂":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}},"docs":{}}}},"任":{"docs":{},"意":{"docs":{},"一":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"x":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"都":{"docs":{},"可":{"docs":{},"以":{"docs":{},"求":{"docs":{},"这":{"docs":{},"样":{"docs":{},"一":{"docs":{},"个":{"docs":{},"值":{"docs":{},"，":{"docs":{},"他":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"值":{"docs":{},"x":{"docs":{},"都":{"docs":{},"对":{"docs":{},"他":{"docs":{},"的":{"docs":{},"p":{"docs":{},"次":{"docs":{},"方":{"docs":{},"进":{"docs":{},"行":{"docs":{},"求":{"docs":{},"和":{"docs":{},"，":{"docs":{},"再":{"docs":{},"开":{"docs":{},"p":{"docs":{},"次":{"docs":{},"方":{"docs":{},"根":{"docs":{},"。":{"docs":{},"通":{"docs":{},"常":{"docs":{},"将":{"docs":{},"这":{"docs":{},"个":{"docs":{},"式":{"docs":{},"子":{"docs":{},"成":{"docs":{},"为":{"docs":{},"l":{"docs":{},"p":{"docs":{},"范":{"docs":{},"数":{"docs":{},"。":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"明":{"docs":{},"克":{"docs":{},"夫":{"docs":{},"斯":{"docs":{},"基":{"docs":{},"距":{"docs":{},"离":{"docs":{},"进":{"docs":{},"一":{"docs":{},"步":{"docs":{},"泛":{"docs":{},"化":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}},"比":{"docs":{},"s":{"docs":{},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"的":{"docs":{},"划":{"docs":{},"分":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"在":{"docs":{},"横":{"docs":{},"轴":{"docs":{},"上":{"docs":{},"（":{"docs":{},"第":{"0":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"）":{"docs":{},"，":{"docs":{},"大":{"docs":{},"约":{"2":{"docs":{},".":{"4":{"5":{"docs":{},"的":{"docs":{},"位":{"docs":{},"置":{"docs":{},"进":{"docs":{},"行":{"docs":{},"了":{"docs":{},"划":{"docs":{},"分":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}}}}}}}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"特":{"docs":{},"征":{"docs":{},"进":{"docs":{},"行":{"docs":{},"随":{"docs":{},"机":{"docs":{},"取":{"docs":{},"样":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}}}}}}}}}},"随":{"docs":{},"机":{"docs":{},"取":{"docs":{},"样":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}}}}}}}},"将":{"5":{"2":{"docs":{},"的":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"进":{"docs":{},"行":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"转":{"docs":{},"换":{"docs":{},"后":{"docs":{},"变":{"docs":{},"成":{"docs":{},"了":{"5":{"6":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}},"docs":{}},"docs":{},"给":{"docs":{},"定":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"分":{"docs":{},"类":{"docs":{},"，":{"docs":{},"比":{"docs":{},"如":{"docs":{},"区":{"docs":{},"分":{"docs":{},"猫":{"docs":{},"和":{"docs":{},"狗":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}},"横":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"作":{"docs":{},"为":{"docs":{},"x":{"docs":{},"轴":{"docs":{},"，":{"docs":{},"纵":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"作":{"docs":{},"为":{"docs":{},"y":{"docs":{},"轴":{"docs":{},"，":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"点":{"docs":{},"为":{"docs":{},"（":{"docs":{},"x":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{"线性回归算法/1.线性回归算法简介.html":{"ref":"线性回归算法/1.线性回归算法简介.html","tf":0.125}}}}}}}}}}}}}}}}}}}}}}}}}}}},"计":{"docs":{},"算":{"docs":{},"分":{"docs":{},"数":{"docs":{},"方":{"docs":{},"法":{"docs":{},"封":{"docs":{},"装":{"docs":{},"到":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"s":{"docs":{},"i":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"中":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"样":{"docs":{},"例":{"docs":{},"的":{"docs":{},"均":{"docs":{},"值":{"docs":{},"归":{"docs":{},"为":{"0":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}},"docs":{}}}}}}},"本":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"和":{"docs":{},"样":{"docs":{},"本":{"docs":{},"发":{"docs":{},"生":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"联":{"docs":{},"系":{"docs":{},"起":{"docs":{},"来":{"docs":{},"，":{"docs":{},"概":{"docs":{},"率":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"。":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}}}}}},"对":{"docs":{},"角":{"docs":{},"线":{"docs":{},"上":{"docs":{},"的":{"docs":{},"数":{"docs":{},"字":{"docs":{},"全":{"docs":{},"都":{"docs":{},"填":{"docs":{},"成":{"docs":{},"是":{"0":{"docs":{},"，":{"docs":{},"剩":{"docs":{},"下":{"docs":{},"的":{"docs":{},"其":{"docs":{},"他":{"docs":{},"的":{"docs":{},"格":{"docs":{},"子":{"docs":{},"就":{"docs":{},"是":{"docs":{},"犯":{"docs":{},"错":{"docs":{},"误":{"docs":{},"的":{"docs":{},"百":{"docs":{},"分":{"docs":{},"比":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}},"两":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"的":{"docs":{},"平":{"docs":{},"均":{"docs":{},"值":{"docs":{},"作":{"docs":{},"为":{"docs":{},"切":{"docs":{},"分":{"docs":{},"点":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}}}}},"市":{"docs":{},"场":{"docs":{},"分":{"docs":{},"析":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}},"积":{"docs":{},"累":{"docs":{},"了":{"docs":{},"房":{"docs":{},"屋":{"docs":{},"的":{"docs":{},"基":{"docs":{},"本":{"docs":{},"信":{"docs":{},"息":{"docs":{},"和":{"docs":{},"最":{"docs":{},"终":{"docs":{},"成":{"docs":{},"交":{"docs":{},"的":{"docs":{},"金":{"docs":{},"额":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}}}}},"异":{"docs":{},"常":{"docs":{},"检":{"docs":{},"测":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}},"房":{"docs":{},"屋":{"docs":{},"价":{"docs":{},"格":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}},"面":{"docs":{},"积":{"docs":{"线性回归算法/1.线性回归算法简介.html":{"ref":"线性回归算法/1.线性回归算法简介.html","tf":0.125}}}}}},"数":{"docs":{},"字":{"docs":{},"识":{"docs":{},"别":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}},"据":{"docs":{},"基":{"docs":{},"础":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":5}}}},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}},"总":{"docs":{},"过":{"docs":{},"程":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}},"量":{"docs":{},"太":{"docs":{},"大":{"docs":{},"会":{"docs":{},"报":{"docs":{},"错":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}},"标":{"docs":{},"准":{"docs":{},"化":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}},"方":{"docs":{},"便":{"docs":{},"可":{"docs":{},"视":{"docs":{},"化":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}},"差":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}},"无":{"docs":{},"人":{"docs":{},"驾":{"docs":{},"驶":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}},"更":{"docs":{},"常":{"docs":{},"见":{"docs":{},"：":{"docs":{},"各":{"docs":{},"种":{"docs":{},"原":{"docs":{},"因":{"docs":{},"产":{"docs":{},"生":{"docs":{},"的":{"docs":{},"标":{"docs":{},"记":{"docs":{},"缺":{"docs":{},"失":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}},"多":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"定":{"docs":{},"义":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}},"合":{"docs":{},"理":{"docs":{},"的":{"docs":{},"投":{"docs":{},"票":{"docs":{},"，":{"docs":{},"应":{"docs":{},"该":{"docs":{},"有":{"docs":{},"权":{"docs":{},"值":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}},"快":{"docs":{},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{},"速":{"docs":{},"度":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}},"机":{"docs":{},"器":{"docs":{},"人":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}},"学":{"docs":{},"习":{"docs":{},"中":{"docs":{},"监":{"docs":{},"督":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"基":{"docs":{},"本":{"docs":{},"任":{"docs":{},"务":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":10}}}}}}}}}}}},"总":{"docs":{},"过":{"docs":{},"程":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}},"流":{"docs":{},"程":{"docs":{},"回":{"docs":{},"顾":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}},"的":{"docs":{},"主":{"docs":{},"要":{"docs":{},"调":{"docs":{},"整":{"docs":{},"来":{"docs":{},"源":{"docs":{},"于":{"docs":{},"方":{"docs":{},"差":{"docs":{},"（":{"docs":{},"这":{"docs":{},"是":{"docs":{},"站":{"docs":{},"在":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"角":{"docs":{},"度":{"docs":{},"上":{"docs":{},"，":{"docs":{},"而":{"docs":{},"不":{"docs":{},"是":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"角":{"docs":{},"度":{"docs":{},"上":{"docs":{},",":{"docs":{},"比":{"docs":{},"如":{"docs":{},"对":{"docs":{},"金":{"docs":{},"融":{"docs":{},"市":{"docs":{},"场":{"docs":{},"的":{"docs":{},"理":{"docs":{},"解":{"docs":{},"，":{"docs":{},"很":{"docs":{},"多":{"docs":{},"人":{"docs":{},"尝":{"docs":{},"试":{"docs":{},"用":{"docs":{},"历":{"docs":{},"史":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"预":{"docs":{},"测":{"docs":{},"未":{"docs":{},"来":{"docs":{},"的":{"docs":{},"金":{"docs":{},"融":{"docs":{},"走":{"docs":{},"势":{"docs":{},"，":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"尝":{"docs":{},"试":{"docs":{},"通":{"docs":{},"常":{"docs":{},"都":{"docs":{},"不":{"docs":{},"太":{"docs":{},"理":{"docs":{},"想":{"docs":{},"。":{"docs":{},"很":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"因":{"docs":{},"为":{"docs":{},"历":{"docs":{},"史":{"docs":{},"的":{"docs":{},"金":{"docs":{},"融":{"docs":{},"趋":{"docs":{},"势":{"docs":{},"不":{"docs":{},"能":{"docs":{},"很":{"docs":{},"好":{"docs":{},"的":{"docs":{},"反":{"docs":{},"应":{"docs":{},"未":{"docs":{},"来":{"docs":{},"的":{"docs":{},"走":{"docs":{},"向":{"docs":{},"，":{"docs":{},"这":{"docs":{},"种":{"docs":{},"预":{"docs":{},"测":{"docs":{},"方":{"docs":{},"法":{"docs":{},"本":{"docs":{},"身":{"docs":{},"带":{"docs":{},"来":{"docs":{},"的":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{},"的":{"docs":{},"偏":{"docs":{},"差":{"docs":{},"）":{"docs":{},"换":{"docs":{},"句":{"docs":{},"话":{"docs":{},"说":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"很":{"docs":{},"容":{"docs":{},"易":{"docs":{},"让":{"docs":{},"模":{"docs":{},"型":{"docs":{},"变":{"docs":{},"的":{"docs":{},"很":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"，":{"docs":{},"从":{"docs":{},"而":{"docs":{},"降":{"docs":{},"低":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"偏":{"docs":{},"差":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"由":{"docs":{},"于":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"方":{"docs":{},"差":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"大":{"docs":{},"，":{"docs":{},"最":{"docs":{},"终":{"docs":{},"也":{"docs":{},"没":{"docs":{},"有":{"docs":{},"很":{"docs":{},"好":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"。":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"根":{"docs":{},"据":{"docs":{},"周":{"docs":{},"围":{"docs":{},"环":{"docs":{},"境":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"，":{"docs":{},"采":{"docs":{},"取":{"docs":{},"行":{"docs":{},"动":{"docs":{},"，":{"docs":{},"根":{"docs":{},"据":{"docs":{},"采":{"docs":{},"取":{"docs":{},"行":{"docs":{},"动":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"学":{"docs":{},"习":{"docs":{},"行":{"docs":{},"动":{"docs":{},"方":{"docs":{},"式":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}},"某":{"docs":{},"一":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"d":{"docs":{},"和":{"docs":{},"某":{"docs":{},"一":{"docs":{},"个":{"docs":{},"阈":{"docs":{},"值":{"docs":{},"v":{"docs":{},"进":{"docs":{},"行":{"docs":{},"二":{"docs":{},"分":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}}}}}}},"特":{"docs":{},"征":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}},"压":{"docs":{},"缩":{"docs":{},"：":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}},"提":{"docs":{},"取":{"docs":{},"：":{"docs":{},"信":{"docs":{},"用":{"docs":{},"卡":{"docs":{},"的":{"docs":{},"信":{"docs":{},"用":{"docs":{},"评":{"docs":{},"级":{"docs":{},"和":{"docs":{},"人":{"docs":{},"的":{"docs":{},"胖":{"docs":{},"瘦":{"docs":{},"无":{"docs":{},"关":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}}}}},"监":{"docs":{},"督":{"docs":{},"学":{"docs":{},"习":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}},"和":{"docs":{},"半":{"docs":{},"监":{"docs":{},"督":{"docs":{},"学":{"docs":{},"习":{"docs":{},"是":{"docs":{},"基":{"docs":{},"础":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}},"结":{"docs":{},"果":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"连":{"docs":{},"续":{"docs":{},"数":{"docs":{},"字":{"docs":{},"的":{"docs":{},"值":{"docs":{},"，":{"docs":{},"而":{"docs":{},"非":{"docs":{},"一":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}},"向":{"docs":{},"量":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}},"即":{"docs":{},"为":{"docs":{},"对":{"docs":{},"于":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"在":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"中":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"值":{"docs":{},"是":{"docs":{},"什":{"docs":{},"么":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"给":{"docs":{},"机":{"docs":{},"器":{"docs":{},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"拥":{"docs":{},"有":{"docs":{},"“":{"docs":{},"标":{"docs":{},"记":{"docs":{},"”":{"docs":{},"或":{"docs":{},"者":{"docs":{},"“":{"docs":{},"答":{"docs":{},"案":{"docs":{},"”":{"docs":{},"，":{"docs":{},"人":{"docs":{},"类":{"docs":{},"已":{"docs":{},"经":{"docs":{},"给":{"docs":{},"机":{"docs":{},"器":{"docs":{},"对":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"了":{"docs":{},"正":{"docs":{},"确":{"docs":{},"答":{"docs":{},"案":{"docs":{},"的":{"docs":{},"划":{"docs":{},"分":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"答":{"docs":{},"案":{"docs":{},"的":{"docs":{},"划":{"docs":{},"分":{"docs":{},"本":{"docs":{},"身":{"docs":{},"就":{"docs":{},"是":{"docs":{},"监":{"docs":{},"督":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"没":{"docs":{},"有":{"docs":{},"任":{"docs":{},"何":{"docs":{},"的":{"docs":{},"“":{"docs":{},"答":{"docs":{},"案":{"docs":{},"”":{"docs":{},"和":{"docs":{},"“":{"docs":{},"标":{"docs":{},"记":{"docs":{},"”":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}}}}}},"出":{"docs":{},"从":{"0":{"docs":{},"到":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}},"docs":{}}},"消":{"docs":{},"除":{"docs":{},"了":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}},"聚":{"docs":{},"类":{"docs":{},"分":{"docs":{},"析":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}},"股":{"docs":{},"票":{"docs":{},"价":{"docs":{},"格":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}},"银":{"docs":{},"行":{"docs":{},"已":{"docs":{},"经":{"docs":{},"积":{"docs":{},"累":{"docs":{},"了":{"docs":{},"一":{"docs":{},"定":{"docs":{},"的":{"docs":{},"客":{"docs":{},"户":{"docs":{},"信":{"docs":{},"息":{"docs":{},"和":{"docs":{},"他":{"docs":{},"们":{"docs":{},"信":{"docs":{},"息":{"docs":{},"卡":{"docs":{},"的":{"docs":{},"信":{"docs":{},"用":{"docs":{},"情":{"docs":{},"况":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}}}}}}}}}}}}}}}}}}}}}}},"非":{"docs":{},"监":{"docs":{},"督":{"docs":{},"学":{"docs":{},"习":{"docs":{"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"ref":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","tf":0.02127659574468085}}}}}},"参":{"docs":{},"数":{"docs":{},"学":{"docs":{},"习":{"docs":{},"通":{"docs":{},"常":{"docs":{},"都":{"docs":{},"是":{"docs":{},"高":{"docs":{},"方":{"docs":{},"差":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"。":{"docs":{},"因":{"docs":{},"为":{"docs":{},"不":{"docs":{},"对":{"docs":{},"数":{"docs":{},"据":{"docs":{},"做":{"docs":{},"任":{"docs":{},"何":{"docs":{},"假":{"docs":{},"设":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}}}}}}}}}}}}}}}}}},"常":{"docs":{},"接":{"docs":{},"近":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}},"好":{"docs":{},"的":{"docs":{},"可":{"docs":{},"解":{"docs":{},"释":{"docs":{},"性":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}},"%":{"docs":{},"%":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}},"i":{"docs":{},"t":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}},"r":{"docs":{},"u":{"docs":{},"n":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.07142857142857142}}}}},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.025974025974025976}},"i":{"docs":{},"t":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}}}}}}}},".":{"docs":{},"/":{"docs":{},"h":{"docs":{},"e":{"docs":{},"l":{"docs":{},"l":{"docs":{},"o":{"docs":{},".":{"docs":{},"p":{"docs":{},"i":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"c":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}},".":{"docs":{},".":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}},"j":{"docs":{},"u":{"docs":{},"p":{"docs":{},"y":{"docs":{},"t":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":5}}}}},"m":{"docs":{},"a":{"docs":{},"'":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{},":":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}},"_":{"2":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{},")":{"docs":{},"/":{"docs":{},"(":{"2":{"docs":{},"*":{"docs":{},"e":{"docs":{},"p":{"docs":{},"s":{"docs":{},"i":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}},"docs":{}}}}}}}}}}}},"docs":{}}}}}}}}},"μ":{"docs":{},"s":{"docs":{},"（":{"docs":{},"有":{"docs":{},"多":{"docs":{},"少":{"docs":{},"次":{"docs":{},"循":{"docs":{},"环":{"docs":{},"是":{"docs":{},"由":{"docs":{},"j":{"docs":{},"u":{"docs":{},"p":{"docs":{},"y":{"docs":{},"t":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}},"可":{"docs":{},"以":{"docs":{},"直":{"docs":{},"接":{"docs":{},"使":{"docs":{},"用":{"docs":{},"i":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"命":{"docs":{},"令":{"docs":{},"导":{"docs":{},"入":{"docs":{},"本":{"docs":{},"机":{"docs":{},"目":{"docs":{},"录":{"docs":{},"下":{"docs":{},"的":{"docs":{},"包":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}},"说":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"不":{"docs":{},"需":{"docs":{},"要":{"docs":{},"训":{"docs":{},"练":{"docs":{},"过":{"docs":{},"程":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}}}}}},"看":{"docs":{},"出":{"docs":{},"，":{"docs":{},"向":{"docs":{},"量":{"docs":{},"化":{"docs":{},"的":{"docs":{},"运":{"docs":{},"行":{"docs":{},"速":{"docs":{},"度":{"docs":{},"比":{"docs":{},"循":{"docs":{},"环":{"docs":{},"的":{"docs":{},"形":{"docs":{},"式":{"docs":{},"速":{"docs":{},"度":{"docs":{},"要":{"docs":{},"快":{"8":{"0":{"docs":{},"倍":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}},"当":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"取":{"1":{"docs":{},".":{"1":{"docs":{},"，":{"docs":{},"函":{"docs":{},"数":{"docs":{},"会":{"docs":{},"循":{"docs":{},"环":{"docs":{},"直":{"docs":{},"至":{"docs":{},"终":{"docs":{},"止":{"docs":{},"，":{"docs":{},"这":{"docs":{},"是":{"docs":{},"由":{"docs":{},"于":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"η":{"docs":{},"设":{"docs":{},"置":{"docs":{},"过":{"docs":{},"大":{"docs":{},"，":{"docs":{},"导":{"docs":{},"致":{"docs":{},"每":{"docs":{},"次":{"docs":{},"循":{"docs":{},"环":{"docs":{},"过":{"docs":{},"后":{"docs":{},"，":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"j":{"docs":{},"的":{"docs":{},"值":{"docs":{},"都":{"docs":{},"向":{"docs":{},"大":{"docs":{},"的":{"docs":{},"方":{"docs":{},"向":{"docs":{},"变":{"docs":{},"化":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}},"成":{"docs":{},"是":{"docs":{},"(":{"docs":{},"x":{"docs":{},"·":{"docs":{},"w":{"docs":{},")":{"docs":{},"这":{"docs":{},"个":{"docs":{},"向":{"docs":{},"量":{"docs":{},"的":{"docs":{},"转":{"docs":{},"置":{"docs":{},"（":{"docs":{},"本":{"docs":{},"来":{"docs":{},"是":{"docs":{},"个":{"docs":{},"行":{"docs":{},"向":{"docs":{},"量":{"docs":{},"，":{"docs":{},"转":{"docs":{},"置":{"docs":{},"后":{"docs":{},"是":{"1":{"docs":{},"行":{"docs":{},"m":{"docs":{},"列":{"docs":{},"的":{"docs":{},"列":{"docs":{},"向":{"docs":{},"量":{"docs":{},"）":{"docs":{},"与":{"docs":{},"x":{"docs":{},"这":{"docs":{},"个":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"（":{"docs":{},"m":{"docs":{},"行":{"docs":{},"n":{"docs":{},"列":{"docs":{},"）":{"docs":{},"做":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"等":{"docs":{},"到":{"docs":{},"的":{"docs":{},"其":{"docs":{},"中":{"docs":{},"一":{"docs":{},"项":{"docs":{},"的":{"docs":{},"相":{"docs":{},"乘":{"docs":{},"相":{"docs":{},"加":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{"PCA/2.使用梯度上升法解决PCA问题.html":{"ref":"PCA/2.使用梯度上升法解决PCA问题.html","tf":0.16666666666666666}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}},"到":{"docs":{},"排":{"docs":{},"在":{"docs":{},"前":{"docs":{},"面":{"docs":{},"的":{"docs":{},"这":{"docs":{},"些":{"docs":{},"脸":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"比":{"docs":{},"较":{"docs":{},"笼":{"docs":{},"统":{"docs":{},"，":{"docs":{},"排":{"docs":{},"名":{"docs":{},"第":{"docs":{},"一":{"docs":{},"的":{"docs":{},"这":{"docs":{},"张":{"docs":{},"脸":{"docs":{},"，":{"docs":{},"告":{"docs":{},"诉":{"docs":{},"我":{"docs":{},"们":{"docs":{},"人":{"docs":{},"脸":{"docs":{},"大":{"docs":{},"概":{"docs":{},"就":{"docs":{},"是":{"docs":{},"这":{"docs":{},"个":{"docs":{},"位":{"docs":{},"置":{"docs":{},"，":{"docs":{},"大":{"docs":{},"概":{"docs":{},"有":{"docs":{},"这":{"docs":{},"样":{"docs":{},"一":{"docs":{},"个":{"docs":{},"轮":{"docs":{},"廓":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"发":{"docs":{},"现":{"docs":{},"，":{"docs":{},"只":{"docs":{},"要":{"docs":{},"η":{"docs":{},"不":{"docs":{},"超":{"docs":{},"过":{"docs":{},"一":{"docs":{},"个":{"docs":{},"限":{"docs":{},"度":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"编":{"docs":{},"写":{"docs":{},"的":{"docs":{},"函":{"docs":{},"数":{"docs":{},"都":{"docs":{},"可":{"docs":{},"以":{"docs":{},"在":{"docs":{},"有":{"docs":{},"限":{"docs":{},"次":{"docs":{},"数":{"docs":{},"之":{"docs":{},"后":{"docs":{},"找":{"docs":{},"到":{"docs":{},"最":{"docs":{},"优":{"docs":{},"解":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"η":{"docs":{},"越":{"docs":{},"小":{"docs":{},"，":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"次":{"docs":{},"数":{"docs":{},"越":{"docs":{},"多":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"e":{"docs":{},"x":{"docs":{},"p":{"docs":{},"l":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"d":{"docs":{},"v":{"docs":{},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"_":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"这":{"docs":{},"个":{"docs":{},"参":{"docs":{},"数":{"docs":{},"来":{"docs":{},"查":{"docs":{},"看":{"docs":{},"每":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"所":{"docs":{},"解":{"docs":{},"释":{"docs":{},"的":{"docs":{},"原":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"来":{"docs":{},"判":{"docs":{},"断":{"docs":{},"要":{"docs":{},"取":{"docs":{},"多":{"docs":{},"少":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"方":{"docs":{},"便":{"docs":{},"可":{"docs":{},"视":{"docs":{},"化":{"docs":{},"展":{"docs":{},"示":{"docs":{},"，":{"docs":{},"帮":{"docs":{},"助":{"docs":{},"人":{"docs":{},"们":{"docs":{},"理":{"docs":{},"解":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}},"想":{"docs":{},"象":{"docs":{},"如":{"docs":{},"果":{"docs":{},"将":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"设":{"docs":{},"置":{"docs":{},"为":{"3":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"将":{"docs":{},"产":{"docs":{},"生":{"docs":{},"一":{"docs":{},"下":{"1":{"0":{"docs":{},"个":{"docs":{},"元":{"docs":{},"素":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}},"docs":{}},"docs":{}}}}}}}}}},"docs":{}}}}}}}}}}}}}}},"解":{"docs":{},"决":{"docs":{},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{},"（":{"docs":{},"将":{"docs":{},"最":{"docs":{},"终":{"docs":{},"预":{"docs":{},"测":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"落":{"docs":{},"在":{"docs":{},"叶":{"docs":{},"子":{"docs":{},"节":{"docs":{},"点":{"docs":{},"所":{"docs":{},"有":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"的":{"docs":{},"平":{"docs":{},"均":{"docs":{},"值":{"docs":{},"作":{"docs":{},"为":{"docs":{},"预":{"docs":{},"测":{"docs":{},"值":{"docs":{},"）":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"能":{"docs":{},"预":{"docs":{},"测":{"docs":{},"房":{"docs":{},"源":{"docs":{},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{},"，":{"docs":{},"r":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"或":{"docs":{},"者":{"docs":{},"m":{"docs":{},"a":{"docs":{},"e":{"docs":{},"的":{"docs":{},"值":{"docs":{},"为":{"5":{"docs":{},"，":{"docs":{},"预":{"docs":{},"测":{"docs":{},"学":{"docs":{},"生":{"docs":{},"的":{"docs":{},"分":{"docs":{},"数":{"docs":{},"，":{"docs":{},"结":{"docs":{},"果":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"是":{"1":{"0":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"5":{"docs":{},"和":{"1":{"0":{"docs":{},"没":{"docs":{},"有":{"docs":{},"判":{"docs":{},"断":{"docs":{},"性":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"5":{"docs":{},"和":{"1":{"0":{"docs":{},"对":{"docs":{},"应":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"单":{"docs":{},"位":{"docs":{},"和":{"docs":{},"量":{"docs":{},"纲":{"docs":{},"，":{"docs":{},"无":{"docs":{},"法":{"docs":{},"比":{"docs":{},"较":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}},"docs":{}},"docs":{}}},"docs":{}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"我":{"docs":{},"们":{"docs":{},"计":{"docs":{},"算":{"docs":{},"出":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"的":{"docs":{},"公":{"docs":{},"式":{"docs":{},"，":{"docs":{},"并":{"docs":{},"使":{"docs":{},"用":{"docs":{},"p":{"docs":{},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"n":{"docs":{},"编":{"docs":{},"程":{"docs":{},"实":{"docs":{},"现":{"docs":{},"，":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"程":{"docs":{},"序":{"docs":{},"并":{"docs":{},"没":{"docs":{},"有":{"docs":{},"报":{"docs":{},"错":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"可":{"docs":{},"能":{"docs":{},"我":{"docs":{},"们":{"docs":{},"需":{"docs":{},"要":{"docs":{},"求":{"docs":{},"的":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"是":{"docs":{},"错":{"docs":{},"误":{"docs":{},"的":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"时":{"docs":{},"候":{"docs":{},"需":{"docs":{},"要":{"docs":{},"怎":{"docs":{},"么":{"docs":{},"样":{"docs":{},"去":{"docs":{},"调":{"docs":{},"试":{"docs":{},"发":{"docs":{},"现":{"docs":{},"错":{"docs":{},"误":{"docs":{},"呢":{"docs":{},"。":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"在":{"1":{"0":{"0":{"0":{"0":{"docs":{},"个":{"docs":{},"癌":{"docs":{},"症":{"docs":{},"患":{"docs":{},"者":{"docs":{},"中":{"docs":{},"，":{"docs":{},"一":{"docs":{},"共":{"docs":{},"有":{"1":{"0":{"docs":{},"个":{"docs":{},"癌":{"docs":{},"症":{"docs":{},"患":{"docs":{},"者":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"成":{"docs":{},"功":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"出":{"docs":{},"了":{"8":{"docs":{},"个":{"docs":{"评价分类结果/9.2 精准率和召回率.html":{"ref":"评价分类结果/9.2 精准率和召回率.html","tf":0.1111111111111111}}}},"docs":{}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{},"使":{"docs":{},"用":{"2":{"0":{"docs":{},"阶":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"可":{"docs":{},"以":{"docs":{},"发":{"docs":{},"现":{"docs":{},"，":{"docs":{},"在":{"docs":{},"数":{"docs":{},"据":{"docs":{},"量":{"docs":{},"偏":{"docs":{},"多":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"的":{"docs":{},"是":{"docs":{},"比":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"相":{"docs":{},"对":{"docs":{},"来":{"docs":{},"说":{"docs":{},"增":{"docs":{},"大":{"docs":{},"了":{"docs":{},"很":{"docs":{},"多":{"docs":{},"，":{"docs":{},"离":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"比":{"docs":{},"较":{"docs":{},"远":{"docs":{},"，":{"docs":{},"通":{"docs":{},"常":{"docs":{},"这":{"docs":{},"就":{"docs":{},"是":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"他":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"是":{"docs":{},"不":{"docs":{},"够":{"docs":{},"的":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{},"方":{"docs":{},"法":{"docs":{},"后":{"docs":{},"面":{"docs":{},"加":{"docs":{},"?":{"docs":{},"查":{"docs":{},"看":{"docs":{},"文":{"docs":{},"档":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}},"v":{"docs":{},"m":{"docs":{},"中":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"不":{"docs":{},"使":{"docs":{},"用":{"docs":{},"现":{"docs":{},"使":{"docs":{},"用":{"docs":{},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"f":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"再":{"docs":{},"扔":{"docs":{},"给":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},"这":{"docs":{},"种":{"docs":{},"方":{"docs":{},"式":{"docs":{},"进":{"docs":{},"行":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"曲":{"docs":{},"线":{"docs":{},"方":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"导":{"docs":{},"数":{"docs":{},"代":{"docs":{},"表":{"docs":{},"切":{"docs":{},"线":{"docs":{},"斜":{"docs":{},"率":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}}}}}}}}}}}}}},"直":{"docs":{},"线":{"docs":{},"方":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"导":{"docs":{},"数":{"docs":{},"代":{"docs":{},"表":{"docs":{},"斜":{"docs":{},"率":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}}}}}}}}}}}},"随":{"docs":{},"机":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"中":{"docs":{},"，":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"代":{"docs":{},"表":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"会":{"docs":{},"被":{"docs":{},"看":{"docs":{},"几":{"docs":{},"圈":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"人":{"docs":{},"脸":{"docs":{},"识":{"docs":{},"别":{"docs":{},"领":{"docs":{},"域":{"docs":{},"中":{"docs":{},"，":{"docs":{},"x":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"都":{"docs":{},"是":{"docs":{},"人":{"docs":{},"脸":{"docs":{},"，":{"docs":{},"而":{"docs":{},"w":{"docs":{},"中":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"，":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"也":{"docs":{},"可":{"docs":{},"以":{"docs":{},"理":{"docs":{},"解":{"docs":{},"为":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"人":{"docs":{},"脸":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"特":{"docs":{},"征":{"docs":{},"脸":{"docs":{},"。":{"docs":{},"之":{"docs":{},"所":{"docs":{},"以":{"docs":{},"叫":{"docs":{},"特":{"docs":{},"征":{"docs":{},"脸":{"docs":{},"就":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"，":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"都":{"docs":{},"能":{"docs":{},"反":{"docs":{},"应":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"重":{"docs":{},"要":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"。":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"用":{"docs":{},"新":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"进":{"docs":{},"行":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}},"最":{"docs":{},"终":{"docs":{},"，":{"docs":{},"测":{"docs":{},"试":{"docs":{},"误":{"docs":{},"差":{"docs":{},"和":{"docs":{},"训":{"docs":{},"练":{"docs":{},"误":{"docs":{},"差":{"docs":{},"趋":{"docs":{},"于":{"docs":{},"相":{"docs":{},"等":{"docs":{},"，":{"docs":{},"不":{"docs":{},"过":{"docs":{},"测":{"docs":{},"试":{"docs":{},"误":{"docs":{},"差":{"docs":{},"还":{"docs":{},"是":{"docs":{},"高":{"docs":{},"于":{"docs":{},"训":{"docs":{},"练":{"docs":{},"误":{"docs":{},"差":{"docs":{},"一":{"docs":{},"些":{"docs":{},"，":{"docs":{},"这":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"，":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"在":{"docs":{},"数":{"docs":{},"据":{"docs":{},"非":{"docs":{},"常":{"docs":{},"多":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"将":{"docs":{},"数":{"docs":{},"据":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"的":{"docs":{},"比":{"docs":{},"较":{"docs":{},"好":{"docs":{},"，":{"docs":{},"误":{"docs":{},"差":{"docs":{},"小":{"docs":{},"一":{"docs":{},"些":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"到":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"还":{"docs":{},"是":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"多":{"docs":{},"一":{"docs":{},"些":{"docs":{},"误":{"docs":{},"差":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"上":{"docs":{},"，":{"docs":{},"在":{"docs":{},"使":{"docs":{},"用":{"docs":{},"非":{"docs":{},"常":{"docs":{},"少":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"进":{"docs":{},"行":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"刚":{"docs":{},"开":{"docs":{},"始":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{},"误":{"docs":{},"差":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"大":{"docs":{},"，":{"docs":{},"当":{"docs":{},"训":{"docs":{},"练":{"docs":{},"样":{"docs":{},"本":{"docs":{},"大":{"docs":{},"到":{"docs":{},"一":{"docs":{},"定":{"docs":{},"程":{"docs":{},"度":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{},"误":{"docs":{},"差":{"docs":{},"就":{"docs":{},"会":{"docs":{},"逐":{"docs":{},"渐":{"docs":{},"减":{"docs":{},"小":{"docs":{},"，":{"docs":{},"减":{"docs":{},"小":{"docs":{},"到":{"docs":{},"一":{"docs":{},"定":{"docs":{},"程":{"docs":{},"度":{"docs":{},"后":{"docs":{},"，":{"docs":{},"也":{"docs":{},"不":{"docs":{},"会":{"docs":{},"小":{"docs":{},"太":{"docs":{},"多":{"docs":{},"，":{"docs":{},"达":{"docs":{},"到":{"docs":{},"一":{"docs":{},"种":{"docs":{},"相":{"docs":{},"对":{"docs":{},"稳":{"docs":{},"定":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"上":{"docs":{},"，":{"docs":{},"误":{"docs":{},"差":{"docs":{},"是":{"docs":{},"逐":{"docs":{},"渐":{"docs":{},"升":{"docs":{},"高":{"docs":{},"的":{"docs":{},"。":{"docs":{},"这":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"越":{"docs":{},"来":{"docs":{},"越":{"docs":{},"多":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"越":{"docs":{},"难":{"docs":{},"得":{"docs":{},"到":{"docs":{},"全":{"docs":{},"部":{"docs":{},"的":{"docs":{},"累":{"docs":{},"积":{"docs":{},"，":{"docs":{},"不":{"docs":{},"过":{"docs":{},"整":{"docs":{},"体":{"docs":{},"而":{"docs":{},"言":{"docs":{},"，":{"docs":{},"在":{"docs":{},"刚":{"docs":{},"开":{"docs":{},"始":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"误":{"docs":{},"差":{"docs":{},"变":{"docs":{},"化":{"docs":{},"的":{"docs":{},"比":{"docs":{},"较":{"docs":{},"快":{"docs":{},"，":{"docs":{},"后":{"docs":{},"来":{"docs":{},"就":{"docs":{},"几":{"docs":{},"乎":{"docs":{},"不":{"docs":{},"变":{"docs":{},"了":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"是":{"docs":{},"百":{"docs":{},"分":{"docs":{},"之":{"docs":{},"百":{"docs":{},"的":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"下":{"docs":{},"，":{"docs":{},"添":{"docs":{},"加":{"docs":{},"上":{"docs":{},"一":{"docs":{},"个":{"docs":{},"l":{"1":{"docs":{},"正":{"docs":{},"则":{"docs":{},"项":{"docs":{},"和":{"docs":{},"一":{"docs":{},"个":{"docs":{},"l":{"2":{"docs":{},"正":{"docs":{},"则":{"docs":{},"项":{"docs":{},"，":{"docs":{},"并":{"docs":{},"引":{"docs":{},"入":{"docs":{},"一":{"docs":{},"个":{"docs":{},"参":{"docs":{},"数":{"docs":{},"r":{"docs":{},"来":{"docs":{},"表":{"docs":{},"示":{"docs":{},"他":{"docs":{},"们":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"比":{"docs":{},"例":{"docs":{},"。":{"docs":{},"同":{"docs":{},"时":{"docs":{},"结":{"docs":{},"合":{"docs":{},"了":{"docs":{},"岭":{"docs":{},"回":{"docs":{},"归":{"docs":{},"和":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"优":{"docs":{},"势":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}},"docs":{}}}}}}}}}}}}},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"领":{"docs":{},"域":{"docs":{},"中":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"会":{"docs":{},"发":{"docs":{},"明":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"名":{"docs":{},"词":{"docs":{},"来":{"docs":{},"描":{"docs":{},"述":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"标":{"docs":{},"准":{"docs":{},"，":{"docs":{},"比":{"docs":{},"如":{"docs":{},"用":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"docs":{},"和":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"来":{"docs":{},"衡":{"docs":{},"量":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"的":{"docs":{},"这":{"docs":{},"一":{"docs":{},"项":{"docs":{},"；":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"和":{"docs":{},"m":{"docs":{},"a":{"docs":{},"e":{"docs":{},"用":{"docs":{},"来":{"docs":{},"衡":{"docs":{},"量":{"docs":{},"回":{"docs":{},"归":{"docs":{},"结":{"docs":{},"果":{"docs":{},"的":{"docs":{},"好":{"docs":{},"坏":{"docs":{},"，":{"docs":{},"欧":{"docs":{},"拉":{"docs":{},"距":{"docs":{},"离":{"docs":{},"和":{"docs":{},"曼":{"docs":{},"哈":{"docs":{},"顿":{"docs":{},"距":{"docs":{},"离":{"docs":{},"用":{"docs":{},"来":{"docs":{},"衡":{"docs":{},"量":{"docs":{},"两":{"docs":{},"点":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"。":{"docs":{},"但":{"docs":{},"是":{"docs":{},"他":{"docs":{},"们":{"docs":{},"背":{"docs":{},"后":{"docs":{},"的":{"docs":{},"数":{"docs":{},"学":{"docs":{},"思":{"docs":{},"想":{"docs":{},"是":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"类":{"docs":{},"似":{"docs":{},"的":{"docs":{},"，":{"docs":{},"表":{"docs":{},"达":{"docs":{},"出":{"docs":{},"的":{"docs":{},"数":{"docs":{},"学":{"docs":{},"含":{"docs":{},"义":{"docs":{},"也":{"docs":{},"是":{"docs":{},"一":{"docs":{},"致":{"docs":{},"的":{"docs":{},"。":{"docs":{},"只":{"docs":{},"不":{"docs":{},"过":{"docs":{},"应":{"docs":{},"用":{"docs":{},"到":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"场":{"docs":{},"景":{"docs":{},"中":{"docs":{},"产":{"docs":{},"生":{"docs":{},"了":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"平":{"docs":{},"面":{"docs":{},"上":{"docs":{},"，":{"docs":{},"分":{"docs":{},"布":{"docs":{},"着":{"docs":{},"很":{"docs":{},"多":{"docs":{},"点":{"docs":{},"，":{"docs":{},"对":{"docs":{},"于":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"点":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"都":{"docs":{},"使":{"docs":{},"用":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"来":{"docs":{},"判":{"docs":{},"断":{"docs":{},"他":{"docs":{},"分":{"docs":{},"为":{"docs":{},"哪":{"docs":{},"一":{"docs":{},"类":{"docs":{},"。":{"docs":{},"然":{"docs":{},"后":{"docs":{},"将":{"docs":{},"这":{"docs":{},"些":{"docs":{},"颜":{"docs":{},"色":{"docs":{},"绘":{"docs":{},"制":{"docs":{},"出":{"docs":{},"来":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"就":{"docs":{},"是":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"之":{"docs":{},"前":{"docs":{},"推":{"docs":{},"导":{"docs":{},"的":{"docs":{},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{},"基":{"docs":{},"础":{"docs":{},"上":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"适":{"docs":{},"当":{"docs":{},"的":{"docs":{},"将":{"docs":{},"条":{"docs":{},"件":{"docs":{},"放":{"docs":{},"宽":{"docs":{},"松":{"docs":{},"一":{"docs":{},"些":{"docs":{},"，":{"docs":{},"允":{"docs":{},"许":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"犯":{"docs":{},"一":{"docs":{},"些":{"docs":{},"错":{"docs":{},"误":{"docs":{"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":0.0625}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"最":{"docs":{},"优":{"docs":{},"化":{"docs":{},"函":{"docs":{},"数":{"docs":{},"基":{"docs":{},"础":{"docs":{},"上":{"docs":{},"，":{"docs":{},"需":{"docs":{},"要":{"docs":{},"进":{"docs":{},"行":{"docs":{},"进":{"docs":{},"一":{"docs":{},"步":{"docs":{},"的":{"docs":{},"变":{"docs":{},"形":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}},"调":{"docs":{},"小":{"docs":{},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"想":{"docs":{},"象":{"docs":{},"成":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"蓝":{"docs":{},"色":{"docs":{},"点":{"docs":{},"周":{"docs":{},"围":{"docs":{},"的":{"docs":{},"中":{"docs":{},"型":{"docs":{},"图":{"docs":{},"案":{"docs":{},"变":{"docs":{},"宽":{"docs":{},"了":{"docs":{},"一":{"docs":{},"些":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"这":{"docs":{},"些":{"docs":{},"蓝":{"docs":{},"色":{"docs":{},"的":{"docs":{},"点":{"docs":{},"离":{"docs":{},"的":{"docs":{},"比":{"docs":{},"较":{"docs":{},"近":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"图":{"docs":{},"案":{"docs":{},"连":{"docs":{},"在":{"docs":{},"了":{"docs":{},"一":{"docs":{},"起":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"及":{"docs":{},"其":{"docs":{},"有":{"docs":{},"偏":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"中":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"不":{"docs":{},"看":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"，":{"docs":{},"而":{"docs":{},"看":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"，":{"docs":{},"才":{"docs":{},"能":{"docs":{},"分":{"docs":{},"析":{"docs":{},"出":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"好":{"docs":{},"坏":{"docs":{"评价分类结果/9.2 精准率和召回率.html":{"ref":"评价分类结果/9.2 精准率和召回率.html","tf":0.1111111111111111}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"有":{"docs":{},"偏":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"中":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"将":{"docs":{},"分":{"docs":{},"类":{"1":{"docs":{},"作":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"关":{"docs":{},"注":{"docs":{},"的":{"docs":{},"对":{"docs":{},"象":{"docs":{},"，":{"docs":{},"例":{"docs":{},"如":{"docs":{},"在":{"docs":{},"医":{"docs":{},"疗":{"docs":{},"中":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"就":{"docs":{},"是":{"docs":{},"指":{"docs":{},"我":{"docs":{},"们":{"docs":{},"预":{"docs":{},"测":{"docs":{},"癌":{"docs":{},"症":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"成":{"docs":{},"功":{"docs":{},"率":{"docs":{"评价分类结果/9.2 精准率和召回率.html":{"ref":"评价分类结果/9.2 精准率和召回率.html","tf":0.1111111111111111}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}},"f":{"docs":{},"p":{"docs":{},"r":{"docs":{},"越":{"docs":{},"低":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"（":{"docs":{},"犯":{"docs":{},"f":{"docs":{},"p":{"docs":{},"错":{"docs":{},"误":{"docs":{},"的":{"docs":{},"次":{"docs":{},"数":{"docs":{},"越":{"docs":{},"少":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"）":{"docs":{},"，":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"f":{"docs":{},"p":{"docs":{},"r":{"docs":{},"越":{"docs":{},"高":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"这":{"docs":{},"根":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"整":{"docs":{},"体":{"docs":{},"就":{"docs":{},"会":{"docs":{},"被":{"docs":{},"抬":{"docs":{},"的":{"docs":{},"越":{"docs":{},"高":{"docs":{},"，":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"生":{"docs":{},"成":{"docs":{},"规":{"docs":{},"则":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"基":{"docs":{},"础":{"docs":{},"上":{"docs":{},"，":{"docs":{},"增":{"docs":{},"加":{"docs":{},"标":{"docs":{},"准":{"docs":{},"差":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}}}}}},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"求":{"docs":{},"解":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"需":{"docs":{},"要":{"docs":{},"对":{"docs":{},"x":{"docs":{},"i":{"docs":{},"添":{"docs":{},"加":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"项":{"docs":{},"形":{"docs":{},"成":{"docs":{},"x":{"docs":{},"`":{"docs":{},"i":{"docs":{},",":{"docs":{},"对":{"docs":{},"x":{"docs":{},"j":{"docs":{},"添":{"docs":{},"加":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"项":{"docs":{},"形":{"docs":{},"成":{"docs":{},"x":{"docs":{},"`":{"docs":{},"j":{"docs":{},"。":{"docs":{},"然":{"docs":{},"后":{"docs":{},"再":{"docs":{},"将":{"docs":{},"二":{"docs":{},"者":{"docs":{},"进":{"docs":{},"行":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"进":{"docs":{},"行":{"docs":{},"这":{"docs":{},"一":{"docs":{},"项":{"docs":{},"变":{"docs":{},"化":{"docs":{},"之":{"docs":{},"后":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"发":{"docs":{},"现":{"docs":{},"，":{"docs":{},"在":{"docs":{},"第":{"docs":{},"二":{"docs":{},"项":{"docs":{},"中":{"docs":{},"其":{"docs":{},"中":{"docs":{},"有":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"是":{"docs":{},"x":{"docs":{},"i":{"docs":{},"x":{"docs":{},"j":{"docs":{},",":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"对":{"docs":{},"于":{"docs":{},"任":{"docs":{},"意":{"docs":{},"两":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"的":{"docs":{},"x":{"docs":{},"值":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"具":{"docs":{},"体":{"docs":{},"训":{"docs":{},"练":{"docs":{},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"对":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"是":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"指":{"docs":{},"定":{"docs":{},"的":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"在":{"docs":{},"这":{"docs":{},"里":{"docs":{},"引":{"docs":{},"入":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"ɛ":{"docs":{},"，":{"docs":{},"代":{"docs":{},"表":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"范":{"docs":{},"围":{"docs":{},"的":{"docs":{},"两":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"的":{"docs":{},"任":{"docs":{},"意":{"docs":{},"一":{"docs":{},"跟":{"docs":{},"到":{"docs":{},"中":{"docs":{},"间":{"docs":{},"直":{"docs":{},"线":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"这":{"docs":{},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{},"干":{"docs":{},"的":{"docs":{},"事":{"docs":{},"，":{"docs":{},"和":{"docs":{},"解":{"docs":{},"决":{"docs":{},"分":{"docs":{},"类":{"docs":{},"算":{"docs":{},"法":{"docs":{},"是":{"docs":{},"相":{"docs":{},"反":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"期":{"docs":{},"望":{"docs":{},"的":{"docs":{},"是":{"docs":{},"在":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"范":{"docs":{},"围":{"docs":{},"里":{"docs":{},"，":{"docs":{},"包":{"docs":{},"围":{"docs":{},"的":{"docs":{},"点":{"docs":{},"越":{"docs":{},"多":{"docs":{},"越":{"docs":{},"好":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"结":{"docs":{},"点":{"docs":{},"上":{"docs":{},"，":{"docs":{},"他":{"docs":{},"首":{"docs":{},"先":{"docs":{},"选":{"docs":{},"择":{"docs":{},"一":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"以":{"docs":{},"及":{"docs":{},"这":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"上":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"阈":{"docs":{},"值":{"docs":{},"，":{"docs":{},"分":{"docs":{},"成":{"docs":{},"两":{"docs":{},"支":{"docs":{},"，":{"docs":{},"循":{"docs":{},"环":{"docs":{},"往":{"docs":{},"复":{"docs":{},"，":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"分":{"docs":{},"类":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"0":{"docs":{},".":{"7":{"5":{"docs":{},"的":{"docs":{},"位":{"docs":{},"置":{"docs":{},"进":{"docs":{},"行":{"docs":{},"划":{"docs":{},"分":{"docs":{},"，":{"docs":{},"划":{"docs":{},"分":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"为":{"0":{"docs":{},".":{"4":{"1":{"docs":{},"左":{"docs":{},"右":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}},"docs":{}}}}}},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"建":{"docs":{},"立":{"docs":{},"之":{"docs":{},"后":{"docs":{},"，":{"docs":{},"每":{"docs":{},"个":{"docs":{},"叶":{"docs":{},"子":{"docs":{},"结":{"docs":{},"点":{"docs":{},"都":{"docs":{},"包":{"docs":{},"含":{"docs":{},"了":{"docs":{},"一":{"docs":{},"些":{"docs":{},"数":{"docs":{},"据":{"docs":{},"。":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}}}}}}}},"执":{"docs":{},"行":{"docs":{},"p":{"docs":{},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"n":{"docs":{},"脚":{"docs":{},"本":{"docs":{},"，":{"docs":{},"并":{"docs":{},"将":{"docs":{},"脚":{"docs":{},"本":{"docs":{},"中":{"docs":{},"的":{"docs":{},"函":{"docs":{},"数":{"docs":{},"加":{"docs":{},"载":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}},"测":{"docs":{},"试":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}},"一":{"docs":{},"个":{"docs":{},"排":{"docs":{},"序":{"docs":{},"算":{"docs":{},"法":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"第":{"docs":{},"一":{"docs":{},"次":{"docs":{},"执":{"docs":{},"行":{"docs":{},"完":{"docs":{},"毕":{"docs":{},"后":{"docs":{},"数":{"docs":{},"组":{"docs":{},"已":{"docs":{},"经":{"docs":{},"排":{"docs":{},"好":{"docs":{},"序":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"在":{"docs":{},"后":{"docs":{},"面":{"docs":{},"执":{"docs":{},"行":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"使":{"docs":{},"用":{"docs":{},"插":{"docs":{},"入":{"docs":{},"排":{"docs":{},"序":{"docs":{},"等":{"docs":{},"算":{"docs":{},"法":{"docs":{},"就":{"docs":{},"会":{"docs":{},"导":{"docs":{},"致":{"docs":{},"后":{"docs":{},"面":{"9":{"9":{"9":{"docs":{},"次":{"docs":{},"的":{"docs":{},"时":{"docs":{},"间":{"docs":{},"非":{"docs":{},"常":{"docs":{},"短":{"docs":{},"，":{"docs":{},"导":{"docs":{},"致":{"docs":{},"测":{"docs":{},"试":{"docs":{},"值":{"docs":{},"不":{"docs":{},"准":{"docs":{},"确":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"代":{"docs":{},"码":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}},"多":{"docs":{},"次":{"docs":{},"在":{"docs":{},"每":{"docs":{},"次":{"docs":{},"测":{"docs":{},"试":{"docs":{},"的":{"docs":{},"执":{"docs":{},"行":{"docs":{},"性":{"docs":{},"能":{"docs":{},"不":{"docs":{},"一":{"docs":{},"样":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"测":{"docs":{},"试":{"docs":{},"结":{"docs":{},"果":{"docs":{},"会":{"docs":{},"不":{"docs":{},"准":{"docs":{},"确":{"docs":{},"。":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}}}}}}},"维":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}},"整":{"docs":{},"个":{"docs":{},"代":{"docs":{},"码":{"docs":{},"块":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}},"结":{"docs":{},"果":{"docs":{},"表":{"docs":{},"明":{"docs":{},"，":{"docs":{},"运":{"docs":{},"行":{"docs":{},"了":{"docs":{},"一":{"docs":{},"千":{"docs":{},"次":{"docs":{},"，":{"docs":{},"取":{"docs":{},"有":{"docs":{},"价":{"docs":{},"值":{"docs":{},"的":{"7":{"docs":{},"次":{"docs":{},"，":{"docs":{},"平":{"docs":{},"均":{"docs":{},"每":{"docs":{},"次":{"docs":{},"耗":{"docs":{},"时":{"3":{"2":{"4":{"docs":{},"+":{"docs":{},"/":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"算":{"docs":{},"法":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}},"算":{"docs":{},"法":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"比":{"docs":{},"例":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}},"对":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"是":{"docs":{},"全":{"docs":{},"新":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"使":{"docs":{},"用":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"获":{"docs":{},"得":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"面":{"docs":{},"对":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"也":{"docs":{},"能":{"docs":{},"获":{"docs":{},"得":{"docs":{},"很":{"docs":{},"好":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"说":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"是":{"docs":{},"很":{"docs":{},"强":{"docs":{},"的":{"docs":{},"。":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"考":{"docs":{},"虑":{"docs":{},"用":{"docs":{},"%":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{},"i":{"docs":{},"t":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","tf":0.03571428571428571}}}}}}}}}},"距":{"docs":{},"离":{"docs":{},"？":{"docs":{},"不":{"docs":{},"考":{"docs":{},"虑":{"docs":{},"距":{"docs":{},"离":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}},"下":{"docs":{},"面":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"虽":{"docs":{},"然":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"使":{"docs":{},"用":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"来":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"这":{"docs":{},"些":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"这":{"docs":{},"些":{"docs":{},"数":{"docs":{},"据":{"docs":{},"更":{"docs":{},"像":{"docs":{},"是":{"docs":{},"一":{"docs":{},"条":{"docs":{},"二":{"docs":{},"次":{"docs":{},"曲":{"docs":{},"线":{"docs":{},",":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"方":{"docs":{},"程":{"docs":{},"是":{"docs":{},"y":{"docs":{},"=":{"docs":{},"a":{"docs":{},"x":{"2":{"docs":{},"+":{"docs":{},"b":{"docs":{},"x":{"docs":{},"+":{"docs":{},"c":{"docs":{},",":{"docs":{},"这":{"docs":{},"是":{"docs":{},"式":{"docs":{},"子":{"docs":{},"虽":{"docs":{},"然":{"docs":{},"可":{"docs":{},"以":{"docs":{},"理":{"docs":{},"解":{"docs":{},"为":{"docs":{},"二":{"docs":{},"次":{"docs":{},"方":{"docs":{},"程":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"呢":{"docs":{},"可":{"docs":{},"以":{"docs":{},"从":{"docs":{},"另":{"docs":{},"外":{"docs":{},"一":{"docs":{},"个":{"docs":{},"角":{"docs":{},"度":{"docs":{},"来":{"docs":{},"理":{"docs":{},"解":{"docs":{},"这":{"docs":{},"个":{"docs":{},"式":{"docs":{},"子":{"docs":{},"：":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"查":{"docs":{},"看":{"docs":{},"数":{"docs":{},"组":{"docs":{},"元":{"docs":{},"素":{"docs":{},"类":{"docs":{},"型":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":0.1}}}}}}}}}},"由":{"docs":{},"于":{"docs":{},"p":{"docs":{},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"n":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":0.1}}}}}}}},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"和":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"分":{"docs":{},"割":{"docs":{},"和":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"稍":{"docs":{},"有":{"docs":{},"不":{"docs":{},"同":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"结":{"docs":{},"果":{"docs":{},"会":{"docs":{},"略":{"docs":{},"有":{"docs":{},"不":{"docs":{},"同":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"事":{"docs":{},"随":{"docs":{},"机":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"导":{"docs":{},"致":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"最":{"docs":{},"终":{"docs":{},"结":{"docs":{},"果":{"docs":{},"不":{"docs":{},"会":{"docs":{},"像":{"docs":{},"批":{"docs":{},"量":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"一":{"docs":{},"样":{"docs":{},"准":{"docs":{},"确":{"docs":{},"的":{"docs":{},"朝":{"docs":{},"着":{"docs":{},"一":{"docs":{},"个":{"docs":{},"方":{"docs":{},"向":{"docs":{},"运":{"docs":{},"算":{"docs":{},"，":{"docs":{},"而":{"docs":{},"是":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"行":{"docs":{},"下":{"docs":{},"降":{"docs":{},"，":{"docs":{},"这":{"docs":{},"时":{"docs":{},"候":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"希":{"docs":{},"望":{"docs":{},"，":{"docs":{},"越":{"docs":{},"到":{"docs":{},"下":{"docs":{},"面":{"docs":{},"，":{"docs":{},"η":{"docs":{},"值":{"docs":{},"相":{"docs":{},"应":{"docs":{},"减":{"docs":{},"小":{"docs":{},"，":{"docs":{},"事":{"docs":{},"运":{"docs":{},"算":{"docs":{},"次":{"docs":{},"数":{"docs":{},"变":{"docs":{},"多":{"docs":{},"，":{"docs":{},"从":{"docs":{},"而":{"docs":{},"精":{"docs":{},"确":{"docs":{},"计":{"docs":{},"算":{"docs":{},"结":{"docs":{},"果":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"求":{"docs":{},"平":{"docs":{},"均":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"不":{"docs":{},"会":{"docs":{},"由":{"docs":{},"于":{"docs":{},"一":{"docs":{},"份":{"docs":{},"验":{"docs":{},"证":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"中":{"docs":{},"有":{"docs":{},"比":{"docs":{},"较":{"docs":{},"极":{"docs":{},"端":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"而":{"docs":{},"导":{"docs":{},"致":{"docs":{},"模":{"docs":{},"型":{"docs":{},"有":{"docs":{},"过":{"docs":{},"大":{"docs":{},"的":{"docs":{},"偏":{"docs":{},"差":{"docs":{},"，":{"docs":{},"这":{"docs":{},"比":{"docs":{},"我":{"docs":{},"们":{"docs":{},"只":{"docs":{},"分":{"docs":{},"成":{"docs":{},"训":{"docs":{},"练":{"docs":{},"、":{"docs":{},"验":{"docs":{},"证":{"docs":{},"、":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"要":{"docs":{},"更":{"docs":{},"加":{"docs":{},"准":{"docs":{},"确":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"f":{"docs":{},"e":{"docs":{},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{},"_":{"docs":{},"l":{"docs":{},"f":{"docs":{},"w":{"docs":{},"_":{"docs":{},"p":{"docs":{},"e":{"docs":{},"o":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"这":{"docs":{},"个":{"docs":{},"库":{"docs":{},"的":{"docs":{},"人":{"docs":{},"脸":{"docs":{},"是":{"docs":{},"分":{"docs":{},"布":{"docs":{},"不":{"docs":{},"均":{"docs":{},"匀":{"docs":{},"的":{"docs":{},"，":{"docs":{},"有":{"docs":{},"的":{"docs":{},"人":{"docs":{},"可":{"docs":{},"能":{"docs":{},"只":{"docs":{},"有":{"docs":{},"一":{"docs":{},"张":{"docs":{},"图":{"docs":{},"片":{"docs":{},"，":{"docs":{},"有":{"docs":{},"的":{"docs":{},"人":{"docs":{},"有":{"docs":{},"几":{"docs":{},"十":{"docs":{},"张":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"x":{"docs":{},"是":{"docs":{},"乱":{"docs":{},"的":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"应":{"docs":{},"该":{"docs":{},"进":{"docs":{},"行":{"docs":{},"排":{"docs":{},"序":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}},"此":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"出":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"d":{"docs":{},"_":{"docs":{},"j":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"b":{"docs":{},"u":{"docs":{},"g":{"docs":{},"和":{"docs":{},"d":{"docs":{},"_":{"docs":{},"j":{"docs":{},"_":{"docs":{},"m":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"是":{"docs":{},"相":{"docs":{},"近":{"docs":{},"的":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"d":{"docs":{},"_":{"docs":{},"j":{"docs":{},"_":{"docs":{},"m":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"的":{"docs":{},"数":{"docs":{},"学":{"docs":{},"推":{"docs":{},"导":{"docs":{},"是":{"docs":{},"没":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"。":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"上":{"docs":{},"图":{"docs":{},"可":{"docs":{},"知":{"docs":{},"，":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"的":{"docs":{},"关":{"docs":{},"系":{"docs":{},"。":{"docs":{},"随":{"docs":{},"着":{"docs":{},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"增":{"docs":{},"大":{"docs":{},"，":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"在":{"docs":{},"不":{"docs":{},"断":{"docs":{},"的":{"docs":{},"变":{"docs":{},"高":{"docs":{},"，":{"docs":{},"而":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"在":{"docs":{},"不":{"docs":{},"断":{"docs":{},"的":{"docs":{},"降":{"docs":{},"低":{"docs":{},"。":{"docs":{},"由":{"docs":{},"此":{"docs":{},"说":{"docs":{},"明":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"是":{"docs":{},"相":{"docs":{},"互":{"docs":{},"矛":{"docs":{},"盾":{"docs":{},"的":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}},"特":{"docs":{},"点":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","tf":0.1}}}},"方":{"docs":{},"法":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":5}}}},"基":{"docs":{},"本":{"docs":{},"操":{"docs":{},"作":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html","tf":5}}}}}},"实":{"docs":{},"现":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}},"缺":{"docs":{},"点":{"docs":{},"，":{"docs":{},"量":{"docs":{},"纲":{"docs":{},"不":{"docs":{},"准":{"docs":{},"确":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"y":{"docs":{},"的":{"docs":{},"单":{"docs":{},"位":{"docs":{},"是":{"docs":{},"万":{"docs":{},"元":{"docs":{},"，":{"docs":{},"平":{"docs":{},"方":{"docs":{},"后":{"docs":{},"就":{"docs":{},"变":{"docs":{},"成":{"docs":{},"了":{"docs":{},"万":{"docs":{},"元":{"docs":{},"的":{"docs":{},"平":{"docs":{},"方":{"docs":{},"，":{"docs":{},"这":{"docs":{},"可":{"docs":{},"能":{"docs":{},"会":{"docs":{},"给":{"docs":{},"我":{"docs":{},"们":{"docs":{},"带":{"docs":{},"来":{"docs":{},"一":{"docs":{},"些":{"docs":{},"麻":{"docs":{},"烦":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"调":{"docs":{},"试":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}},"分":{"docs":{},"量":{"docs":{},"矩":{"docs":{},"阵":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}},"作":{"docs":{},"用":{"docs":{},"就":{"docs":{},"是":{"docs":{},"把":{"docs":{},"上":{"docs":{},"面":{"docs":{},"的":{"docs":{},"三":{"docs":{},"个":{"docs":{},"步":{"docs":{},"骤":{"docs":{},"合":{"docs":{},"并":{"docs":{},"，":{"docs":{},"使":{"docs":{},"得":{"docs":{},"我":{"docs":{},"们":{"docs":{},"不":{"docs":{},"用":{"docs":{},"一":{"docs":{},"直":{"docs":{},"重":{"docs":{},"复":{"docs":{},"这":{"docs":{},"三":{"docs":{},"步":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"意":{"docs":{},"思":{"docs":{},"就":{"docs":{},"是":{"docs":{},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{},"中":{"docs":{},"分":{"docs":{},"割":{"docs":{},"了":{"docs":{},"三":{"docs":{},"组":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"而":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"组":{"docs":{},"合":{"docs":{},"为":{"8":{"docs":{},"*":{"5":{"docs":{},"=":{"4":{"0":{"docs":{},"中":{"docs":{},"组":{"docs":{},"合":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"值":{"docs":{},"等":{"docs":{},"于":{"1":{"docs":{},"，":{"docs":{},"均":{"docs":{},"差":{"docs":{},"误":{"docs":{},"差":{"docs":{},"更":{"docs":{},"加":{"docs":{},"的":{"docs":{},"缩":{"docs":{},"小":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"越":{"docs":{},"来":{"docs":{},"越":{"docs":{},"趋":{"docs":{},"近":{"docs":{},"于":{"docs":{},"一":{"docs":{},"根":{"docs":{},"倾":{"docs":{},"斜":{"docs":{},"的":{"docs":{},"直":{"docs":{},"线":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"预":{"docs":{},"测":{"docs":{},"值":{"docs":{},"都":{"docs":{},"是":{"0":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}},"docs":{}}}}}}}}},"混":{"docs":{},"淆":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"天":{"docs":{},"然":{"docs":{},"支":{"docs":{},"持":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}},"数":{"docs":{},"据":{"docs":{},"映":{"docs":{},"射":{"docs":{},"成":{"docs":{},"了":{"docs":{},"m":{"docs":{},"m":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"m":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"大":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"经":{"docs":{},"过":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"后":{"docs":{},"，":{"docs":{},"就":{"docs":{},"映":{"docs":{},"射":{"docs":{},"成":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"非":{"docs":{},"常":{"docs":{},"非":{"docs":{},"常":{"docs":{},"高":{"docs":{},"维":{"docs":{},"空":{"docs":{},"间":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"达":{"docs":{},"到":{"docs":{},"了":{"docs":{},"最":{"docs":{},"大":{"docs":{},"值":{"docs":{},"（":{"docs":{},"当":{"docs":{},"两":{"docs":{},"类":{"docs":{},"样":{"docs":{},"本":{"docs":{},"各":{"docs":{},"占":{"docs":{},"一":{"docs":{},"半":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"）":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"说":{"docs":{},"这":{"docs":{},"个":{"docs":{},"时":{"docs":{},"候":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"是":{"docs":{},"最":{"docs":{},"不":{"docs":{},"稳":{"docs":{},"定":{"docs":{},"的":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"原":{"docs":{},"理":{"docs":{},"，":{"docs":{},"只":{"docs":{},"要":{"docs":{},"大":{"docs":{},"多":{"docs":{},"数":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"能":{"docs":{},"力":{"docs":{},"比":{"docs":{},"扔":{"docs":{},"硬":{"docs":{},"币":{"docs":{},"的":{"docs":{},"能":{"docs":{},"力":{"docs":{},"好":{"docs":{},"一":{"docs":{},"点":{"docs":{},"就":{"docs":{},"够":{"docs":{},"了":{"docs":{},")":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"其":{"docs":{},"他":{"docs":{},"创":{"docs":{},"建":{"docs":{},"n":{"docs":{},"u":{"docs":{},"m":{"docs":{},"p":{"docs":{},"y":{"docs":{},".":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":5}}}}}}}}}}}}}}},"注":{"docs":{},"意":{"docs":{},"事":{"docs":{},"项":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}}}}},"中":{"docs":{},"衡":{"docs":{},"量":{"docs":{},"标":{"docs":{},"准":{"docs":{},"是":{"docs":{},"和":{"docs":{},"m":{"docs":{},"有":{"docs":{},"关":{"docs":{},"的":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"越":{"docs":{},"多":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"量":{"docs":{},"产":{"docs":{},"生":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"和":{"docs":{},"可":{"docs":{},"能":{"docs":{},"会":{"docs":{},"更":{"docs":{},"大":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"毫":{"docs":{},"无":{"docs":{},"疑":{"docs":{},"问":{"docs":{},"越":{"docs":{},"多":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"量":{"docs":{},"训":{"docs":{},"练":{"docs":{},"出":{"docs":{},"来":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"更":{"docs":{},"好":{"docs":{},"，":{"docs":{},"为":{"docs":{},"此":{"docs":{},"需":{"docs":{},"要":{"docs":{},"一":{"docs":{},"个":{"docs":{},"取":{"docs":{},"消":{"docs":{},"误":{"docs":{},"差":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"，":{"docs":{},"如":{"docs":{},"下":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"创":{"docs":{},"建":{"docs":{},"n":{"docs":{},"维":{"0":{"docs":{},"数":{"docs":{},"组":{"docs":{},",":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"参":{"docs":{},"数":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"是":{"docs":{},"数":{"docs":{},"组":{"docs":{},"维":{"docs":{},"度":{"docs":{},"，":{"docs":{},"第":{"docs":{},"二":{"docs":{},"个":{"docs":{},"参":{"docs":{},"数":{"docs":{},"是":{"docs":{},"类":{"docs":{},"型":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"一":{"docs":{},"维":{"0":{"docs":{},"数":{"docs":{},"组":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}}},"docs":{}}},"更":{"docs":{},"多":{"docs":{},"的":{"docs":{},"子":{"docs":{},"模":{"docs":{},"型":{"docs":{},"！":{"docs":{},"集":{"docs":{},"成":{"docs":{},"更":{"docs":{},"多":{"docs":{},"的":{"docs":{},"子":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"意":{"docs":{},"见":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}}}}}}}}}}}}}},"注":{"docs":{},"意":{"2":{"docs":{},"：":{"docs":{},"不":{"docs":{},"能":{"docs":{},"从":{"0":{"docs":{},"向":{"docs":{},"量":{"docs":{},"开":{"docs":{},"始":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}},"docs":{}}}}}},"3":{"docs":{},"：":{"docs":{},"不":{"docs":{},"能":{"docs":{},"使":{"docs":{},"用":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{},"r":{"docs":{},"标":{"docs":{},"准":{"docs":{},"化":{"docs":{},"数":{"docs":{},"据":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{},"f":{"docs":{},"u":{"docs":{},"l":{"docs":{},"l":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}}}},"这":{"docs":{},"个":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"在":{"docs":{},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"是":{"docs":{},"完":{"docs":{},"全":{"docs":{},"没":{"docs":{},"有":{"docs":{},"用":{"docs":{},"过":{"docs":{},"的":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"说":{"docs":{},"我":{"docs":{},"们":{"docs":{},"这":{"docs":{},"样":{"docs":{},"得":{"docs":{},"出":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"是":{"docs":{},"可":{"docs":{},"信":{"docs":{},"的":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"后":{"docs":{},"面":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"是":{"docs":{},"所":{"docs":{},"有":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"的":{"docs":{},"平":{"docs":{},"方":{"docs":{},"和":{"docs":{},"，":{"docs":{},"而":{"docs":{},"对":{"docs":{},"于":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"岭":{"docs":{},"回":{"docs":{},"归":{"docs":{},"之":{"docs":{},"前":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"θ":{"docs":{},"都":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"：":{"docs":{},"这":{"docs":{},"里":{"docs":{},"由":{"docs":{},"于":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"比":{"docs":{},"较":{"docs":{},"小":{"docs":{},"，":{"docs":{},"耗":{"docs":{},"时":{"docs":{},"的":{"docs":{},"差":{"docs":{},"别":{"docs":{},"比":{"docs":{},"较":{"docs":{},"小":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}},"：":{"docs":{},"|":{"docs":{},"x":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"j":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"|":{"docs":{},"的":{"docs":{},"平":{"docs":{},"均":{"docs":{},"值":{"docs":{},"也":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"向":{"docs":{},"量":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}},"有":{"docs":{},"了":{"docs":{},"l":{"1":{"docs":{},",":{"docs":{},"l":{"2":{"docs":{},"正":{"docs":{},"则":{"docs":{},"项":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"进":{"docs":{},"一":{"docs":{},"步":{"docs":{},"得":{"docs":{},"到":{"docs":{},"l":{"docs":{},"n":{"docs":{},"正":{"docs":{},"则":{"docs":{},"项":{"docs":{},"，":{"docs":{},"虽":{"docs":{},"然":{"docs":{},"实":{"docs":{},"际":{"docs":{},"应":{"docs":{},"用":{"docs":{},"中":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"n":{"docs":{},"不":{"docs":{},"会":{"docs":{},"超":{"docs":{},"过":{"2":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"在":{"docs":{},"数":{"docs":{},"学":{"docs":{},"推":{"docs":{},"导":{"docs":{},"中":{"docs":{},"是":{"docs":{},"有":{"docs":{},"意":{"docs":{},"义":{"docs":{},"的":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}},"docs":{}}}}}},"随":{"docs":{},"机":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}},"整":{"docs":{},"数":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}},"浮":{"docs":{},"点":{"docs":{},"数":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}}},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.016}},"介":{"docs":{},"绍":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}},"实":{"docs":{},"现":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}},"的":{"docs":{},"封":{"docs":{},"装":{"docs":{},"和":{"docs":{},"测":{"docs":{},"试":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}},"优":{"docs":{},"点":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}},"的":{"docs":{},"检":{"docs":{},"查":{"docs":{},"了":{"3":{"docs":{},"分":{"docs":{},"之":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"总":{"docs":{},"量":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}},"docs":{}}}}},"获":{"docs":{},"取":{"3":{"6":{"docs":{},"张":{"docs":{},"脸":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}},"docs":{}},"docs":{}}},"森":{"docs":{},"林":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":5.010989010989011}},"包":{"docs":{},"含":{"docs":{},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}}}}}},"着":{"docs":{},"训":{"docs":{},"练":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"主":{"docs":{},"键":{"docs":{},"增":{"docs":{},"多":{"docs":{},"，":{"docs":{},"算":{"docs":{},"法":{"docs":{},"训":{"docs":{},"练":{"docs":{},"出":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"表":{"docs":{},"现":{"docs":{},"能":{"docs":{},"力":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"逐":{"docs":{},"渐":{"docs":{},"降":{"docs":{},"低":{"docs":{},"，":{"docs":{},"t":{"docs":{},"p":{"docs":{},"r":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}},"默":{"docs":{},"认":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"类":{"docs":{},"型":{"docs":{},"是":{"docs":{},"整":{"docs":{},"形":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","tf":0.08333333333333333}}}}}}}}}},"s":{"docs":{},"o":{"docs":{},"l":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"=":{"docs":{},"'":{"docs":{},"l":{"docs":{},"i":{"docs":{},"b":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"'":{"docs":{},",":{"docs":{},"为":{"docs":{},"了":{"docs":{},"正":{"docs":{},"确":{"docs":{},"的":{"docs":{},"调":{"docs":{},"用":{"docs":{},"o":{"docs":{},"v":{"docs":{},"o":{"docs":{},"，":{"docs":{},"缓":{"docs":{},"存":{"docs":{},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{},"t":{"docs":{},"o":{"docs":{},"n":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"支":{"docs":{},"持":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"而":{"docs":{},"且":{"docs":{},"默":{"docs":{},"认":{"docs":{},"使":{"docs":{},"用":{"docs":{},"o":{"docs":{},"v":{"docs":{},"r":{"docs":{},"方":{"docs":{},"式":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"基":{"docs":{},"尼":{"docs":{},"系":{"docs":{},"数":{"docs":{},"划":{"docs":{},"分":{"docs":{},"数":{"docs":{},"据":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}},"改":{"docs":{},"变":{"docs":{},"维":{"docs":{},"度":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html","tf":0.2}}}}}},"访":{"docs":{},"问":{"docs":{},"数":{"docs":{},"组":{"docs":{"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html":{"ref":"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html","tf":0.2}}}}}},",":{"3":{"docs":{},">":{"0":{"docs":{},",":{"docs":{},"所":{"docs":{},"以":{"docs":{},"判":{"docs":{},"断":{"docs":{},"这":{"docs":{},"个":{"docs":{},"新":{"docs":{},"病":{"docs":{},"人":{"docs":{},"幻":{"docs":{},"的":{"docs":{},"事":{"docs":{},"恶":{"docs":{},"性":{"docs":{},"肿":{"docs":{},"瘤":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.050110537951363304},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.04040404040404041},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.16}},"y":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"）":{"docs":{},",":{"docs":{},"那":{"docs":{},"么":{"docs":{},"我":{"docs":{},"们":{"docs":{},"期":{"docs":{},"望":{"docs":{},"寻":{"docs":{},"找":{"docs":{},"的":{"docs":{},"直":{"docs":{},"线":{"docs":{},"就":{"docs":{},"是":{"docs":{},"y":{"docs":{},"=":{"docs":{},"a":{"docs":{},"x":{"docs":{},"+":{"docs":{},"b":{"docs":{},"，":{"docs":{},"当":{"docs":{},"给":{"docs":{},"出":{"docs":{},"一":{"docs":{},"个":{"docs":{},"新":{"docs":{},"的":{"docs":{},"点":{"docs":{},"x":{"docs":{},"(":{"docs":{},"j":{"docs":{},")":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"希":{"docs":{},"望":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"y":{"docs":{},"^":{"docs":{},"(":{"docs":{},"j":{"docs":{},")":{"docs":{},"=":{"docs":{},"a":{"docs":{},"x":{"docs":{},"(":{"docs":{},"j":{"docs":{},")":{"docs":{},"+":{"docs":{},"b":{"docs":{"线性回归算法/1.线性回归算法简介.html":{"ref":"线性回归算法/1.线性回归算法简介.html","tf":0.125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"d":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}},"=":{"0":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"6":{"6":{"6":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}},"docs":{}},"docs":{}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333},"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.07853403141361257},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.038135593220338986},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.10317460317460317},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.01694915254237288},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.08247422680412371},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0703125},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.05813953488372093},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.13636363636363635},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.07692307692307693},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.08835341365461848},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.08695652173913043},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.016},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.12},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.13986013986013987},"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.07142857142857142},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.05501618122977346},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.07142857142857142},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.14545454545454545},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.06802721088435375},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.09876543209876543},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.06060606060606061},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.11214953271028037},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.096},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.1103202846975089},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.09142857142857143},"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.08333333333333333},"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.07142857142857142},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.06140350877192982},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.04672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.05714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.037142857142857144},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0867579908675799},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.06666666666666667},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.128},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.05194805194805195},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.09352517985611511},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.05},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.024},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.08403361344537816},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.04433497536945813},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.07},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.05188679245283019},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.07954545454545454},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.08035714285714286},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.09623430962343096},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.09836065573770492},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.054455445544554455},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.08823529411764706},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.06756756756756757},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857},"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.038461538461538464},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.03875968992248062},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.03296703296703297},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}},"=":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.01171875},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.022222222222222223}}},"[":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}},">":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}},"优":{"docs":{},"点":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333}}}},"假":{"docs":{},"设":{"docs":{},"现":{"docs":{},"在":{"docs":{},"设":{"docs":{},"计":{"docs":{},"一":{"docs":{},"个":{"docs":{},"程":{"docs":{},"序":{"docs":{},"判":{"docs":{},"断":{"docs":{},"一":{"docs":{},"个":{"docs":{},"新":{"docs":{},"的":{"docs":{},"肿":{"docs":{},"瘤":{"docs":{},"病":{"docs":{},"人":{"docs":{},"是":{"docs":{},"良":{"docs":{},"性":{"docs":{},"肿":{"docs":{},"瘤":{"docs":{},"还":{"docs":{},"是":{"docs":{},"恶":{"docs":{},"性":{"docs":{},"肿":{"docs":{},"瘤":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"一":{"docs":{},"共":{"docs":{},"有":{"4":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"，":{"docs":{},"选":{"docs":{},"取":{"docs":{},"其":{"docs":{},"中":{"docs":{},"的":{"docs":{},"某":{"docs":{},"一":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"（":{"docs":{},"假":{"docs":{},"设":{"docs":{},"红":{"docs":{},"色":{"docs":{},"）":{"docs":{},"（":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"）":{"docs":{},"，":{"docs":{},"而":{"docs":{},"对":{"docs":{},"于":{"docs":{},"剩":{"docs":{},"下":{"docs":{},"的":{"docs":{},"类":{"docs":{},"别":{"docs":{},"，":{"docs":{},"把":{"docs":{},"他":{"docs":{},"们":{"docs":{},"融":{"docs":{},"合":{"docs":{},"在":{"docs":{},"一":{"docs":{},"起":{"docs":{},"，":{"docs":{},"把":{"docs":{},"他":{"docs":{},"称":{"docs":{},"之":{"docs":{},"为":{"docs":{},"其":{"docs":{},"他":{"docs":{},"的":{"docs":{},"类":{"docs":{},"别":{"docs":{},"（":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"）":{"docs":{},"，":{"docs":{},"这":{"docs":{},"样":{"docs":{},"就":{"docs":{},"把":{"docs":{},"一":{"docs":{},"个":{"docs":{},"四":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"转":{"docs":{},"换":{"docs":{},"成":{"docs":{},"了":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"转":{"docs":{},"换":{"docs":{},"成":{"docs":{},"了":{"docs":{},"使":{"docs":{},"红":{"docs":{},"色":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"是":{"docs":{},"多":{"docs":{},"少":{"docs":{},"，":{"docs":{},"是":{"docs":{},"非":{"docs":{},"红":{"docs":{},"色":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"是":{"docs":{},"多":{"docs":{},"少":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}},"先":{"docs":{},"基":{"docs":{},"于":{"docs":{},"原":{"docs":{},"有":{"docs":{},"的":{"docs":{},"肿":{"docs":{},"瘤":{"docs":{},"病":{"docs":{},"人":{"docs":{},"的":{"docs":{},"发":{"docs":{},"现":{"docs":{},"时":{"docs":{},"间":{"docs":{},"和":{"docs":{},"肿":{"docs":{},"瘤":{"docs":{},"大":{"docs":{},"小":{"docs":{},"（":{"docs":{},"特":{"docs":{},"征":{"docs":{},"）":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"良":{"docs":{},"性":{"docs":{},"/":{"docs":{},"恶":{"docs":{},"性":{"docs":{},"（":{"docs":{},"值":{"docs":{},"）":{"docs":{},"建":{"docs":{},"立":{"docs":{},"了":{"docs":{},"一":{"docs":{},"张":{"docs":{},"散":{"docs":{},"点":{"docs":{},"图":{"docs":{},"，":{"docs":{},"横":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"是":{"docs":{},"肿":{"docs":{},"瘤":{"docs":{},"大":{"docs":{},"小":{"docs":{},"，":{"docs":{},"纵":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"是":{"docs":{},"发":{"docs":{},"现":{"docs":{},"时":{"docs":{},"间":{"docs":{},"，":{"docs":{},"红":{"docs":{},"色":{"docs":{},"代":{"docs":{},"表":{"docs":{},"良":{"docs":{},"性":{"docs":{},"，":{"docs":{},"蓝":{"docs":{},"色":{"docs":{},"代":{"docs":{},"表":{"docs":{},"恶":{"docs":{},"性":{"docs":{},"，":{"docs":{},"现":{"docs":{},"在":{"docs":{},"要":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"病":{"docs":{},"人":{"docs":{},"的":{"docs":{},"颜":{"docs":{},"色":{"docs":{},"为":{"docs":{},"绿":{"docs":{},"色":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{},"一":{"docs":{},"个":{"docs":{},"默":{"docs":{},"认":{"docs":{},"的":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"e":{"docs":{},"r":{"docs":{},"对":{"docs":{},"象":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}}}}}}},"原":{"docs":{},"理":{"docs":{},"案":{"docs":{},"例":{"docs":{},"介":{"docs":{},"绍":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333}}}}}}},"始":{"docs":{},"集":{"docs":{},"合":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"来":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"都":{"docs":{},"在":{"docs":{},"x":{"docs":{},"中":{"docs":{},"，":{"docs":{},"现":{"docs":{},"在":{"docs":{},"对":{"docs":{},"x":{"docs":{},"中":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"都":{"docs":{},"进":{"docs":{},"行":{"docs":{},"平":{"docs":{},"方":{"docs":{},"，":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"如":{"docs":{},"果":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"在":{"docs":{},"特":{"docs":{},"征":{"docs":{},"空":{"docs":{},"间":{"docs":{},"中":{"docs":{},"的":{"docs":{},"k":{"docs":{},"个":{"docs":{},"最":{"docs":{},"相":{"docs":{},"似":{"docs":{},"(":{"docs":{},"即":{"docs":{},"特":{"docs":{},"征":{"docs":{},"空":{"docs":{},"间":{"docs":{},"中":{"docs":{},"最":{"docs":{},"邻":{"docs":{},"近":{"docs":{},")":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"中":{"docs":{},"的":{"docs":{},"大":{"docs":{},"多":{"docs":{},"数":{"docs":{},"属":{"docs":{},"于":{"docs":{},"某":{"docs":{},"一":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"，":{"docs":{},"则":{"docs":{},"该":{"docs":{},"样":{"docs":{},"本":{"docs":{},"也":{"docs":{},"属":{"docs":{},"于":{"docs":{},"这":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"。":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"k":{"docs":{},"=":{"1":{"0":{"docs":{},"，":{"docs":{},"则":{"docs":{},"有":{"docs":{},"必":{"docs":{},"要":{"docs":{},"对":{"1":{"0":{"docs":{},"以":{"docs":{},"上":{"docs":{},"的":{"docs":{},"数":{"docs":{},"字":{"docs":{},"进":{"docs":{},"行":{"docs":{},"搜":{"docs":{},"索":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}},"docs":{}},"docs":{}}},"样":{"docs":{},"本":{"docs":{},"数":{"docs":{},"非":{"docs":{},"常":{"docs":{},"多":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"即":{"docs":{},"使":{"docs":{},"使":{"docs":{},"用":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"也":{"docs":{},"会":{"docs":{},"导":{"docs":{},"致":{"docs":{},"速":{"docs":{},"度":{"docs":{},"比":{"docs":{},"较":{"docs":{},"慢":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"在":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"中":{"docs":{},"，":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"都":{"docs":{},"要":{"docs":{},"参":{"docs":{},"与":{"docs":{},"运":{"docs":{},"算":{"docs":{},"。":{"docs":{},"这":{"docs":{},"时":{"docs":{},"候":{"docs":{},"需":{"docs":{},"要":{"docs":{},"采":{"docs":{},"用":{"docs":{},"随":{"docs":{},"机":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"将":{"docs":{},"在":{"docs":{},"下":{"docs":{},"一":{"docs":{},"小":{"docs":{},"节":{"docs":{},"进":{"docs":{},"行":{"docs":{},"介":{"docs":{},"绍":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"将":{"docs":{},"w":{"docs":{},"中":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"都":{"docs":{},"看":{"docs":{},"作":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"我":{"docs":{},"们":{"docs":{},"也":{"docs":{},"可":{"docs":{},"以":{"docs":{},"说":{"docs":{},"，":{"docs":{},"第":{"docs":{},"一":{"docs":{},"行":{"docs":{},"所":{"docs":{},"代":{"docs":{},"表":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"是":{"docs":{},"最":{"docs":{},"重":{"docs":{},"要":{"docs":{},"的":{"docs":{},"那":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"，":{"docs":{},"最":{"docs":{},"能":{"docs":{},"反":{"docs":{},"应":{"docs":{},"x":{"docs":{},"这":{"docs":{},"个":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"那":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"x":{"2":{"docs":{},"理":{"docs":{},"解":{"docs":{},"为":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"，":{"docs":{},"将":{"docs":{},"x":{"docs":{},"理":{"docs":{},"解":{"docs":{},"为":{"docs":{},"另":{"docs":{},"外":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},",":{"docs":{},"换":{"docs":{},"句":{"docs":{},"话":{"docs":{},"说":{"docs":{},"，":{"docs":{},"本":{"docs":{},"来":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"只":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"x":{"docs":{},"，":{"docs":{},"现":{"docs":{},"在":{"docs":{},"我":{"docs":{},"们":{"docs":{},"把":{"docs":{},"他":{"docs":{},"看":{"docs":{},"成":{"docs":{},"有":{"docs":{},"两":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"。":{"docs":{},"多":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"x":{"2":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"从":{"docs":{},"这":{"docs":{},"个":{"docs":{},"角":{"docs":{},"度":{"docs":{},"来":{"docs":{},"看":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"式":{"docs":{},"子":{"docs":{},"依":{"docs":{},"旧":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"式":{"docs":{},"子":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"从":{"docs":{},"x":{"docs":{},"的":{"docs":{},"角":{"docs":{},"度":{"docs":{},"来":{"docs":{},"看":{"docs":{},"，":{"docs":{},"他":{"docs":{},"就":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"二":{"docs":{},"次":{"docs":{},"的":{"docs":{},"方":{"docs":{},"程":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"面":{"docs":{},"对":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"结":{"docs":{},"果":{"docs":{},"很":{"docs":{},"差":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"他":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"就":{"docs":{},"很":{"docs":{},"弱":{"docs":{},"。":{"docs":{},"事":{"docs":{},"实":{"docs":{},"上":{"docs":{},"，":{"docs":{},"这":{"docs":{},"是":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"更":{"docs":{},"大":{"docs":{},"的":{"docs":{},"意":{"docs":{},"义":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"数":{"docs":{},"据":{"docs":{},"输":{"docs":{},"出":{"docs":{},"的":{"docs":{},"是":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"那":{"docs":{},"就":{"docs":{},"是":{"docs":{},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{},"所":{"docs":{},"解":{"docs":{},"决":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"一":{"docs":{},"个":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"来":{"docs":{},"临":{"docs":{},"之":{"docs":{},"后":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"到":{"docs":{},"达":{"docs":{},"了":{"docs":{},"某":{"docs":{},"一":{"docs":{},"个":{"docs":{},"叶":{"docs":{},"子":{"docs":{},"节":{"docs":{},"点":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"将":{"docs":{},"这":{"docs":{},"些":{"docs":{},"叶":{"docs":{},"子":{"docs":{},"节":{"docs":{},"点":{"docs":{},"包":{"docs":{},"含":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"平":{"docs":{},"均":{"docs":{},"值":{"docs":{},"作":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"值":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"类":{"docs":{},"别":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"让":{"docs":{},"这":{"docs":{},"些":{"docs":{},"叶":{"docs":{},"子":{"docs":{},"结":{"docs":{},"点":{"docs":{},"包":{"docs":{},"含":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"投":{"docs":{},"票":{"docs":{},"，":{"docs":{},"票":{"docs":{},"数":{"docs":{},"最":{"docs":{},"多":{"docs":{},"的":{"docs":{},"即":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"输":{"docs":{},"出":{"docs":{},"的":{"docs":{},"分":{"docs":{},"类":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"真":{"docs":{},"的":{"docs":{},"进":{"docs":{},"行":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"算":{"docs":{},"法":{"docs":{},"进":{"docs":{},"行":{"docs":{},"训":{"docs":{},"练":{"docs":{},"，":{"docs":{},"而":{"docs":{},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{},"是":{"9":{"9":{"docs":{},".":{"9":{"docs":{},"%":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"说":{"docs":{},"明":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"算":{"docs":{},"法":{"docs":{},"是":{"docs":{},"失":{"docs":{},"败":{"docs":{},"的":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"他":{"docs":{},"比":{"docs":{},"纯":{"docs":{},"粹":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"一":{"docs":{},"个":{"docs":{},"人":{"docs":{},"是":{"docs":{},"监":{"docs":{},"控":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"更":{"docs":{},"低":{"docs":{},"。":{"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}},"设":{"docs":{},"置":{"docs":{},"任":{"docs":{},"意":{"docs":{},"一":{"docs":{},"个":{"docs":{},"常":{"docs":{},"量":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}},"想":{"docs":{},"让":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"变":{"docs":{},"高":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"其":{"docs":{},"实":{"docs":{},"就":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"只":{"docs":{},"能":{"docs":{},"讲":{"docs":{},"那":{"docs":{},"些":{"docs":{},"特":{"docs":{},"别":{"docs":{},"有":{"docs":{},"把":{"docs":{},"握":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"分":{"docs":{},"类":{"docs":{},"为":{"1":{"docs":{},"。":{"docs":{},"在":{"docs":{},"这":{"docs":{},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"我":{"docs":{},"们":{"docs":{},"做":{"docs":{},"的":{"docs":{},"事":{"docs":{},"情":{"docs":{},"是":{"docs":{},"让":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"做":{"docs":{},"的":{"docs":{},"事":{"docs":{},"让":{"docs":{},"其":{"docs":{},"判":{"docs":{},"断":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"是":{"docs":{},"百":{"docs":{},"分":{"docs":{},"之":{"9":{"0":{"docs":{},"甚":{"docs":{},"至":{"docs":{},"百":{"docs":{},"分":{"docs":{},"之":{"9":{"9":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"才":{"docs":{},"预":{"docs":{},"测":{"docs":{},"他":{"docs":{},"为":{"1":{"docs":{},"，":{"docs":{},"在":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"，":{"docs":{},"很":{"docs":{},"显":{"docs":{},"然":{"docs":{},"有":{"docs":{},"很":{"docs":{},"多":{"docs":{},"真":{"docs":{},"实":{"docs":{},"为":{"1":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"就":{"docs":{},"被":{"docs":{},"排":{"docs":{},"除":{"docs":{},"在":{"docs":{},"了":{"docs":{},"外":{"docs":{},"面":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"就":{"docs":{},"会":{"docs":{},"变":{"docs":{},"低":{"docs":{},"。":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"要":{"docs":{},"让":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"升":{"docs":{},"高":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"要":{"docs":{},"降":{"docs":{},"低":{"docs":{},"算":{"docs":{},"法":{"docs":{},"判":{"docs":{},"断":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"的":{"docs":{},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"，":{"docs":{},"这":{"docs":{},"时":{"docs":{},"却":{"docs":{},"是":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"提":{"docs":{},"高":{"docs":{},"了":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"下":{"docs":{},"降":{"docs":{},"了":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"c":{"docs":{},"=":{"1":{"docs":{},"，":{"docs":{},"代":{"docs":{},"表":{"docs":{},"两":{"docs":{},"部":{"docs":{},"分":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"程":{"docs":{},"度":{"docs":{},"是":{"docs":{},"一":{"docs":{},"样":{"docs":{},"的":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"c":{"docs":{},"在":{"0":{"docs":{"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":0.0625}}},"docs":{}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"是":{"docs":{},"在":{"docs":{},"逼":{"docs":{},"迫":{"docs":{},"着":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"℥":{"docs":{},"为":{"0":{"docs":{},"，":{"docs":{},"此":{"docs":{},"时":{"docs":{},"意":{"docs":{},"味":{"docs":{},"着":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"容":{"docs":{},"错":{"docs":{},"空":{"docs":{},"间":{"docs":{},"更":{"docs":{},"小":{"docs":{},"，":{"docs":{},"s":{"docs":{},"o":{"docs":{},"f":{"docs":{},"t":{"docs":{"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":0.0625}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"能":{"docs":{},"找":{"docs":{},"到":{"docs":{},"一":{"docs":{},"个":{"docs":{},"函":{"docs":{},"数":{"docs":{},"k":{"docs":{},"，":{"docs":{},"将":{"docs":{},"x":{"docs":{},"i":{"docs":{},"和":{"docs":{},"x":{"docs":{},"j":{"docs":{},"分":{"docs":{},"别":{"docs":{},"作":{"docs":{},"为":{"docs":{},"参":{"docs":{},"数":{"docs":{},"传":{"docs":{},"入":{"docs":{},"，":{"docs":{},"返":{"docs":{},"回":{"docs":{},"的":{"docs":{},"就":{"docs":{},"是":{"docs":{},"x":{"docs":{},"`":{"docs":{},"i":{"docs":{},"和":{"docs":{},"x":{"docs":{},"`":{"docs":{},"j":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"不":{"docs":{},"需":{"docs":{},"要":{"docs":{},"分":{"docs":{},"别":{"docs":{},"运":{"docs":{},"算":{"docs":{},"再":{"docs":{},"相":{"docs":{},"乘":{"docs":{},"。":{"docs":{},"并":{"docs":{},"且":{"docs":{},"也":{"docs":{},"节":{"docs":{},"省":{"docs":{},"了":{"docs":{},"空":{"docs":{},"间":{"docs":{},"（":{"docs":{},"因":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"不":{"docs":{},"需":{"docs":{},"要":{"docs":{},"将":{"docs":{},"低":{"docs":{},"维":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"变":{"docs":{},"形":{"docs":{},"为":{"docs":{},"高":{"docs":{},"维":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"存":{"docs":{},"储":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"利":{"docs":{},"用":{"docs":{},"k":{"docs":{},"函":{"docs":{},"数":{"docs":{},"直":{"docs":{},"接":{"docs":{},"计":{"docs":{},"算":{"docs":{},"出":{"docs":{},"结":{"docs":{},"果":{"docs":{},"）":{"docs":{},"。":{"docs":{},"这":{"docs":{},"个":{"docs":{},"k":{"docs":{},"就":{"docs":{},"是":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"（":{"docs":{},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"e":{"docs":{},"l":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"每":{"docs":{},"个":{"docs":{},"子":{"docs":{},"模":{"docs":{},"型":{"docs":{},"只":{"docs":{},"有":{"5":{"1":{"docs":{},"%":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}},"docs":{}},"docs":{}}},"有":{"6":{"0":{"docs":{},"%":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}},"docs":{}},"docs":{}}}}}}}},"何":{"docs":{},"定":{"docs":{},"义":{"docs":{},"样":{"docs":{},"本":{"docs":{},"间":{"docs":{},"间":{"docs":{},"距":{"docs":{},"?":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}},"将":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"x":{"docs":{},"从":{"docs":{},"n":{"docs":{},"维":{"docs":{},"转":{"docs":{},"化":{"docs":{},"成":{"docs":{},"k":{"docs":{},"维":{"docs":{},"呢":{"docs":{},"，":{"docs":{},"回":{"docs":{},"忆":{"docs":{},"们":{"docs":{},"之":{"docs":{},"前":{"docs":{},"学":{"docs":{},"到":{"docs":{},"的":{"docs":{},"，":{"docs":{},"对":{"docs":{},"于":{"docs":{},"一":{"docs":{},"个":{"docs":{},"x":{"docs":{},"样":{"docs":{},"本":{"docs":{},"，":{"docs":{},"与":{"docs":{},"一":{"docs":{},"个":{"docs":{},"w":{"docs":{},"进":{"docs":{},"行":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"，":{"docs":{},"其":{"docs":{},"实":{"docs":{},"就":{"docs":{},"是":{"docs":{},"讲":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"了":{"docs":{},"w":{"docs":{},"这":{"docs":{},"个":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"轴":{"docs":{},"，":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"模":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"讲":{"docs":{},"这":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"和":{"docs":{},"这":{"docs":{},"k":{"docs":{},"个":{"docs":{},"w":{"docs":{},"分":{"docs":{},"别":{"docs":{},"做":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"，":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"就":{"docs":{},"是":{"docs":{},"这":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"，":{"docs":{},"在":{"docs":{},"这":{"docs":{},"k":{"docs":{},"个":{"docs":{},"方":{"docs":{},"向":{"docs":{},"上":{"docs":{},"做":{"docs":{},"映":{"docs":{},"射":{"docs":{},"后":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"方":{"docs":{},"向":{"docs":{},"上":{"docs":{},"的":{"docs":{},"大":{"docs":{},"小":{"docs":{},"，":{"docs":{},"这":{"docs":{},"k":{"docs":{},"个":{"docs":{},"元":{"docs":{},"素":{"docs":{},"合":{"docs":{},"在":{"docs":{},"一":{"docs":{},"起":{"docs":{},"，":{"docs":{},"就":{"docs":{},"代":{"docs":{},"表":{"docs":{},"这":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"新":{"docs":{},"的":{"docs":{},"k":{"docs":{},"个":{"docs":{},"轴":{"docs":{},"所":{"docs":{},"代":{"docs":{},"表":{"docs":{},"的":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"上":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"这":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"大":{"docs":{},"小":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"创":{"docs":{},"建":{"docs":{},"差":{"docs":{},"异":{"docs":{},"性":{"docs":{},"？":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"中":{"docs":{},"的":{"docs":{},"k":{"docs":{},"，":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"中":{"docs":{},"使":{"docs":{},"用":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}}}}}}}}}}},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"概":{"docs":{},"率":{"docs":{},"值":{"docs":{},"为":{"0":{"docs":{},".":{"0":{"2":{"9":{"4":{"5":{"7":{"3":{"2":{"docs":{},"，":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"为":{"0":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}},"docs":{}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}},"然":{"docs":{},"后":{"docs":{},"用":{"docs":{},"第":{"docs":{},"一":{"docs":{},"步":{"docs":{},"中":{"docs":{},"取":{"docs":{},"到":{"docs":{},"的":{"docs":{},"三":{"docs":{},"个":{"docs":{},"点":{"docs":{},"进":{"docs":{},"行":{"docs":{},"投":{"docs":{},"票":{"docs":{},"，":{"docs":{},"比":{"docs":{},"如":{"docs":{},"本":{"docs":{},"例":{"docs":{},"中":{"docs":{},"投":{"docs":{},"票":{"docs":{},"结":{"docs":{},"果":{"docs":{},"就":{"docs":{},"是":{"docs":{},"蓝":{"docs":{},"：":{"docs":{},"红":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"缺":{"docs":{},"点":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333}}}},"首":{"docs":{},"先":{"docs":{},"需":{"docs":{},"要":{"docs":{},"取":{"docs":{},"一":{"docs":{},"个":{"docs":{},"k":{"docs":{},"值":{"docs":{},"（":{"docs":{},"这":{"docs":{},"个":{"docs":{},"k":{"docs":{},"值":{"docs":{},"的":{"docs":{},"取":{"docs":{},"法":{"docs":{},"后":{"docs":{},"面":{"docs":{},"会":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"）":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"找":{"docs":{},"到":{"docs":{},"距":{"docs":{},"离":{"docs":{},"要":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"病":{"docs":{},"人":{"docs":{},"的":{"docs":{},"点":{"docs":{},"（":{"docs":{},"绿":{"docs":{},"点":{"docs":{},"）":{"docs":{},"距":{"docs":{},"离":{"docs":{},"最":{"docs":{},"近":{"docs":{},"的":{"docs":{},"k":{"docs":{},"个":{"docs":{},"点":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"以":{"docs":{},"二":{"docs":{},"维":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"平":{"docs":{},"面":{"docs":{},"为":{"docs":{},"例":{"docs":{},"，":{"docs":{},"一":{"docs":{},"个":{"docs":{},"点":{"docs":{},"（":{"docs":{},"o":{"docs":{},"）":{"docs":{},"的":{"docs":{},"导":{"docs":{},"数":{"docs":{},"就":{"docs":{},"是":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"在":{"docs":{},"这":{"docs":{},"个":{"docs":{},"点":{"docs":{},"的":{"docs":{},"切":{"docs":{},"线":{"docs":{},"的":{"docs":{},"斜":{"docs":{},"率":{"docs":{},"，":{"docs":{},"在":{"docs":{},"这":{"docs":{},"个":{"docs":{},"点":{"docs":{},"两":{"docs":{},"侧":{"docs":{},"各":{"docs":{},"取":{"docs":{},"一":{"docs":{},"个":{"docs":{},"点":{"docs":{},"（":{"docs":{},"a":{"docs":{},"b":{"docs":{},"）":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"a":{"docs":{},"b":{"docs":{},"两":{"docs":{},"点":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"直":{"docs":{},"线":{"docs":{},"的":{"docs":{},"斜":{"docs":{},"率":{"docs":{},"应":{"docs":{},"该":{"docs":{},"大":{"docs":{},"体":{"docs":{},"等":{"docs":{},"于":{"docs":{},"o":{"docs":{},"的":{"docs":{},"切":{"docs":{},"线":{"docs":{},"的":{"docs":{},"斜":{"docs":{},"率":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"这":{"docs":{},"a":{"docs":{},"和":{"docs":{},"b":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"越":{"docs":{},"近":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"两":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{},"的":{"docs":{},"斜":{"docs":{},"率":{"docs":{},"就":{"docs":{},"越":{"docs":{},"接":{"docs":{},"近":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"整":{"docs":{},"体":{"docs":{},"从":{"docs":{},"趋":{"docs":{},"势":{"docs":{},"上":{"docs":{},"，":{"docs":{},"和":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"学":{"docs":{},"习":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"是":{"docs":{},"类":{"docs":{},"似":{"docs":{},"的":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}},"讨":{"docs":{},"论":{"docs":{},"二":{"docs":{},"次":{"docs":{},"方":{"docs":{},"的":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"项":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":0.08333333333333333}}}}}}}}}}}}}}},"对":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"进":{"docs":{},"行":{"docs":{},"一":{"docs":{},"下":{"docs":{},"改":{"docs":{},"变":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"把":{"docs":{},"y":{"docs":{},"的":{"docs":{},"值":{"docs":{},"不":{"docs":{},"取":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"，":{"docs":{},"而":{"docs":{},"取":{"docs":{},"固":{"docs":{},"定":{"docs":{},"的":{"docs":{},"的":{"docs":{},"点":{"docs":{},"，":{"docs":{},"取":{"docs":{},"两":{"docs":{},"个":{"docs":{},"固":{"docs":{},"定":{"docs":{},"的":{"docs":{},"点":{"docs":{},"分":{"docs":{},"别":{"docs":{},"叫":{"docs":{},"l":{"1":{"docs":{},"，":{"docs":{},"l":{"2":{"docs":{},"（":{"docs":{},"l":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"k":{"docs":{},"）":{"docs":{},"。":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"做":{"docs":{},"的":{"docs":{},"升":{"docs":{},"维":{"docs":{},"过":{"docs":{},"程":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"对":{"docs":{},"于":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"x":{"docs":{},"的":{"docs":{},"值":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"他":{"docs":{},"有":{"docs":{},"两":{"docs":{},"个":{"docs":{},"地":{"docs":{},"标":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"就":{"docs":{},"把":{"docs":{},"他":{"docs":{},"们":{"docs":{},"升":{"docs":{},"维":{"docs":{},"成":{"docs":{},"一":{"docs":{},"个":{"docs":{},"二":{"docs":{},"维":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"\"":{"docs":{},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}},"\"":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.015151515151515152},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.024096385542168676},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.048},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.008771929824561403},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.005714285714285714},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.011204481792717087},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.017857142857142856},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.009900990099009901}},"初":{"docs":{},"始":{"docs":{},"化":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"分":{"docs":{},"类":{"docs":{},"器":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}},"s":{"docs":{},"i":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}},"o":{"docs":{},"g":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}},"根":{"docs":{},"据":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"和":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"训":{"docs":{},"练":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"分":{"docs":{},"类":{"docs":{},"器":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}}}}}}}},"集":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"，":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}},"x":{"docs":{},"获":{"docs":{},"得":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"均":{"docs":{},"值":{"docs":{},"和":{"docs":{},"方":{"docs":{},"差":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}}}}}}}}}},"将":{"docs":{},"数":{"docs":{},"据":{"docs":{},"x":{"docs":{},"和":{"docs":{},"y":{"docs":{},"按":{"docs":{},"照":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"r":{"docs":{},"a":{"docs":{},"d":{"docs":{},"i":{"docs":{},"o":{"docs":{},"分":{"docs":{},"割":{"docs":{},"成":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"x":{"docs":{},"根":{"docs":{},"据":{"docs":{},"这":{"docs":{},"个":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{},"r":{"docs":{},"进":{"docs":{},"行":{"docs":{},"均":{"docs":{},"值":{"docs":{},"方":{"docs":{},"差":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{},"处":{"docs":{},"理":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"给":{"docs":{},"定":{"docs":{},"单":{"docs":{},"个":{"docs":{},"待":{"docs":{},"预":{"docs":{},"测":{"docs":{},"数":{"docs":{},"据":{"docs":{},"x":{"docs":{},"_":{"docs":{},"s":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"l":{"docs":{},"e":{"docs":{},"，":{"docs":{},"返":{"docs":{},"回":{"docs":{},"x":{"docs":{},"_":{"docs":{},"s":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"l":{"docs":{},"e":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"结":{"docs":{},"果":{"docs":{},"值":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"待":{"docs":{},"预":{"docs":{},"测":{"docs":{},"集":{"docs":{},"x":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"，":{"docs":{},"返":{"docs":{},"回":{"docs":{},"x":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"结":{"docs":{},"果":{"docs":{},"值":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"x":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"，":{"docs":{},"返":{"docs":{},"回":{"docs":{},"表":{"docs":{},"示":{"docs":{},"x":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"向":{"docs":{},"量":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"计":{"docs":{},"算":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},"和":{"docs":{},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}}}}}},"r":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353}}}}}}}}}}}}}}}}}}}}}}}}}}}},"单":{"docs":{},"位":{"docs":{},"向":{"docs":{},"量":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}},"的":{"docs":{},"导":{"docs":{},"数":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}}},"获":{"docs":{},"得":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"x":{"docs":{},"的":{"docs":{},"前":{"docs":{},"n":{"docs":{},"个":{"docs":{},"元":{"docs":{},"素":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}},"k":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}},"m":{"docs":{},"u":{"docs":{},"s":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}},"s":{"docs":{},"i":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"1":{"docs":{},"(":{"docs":{},")":{"docs":{},"\"":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},")":{"docs":{},"\"":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"_":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"\"":{"docs":{},":":{"docs":{},"[":{"docs":{},"i":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}},"p":{"docs":{},"\"":{"docs":{},":":{"docs":{},"[":{"docs":{},"i":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}},"w":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"s":{"docs":{},"\"":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093}},":":{"docs":{},"[":{"docs":{},"'":{"docs":{},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"'":{"docs":{},"]":{"docs":{},",":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}},",":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"k":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}}},"p":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}}}}}}}}}}}}}},"*":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.02197802197802198},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.024096385542168676},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.016},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0196078431372549},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.05714285714285714}},"*":{"2":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}},"k":{"docs":{},"w":{"docs":{},"d":{"docs":{},"s":{"docs":{},")":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}},"w":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}},">":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}},"=":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"正":{"docs":{},"合":{"docs":{},"适":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"）":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}},"o":{"docs":{},"(":{"docs":{},"n":{"docs":{},")":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}},"[":{"0":{"docs":{},",":{"0":{"docs":{},",":{"0":{"docs":{},",":{"0":{"docs":{},",":{"0":{"docs":{},",":{"1":{"docs":{},",":{"1":{"docs":{},",":{"1":{"docs":{},",":{"1":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},".":{"0":{"0":{"6":{"7":{"5":{"6":{"7":{"6":{"docs":{},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.008}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"6":{"0":{"6":{"0":{"6":{"0":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"1":{"5":{"1":{"5":{"1":{"5":{"1":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"5":{"2":{"5":{"2":{"5":{"2":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"2":{"7":{"2":{"7":{"2":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"3":{"3":{"3":{"3":{"3":{"3":{"3":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"9":{"3":{"9":{"3":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"4":{"9":{"4":{"9":{"4":{"9":{"4":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"3":{"7":{"3":{"7":{"3":{"7":{"4":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"8":{"9":{"8":{"9":{"8":{"9":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.016}}},"]":{"docs":{},"*":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"[":{"docs":{},"y":{"docs":{},"=":{"docs":{},"=":{"0":{"docs":{},"]":{"docs":{},")":{"docs":{},")":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}},"1":{"docs":{},"]":{"docs":{},")":{"docs":{},")":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}},"docs":{}}}}}}}}}}}}},"1":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.00872093023255814},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}},".":{"3":{"4":{"3":{"8":{"0":{"8":{"8":{"3":{"1":{"docs":{},",":{"3":{"docs":{},".":{"3":{"6":{"8":{"3":{"6":{"0":{"9":{"5":{"4":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"2":{"5":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{},".":{"1":{"1":{"0":{"0":{"7":{"3":{"4":{"8":{"3":{"docs":{},",":{"1":{"docs":{},".":{"7":{"8":{"1":{"5":{"3":{"9":{"6":{"3":{"8":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"2":{"8":{"0":{"3":{"6":{"2":{"4":{"3":{"9":{"docs":{},",":{"2":{"docs":{},".":{"8":{"6":{"6":{"9":{"9":{"0":{"2":{"6":{"3":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},",":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}},"3":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{},".":{"5":{"8":{"2":{"2":{"9":{"4":{"0":{"4":{"2":{"docs":{},",":{"4":{"docs":{},".":{"6":{"7":{"9":{"1":{"7":{"9":{"1":{"1":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"4":{"9":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{},".":{"8":{"1":{"2":{"5":{"6":{"6":{"9":{"0":{"7":{"6":{"0":{"9":{"8":{"7":{"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"5":{"docs":{},".":{"7":{"4":{"5":{"0":{"5":{"1":{"9":{"9":{"7":{"docs":{},",":{"3":{"docs":{},".":{"5":{"3":{"3":{"9":{"8":{"9":{"8":{"0":{"3":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"7":{"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{},".":{"4":{"2":{"3":{"4":{"3":{"6":{"9":{"4":{"2":{"docs":{},",":{"4":{"docs":{},".":{"6":{"9":{"6":{"5":{"2":{"2":{"8":{"7":{"5":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"9":{"2":{"7":{"8":{"3":{"4":{"8":{"1":{"docs":{},",":{"3":{"docs":{},".":{"4":{"2":{"4":{"0":{"8":{"8":{"9":{"4":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"3":{"9":{"8":{"2":{"0":{"8":{"1":{"7":{"docs":{},",":{"0":{"docs":{},".":{"7":{"9":{"1":{"6":{"3":{"7":{"2":{"3":{"1":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"9":{"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},".":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"docs":{},".":{"1":{"7":{"2":{"1":{"6":{"8":{"6":{"2":{"2":{"docs":{},",":{"2":{"docs":{},".":{"5":{"1":{"1":{"1":{"0":{"1":{"0":{"4":{"5":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.1105379513633014},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.06060606060606061},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.024}},"(":{"1":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}},"docs":{},"x":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"[":{"3":{"docs":{},".":{"3":{"9":{"3":{"5":{"3":{"3":{"2":{"1":{"1":{"docs":{},",":{"2":{"docs":{},".":{"3":{"3":{"1":{"2":{"7":{"3":{"3":{"8":{"1":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.045454545454545456},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.024691358024691357},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.010101010101010102},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.014018691588785047},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.016},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.02877697841726619},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.05}}},"s":{"docs":{},"q":{"docs":{},"r":{"docs":{},"t":{"docs":{},"(":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{},"(":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}}}}}}}}},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"[":{"docs":{},"i":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}},"\"":{"docs":{},"u":{"docs":{},"n":{"docs":{},"i":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"\"":{"docs":{},",":{"docs":{},"\"":{"docs":{},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"\"":{"docs":{},"]":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}},"]":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"\"":{"docs":{},"]":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}},"'":{"docs":{},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"'":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}},"u":{"docs":{},"n":{"docs":{},"i":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"'":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}},"i":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.030927835051546393}}},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{},"(":{"docs":{},"[":{"0":{"docs":{},".":{"7":{"5":{"9":{"3":{"4":{"0":{"7":{"7":{"docs":{},",":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"9":{"5":{"9":{"5":{"9":{"5":{"9":{"6":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}}}}}}},"p":{"docs":{},"a":{"docs":{},"r":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"e":{"docs":{},"l":{"docs":{},"(":{"docs":{},"n":{"docs":{},"_":{"docs":{},"j":{"docs":{},"o":{"docs":{},"b":{"docs":{},"s":{"docs":{},"=":{"1":{"docs":{},")":{"docs":{},"]":{"docs":{},":":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}},"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{},"(":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},".":{"docs":{},"e":{"docs":{},"x":{"docs":{},"p":{"docs":{},"l":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"v":{"docs":{},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"_":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"_":{"docs":{},"[":{"docs":{},":":{"docs":{},"i":{"docs":{},"+":{"1":{"docs":{},"]":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"f":{"docs":{},"n":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}}},"t":{"docs":{},"n":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556}}}}}}}}}}}}},"\\":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.014738393515106854},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.008},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.011428571428571429},"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.008571428571428572},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}},"]":{"docs":{},")":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}},"_":{"docs":{},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"_":{"docs":{},"_":{"docs":{},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{},"k":{"docs":{},")":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}},")":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"p":{"docs":{},"r":{"docs":{},"_":{"docs":{},"_":{"docs":{},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},")":{"docs":{},":":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"m":{"docs":{},"o":{"docs":{},"i":{"docs":{},"d":{"docs":{},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},",":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}}},"e":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541}},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"l":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"r":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"=":{"docs":{},"'":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"s":{"docs":{},"e":{"docs":{},"'":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}},".":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}},"_":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"k":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"a":{"docs":{},"l":{"docs":{},"g":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"m":{"docs":{},"=":{"docs":{},"'":{"docs":{},"a":{"docs":{},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{},"'":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"(":{"docs":{},"a":{"docs":{},"l":{"docs":{},"g":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"m":{"docs":{},"=":{"docs":{},"'":{"docs":{},"a":{"docs":{},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{},"'":{"docs":{},",":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},":":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}}}},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}}}}},"l":{"docs":{},"a":{"docs":{},"p":{"docs":{},"s":{"docs":{},"e":{"docs":{},"d":{"docs":{},":":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}},"s":{"docs":{},"t":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}},"p":{"docs":{},"s":{"docs":{},"i":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016}},":":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},"=":{"0":{"docs":{},".":{"1":{"docs":{},",":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}},"docs":{}}},"1":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},"docs":{}},")":{"docs":{},")":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}},"t":{"docs":{},"a":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.045454545454545456},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}},":":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"η":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},"=":{"0":{"docs":{},".":{"0":{"1":{"docs":{},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}},"docs":{}},"docs":{}}},"docs":{}},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}}}},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},",":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}},"o":{"docs":{},"o":{"docs":{},"b":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"_":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}}}}}}}}}}},"x":{"docs":{},"c":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},":":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}}}}}}},"p":{"docs":{},"l":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"d":{"docs":{},"v":{"docs":{},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"_":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}},"a":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"_":{"docs":{},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.01818181818181818}},"s":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}},"f":{"docs":{},"a":{"docs":{},"c":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}},"e":{"docs":{},"s":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}},"t":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"s":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"t":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"p":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}}}}}}}}}},"n":{"docs":{},"_":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"=":{"5":{"0":{"0":{"docs":{},",":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"u":{"docs":{},"m":{"docs":{},"e":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"(":{"docs":{},"a":{"docs":{},"x":{"docs":{},"e":{"docs":{},"s":{"docs":{},".":{"docs":{},"f":{"docs":{},"l":{"docs":{},"a":{"docs":{},"t":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}},"x":{"docs":{},")":{"docs":{},":":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}},"s":{"docs":{},"u":{"docs":{},"r":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}},"t":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{},"y":{"docs":{},"(":{"docs":{},"p":{"docs":{},")":{"docs":{},":":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}}},"x":{"docs":{},")":{"docs":{},")":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}}},"y":{"1":{"docs":{},"_":{"docs":{},"l":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}},"r":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}}},"2":{"docs":{},"_":{"docs":{},"l":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}},"r":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}}},"docs":{},")":{"docs":{},":":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}},"_":{"docs":{},"l":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}},"r":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}}}}}}}}}},"c":{"docs":{},"k":{"docs":{},"h":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"'":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}},"m":{"docs":{},"p":{"docs":{},"t":{"docs":{},"i":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"y":{"docs":{},".":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}},"x":{"0":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.011204481792717087}},",":{"docs":{},"x":{"1":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}},"docs":{}}}},"1":{"3":{"docs":{},",":{"docs":{},"x":{"2":{"3":{"docs":{},",":{"docs":{},"x":{"1":{"2":{"docs":{},"x":{"2":{"docs":{},",":{"docs":{},"x":{"2":{"2":{"docs":{},"x":{"1":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"docs":{}}},"docs":{}},"docs":{}}}},"docs":{}}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}}}},"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435}},"分":{"docs":{},"别":{"docs":{},"乘":{"docs":{},"以":{"docs":{},"w":{"1":{"docs":{},"到":{"docs":{},"w":{"docs":{},"n":{"docs":{},"，":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"k":{"docs":{},"个":{"docs":{},"数":{"docs":{},"组":{"docs":{},"成":{"docs":{},"的":{"docs":{},"向":{"docs":{},"量":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"样":{"docs":{},"本":{"1":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"w":{"docs":{},"k":{"docs":{},"这":{"docs":{},"个":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"上":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"k":{"docs":{},"维":{"docs":{},"的":{"docs":{},"向":{"docs":{},"量":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"k":{"docs":{},"t":{"docs":{},"*":{"docs":{},"*":{"docs":{},"(":{"docs":{},"为":{"docs":{},"什":{"docs":{},"么":{"docs":{},"是":{"docs":{},"转":{"docs":{},"置":{"docs":{},"呢":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"是":{"docs":{},"拿":{"docs":{},"x":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"去":{"docs":{},"和":{"docs":{},"w":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"做":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"的":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"乘":{"docs":{},"法":{"docs":{},"规":{"docs":{},"定":{"docs":{},"是":{"docs":{},"拿":{"docs":{},"x":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"和":{"docs":{},"w":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"列":{"docs":{},"做":{"docs":{},"乘":{"docs":{},"法":{"docs":{},")":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}},"_":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}},"l":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"r":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}},"2":{"1":{"docs":{},",":{"docs":{},"x":{"2":{"2":{"docs":{},",":{"docs":{},"x":{"1":{"docs":{},"*":{"docs":{},"x":{"2":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"docs":{}}}},"docs":{}}}},"docs":{}},"docs":{}}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.013986013986013986},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.015151515151515152}},"[":{"docs":{},":":{"5":{"docs":{},"]":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}},"docs":{},",":{"0":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"1":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}}},"i":{"docs":{},"]":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}},"(":{"docs":{},"x":{"1":{"docs":{},")":{"docs":{},":":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}},"_":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}},"docs":{}}},"_":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}},"l":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"r":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00423728813559322},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0058953574060427415},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.016483516483516484},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.01818181818181818},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.037037037037037035},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.025252525252525252},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.014018691588785047},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.024},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.017142857142857144},"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.008771929824561403},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.014388489208633094},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.02},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}},")":{"docs":{},"*":{"docs":{},"*":{"2":{"docs":{},")":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.010471204188481676}}}}},"docs":{}}},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.020942408376963352},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}}}}}}}}}},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}}}}}}}}}}}}},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}},"e":{"docs":{},"[":{"0":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}},"docs":{}}}}}}},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"(":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}},"n":{"docs":{},"d":{"docs":{},"i":{"docs":{},"m":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}},"\"":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}},",":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.006472491909385114},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}},"i":{"docs":{},",":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}},"*":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"i":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},")":{"docs":{},":":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},".":{"docs":{},"n":{"docs":{},"d":{"docs":{},"i":{"docs":{},"m":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"[":{"1":{"docs":{},"]":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"docs":{}}}}}}}},"]":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}},"c":{"docs":{},"a":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.027972027972027972}},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"w":{"docs":{},")":{"docs":{},".":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"(":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}},"s":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"l":{"docs":{},"e":{"docs":{},")":{"docs":{},":":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}},"b":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304}},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},".":{"docs":{},"_":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{},")":{"docs":{},"*":{"docs":{},"*":{"2":{"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}},"/":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}},"docs":{}}}}}}}}},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},"_":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}}},"t":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{},".":{"docs":{},"_":{"docs":{},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"m":{"docs":{},"o":{"docs":{},"i":{"docs":{},"d":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.016483516483516484},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.008771929824561403}}},":":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},"[":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},"e":{"docs":{},"s":{"docs":{},"]":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}},"_":{"docs":{},"i":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}},".":{"docs":{},"t":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},"_":{"docs":{},"i":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}}}}}}}}}}}}},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}},"[":{"docs":{},"i":{"docs":{},"]":{"docs":{},",":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}},"k":{"docs":{},",":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}},"d":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"n":{"docs":{},"u":{"docs":{},"m":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}},"e":{"docs":{},"w":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}},"[":{"docs":{},"i":{"docs":{},",":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.02}}}}}}}},"l":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}},"[":{"0":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"1":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}}}}}}},"n":{"docs":{},"d":{"docs":{},"i":{"docs":{},"m":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"(":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}},"t":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"w":{"docs":{},")":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}},"c":{"docs":{},"o":{"docs":{},"p":{"docs":{},"y":{"docs":{},"(":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"w":{"docs":{},")":{"docs":{},".":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"(":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}},"[":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},"e":{"docs":{},"s":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},"e":{"docs":{},"s":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}}}}},":":{"1":{"0":{"docs":{},",":{"docs":{},":":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.002210759027266028}}}}}},"docs":{}},"3":{"6":{"docs":{},",":{"docs":{},":":{"docs":{},"]":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}},"docs":{}},"docs":{},",":{"0":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}},"1":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}},"*":{"docs":{},"*":{"2":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}},"docs":{}}}}},"docs":{}}},"y":{"4":{"docs":{},".":{"3":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}},"docs":{}}},"docs":{}},"i":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}},"]":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"w":{"docs":{},")":{"docs":{},"*":{"docs":{},"w":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},"_":{"docs":{},"a":{"docs":{},"]":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}},"b":{"docs":{},"]":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},"[":{"docs":{},"i":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},"]":{"docs":{},",":{"docs":{},"d":{"docs":{},"]":{"docs":{},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},":":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}},"特":{"docs":{},"征":{"docs":{},"矩":{"docs":{},"阵":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}},"]":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}},"(":{"docs":{},"i":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"w":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"就":{"docs":{},"是":{"docs":{},"x":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"与":{"docs":{},"w":{"docs":{},"的":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"（":{"docs":{},"蓝":{"docs":{},"色":{"docs":{},"的":{"docs":{},"线":{"docs":{},"）":{"docs":{},"，":{"docs":{},"根":{"docs":{},"据":{"docs":{},"定":{"docs":{},"义":{"docs":{},"推":{"docs":{},"导":{"docs":{},"，":{"docs":{},"其":{"docs":{},"值":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"就":{"docs":{},"是":{"docs":{},"x":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"j":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"·":{"docs":{},"w":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"i":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.013605442176870748},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"w":{"docs":{},",":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},",":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"t":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"j":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"实":{"docs":{},"现":{"docs":{},"将":{"docs":{},"x":{"docs":{},"样":{"docs":{},"本":{"docs":{},"在":{"docs":{},"x":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"j":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"相":{"docs":{},"应":{"docs":{},"上":{"docs":{},"的":{"docs":{},"分":{"docs":{},"量":{"docs":{},"去":{"docs":{},"掉":{"docs":{},"，":{"docs":{},"相":{"docs":{},"减":{"docs":{},"之":{"docs":{},"后":{"docs":{},"的":{"docs":{},"集":{"docs":{},"合":{"docs":{},"意":{"docs":{},"义":{"docs":{},"就":{"docs":{},"是":{"docs":{},"讲":{"docs":{},"x":{"docs":{},"样":{"docs":{},"本":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"了":{"docs":{},"x":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"j":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"向":{"docs":{},"量":{"docs":{},"相":{"docs":{},"垂":{"docs":{},"直":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"轴":{"docs":{},"上":{"docs":{},"，":{"docs":{},"记":{"docs":{},"为":{"docs":{},"x":{"docs":{},"`":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"*":{"docs":{},"*":{"2":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}},"docs":{}}}},"y":{"1":{"0":{"0":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}}}}}}}}}}},"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}}}}}}}}}}},"docs":{},"_":{"docs":{},"l":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"r":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}},"2":{"0":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}},"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}}}}}}}}},"l":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"r":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.02},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},")":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.029411764705882353},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.02702702702702703},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857},"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.038461538461538464},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"[":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}},"docs":{}}}}}}},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"(":{"docs":{},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.012048192771084338},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.016},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}},"u":{"docs":{},"e":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353}}}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.006355932203389831},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.008771929824561403},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},")":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.02702702702702703},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857},"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.038461538461538464},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.023255813953488372}},":":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"1":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"2":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},")":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}},")":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.025974025974025976}}}}},"3":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00847457627118644},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.014018691588785047},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"\"":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353}}},")":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.022222222222222223},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.005333333333333333},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},")":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.014388489208633094},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.025}}},"*":{"docs":{},"*":{"2":{"docs":{},")":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}},"docs":{}}},":":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.04411764705882353},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.03888888888888889}}},"/":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"v":{"docs":{},"a":{"docs":{},"r":{"docs":{},"(":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},")":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}}}}}}}}}}}}}}},",":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}}}},".":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"0":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}},"docs":{}}}}}}}}}}},"=":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}}}}}},",":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}},"[":{"docs":{},":":{"1":{"0":{"docs":{},"]":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}},"docs":{}},"docs":{}}}}}}}}},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}},"h":{"docs":{},"a":{"docs":{},"t":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}},")":{"docs":{},")":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}},"i":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224}},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}},")":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}}}}}},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}},"[":{"docs":{},"i":{"docs":{},"]":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}},"k":{"docs":{},")":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}},":":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}},")":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.022222222222222223},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"r":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"\"":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.016304347826086956},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}},"[":{"0":{"docs":{},"]":{"docs":{},",":{"docs":{},"\\":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}},"docs":{}}}}}}}},"[":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},"e":{"docs":{},"s":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},"e":{"docs":{},"s":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},"e":{"docs":{},"s":{"docs":{},"]":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}},"_":{"docs":{},"a":{"docs":{},"]":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}},"b":{"docs":{},"]":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"s":{"docs":{},".":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},"!":{"docs":{},"=":{"9":{"docs":{},"]":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}},"docs":{}}},"=":{"docs":{},"=":{"9":{"docs":{},"]":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}},"docs":{}}}}}}}}}}}}}}}}},")":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.016483516483516484},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.018867924528301886},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.03875968992248062},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.03296703296703297}},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099}}}}}}}}}}}}},":":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.016483516483516484},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.008771929824561403},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},"*":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"1":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},"docs":{}}}}}}}}}},":":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}},"为":{"docs":{},"了":{"docs":{},"和":{"docs":{},"其":{"docs":{},"他":{"docs":{},"算":{"docs":{},"法":{"docs":{},"统":{"docs":{},"一":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"认":{"docs":{},"为":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"就":{"docs":{},"是":{"docs":{},"模":{"docs":{},"型":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}}}}}}}}}},"减":{"docs":{},"慢":{"docs":{},"变":{"docs":{},"化":{"docs":{},"速":{"docs":{},"度":{"docs":{},"，":{"docs":{},"t":{"0":{"docs":{},"为":{"docs":{},"了":{"docs":{},"增":{"docs":{},"加":{"docs":{},"随":{"docs":{},"机":{"docs":{},"性":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}},"docs":{}}}}}}}}},"可":{"docs":{},"视":{"docs":{},"化":{"docs":{},"，":{"docs":{},"先":{"docs":{},"使":{"docs":{},"用":{"docs":{},"两":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}},"提":{"docs":{},"高":{"docs":{},"t":{"docs":{},"p":{"docs":{},"r":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"要":{"docs":{},"将":{"docs":{},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"拉":{"docs":{},"低":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"在":{"docs":{},"拉":{"docs":{},"低":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"犯":{"docs":{},"f":{"docs":{},"p":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"也":{"docs":{},"会":{"docs":{},"增":{"docs":{},"高":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"添":{"docs":{},"加":{"docs":{},"一":{"docs":{},"些":{"docs":{},"噪":{"docs":{},"音":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}},"再":{"docs":{},"看":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}},"将":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"与":{"docs":{},"原":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"进":{"docs":{},"行":{"docs":{},"拼":{"docs":{},"接":{"docs":{},"，":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}},"实":{"docs":{},"现":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}},"过":{"docs":{},"程":{"docs":{},"简":{"docs":{},"单":{"docs":{},"编":{"docs":{},"码":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}},"自":{"docs":{},"己":{"docs":{},"的":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}}}},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}},"d":{"docs":{},"e":{"docs":{},"b":{"docs":{},"u":{"docs":{},"g":{"docs":{},"模":{"docs":{},"式":{"docs":{},"的":{"docs":{},"d":{"docs":{},"j":{"docs":{},"(":{"docs":{},"θ":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}},"j":{"docs":{},"(":{"docs":{},"θ":{"docs":{},")":{"docs":{},"函":{"docs":{},"数":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}},"数":{"docs":{},"学":{"docs":{},"推":{"docs":{},"导":{"docs":{},"出":{"docs":{},"的":{"docs":{},"d":{"docs":{},"j":{"docs":{},"(":{"docs":{},"θ":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}},"混":{"docs":{},"淆":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"，":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":5.0055555555555555}}}}}}}}}}}}}}},"验":{"docs":{},"搜":{"docs":{},"索":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}},"际":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"应":{"docs":{},"该":{"docs":{},"多":{"docs":{},"试":{"docs":{},"一":{"docs":{},"些":{"docs":{},"数":{"docs":{},"字":{"docs":{},"，":{"docs":{},"找":{"docs":{},"到":{"docs":{},"最":{"docs":{},"合":{"docs":{},"适":{"docs":{},"的":{"docs":{},"数":{"docs":{},"字":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}},"编":{"docs":{},"程":{"docs":{},"（":{"docs":{},"准":{"docs":{},"备":{"docs":{},"代":{"docs":{},"码":{"docs":{},"参":{"docs":{},"考":{"docs":{},"上":{"docs":{},"一":{"docs":{},"节":{"docs":{},"岭":{"docs":{},"回":{"docs":{},"归":{"docs":{},"）":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}},"应":{"docs":{},"用":{"docs":{},"中":{"docs":{},"，":{"docs":{},"通":{"docs":{},"常":{"docs":{},"应":{"docs":{},"该":{"docs":{},"先":{"docs":{},"尝":{"docs":{},"试":{"docs":{},"一":{"docs":{},"下":{"docs":{},"岭":{"docs":{},"回":{"docs":{},"归":{"docs":{},"（":{"docs":{},"如":{"docs":{},"果":{"docs":{},"计":{"docs":{},"算":{"docs":{},"能":{"docs":{},"力":{"docs":{},"足":{"docs":{},"够":{"docs":{},"的":{"docs":{},"话":{"docs":{},"）":{"docs":{},"。":{"docs":{},"但":{"docs":{},"是":{"docs":{},"如":{"docs":{},"果":{"docs":{},"θ":{"docs":{},"数":{"docs":{},"量":{"docs":{},"太":{"docs":{},"大":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"消":{"docs":{},"耗":{"docs":{},"计":{"docs":{},"算":{"docs":{},"资":{"docs":{},"源":{"docs":{},"可":{"docs":{},"能":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{},"，":{"docs":{},"而":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"由":{"docs":{},"于":{"docs":{},"有":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"急":{"docs":{},"于":{"docs":{},"把":{"docs":{},"一":{"docs":{},"些":{"docs":{},"θ":{"docs":{},"化":{"docs":{},"为":{"0":{"docs":{},"，":{"docs":{},"可":{"docs":{},"能":{"docs":{},"会":{"docs":{},"导":{"docs":{},"致":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"偏":{"docs":{},"差":{"docs":{},"比":{"docs":{},"价":{"docs":{},"大":{"docs":{},"。":{"docs":{},"这":{"docs":{},"个":{"docs":{},"时":{"docs":{},"候":{"docs":{},"需":{"docs":{},"要":{"docs":{},"使":{"docs":{},"用":{"docs":{},"弹":{"docs":{},"性":{"docs":{},"网":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"所":{"docs":{},"述":{"docs":{},"类":{"docs":{},"别":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"以":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"分":{"docs":{},"析":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"就":{"docs":{},"是":{"docs":{},"把":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"转":{"docs":{},"换":{"docs":{},"到":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"殊":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"向":{"docs":{},"量":{"docs":{},"空":{"docs":{},"间":{"docs":{},"；":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"为":{"docs":{},"了":{"docs":{},"限":{"docs":{},"制":{"docs":{},"让":{"docs":{},"他":{"docs":{},"们":{"docs":{},"比":{"docs":{},"较":{"docs":{},"小":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"前":{"docs":{},"面":{"docs":{},"系":{"docs":{},"数":{"docs":{},"可":{"docs":{},"以":{"docs":{},"取":{"docs":{},"的":{"docs":{},"小":{"docs":{},"一":{"docs":{},"些":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}},"从":{"docs":{},"计":{"docs":{},"算":{"docs":{},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{},"上":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"应":{"docs":{},"该":{"docs":{},"更":{"docs":{},"加":{"docs":{},"倾":{"docs":{},"向":{"docs":{},"于":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"如":{"docs":{},"果":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"维":{"docs":{},"度":{"docs":{},"比":{"docs":{},"较":{"docs":{},"多":{"docs":{},"，":{"docs":{},"样":{"docs":{},"本":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{},"（":{"docs":{},"比":{"docs":{},"如":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"时":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"=":{"1":{"0":{"0":{"docs":{},"）":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"完":{"docs":{},"整":{"docs":{},"的":{"docs":{},"s":{"docs":{},"o":{"docs":{},"f":{"docs":{},"t":{"docs":{"支撑向量机SVM/11.3 Soft Margin SVM.html":{"ref":"支撑向量机SVM/11.3 Soft Margin SVM.html","tf":0.0625}}}}}}}}}}},"投":{"docs":{},"票":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}},"欧":{"docs":{},"拉":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}},"距":{"docs":{},"离":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}},"求":{"docs":{},"出":{"docs":{},"距":{"docs":{},"离":{"docs":{},"测":{"docs":{},"试":{"docs":{},"点":{"docs":{},"最":{"docs":{},"近":{"docs":{},"的":{"6":{"docs":{},"个":{"docs":{},"点":{"docs":{},"的":{"docs":{},"类":{"docs":{},"别":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}},"docs":{}}}}}}}}},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}},"第":{"docs":{},"一":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"如":{"docs":{},"何":{"docs":{},"求":{"docs":{},"出":{"docs":{},"下":{"docs":{},"一":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"?":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}},"二":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}},"平":{"docs":{},"方":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}},"均":{"docs":{},"值":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}},"生":{"docs":{},"成":{"docs":{},"表":{"docs":{},"达":{"docs":{},"式":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}},"一":{"docs":{},"个":{"docs":{},"一":{"docs":{},"维":{"docs":{},"向":{"docs":{},"量":{"docs":{},"进":{"docs":{},"行":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}},"二":{"docs":{},"维":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"进":{"docs":{},"行":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}},"不":{"docs":{},"真":{"docs":{},"实":{"docs":{},"的":{"docs":{},"非":{"docs":{},"线":{"docs":{},"性":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}}}},"绘":{"docs":{},"制":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"及":{"docs":{},"要":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"点":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}},"我":{"docs":{},"们":{"docs":{},"模":{"docs":{},"拟":{"docs":{},"的":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}},"曲":{"docs":{},"线":{"docs":{},"观":{"docs":{},"察":{"docs":{},"取":{"docs":{},"前":{"docs":{},"i":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"所":{"docs":{},"能":{"docs":{},"解":{"docs":{},"释":{"docs":{},"的":{"docs":{},"原":{"docs":{},"数":{"docs":{},"据":{"docs":{},"比":{"docs":{},"例":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}}}}}}}}}}},"学":{"docs":{},"习":{"docs":{},"曲":{"docs":{},"线":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}},"样":{"docs":{},"本":{"docs":{},"曲":{"docs":{},"线":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}},"模":{"docs":{},"型":{"docs":{},"曲":{"docs":{},"线":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"m":{"docs":{},"o":{"docs":{},"i":{"docs":{},"d":{"docs":{},"函":{"docs":{},"数":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}}}}}}}},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"。":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"到":{"docs":{},"是":{"docs":{},"一":{"docs":{},"根":{"docs":{},"弯":{"docs":{},"弯":{"docs":{},"曲":{"docs":{},"曲":{"docs":{},"的":{"docs":{},"曲":{"docs":{},"线":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}},"三":{"docs":{},"个":{"docs":{},"不":{"docs":{},"同":{"docs":{},"类":{"docs":{},"别":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"曲":{"docs":{},"线":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}},"混":{"docs":{},"淆":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"，":{"docs":{},"越":{"docs":{},"亮":{"docs":{},"的":{"docs":{},"地":{"docs":{},"方":{"docs":{},"说":{"docs":{},"明":{"docs":{},"数":{"docs":{},"值":{"docs":{},"越":{"docs":{},"大":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}}},"上":{"docs":{},"下":{"docs":{},"两":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}},"要":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"点":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}},"求":{"docs":{},"集":{"docs":{},"合":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"都":{"docs":{},"能":{"docs":{},"估":{"docs":{},"算":{"docs":{},"概":{"docs":{},"率":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}},"训":{"docs":{},"练":{"docs":{},"集":{"docs":{},"合":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}},"s":{"docs":{},"i":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"和":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"意":{"docs":{},"义":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}},"模":{"docs":{},"型":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"，":{"docs":{},"是":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"使":{"docs":{},"用":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"，":{"docs":{},"以":{"docs":{},"计":{"docs":{},"算":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"s":{"docs":{},"p":{"docs":{},"i":{"docs":{},"l":{"docs":{},"t":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}},"一":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"m":{"1":{"docs":{},",":{"docs":{},"产":{"docs":{},"生":{"docs":{},"错":{"docs":{},"误":{"docs":{},"e":{"1":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}},"docs":{}}}}}}}},"docs":{}}}}}}}},"返":{"docs":{},"回":{"docs":{},"排":{"docs":{},"序":{"docs":{},"后":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"的":{"docs":{},"索":{"docs":{},"引":{"docs":{},",":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"距":{"docs":{},"离":{"docs":{},"测":{"docs":{},"试":{"docs":{},"点":{"docs":{},"距":{"docs":{},"离":{"docs":{},"最":{"docs":{},"近":{"docs":{},"的":{"docs":{},"点":{"docs":{},"的":{"docs":{},"排":{"docs":{},"序":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"数":{"docs":{},"组":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","tf":0.005235602094240838}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"组":{"docs":{},"，":{"docs":{},"有":{"docs":{},"三":{"docs":{},"个":{"docs":{},"元":{"docs":{},"素":{"docs":{},"，":{"docs":{},"说":{"docs":{},"明":{"docs":{},"c":{"docs":{},"r":{"docs":{},"o":{"docs":{},"s":{"docs":{},"s":{"docs":{},"_":{"docs":{},"v":{"docs":{},"a":{"docs":{},"l":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"方":{"docs":{},"法":{"docs":{},"默":{"docs":{},"认":{"docs":{},"将":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"分":{"docs":{},"成":{"docs":{},"了":{"docs":{},"三":{"docs":{},"份":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"(":{"1":{"0":{"0":{"docs":{},",":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}},"docs":{}},"1":{"2":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}},"3":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}},"docs":{}},"3":{"2":{"3":{"3":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.013605442176870748}}}},"docs":{}},"docs":{}},"4":{"7":{"docs":{},",":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}},"docs":{}},"docs":{}},"4":{"4":{"docs":{},",":{"docs":{},")":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}},"5":{"docs":{},",":{"docs":{},")":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.014388489208633094}}}}},"docs":{}},"5":{"0":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}},"docs":{}},"7":{"9":{"7":{"docs":{},",":{"docs":{},")":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}},"docs":{}},"docs":{}},"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}},".":{"docs":{},"+":{"docs":{},"n":{"docs":{},"p":{"docs":{},".":{"docs":{},"e":{"docs":{},"x":{"docs":{},"p":{"docs":{},"(":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}},"2":{"9":{"1":{"4":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}},"docs":{}},"docs":{}},"docs":{},"*":{"docs":{},"e":{"docs":{},"p":{"docs":{},"s":{"docs":{},"i":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}}}},"3":{"6":{"docs":{},",":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}},"7":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}},"8":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}},"docs":{}},"5":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}},"6":{"0":{"0":{"0":{"0":{"docs":{},",":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"7":{"5":{"docs":{},",":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}},"docs":{}},"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}},"(":{"docs":{},"x":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"1":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}}}}},"a":{"docs":{},"b":{"docs":{},")":{"docs":{},"t":{"docs":{},"=":{"docs":{},"b":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},")":{"docs":{},"转":{"docs":{},"换":{"docs":{},"成":{"docs":{},"最":{"docs":{},"后":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{"PCA/2.使用梯度上升法解决PCA问题.html":{"ref":"PCA/2.使用梯度上升法解决PCA问题.html","tf":0.16666666666666666}}}}}}}}}}}}}}}}}}}}},"x":{"2":{"docs":{},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"1":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{}}}}},"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.02}},"[":{"docs":{},":":{"docs":{},",":{"0":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}},"docs":{},"d":{"docs":{},"]":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"]":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"e":{"docs":{},"d":{"docs":{},"_":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},"[":{"docs":{},"i":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}}}}},"_":{"docs":{},"i":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}}}}}},"b":{"docs":{},"_":{"docs":{},"i":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}},"k":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},")":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}}}},"*":{"docs":{},"*":{"2":{"docs":{},")":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}},"docs":{}}}},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}}}}},"θ":{"0":{"docs":{},")":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}},"docs":{}},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{},"_":{"docs":{},"x":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.016}}}}}}}},"t":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}},"p":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112}}}},"j":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"_":{"1":{"docs":{},",":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},",":{"docs":{},"y":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}},"docs":{}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},")":{"docs":{},"（":{"docs":{},"归":{"0":{"docs":{},"：":{"docs":{},"所":{"docs":{},"有":{"docs":{},"样":{"docs":{},"本":{"docs":{},"都":{"docs":{},"减":{"docs":{},"去":{"docs":{},"他":{"docs":{},"们":{"docs":{},"的":{"docs":{},"均":{"docs":{},"值":{"docs":{},"）":{"docs":{},"，":{"docs":{},"使":{"docs":{},"得":{"docs":{},"均":{"docs":{},"值":{"docs":{},"为":{"0":{"docs":{},"，":{"docs":{},"这":{"docs":{},"样":{"docs":{},"可":{"docs":{},"以":{"docs":{},"简":{"docs":{},"化":{"docs":{},"方":{"docs":{},"差":{"docs":{},"的":{"docs":{},"公":{"docs":{},"式":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}},"o":{"docs":{},"w":{"docs":{},"n":{"docs":{},"_":{"docs":{},"i":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}},"f":{"docs":{},"(":{"docs":{},"w":{"docs":{},"_":{"1":{"docs":{},",":{"docs":{},"x":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}},"docs":{}}}}},"~":{"2":{"0":{"0":{"docs":{},"m":{"docs":{},"b":{"docs":{},")":{"docs":{},":":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}},"docs":{}},"docs":{}},"docs":{}},"'":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"'":{"docs":{},",":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},")":{"docs":{},")":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},"'":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}},"_":{"docs":{},"s":{"docs":{},"v":{"docs":{},"r":{"docs":{},"'":{"docs":{},",":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"'":{"docs":{},",":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429}},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},")":{"docs":{},")":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}},"c":{"docs":{},"=":{"docs":{},"c":{"docs":{},")":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}},",":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"t":{"docs":{},"y":{"docs":{},"=":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"t":{"docs":{},"y":{"docs":{},")":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"'":{"docs":{},",":{"docs":{},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"f":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"=":{"2":{"docs":{},")":{"docs":{},")":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}},"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},")":{"docs":{},")":{"docs":{},",":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.008571428571428572}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"d":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{},"r":{"docs":{},"'":{"docs":{},",":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},")":{"docs":{},")":{"docs":{},",":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.008571428571428572}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"v":{"docs":{},"c":{"docs":{},"'":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943}}}}}}}},"\"":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"\"":{"docs":{},",":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"(":{"docs":{},")":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},"\"":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}},"_":{"docs":{},"s":{"docs":{},"v":{"docs":{},"r":{"docs":{},"\"":{"docs":{},",":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"\"":{"docs":{},",":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"(":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"=":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},")":{"docs":{},")":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"g":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{},"\"":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"\"":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"f":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"=":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},")":{"docs":{},")":{"docs":{},",":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.011428571428571429},"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"d":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{},"r":{"docs":{},"\"":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},")":{"docs":{},")":{"docs":{},",":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.011428571428571429},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"\"":{"docs":{},",":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},")":{"docs":{},")":{"docs":{},",":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"v":{"docs":{},"c":{"docs":{},"\"":{"docs":{},",":{"docs":{},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},"(":{"docs":{},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"e":{"docs":{},"l":{"docs":{},"=":{"docs":{},"\"":{"docs":{},"r":{"docs":{},"b":{"docs":{},"f":{"docs":{},"\"":{"docs":{},",":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{},"\"":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{},"\"":{"docs":{},",":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"docs":{},"(":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"=":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},")":{"docs":{},")":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"e":{"docs":{},"l":{"docs":{},"s":{"docs":{},"v":{"docs":{},"c":{"docs":{},"\"":{"docs":{},",":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}}}}}}}},"d":{"docs":{},"t":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{},"\"":{"docs":{},",":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}},"u":{"docs":{},"p":{"docs":{},"_":{"docs":{},"i":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435}}}}}},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":2.007751937984496}}}}}},"w":{"0":{"docs":{},"/":{"docs":{},"w":{"1":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.008403361344537815}}},"docs":{}}}},"1":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}},"2":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.013986013986013986}}},"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.024},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.02097902097902098},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"s":{"docs":{},"=":{"docs":{},"'":{"docs":{},"u":{"docs":{},"n":{"docs":{},"i":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"'":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.012987012987012988},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.009345794392523364}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"'":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"_":{"docs":{},"m":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"d":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},")":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}},")":{"docs":{},",":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.011428571428571429},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.009433962264150943},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.025974025974025976},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}}},"r":{"docs":{},"m":{"docs":{},"_":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"=":{"docs":{},"f":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},"e":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.02197802197802198},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}},")":{"docs":{},"]":{"docs":{},")":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.011428571428571429}}}}}}}}}}}}}}}}}}}}},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"p":{"docs":{},"y":{"docs":{},"(":{"docs":{},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016}}}}}}}},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"w":{"2":{"docs":{},")":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}},"docs":{}}}}}}},"_":{"1":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}},"[":{"docs":{},"i":{"docs":{},"]":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}},"2":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}},"[":{"docs":{},"i":{"docs":{},"]":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}},"docs":{}},"[":{"0":{"docs":{},"]":{"docs":{},"/":{"docs":{},"w":{"docs":{},"[":{"1":{"docs":{},"]":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435}}}},"docs":{}}}}}},"1":{"docs":{},"]":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435}}}},"docs":{}}},"加":{"docs":{},"载":{"docs":{},"鸢":{"docs":{},"尾":{"docs":{},"花":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}}},"波":{"docs":{},"士":{"docs":{},"顿":{"docs":{},"房":{"docs":{},"价":{"docs":{},"数":{"docs":{},"据":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}},"书":{"docs":{},"写":{"docs":{},"识":{"docs":{},"别":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}},"封":{"docs":{},"装":{"docs":{},"我":{"docs":{},"们":{"docs":{},"自":{"docs":{},"己":{"docs":{},"的":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}},"的":{"docs":{},"k":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"e":{"docs":{},"r":{"docs":{},"，":{"docs":{},"在":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"如":{"docs":{},"果":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"较":{"docs":{},"大":{"docs":{},"，":{"docs":{},"会":{"docs":{},"以":{"docs":{},"树":{"docs":{},"结":{"docs":{},"构":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"进":{"docs":{},"行":{"docs":{},"存":{"docs":{},"储":{"docs":{},"，":{"docs":{},"以":{"docs":{},"加":{"docs":{},"快":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"过":{"docs":{},"程":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"会":{"docs":{},"导":{"docs":{},"致":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"过":{"docs":{},"程":{"docs":{},"变":{"docs":{},"慢":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"获":{"docs":{},"取":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}}}}},"矩":{"docs":{},"阵":{"docs":{},"的":{"docs":{},"转":{"docs":{},"置":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"逆":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}},"得":{"docs":{},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"评":{"docs":{},"估":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"返":{"docs":{},"回":{"docs":{},"的":{"docs":{},"是":{"docs":{},"k":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"e":{"docs":{},"r":{"docs":{},"对":{"docs":{},"象":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"直":{"docs":{},"接":{"docs":{},"拿":{"docs":{},"来":{"docs":{},"做":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"预":{"docs":{},"测":{"docs":{},"了":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"前":{"docs":{},"n":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"实":{"docs":{},"现":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}},"每":{"docs":{},"个":{"docs":{},"标":{"docs":{},"记":{"docs":{},"加":{"docs":{},"了":{"docs":{},"噪":{"docs":{},"音":{"docs":{},"的":{"1":{"0":{"docs":{},"个":{"docs":{},"元":{"docs":{},"素":{"docs":{},"，":{"docs":{},"一":{"docs":{},"共":{"1":{"0":{"docs":{},"个":{"docs":{},"标":{"docs":{},"记":{"docs":{},"，":{"docs":{},"公":{"1":{"0":{"0":{"docs":{},"个":{"docs":{},"元":{"docs":{},"素":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}},"docs":{}},"docs":{}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}},"预":{"docs":{},"测":{"docs":{},"结":{"docs":{},"果":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","tf":0.00211864406779661}}}},"波":{"docs":{},"士":{"docs":{},"顿":{"docs":{},"房":{"docs":{},"价":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}},"'":{"docs":{},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"'":{"docs":{},"}":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}},"n":{"docs":{},"_":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"'":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.015873015873015872},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.020618556701030927},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}},"p":{"docs":{},"'":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}}}},"w":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"s":{"docs":{},"'":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},"[":{"docs":{},"'":{"docs":{},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"'":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}},"u":{"docs":{},"n":{"docs":{},"i":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"'":{"docs":{},"]":{"docs":{},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"a":{"docs":{},"r":{"docs":{},"o":{"docs":{},"n":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}},"j":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}},"z":{"docs":{},"u":{"docs":{},"m":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}},"r":{"docs":{},"a":{"docs":{},"b":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}},"y":{"docs":{},"d":{"docs":{},"r":{"docs":{},"u":{"docs":{},"n":{"docs":{},"a":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"'":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"'":{"docs":{},")":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"s":{"docs":{},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},"'":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.018691588785046728},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.017142857142857144},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02358490566037736},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}}}}}}}}}}},"b":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0196078431372549}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"k":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.023809523809523808},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}},"n":{"docs":{},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},")":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"d":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936}}}}}}}},"p":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.023809523809523808},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}},"e":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.011904761904761904},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0071174377224199285}}},",":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"k":{"docs":{},",":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"p":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.014234875444839857}}}}}}}}}}}}}}}}}}}}},"d":{"2":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541}}}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{},"i":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}},"y":{"2":{"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}},"docs":{},",":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}}}}}}},"v":{"2":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}},"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541}},")":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"g":{"2":{"docs":{},",":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},",":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}},"i":{"docs":{},"g":{"docs":{},"_":{"docs":{},"i":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}},"x":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125}}}}},"n":{"docs":{},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}}}}},"o":{"docs":{},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"n":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}},".":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}}}}}}}}}}}}},"o":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},",":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}},"t":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"p":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},")":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.038461538461538464}}},",":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.03875968992248062},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}},"_":{"docs":{},"f":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}},"e":{"docs":{},"s":{"docs":{},"=":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"e":{"docs":{},")":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":3.37984496124031}}}}}},"a":{"docs":{},"s":{"docs":{},"i":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}},"e":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}},"_":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"=":{"docs":{},"d":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"_":{"docs":{},"w":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"=":{"docs":{},"n":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},",":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"g":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":3.352564102564102},"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"2":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}}}}}}}}},"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.023255813953488372}},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}},",":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.023255813953488372}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}}}}}}},"o":{"docs":{},"o":{"docs":{},"b":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"_":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}}}}}}}}},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},")":{"docs":{},",":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.038461538461538464},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.03875968992248062}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"更":{"docs":{},"常":{"docs":{},"用":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}},"的":{"docs":{},"思":{"docs":{},"路":{"docs":{},"极":{"docs":{},"易":{"docs":{},"并":{"docs":{},"行":{"docs":{},"化":{"docs":{},"处":{"docs":{},"理":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}}}}}}}},")":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":2.007751937984496}}},".":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}},"{":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.007936507936507936},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.020618556701030927},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},"'":{"docs":{},"n":{"docs":{},"_":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"'":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}},"w":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"s":{"docs":{},"'":{"docs":{},":":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}},"}":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},",":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"不":{"docs":{},"是":{"docs":{},"用":{"docs":{},"户":{"docs":{},"传":{"docs":{},"入":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"而":{"docs":{},"是":{"docs":{},"根":{"docs":{},"据":{"docs":{},"用":{"docs":{},"户":{"docs":{},"传":{"docs":{},"入":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"计":{"docs":{},"算":{"docs":{},"出":{"docs":{},"来":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"以":{"docs":{},"_":{"docs":{},"结":{"docs":{},"尾":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"直":{"docs":{},"接":{"docs":{},"相":{"docs":{},"减":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"差":{"docs":{},"值":{"docs":{},"有":{"docs":{},"正":{"docs":{},"有":{"docs":{},"负":{"docs":{},"，":{"docs":{},"会":{"docs":{},"抵":{"docs":{},"消":{"docs":{"线性回归算法/1.线性回归算法简介.html":{"ref":"线性回归算法/1.线性回归算法简介.html","tf":0.125}}}}}}}}}}}}}}}}}}}}}},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"而":{"docs":{},"使":{"docs":{},"用":{"docs":{},"这":{"docs":{},"部":{"docs":{},"分":{"docs":{},"没":{"docs":{},"有":{"docs":{},"取":{"docs":{},"到":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"做":{"docs":{},"测":{"docs":{},"试":{"docs":{},"/":{"docs":{},"验":{"docs":{},"证":{"docs":{},"。":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"适":{"docs":{},"用":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"值":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"值":{"docs":{},"函":{"docs":{},"数":{"docs":{},"存":{"docs":{},"在":{"docs":{},"不":{"docs":{},"可":{"docs":{},"导":{"docs":{},"的":{"docs":{},"点":{"docs":{"线性回归算法/1.线性回归算法简介.html":{"ref":"线性回归算法/1.线性回归算法简介.html","tf":0.125}}}}}}}}}}}}}}}}}}}}}}}}},"过":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"我":{"docs":{},"们":{"docs":{},"是":{"docs":{},"很":{"docs":{},"少":{"docs":{},"使":{"docs":{},"用":{"docs":{},"l":{"0":{"docs":{},"正":{"docs":{},"则":{"docs":{},"的":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"l":{"0":{"docs":{},"正":{"docs":{},"则":{"docs":{},"的":{"docs":{},"优":{"docs":{},"化":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"n":{"docs":{},"p":{"docs":{},"难":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"不":{"docs":{},"能":{"docs":{},"使":{"docs":{},"用":{"docs":{},"诸":{"docs":{},"如":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"甚":{"docs":{},"至":{"docs":{},"数":{"docs":{},"学":{"docs":{},"公":{"docs":{},"式":{"docs":{},"来":{"docs":{},"找":{"docs":{},"到":{"docs":{},"一":{"docs":{},"个":{"docs":{},"最":{"docs":{},"优":{"docs":{},"解":{"docs":{},"。":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}},"docs":{}}}}}}}}}}}},"无":{"docs":{},"论":{"docs":{},"是":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"，":{"docs":{},"还":{"docs":{},"是":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"依":{"docs":{},"然":{"docs":{},"可":{"docs":{},"以":{"docs":{},"加":{"docs":{},"入":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"项":{"docs":{},"，":{"docs":{},"使":{"docs":{},"得":{"docs":{},"他":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"不":{"docs":{},"再":{"docs":{},"是":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"对":{"docs":{},"于":{"docs":{},"这":{"docs":{},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"不":{"docs":{},"能":{"docs":{},"简":{"docs":{},"单":{"docs":{},"的":{"docs":{},"求":{"docs":{},"出":{"docs":{},"这":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"的":{"docs":{},"方":{"docs":{},"程":{"docs":{},"。":{"docs":{},"然":{"docs":{},"后":{"docs":{},"将":{"docs":{},"整":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"画":{"docs":{},"出":{"docs":{},"来":{"docs":{},"来":{"docs":{},"看":{"docs":{},"到":{"docs":{},"这":{"docs":{},"个":{"docs":{},"决":{"docs":{},"策":{"docs":{},"的":{"docs":{},"边":{"docs":{},"界":{"docs":{},"。":{"docs":{},"这":{"docs":{},"个":{"docs":{},"时":{"docs":{},"候":{"docs":{},"我":{"docs":{},"们":{"docs":{},"需":{"docs":{},"要":{"docs":{},"一":{"docs":{},"个":{"docs":{},"绘":{"docs":{},"制":{"docs":{},"不":{"docs":{},"规":{"docs":{},"则":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"即":{"docs":{},"使":{"docs":{},"如":{"docs":{},"此":{"docs":{},"还":{"docs":{},"是":{"docs":{},"存":{"docs":{},"在":{"docs":{},"一":{"docs":{},"个":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"直":{"docs":{},"线":{"docs":{},"太":{"docs":{},"简":{"docs":{},"单":{"docs":{},"了":{"docs":{},"，":{"docs":{},"比":{"docs":{},"如":{"docs":{},"如":{"docs":{},"下":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"规":{"docs":{},"则":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"的":{"docs":{},"绘":{"docs":{},"制":{"docs":{},"方":{"docs":{},"法":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}},"传":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{},"h":{"docs":{},"会":{"docs":{},"一":{"docs":{},"直":{"docs":{},"划":{"docs":{},"分":{"docs":{},"直":{"docs":{},"到":{"docs":{},"基":{"docs":{},"尼":{"docs":{},"系":{"docs":{},"数":{"docs":{},"为":{"0":{"docs":{},"为":{"docs":{},"止":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}},"两":{"docs":{},"种":{"docs":{},"距":{"docs":{},"离":{"docs":{},"的":{"docs":{},"整":{"docs":{},"理":{"docs":{},"对":{"docs":{},"比":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"加":{"docs":{},"起":{"docs":{},"来":{"docs":{},"可":{"docs":{},"以":{"docs":{},"解":{"docs":{},"释":{"docs":{},"百":{"docs":{},"分":{"docs":{},"之":{"2":{"7":{"docs":{},"的":{"docs":{},"原":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"而":{"docs":{},"其":{"docs":{},"他":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"丢":{"docs":{},"失":{"docs":{},"了":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"x":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}}},"什":{"docs":{},"么":{"docs":{},"是":{"docs":{},"距":{"docs":{},"离":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}},"学":{"docs":{},"习":{"docs":{},"曲":{"docs":{},"线":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":5.05}}}}},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":5.008928571428571}}}}},"集":{"docs":{},"成":{"docs":{},"学":{"docs":{},"习":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":5.006756756756757}}}}}}}}},"到":{"docs":{},"这":{"docs":{},"里":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"获":{"docs":{},"得":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"新":{"docs":{},"的":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}}}},"现":{"docs":{},"在":{"docs":{},"我":{"docs":{},"们":{"docs":{},"所":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"的":{"docs":{},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{},"，":{"docs":{},"都":{"docs":{},"必":{"docs":{},"须":{"docs":{},"在":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"中":{"docs":{},"能":{"docs":{},"求":{"docs":{},"出":{"docs":{},"一":{"docs":{},"根":{"docs":{},"确":{"docs":{},"确":{"docs":{},"实":{"docs":{},"实":{"docs":{},"的":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"满":{"docs":{},"足":{"docs":{},"以":{"docs":{},"上":{"docs":{},"的":{"docs":{},"条":{"docs":{},"件":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"线":{"docs":{},"性":{"docs":{},"可":{"docs":{},"分":{"docs":{},"的":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"底":{"docs":{},"什":{"docs":{},"么":{"docs":{},"是":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":5.083333333333333}}}}}}}}}},"寻":{"docs":{},"找":{"docs":{},"好":{"docs":{},"的":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"k":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}},"思":{"docs":{},"路":{"docs":{},"，":{"docs":{},"遍":{"docs":{},"历":{"1":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}},"docs":{}}}}}},"搜":{"docs":{},"索":{"docs":{},"明":{"docs":{},"可":{"docs":{},"夫":{"docs":{},"斯":{"docs":{},"基":{"docs":{},"距":{"docs":{},"离":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"p":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}},"明":{"docs":{},"克":{"docs":{},"夫":{"docs":{},"斯":{"docs":{},"基":{"docs":{},"距":{"docs":{},"离":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}},"是":{"docs":{},"否":{"docs":{},"打":{"docs":{},"印":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"信":{"docs":{},"息":{"docs":{},",":{"docs":{},"传":{"docs":{},"入":{"docs":{},"值":{"docs":{},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"输":{"docs":{},"出":{"docs":{},"信":{"docs":{},"息":{"docs":{},"越":{"docs":{},"详":{"docs":{},"细":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}}}}}}}}},"放":{"docs":{},"回":{"docs":{},"取":{"docs":{},"样":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}},"x":{"docs":{},"中":{"docs":{},"的":{"docs":{},"所":{"docs":{},"有":{"docs":{},"样":{"docs":{},"本":{"docs":{},"都":{"docs":{},"去":{"docs":{},"除":{"docs":{},"了":{"docs":{},"第":{"docs":{},"一":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"上":{"docs":{},"的":{"docs":{},"分":{"docs":{},"量":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"要":{"docs":{},"求":{"docs":{},"第":{"docs":{},"二":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"，":{"docs":{},"只":{"docs":{},"要":{"docs":{},"在":{"docs":{},"新":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"上":{"docs":{},"，":{"docs":{},"重":{"docs":{},"新":{"docs":{},"求":{"docs":{},"一":{"docs":{},"下":{"docs":{},"第":{"docs":{},"一":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"噪":{"docs":{},"音":{"docs":{},"，":{"docs":{},"这":{"docs":{},"也":{"docs":{},"解":{"docs":{},"释":{"docs":{},"了":{"docs":{},"为":{"docs":{},"什":{"docs":{},"么":{"docs":{},"我":{"docs":{},"们":{"docs":{},"在":{"docs":{},"上":{"docs":{},"一":{"docs":{},"节":{"docs":{},"降":{"docs":{},"维":{"docs":{},"处":{"docs":{},"理":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"反":{"docs":{},"而":{"docs":{},"识":{"docs":{},"别":{"docs":{},"率":{"docs":{},"提":{"docs":{},"高":{"docs":{},"了":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"有":{"docs":{},"噪":{"docs":{},"音":{"docs":{},"的":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}},"吻":{"docs":{},"合":{"docs":{},"的":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}},"曼":{"docs":{},"哈":{"docs":{},"顿":{"docs":{},"距":{"docs":{},"离":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"分":{"docs":{},"数":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}},"参":{"docs":{},"数":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}},"评":{"docs":{},"估":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"返":{"docs":{},"回":{"docs":{},"的":{"docs":{},"是":{"docs":{},"k":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"e":{"docs":{},"r":{"docs":{},"对":{"docs":{},"象":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"值":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}}}},"大":{"docs":{},"循":{"docs":{},"环":{"docs":{},"次":{"docs":{},"数":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}},"终":{"docs":{},"预":{"docs":{},"测":{"docs":{},"结":{"docs":{},"果":{"docs":{},"是":{"docs":{},":":{"docs":{},"m":{"1":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}},"docs":{}}}}}}}}}},"经":{"docs":{},"验":{"docs":{},"数":{"docs":{},"值":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}},"网":{"docs":{},"格":{"docs":{},"搜":{"docs":{},"索":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}},"调":{"docs":{},"用":{"docs":{},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{},"方":{"docs":{},"法":{"docs":{},"执":{"docs":{},"行":{"docs":{},"网":{"docs":{},"格":{"docs":{},"搜":{"docs":{},"索":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}},"g":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"s":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{},"c":{"docs":{},"v":{"docs":{},"创":{"docs":{},"建":{"docs":{},"网":{"docs":{},"格":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"对":{"docs":{},"象":{"docs":{},"，":{"docs":{},"传":{"docs":{},"入":{"docs":{},"参":{"docs":{},"数":{"docs":{},"为":{"docs":{},"c":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"e":{"docs":{},"r":{"docs":{},"对":{"docs":{},"象":{"docs":{},"以":{"docs":{},"及":{"docs":{},"参":{"docs":{},"数":{"docs":{},"列":{"docs":{},"表":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}}}}}},"整":{"docs":{},"k":{"docs":{},"为":{"5":{"0":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}},"docs":{}},"docs":{}}},"过":{"docs":{},"后":{"docs":{},"的":{"docs":{},"样":{"docs":{},"子":{"docs":{},"已":{"docs":{},"经":{"docs":{},"比":{"docs":{},"上":{"docs":{},"面":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"规":{"docs":{},"整":{"docs":{},"了":{"docs":{},"很":{"docs":{},"多":{"docs":{},"。":{"docs":{},"整":{"docs":{},"体":{"docs":{},"分":{"docs":{},"成":{"docs":{},"了":{"docs":{},"三":{"docs":{},"大":{"docs":{},"块":{"docs":{},"，":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"清":{"docs":{},"晰":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"和":{"docs":{},"平":{"docs":{},"均":{"docs":{},"值":{"docs":{},"的":{"docs":{},"特":{"docs":{},"点":{"docs":{},"：":{"docs":{},"如":{"docs":{},"果":{"docs":{},"一":{"docs":{},"个":{"docs":{},"值":{"docs":{},"特":{"docs":{},"别":{"docs":{},"高":{"docs":{},"，":{"docs":{},"一":{"docs":{},"个":{"docs":{},"值":{"docs":{},"特":{"docs":{},"别":{"docs":{},"低":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"我":{"docs":{},"们":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"f":{"1":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"和":{"docs":{},"模":{"docs":{},"型":{"docs":{},"参":{"docs":{},"数":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":5}}}}}}}}}},"领":{"docs":{},"域":{"docs":{},"知":{"docs":{},"识":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","tf":0.003968253968253968}}}}}},"/":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.029411764705882353},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.02197802197802198},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.008032128514056224},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.013157894736842105},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541}},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},")":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}},")":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}},"a":{"docs":{},"n":{"docs":{},"a":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"3":{"docs":{},"/":{"docs":{},"l":{"docs":{},"i":{"docs":{},"b":{"docs":{},"/":{"docs":{},"p":{"docs":{},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"n":{"3":{"docs":{},".":{"6":{"docs":{},"/":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.018691588785046728},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.017142857142857144},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02358490566037736},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}},"i":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"l":{"docs":{},"i":{"docs":{},"b":{"docs":{},"/":{"docs":{},"_":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"t":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"p":{"docs":{},".":{"docs":{},"p":{"docs":{},"y":{"docs":{},":":{"2":{"1":{"9":{"docs":{},":":{"docs":{"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}},"docs":{}}}}}}}}}},"之":{"docs":{},"间":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}},"前":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"（":{"docs":{},"通":{"docs":{},"过":{"docs":{},"最":{"docs":{},"小":{"docs":{},"化":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"找":{"docs":{},"到":{"docs":{},"一":{"docs":{},"个":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"来":{"docs":{},"分":{"docs":{},"类":{"docs":{},"数":{"docs":{},"据":{"docs":{},"）":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{},"的":{"docs":{},"不":{"docs":{},"足":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"他":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"非":{"docs":{},"常":{"docs":{},"弱":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"通":{"docs":{},"过":{"docs":{},"已":{"docs":{},"知":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"求":{"docs":{},"出":{"docs":{},"了":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"，":{"docs":{},"而":{"docs":{},"并":{"docs":{},"没":{"docs":{},"有":{"docs":{},"考":{"docs":{},"虑":{"docs":{},"未":{"docs":{},"知":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"均":{"docs":{},"值":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}},"方":{"docs":{},"差":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0014738393515106854}}}}}}},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"出":{"docs":{},"现":{"docs":{},"在":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"是":{"docs":{},"均":{"docs":{},"匀":{"docs":{},"分":{"docs":{},"布":{"docs":{},"的":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}}}}}},"归":{"0":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}},"docs":{}}}},"存":{"docs":{},"放":{"docs":{},"了":{"docs":{},"均":{"docs":{},"值":{"docs":{},"方":{"docs":{},"差":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{},"所":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","tf":0.010309278350515464}}}}}}}}}}}}}}}}}},"把":{"docs":{},"所":{"docs":{},"有":{"docs":{},"数":{"docs":{},"据":{"docs":{},"归":{"docs":{},"一":{"docs":{},"到":{"docs":{},"均":{"docs":{},"值":{"docs":{},"为":{"0":{"docs":{},"方":{"docs":{},"差":{"docs":{},"为":{"1":{"docs":{},"的":{"docs":{},"分":{"docs":{},"布":{"docs":{},"中":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}},"docs":{}}}}},"docs":{}}}}}}}}}}}},"描":{"docs":{},"述":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"分":{"docs":{},"布":{"docs":{},"范":{"docs":{},"围":{"docs":{},"（":{"docs":{},"标":{"docs":{},"准":{"docs":{},"差":{"docs":{},"）":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}},"绘":{"docs":{},"线":{"docs":{},"性":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}},"三":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}},"样":{"docs":{},"本":{"docs":{},"间":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"被":{"docs":{},"一":{"docs":{},"个":{"docs":{},"字":{"docs":{},"段":{"docs":{},"所":{"docs":{},"主":{"docs":{},"导":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}},"特":{"docs":{},"征":{"docs":{},"只":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"的":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"为":{"docs":{},"简":{"docs":{},"单":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"，":{"docs":{},"如":{"docs":{},"房":{"docs":{},"屋":{"docs":{},"价":{"docs":{},"格":{"docs":{"线性回归算法/1.线性回归算法简介.html":{"ref":"线性回归算法/1.线性回归算法简介.html","tf":0.125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"解":{"docs":{},"决":{"docs":{},"方":{"docs":{},"案":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}},"：":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}}},"办":{"docs":{},"法":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"其":{"docs":{},"实":{"docs":{},"就":{"docs":{},"是":{"docs":{},"：":{"docs":{},"我":{"docs":{},"们":{"docs":{},"需":{"docs":{},"要":{"docs":{},"将":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"分":{"docs":{},"为":{"docs":{},"三":{"docs":{},"部":{"docs":{},"分":{"docs":{},"，":{"docs":{},"这":{"docs":{},"三":{"docs":{},"部":{"docs":{},"分":{"docs":{},"分":{"docs":{},"别":{"docs":{},"是":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"验":{"docs":{},"证":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"。":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"高":{"docs":{},"方":{"docs":{},"差":{"docs":{},"的":{"docs":{},"通":{"docs":{},"常":{"docs":{},"手":{"docs":{},"段":{"docs":{},"：":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}},"释":{"docs":{},"方":{"docs":{},"差":{"docs":{},"的":{"docs":{},"比":{"docs":{},"例":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}},"适":{"docs":{},"用":{"docs":{},"于":{"docs":{},"分":{"docs":{},"布":{"docs":{},"有":{"docs":{},"明":{"docs":{},"显":{"docs":{},"边":{"docs":{},"界":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"；":{"docs":{},"受":{"docs":{},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{},"l":{"docs":{},"i":{"docs":{},"e":{"docs":{},"r":{"docs":{},"影":{"docs":{},"响":{"docs":{},"较":{"docs":{},"大":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}}}}}}}}}}}},"数":{"docs":{},"据":{"docs":{},"分":{"docs":{},"布":{"docs":{},"没":{"docs":{},"有":{"docs":{},"明":{"docs":{},"显":{"docs":{},"边":{"docs":{},"界":{"docs":{},"；":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"存":{"docs":{},"在":{"docs":{},"极":{"docs":{},"端":{"docs":{},"情":{"docs":{},"况":{"docs":{},"值":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}}}}}}}}}}}}},"：":{"docs":{},"将":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"同":{"docs":{},"一":{"docs":{},"尺":{"docs":{},"度":{"docs":{"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"ref":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","tf":0.0007369196757553427}}}}}}}}}}}}}}}},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"以":{"docs":{},"一":{"docs":{},"个":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"里":{"docs":{},"一":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"为":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"其":{"docs":{},"他":{"docs":{},"维":{"docs":{},"度":{"docs":{},"为":{"docs":{},"特":{"docs":{},"征":{"docs":{},"（":{"docs":{},"如":{"docs":{},"二":{"docs":{},"维":{"docs":{},"平":{"docs":{},"面":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"中":{"docs":{},"横":{"docs":{},"轴":{"docs":{},"为":{"docs":{},"特":{"docs":{},"征":{"docs":{},"，":{"docs":{},"纵":{"docs":{},"轴":{"docs":{},"为":{"docs":{},"结":{"docs":{},"果":{"docs":{},"）":{"docs":{},"，":{"docs":{},"无":{"docs":{},"数":{"docs":{},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{},"集":{"docs":{},"放":{"docs":{},"在":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"中":{"docs":{},"，":{"docs":{},"发":{"docs":{},"现":{"docs":{},"他":{"docs":{},"们":{"docs":{},"是":{"docs":{},"围":{"docs":{},"绕":{"docs":{},"着":{"docs":{},"一":{"docs":{},"条":{"docs":{},"执":{"docs":{},"行":{"docs":{},"分":{"docs":{},"布":{"docs":{},"。":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"期":{"docs":{},"望":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"寻":{"docs":{},"找":{"docs":{},"一":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"最":{"docs":{},"大":{"docs":{},"程":{"docs":{},"度":{"docs":{},"的":{"docs":{},"“":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"”":{"docs":{},"样":{"docs":{},"本":{"docs":{},"特":{"docs":{},"征":{"docs":{},"和":{"docs":{},"样":{"docs":{},"本":{"docs":{},"输":{"docs":{},"出":{"docs":{},"标":{"docs":{},"记":{"docs":{},"的":{"docs":{},"关":{"docs":{},"系":{"docs":{"线性回归算法/1.线性回归算法简介.html":{"ref":"线性回归算法/1.线性回归算法简介.html","tf":0.125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"中":{"docs":{},"的":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"的":{"docs":{},"实":{"docs":{},"现":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}}}}}}},"计":{"docs":{},"算":{"docs":{},"出":{"docs":{},"来":{"docs":{},"的":{"docs":{},"值":{"docs":{},"域":{"docs":{},"是":{"docs":{},"负":{"docs":{},"无":{"docs":{},"穷":{"docs":{},"到":{"docs":{},"正":{"docs":{},"无":{"docs":{},"穷":{"docs":{},"，":{"docs":{},"而":{"docs":{},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"得":{"docs":{},"出":{"docs":{},"来":{"docs":{},"的":{"docs":{},"p":{"docs":{},"是":{"docs":{},"只":{"docs":{},"取":{"0":{"docs":{},"到":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"个":{"docs":{},"值":{"docs":{},"的":{"docs":{},"。":{"docs":{},"这":{"docs":{},"使":{"docs":{},"得":{"docs":{},"我":{"docs":{},"们":{"docs":{},"不":{"docs":{},"能":{"docs":{},"直":{"docs":{},"接":{"docs":{},"使":{"docs":{},"用":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"，":{"docs":{},"单":{"docs":{},"单":{"docs":{},"从":{"docs":{},"应":{"docs":{},"用":{"docs":{},"的":{"docs":{},"角":{"docs":{},"度":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"这":{"docs":{},"样":{"docs":{},"做":{"docs":{},"不":{"docs":{},"够":{"docs":{},"好":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"值":{"docs":{},"域":{"docs":{},"是":{"docs":{},"有":{"docs":{},"限":{"docs":{},"制":{"docs":{},"的":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"或":{"docs":{},"者":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"出":{"docs":{},"来":{"docs":{},"的":{"docs":{},"直":{"docs":{},"线":{"docs":{},"或":{"docs":{},"者":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"肯":{"docs":{},"定":{"docs":{},"会":{"docs":{},"比":{"docs":{},"较":{"docs":{},"差":{"docs":{},"。":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"添":{"docs":{},"加":{"docs":{},"了":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"项":{"docs":{},"后":{"docs":{},"。":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"这":{"docs":{},"个":{"docs":{},"阶":{"docs":{},"数":{"docs":{},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"模":{"docs":{},"型":{"docs":{},"越":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"，":{"docs":{},"就":{"docs":{},"越":{"docs":{},"容":{"docs":{},"易":{"docs":{},"发":{"docs":{},"生":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"通":{"docs":{},"过":{"docs":{},"上":{"docs":{},"面":{"docs":{},"的":{"docs":{},"推":{"docs":{},"导":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"归":{"docs":{},"纳":{"docs":{},"出":{"docs":{},"一":{"docs":{},"类":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"基":{"docs":{},"本":{"docs":{},"思":{"docs":{},"路":{"docs":{},"，":{"docs":{},"如":{"docs":{},"下":{"docs":{},"图":{"docs":{},"；":{"docs":{},"其":{"docs":{},"中":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"是":{"docs":{},"计":{"docs":{},"算":{"docs":{},"期":{"docs":{},"望":{"docs":{},"值":{"docs":{},"和":{"docs":{},"预":{"docs":{},"测":{"docs":{},"值":{"docs":{},"的":{"docs":{},"差":{"docs":{},"值":{"docs":{},"，":{"docs":{},"期":{"docs":{},"望":{"docs":{},"其":{"docs":{},"差":{"docs":{},"值":{"docs":{},"（":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"损":{"docs":{},"失":{"docs":{},"）":{"docs":{},"越":{"docs":{},"来":{"docs":{},"越":{"docs":{},"小":{"docs":{},"，":{"docs":{},"而":{"docs":{},"效":{"docs":{},"用":{"docs":{},"函":{"docs":{},"数":{"docs":{},"则":{"docs":{},"是":{"docs":{},"描":{"docs":{},"述":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"度":{"docs":{},"，":{"docs":{},"期":{"docs":{},"望":{"docs":{},"契":{"docs":{},"合":{"docs":{},"度":{"docs":{},"越":{"docs":{},"来":{"docs":{},"越":{"docs":{},"好":{"docs":{"线性回归算法/1.线性回归算法简介.html":{"ref":"线性回归算法/1.线性回归算法简介.html","tf":0.125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"推":{"docs":{},"导":{"docs":{},"可":{"docs":{},"以":{"docs":{},"得":{"docs":{},"出":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}},"求":{"docs":{},"特":{"docs":{},"征":{"docs":{},"脸":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}},"这":{"docs":{},"个":{"docs":{},"方":{"docs":{},"法":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"取":{"docs":{},"出":{"docs":{},"至":{"docs":{},"少":{"docs":{},"有":{"6":{"0":{"docs":{},"张":{"docs":{},"脸":{"docs":{},"的":{"docs":{},"人":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}},"样":{"docs":{},"一":{"docs":{},"个":{"docs":{},"例":{"docs":{},"子":{"docs":{},"，":{"docs":{},"再":{"docs":{},"次":{"docs":{},"印":{"docs":{},"证":{"docs":{},"了":{"docs":{},"对":{"docs":{},"于":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"算":{"docs":{},"法":{"docs":{},"来":{"docs":{},"说":{"docs":{},"k":{"docs":{},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"模":{"docs":{},"型":{"docs":{},"越":{"docs":{},"简":{"docs":{},"单":{"docs":{},"，":{"docs":{},"对":{"docs":{},"于":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"的":{"docs":{},"划":{"docs":{},"分":{"docs":{},"就":{"docs":{},"是":{"docs":{},"决":{"docs":{},"策":{"docs":{},"越":{"docs":{},"规":{"docs":{},"整":{"docs":{},"，":{"docs":{},"分":{"docs":{},"块":{"docs":{},"越":{"docs":{},"明":{"docs":{},"显":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"矩":{"docs":{},"阵":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"很":{"docs":{},"清":{"docs":{},"晰":{"docs":{},"的":{"docs":{},"发":{"docs":{},"现":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"更":{"docs":{},"重":{"docs":{},"要":{"docs":{},"的":{"docs":{},"是":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"出":{"docs":{},"具":{"docs":{},"体":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{},"类":{"docs":{},"型":{"docs":{},"，":{"docs":{},"比":{"docs":{},"如":{"docs":{},"有":{"docs":{},"很":{"docs":{},"多":{"docs":{},"的":{"8":{"docs":{},"我":{"docs":{},"们":{"docs":{},"把":{"docs":{},"他":{"docs":{},"规":{"docs":{},"约":{"docs":{},"为":{"docs":{},"了":{"1":{"docs":{},"，":{"docs":{},"有":{"docs":{},"很":{"docs":{},"多":{"1":{"docs":{},"我":{"docs":{},"们":{"docs":{},"规":{"docs":{},"约":{"docs":{},"成":{"docs":{},"了":{"8":{"docs":{},"，":{"docs":{},"有":{"docs":{},"了":{"docs":{},"这":{"docs":{},"些":{"docs":{},"提":{"docs":{},"示":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"进":{"docs":{},"一":{"docs":{},"步":{"docs":{},"改":{"docs":{},"进":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"了":{"docs":{},"。":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}},"docs":{}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"观":{"docs":{},"察":{"docs":{},"两":{"docs":{},"组":{"docs":{},"调":{"docs":{},"参":{"docs":{},"过":{"docs":{},"程":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"可":{"docs":{},"以":{"docs":{},"发":{"docs":{},"现":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}},"上":{"docs":{},"面":{"docs":{},"的":{"docs":{},"图":{"docs":{},"可":{"docs":{},"以":{"docs":{},"发":{"docs":{},"现":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"新":{"docs":{},"来":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"，":{"docs":{},"落":{"docs":{},"在":{"docs":{},"了":{"docs":{},"直":{"docs":{},"线":{"docs":{},"（":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"）":{"docs":{},"的":{"docs":{},"上":{"docs":{},"方":{"docs":{},"，":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"x":{"1":{"docs":{},"θ":{"1":{"docs":{},"+":{"docs":{},"x":{"2":{"docs":{},"θ":{"2":{"docs":{},"+":{"docs":{},"θ":{"docs":{},">":{"0":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"p":{"docs":{},">":{"0":{"docs":{},".":{"5":{"docs":{},",":{"docs":{},"那":{"docs":{},"么":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"将":{"docs":{},"他":{"docs":{},"分":{"docs":{},"类":{"docs":{},"为":{"1":{"docs":{},"；":{"docs":{},"反":{"docs":{},"之":{"docs":{},"，":{"docs":{},"则":{"docs":{},"分":{"docs":{},"类":{"docs":{},"为":{"0":{"docs":{},"。":{"docs":{},"这":{"docs":{},"就":{"docs":{},"是":{"docs":{},"决":{"docs":{},"策":{"docs":{},"曲":{"docs":{},"线":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}},"docs":{}}}}}}}}}},"docs":{}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}},"docs":{}}}}},"docs":{}}},"docs":{}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"岭":{"docs":{},"回":{"docs":{},"归":{"docs":{},"，":{"docs":{},"使":{"docs":{},"得":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"均":{"docs":{},"方":{"docs":{},"误":{"docs":{},"差":{"docs":{},"小":{"docs":{},"了":{"docs":{},"非":{"docs":{},"常":{"docs":{},"多":{"docs":{},",":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"也":{"docs":{},"缓":{"docs":{},"和":{"docs":{},"了":{"docs":{},"非":{"docs":{},"常":{"docs":{},"多":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"绘":{"docs":{},"制":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"出":{"docs":{},"，":{"docs":{},"绘":{"docs":{},"制":{"docs":{},"出":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"是":{"docs":{},"非":{"docs":{},"常":{"docs":{},"不":{"docs":{},"规":{"docs":{},"则":{"docs":{},"的":{"docs":{},"，":{"docs":{},"甚":{"docs":{},"至":{"docs":{},"在":{"docs":{},"黄":{"docs":{},"色":{"docs":{},"的":{"docs":{},"部":{"docs":{},"分":{"docs":{},"还":{"docs":{},"掺":{"docs":{},"杂":{"docs":{},"着":{"docs":{},"一":{"docs":{},"些":{"docs":{},"蓝":{"docs":{},"色":{"docs":{},"的":{"docs":{},"绿":{"docs":{},"色":{"docs":{},"的":{"docs":{},"点":{"docs":{},"。":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"构":{"docs":{},"建":{"docs":{},"新":{"docs":{},"的":{"docs":{},"y":{"docs":{},"_":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"，":{"docs":{},"来":{"docs":{},"实":{"docs":{},"现":{"docs":{},"修":{"docs":{},"改":{"docs":{},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"为":{"5":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"曲":{"docs":{},"线":{"docs":{},"再":{"docs":{},"次":{"docs":{},"印":{"docs":{},"证":{"docs":{},"了":{"docs":{},"，":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"是":{"docs":{},"相":{"docs":{},"互":{"docs":{},"制":{"docs":{},"约":{"docs":{},"的":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"急":{"docs":{},"剧":{"docs":{},"下":{"docs":{},"降":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"点":{"docs":{},"，":{"docs":{},"可":{"docs":{},"能":{"docs":{},"就":{"docs":{},"是":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"平":{"docs":{},"衡":{"docs":{},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"位":{"docs":{},"置":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"传":{"docs":{},"入":{"docs":{},"a":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"a":{"docs":{},"g":{"docs":{},"e":{"docs":{},"参":{"docs":{},"数":{"docs":{},"可":{"docs":{},"以":{"docs":{},"让":{"docs":{},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"处":{"docs":{},"理":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"常":{"docs":{},"对":{"docs":{},"于":{"docs":{},"这":{"docs":{},"样":{"docs":{},"一":{"docs":{},"个":{"docs":{},"图":{"docs":{},"，":{"docs":{},"会":{"docs":{},"有":{"docs":{},"两":{"docs":{},"根":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"：":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}},"+":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.01098901098901099},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.012048192771084338},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909},"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.024691358024691357},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.010101010101010102},"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364},"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.024},"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.011428571428571429},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.005714285714285714},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.011111111111111112},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0196078431372549},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.008368200836820083},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.00819672131147541},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.03488372093023256}},"=":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}},"z":{"docs":{},"i":{"docs":{},"p":{"docs":{},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}},"z":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}},"±":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.015625}}},"µ":{"docs":{},"s":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}},",":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}},"向":{"docs":{},"量":{"docs":{},"化":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}},"实":{"docs":{},"现":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"测":{"docs":{},"试":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}},"改":{"docs":{},"进":{"docs":{},"n":{"docs":{},"u":{"docs":{},"m":{"docs":{},",":{"docs":{},"d":{"docs":{},"的":{"docs":{},"计":{"docs":{},"算":{"docs":{},"方":{"docs":{},"法":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}}}}}}}}},"，":{"docs":{},"x":{"docs":{},".":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{},"(":{"docs":{},"w":{"docs":{},")":{"docs":{},"为":{"docs":{},"m":{"docs":{},"*":{"1":{"docs":{},"的":{"docs":{},"向":{"docs":{},"量":{"docs":{},"，":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"后":{"docs":{},"变":{"docs":{},"成":{"docs":{},"了":{"1":{"docs":{},"*":{"docs":{},"m":{"docs":{},"的":{"docs":{},"列":{"docs":{},"向":{"docs":{},"量":{"docs":{},"，":{"docs":{},"再":{"docs":{},"乘":{"docs":{},"以":{"docs":{},"w":{"docs":{},"（":{"docs":{},"方":{"docs":{},"向":{"docs":{},"）":{"docs":{},"就":{"docs":{},"是":{"docs":{},"x":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"值":{"docs":{},"在":{"docs":{},"w":{"docs":{},"上":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}},"模":{"docs":{},"型":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.0078125},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}},"没":{"docs":{},"有":{"docs":{},"完":{"docs":{},"全":{"docs":{},"的":{"docs":{},"学":{"docs":{},"到":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"中":{"docs":{},"心":{"docs":{},"，":{"docs":{},"而":{"docs":{},"学":{"docs":{},"习":{"docs":{},"到":{"docs":{},"了":{"docs":{},"很":{"docs":{},"多":{"docs":{},"噪":{"docs":{},"音":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}}}}}}}}}}}}}},"误":{"docs":{},"差":{"docs":{},"=":{"docs":{},"偏":{"docs":{},"差":{"docs":{},"（":{"docs":{},"b":{"docs":{},"i":{"docs":{},"a":{"docs":{},"s":{"docs":{},"）":{"docs":{},"均":{"docs":{},"差":{"docs":{},"(":{"docs":{},"v":{"docs":{},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},")":{"docs":{},"+":{"docs":{},"不":{"docs":{},"可":{"docs":{},"避":{"docs":{},"免":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"样":{"docs":{},"本":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"基":{"docs":{},"本":{"docs":{},"原":{"docs":{},"理":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}},"需":{"docs":{},"要":{"docs":{},"做":{"docs":{},"的":{"docs":{},"事":{"docs":{},"情":{"docs":{},"就":{"docs":{},"是":{"docs":{},"限":{"docs":{},"制":{"docs":{},"这":{"docs":{},"些":{"docs":{},"系":{"docs":{},"数":{"docs":{},"的":{"docs":{},"大":{"docs":{},"小":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}},"泛":{"docs":{},"化":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"举":{"docs":{},"例":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"在":{"docs":{},"考":{"docs":{},"试":{"docs":{},"前":{"docs":{},"会":{"docs":{},"做":{"docs":{},"很":{"docs":{},"多":{"docs":{},"练":{"docs":{},"习":{"docs":{},"题":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"做":{"docs":{},"练":{"docs":{},"习":{"docs":{},"题":{"docs":{},"不":{"docs":{},"是":{"docs":{},"为":{"docs":{},"了":{"docs":{},"把":{"docs":{},"全":{"docs":{},"部":{"docs":{},"的":{"docs":{},"练":{"docs":{},"习":{"docs":{},"题":{"docs":{},"（":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"）":{"docs":{},"都":{"docs":{},"得":{"docs":{},"到":{"docs":{},"满":{"docs":{},"分":{"docs":{},"，":{"docs":{},"而":{"docs":{},"是":{"docs":{},"为":{"docs":{},"了":{"docs":{},"在":{"docs":{},"最":{"docs":{},"后":{"docs":{},"的":{"docs":{},"那":{"docs":{},"一":{"docs":{},"场":{"docs":{},"考":{"docs":{},"试":{"docs":{},"（":{"docs":{},"真":{"docs":{},"实":{"docs":{},"数":{"docs":{},"据":{"docs":{},"）":{"docs":{},"中":{"docs":{},"得":{"docs":{},"到":{"docs":{},"满":{"docs":{},"分":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"能":{"docs":{},"力":{"docs":{},"并":{"docs":{},"没":{"docs":{},"有":{"docs":{},"降":{"docs":{},"低":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}},"返":{"docs":{},"回":{"docs":{},"能":{"docs":{},"力":{"docs":{},"变":{"docs":{},"脆":{"docs":{},"。":{"docs":{},"因":{"docs":{},"为":{"docs":{},"出":{"docs":{},"现":{"docs":{},"了":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}},"拟":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"测":{"docs":{},"试":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}},"测":{"docs":{},"试":{"docs":{},"用":{"docs":{},"例":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}},"使":{"docs":{},"用":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"进":{"docs":{},"行":{"docs":{},"划":{"docs":{},"分":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415}}}}}}}}},"基":{"docs":{},"尼":{"docs":{},"系":{"docs":{},"数":{"docs":{},"进":{"docs":{},"行":{"docs":{},"划":{"docs":{},"分":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}},"计":{"docs":{},"算":{"docs":{},"分":{"docs":{},"子":{"docs":{},"分":{"docs":{},"母":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}},"参":{"docs":{},"数":{"docs":{},"a":{"docs":{},"和":{"docs":{},"b":{"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.00390625}}}}}}},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"，":{"docs":{},"t":{"1":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"docs":{}}}},"曲":{"docs":{},"线":{"docs":{},"数":{"docs":{},"据":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}},"方":{"docs":{},"式":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"的":{"docs":{},"面":{"docs":{},"积":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}},"每":{"docs":{},"两":{"docs":{},"个":{"docs":{},"值":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"从":{"1":{"docs":{},"开":{"docs":{},"始":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}},"docs":{}}}}}}}}}}}}}}}}},"输":{"docs":{},"出":{"1":{"0":{"0":{"0":{"1":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"5":{"docs":{},".":{"1":{"8":{"2":{"0":{"6":{"7":{"1":{"3":{"8":{"5":{"0":{"9":{"4":{"3":{"8":{"6":{"docs":{},"e":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{"线性回归算法/2.简单线性回归的实现.html":{"ref":"线性回归算法/2.简单线性回归的实现.html","tf":0.01171875},"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.016},"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.013986013986013986}},"结":{"docs":{},"果":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}},"平":{"docs":{},"方":{"docs":{},"累":{"docs":{},"加":{"docs":{},"后":{"docs":{},"再":{"docs":{},"开":{"docs":{},"根":{"docs":{},"号":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"某":{"docs":{},"些":{"docs":{},"预":{"docs":{},"测":{"docs":{},"结":{"docs":{},"果":{"docs":{},"和":{"docs":{},"真":{"docs":{},"实":{"docs":{},"结":{"docs":{},"果":{"docs":{},"相":{"docs":{},"差":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"r":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"会":{"docs":{},"相":{"docs":{},"对":{"docs":{},"变":{"docs":{},"大":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"r":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"有":{"docs":{},"放":{"docs":{},"大":{"docs":{},"误":{"docs":{},"差":{"docs":{},"的":{"docs":{},"趋":{"docs":{},"势":{"docs":{},"，":{"docs":{},"而":{"docs":{},"m":{"docs":{},"a":{"docs":{},"e":{"docs":{},"没":{"docs":{},"有":{"docs":{},"，":{"docs":{},"他":{"docs":{},"直":{"docs":{},"接":{"docs":{},"就":{"docs":{},"反":{"docs":{},"应":{"docs":{},"的":{"docs":{},"是":{"docs":{},"预":{"docs":{},"测":{"docs":{},"结":{"docs":{},"果":{"docs":{},"和":{"docs":{},"真":{"docs":{},"实":{"docs":{},"结":{"docs":{},"果":{"docs":{},"直":{"docs":{},"接":{"docs":{},"的":{"docs":{},"差":{"docs":{},"距":{"docs":{},"，":{"docs":{},"正":{"docs":{},"因":{"docs":{},"如":{"docs":{},"此":{"docs":{},"，":{"docs":{},"从":{"docs":{},"某":{"docs":{},"种":{"docs":{},"程":{"docs":{},"度":{"docs":{},"上":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"想":{"docs":{},"办":{"docs":{},"法":{"docs":{},"我":{"docs":{},"们":{"docs":{},"让":{"docs":{},"r":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"变":{"docs":{},"的":{"docs":{},"更":{"docs":{},"小":{"docs":{},"小":{"docs":{},"对":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"来":{"docs":{},"说":{"docs":{},"比":{"docs":{},"较":{"docs":{},"有":{"docs":{},"意":{"docs":{},"义":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"这":{"docs":{},"意":{"docs":{},"味":{"docs":{},"着":{"docs":{},"整":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{},"中":{"docs":{},"，":{"docs":{},"那":{"docs":{},"个":{"docs":{},"最":{"docs":{},"值":{"docs":{},"相":{"docs":{},"对":{"docs":{},"比":{"docs":{},"较":{"docs":{},"小":{"docs":{},"，":{"docs":{},"而":{"docs":{},"且":{"docs":{},"我":{"docs":{},"们":{"docs":{},"之":{"docs":{},"前":{"docs":{},"训":{"docs":{},"练":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"目":{"docs":{},"标":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"r":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"根":{"docs":{},"号":{"docs":{},"里":{"docs":{},"面":{"1":{"docs":{},"/":{"docs":{},"m":{"docs":{},"的":{"docs":{},"这":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"，":{"docs":{},"而":{"docs":{},"这":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"的":{"docs":{},"本":{"docs":{},"质":{"docs":{},"和":{"docs":{},"优":{"docs":{},"化":{"docs":{},"r":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"是":{"docs":{},"一":{"docs":{},"样":{"docs":{},"的":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"均":{"docs":{},"大":{"docs":{},"约":{"docs":{},"有":{"3":{"7":{"docs":{},"%":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"没":{"docs":{},"有":{"docs":{},"取":{"docs":{},"到":{"docs":{},"。":{"docs":{},"这":{"docs":{},"些":{"docs":{},"样":{"docs":{},"本":{"docs":{},"就":{"docs":{},"叫":{"docs":{},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}},"衡":{"docs":{},"量":{"docs":{},"标":{"docs":{},"准":{"docs":{"线性回归算法/3.衡量线性回归算法的指标.html":{"ref":"线性回归算法/3.衡量线性回归算法的指标.html","tf":0.014705882352941176}}}}}},"和":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.03773584905660377},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.025},"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":1.6782945736434107}},"我":{"docs":{},"们":{"docs":{},"上":{"docs":{},"面":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"b":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}},"关":{"docs":{},"于":{"docs":{},"b":{"docs":{},"a":{"docs":{},"g":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"的":{"docs":{},"更":{"docs":{},"多":{"docs":{},"讨":{"docs":{},"论":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":2.007751937984496}}}}}}}}}}}}}}}}},"意":{"docs":{},"义":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886}}}},"确":{"docs":{},"定":{"docs":{},"当":{"docs":{},"前":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{},"\"":{"docs":{},"\"":{"docs":{},"\"":{"docs":{"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"ref":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","tf":0.018867924528301886},"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}}}}},":":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.005813953488372093}},"p":{"docs":{},"a":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.03614457831325301},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.08}}}}}}},"r":{"docs":{},"e":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{},":":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.012048192771084338},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.024}}}}}}}}}},"h":{"docs":{},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},"s":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"b":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},".":{"docs":{},"c":{"docs":{},"s":{"docs":{},"d":{"docs":{},"n":{"docs":{},".":{"docs":{},"n":{"docs":{},"e":{"docs":{},"t":{"docs":{},"/":{"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"a":{"docs":{},"d":{"docs":{},"l":{"docs":{},"x":{"5":{"3":{"docs":{},"/":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"l":{"docs":{},"e":{"docs":{},"/":{"docs":{},"d":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{},"s":{"docs":{},"/":{"5":{"0":{"8":{"4":{"9":{"9":{"4":{"1":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"w":{"docs":{},"n":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},".":{"docs":{},"f":{"docs":{},"i":{"docs":{},"g":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},"f":{"docs":{},"i":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"/":{"5":{"9":{"7":{"6":{"0":{"0":{"6":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},"9":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},"docs":{}},"1":{"2":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},"5":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"s":{"docs":{},"c":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{},"t":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}}},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757},"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}},"|":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}},"|":{"docs":{},"x":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"j":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"|":{"docs":{},"|":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.013986013986013986}}}}}}}}}}}}}}}}},"θ":{"docs":{},"向":{"docs":{},"量":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}},"求":{"docs":{},"和":{"docs":{},"的":{"docs":{},"系":{"docs":{},"数":{"docs":{},"二":{"docs":{},"分":{"docs":{},"之":{"docs":{},"一":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"惯":{"docs":{},"例":{"docs":{},"，":{"docs":{},"加":{"docs":{},"不":{"docs":{},"加":{"docs":{},"都":{"docs":{},"可":{"docs":{},"以":{"docs":{},"，":{"docs":{},"加":{"docs":{},"上":{"docs":{},"的":{"docs":{},"原":{"docs":{},"因":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"，":{"docs":{},"将":{"docs":{},"来":{"docs":{},"对":{"docs":{},"θ":{"2":{"docs":{},">":{"docs":{},"求":{"docs":{},"导":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"可":{"docs":{},"以":{"docs":{},"抵":{"docs":{},"消":{"docs":{},"系":{"docs":{},"数":{"2":{"docs":{},"，":{"docs":{},"方":{"docs":{},"便":{"docs":{},"计":{"docs":{},"算":{"docs":{},"。":{"docs":{},"不":{"docs":{},"要":{"docs":{},"也":{"docs":{},"是":{"docs":{},"可":{"docs":{},"以":{"docs":{},"的":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"同":{"docs":{},"样":{"docs":{},"行":{"docs":{},"数":{"docs":{},"的":{"docs":{},"，":{"docs":{},"只":{"docs":{},"有":{"docs":{},"一":{"docs":{},"列":{"docs":{},"的":{"docs":{},"全":{"docs":{},"是":{"1":{"docs":{},"的":{"docs":{},"矩":{"docs":{},"阵":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}},"docs":{}}}}}}}}}}}}}},"截":{"docs":{},"距":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}},"拼":{"docs":{},"接":{"docs":{},"矩":{"docs":{},"阵":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}},"推":{"docs":{},"导":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}},"过":{"docs":{},"程":{"docs":{"梯度下降法/3.多元线性回归中的梯度下降法.html":{"ref":"梯度下降法/3.多元线性回归中的梯度下降法.html","tf":0.2}},"参":{"docs":{},"考":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}},"构":{"docs":{},"造":{"docs":{},"一":{"docs":{},"个":{"docs":{},"和":{"docs":{},"x":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}},"两":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"之":{"docs":{},"间":{"docs":{},"有":{"docs":{},"基":{"docs":{},"本":{"docs":{},"线":{"docs":{},"性":{"docs":{},"关":{"docs":{},"系":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"使":{"docs":{},"得":{"docs":{},"我":{"docs":{},"们":{"docs":{},"降":{"docs":{},"维":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"更":{"docs":{},"加":{"docs":{},"明":{"docs":{},"显":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"建":{"docs":{},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}},"矩":{"docs":{},"阵":{"docs":{},"点":{"docs":{},"乘":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}},"中":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"行":{"docs":{},"的":{"docs":{},"数":{"docs":{},"字":{"docs":{},"都":{"docs":{},"会":{"docs":{},"除":{"docs":{},"以":{"docs":{},"这":{"docs":{},"一":{"docs":{},"行":{"docs":{},"的":{"docs":{},"数":{"docs":{},"字":{"docs":{},"和":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"百":{"docs":{},"分":{"docs":{},"比":{"docs":{},"矩":{"docs":{},"阵":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"系":{"docs":{},"数":{"docs":{},"向":{"docs":{},"量":{"docs":{},"（":{"docs":{},"θ":{"1":{"docs":{},",":{"docs":{},"θ":{"2":{"docs":{},",":{"docs":{},".":{"docs":{},".":{"docs":{},".":{"docs":{},".":{"docs":{},".":{"docs":{},"θ":{"docs":{},"n":{"docs":{},"）":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}}}}}}}}}},"docs":{}}}},"docs":{}}}}}}},"补":{"docs":{},"充":{"docs":{},"（":{"docs":{},"一":{"docs":{},"个":{"1":{"docs":{},"x":{"docs":{},"m":{"docs":{},"的":{"docs":{},"行":{"docs":{},"向":{"docs":{},"量":{"docs":{},"乘":{"docs":{},"以":{"docs":{},"一":{"docs":{},"个":{"docs":{},"m":{"docs":{},"x":{"1":{"docs":{},"的":{"docs":{},"列":{"docs":{},"向":{"docs":{},"量":{"docs":{},"等":{"docs":{},"于":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"）":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}},"docs":{}}},"矩":{"docs":{},"阵":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"：":{"docs":{},"a":{"docs":{},"（":{"docs":{},"m":{"docs":{},"行":{"docs":{},"）":{"docs":{},"·":{"docs":{},"b":{"docs":{},"（":{"docs":{},"n":{"docs":{},"列":{"docs":{},"）":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}}}}}}}}},"运":{"docs":{},"用":{"docs":{},"了":{"docs":{},"c":{"docs":{},"v":{"docs":{},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{"线性回归算法/5.多元线性回归.html":{"ref":"线性回归算法/5.多元线性回归.html","tf":0.0029069767441860465}}}}}}}}}}}}}},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{"梯度下降法/":{"ref":"梯度下降法/","tf":1},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}},"的":{"docs":{},"初":{"docs":{},"始":{"docs":{},"点":{"docs":{},"也":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}}}}}}}}}}},"简":{"docs":{},"介":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}},"封":{"docs":{},"装":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}}},"模":{"docs":{},"拟":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}},"调":{"docs":{},"试":{"docs":{},"的":{"docs":{},"原":{"docs":{},"理":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}},"实":{"docs":{},"现":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}},"上":{"docs":{},"升":{"docs":{},"法":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008},"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}},"η":{"docs":{},"太":{"docs":{},"大":{"docs":{},"，":{"docs":{},"甚":{"docs":{},"至":{"docs":{},"导":{"docs":{},"致":{"docs":{},"不":{"docs":{},"收":{"docs":{},"敛":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}}}}}}}}},"小":{"docs":{},"，":{"docs":{},"会":{"docs":{},"减":{"docs":{},"慢":{"docs":{},"收":{"docs":{},"敛":{"docs":{},"学":{"docs":{},"习":{"docs":{},"速":{"docs":{},"度":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}}}}}}}}}}}}},"导":{"docs":{},"数":{"docs":{},"代":{"docs":{},"表":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"单":{"docs":{},"位":{"docs":{},"变":{"docs":{},"化":{"docs":{},"时":{"docs":{},"，":{"docs":{},"j":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"变":{"docs":{},"化":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}}}}}}}}}}}}}}}}}}}}},"并":{"docs":{},"不":{"docs":{},"是":{"docs":{},"所":{"docs":{},"有":{"docs":{},"函":{"docs":{},"数":{"docs":{},"都":{"docs":{},"有":{"docs":{},"唯":{"docs":{},"一":{"docs":{},"的":{"docs":{},"极":{"docs":{},"值":{"docs":{},"点":{"docs":{"梯度下降法/1.梯度下降法简介.html":{"ref":"梯度下降法/1.梯度下降法简介.html","tf":0.07692307692307693}}}}}}}}}}}}}}}}},"下":{"docs":{},"面":{"docs":{},"来":{"docs":{},"看":{"docs":{},"一":{"docs":{},"下":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"取":{"docs":{},"较":{"docs":{},"大":{"docs":{},"值":{"1":{"docs":{},".":{"1":{"docs":{},"，":{"docs":{},"会":{"docs":{},"出":{"docs":{},"现":{"docs":{},"什":{"docs":{},"么":{"docs":{},"情":{"docs":{},"况":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}},"下":{"docs":{},"二":{"docs":{},"者":{"docs":{},"的":{"docs":{},"对":{"docs":{},"比":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}},"尝":{"docs":{},"试":{"docs":{},"真":{"docs":{},"正":{"docs":{},"还":{"docs":{},"原":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"（":{"docs":{},"构":{"docs":{},"造":{"docs":{},"均":{"docs":{},"匀":{"docs":{},"分":{"docs":{},"布":{"docs":{},"的":{"docs":{},"原":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"）":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}},"图":{"docs":{},"分":{"docs":{},"别":{"docs":{},"是":{"docs":{},"扔":{"docs":{},"掉":{"docs":{},"了":{"docs":{},"特":{"docs":{},"征":{"docs":{},"一":{"docs":{},"和":{"docs":{},"特":{"docs":{},"征":{"docs":{},"二":{"docs":{},"的":{"docs":{},"两":{"docs":{},"种":{"docs":{},"方":{"docs":{},"案":{"docs":{},"，":{"docs":{},"很":{"docs":{},"明":{"docs":{},"显":{"docs":{},"右":{"docs":{},"边":{"docs":{},"这":{"docs":{},"种":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"会":{"docs":{},"更":{"docs":{},"好":{"docs":{},"一":{"docs":{},"些":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"访":{"docs":{},"问":{"docs":{},"二":{"docs":{},"扔":{"docs":{},"掉":{"docs":{},"特":{"docs":{},"征":{"docs":{},"二":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"点":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"分":{"docs":{},"布":{"docs":{},"情":{"docs":{},"况":{"docs":{},"更":{"docs":{},"接":{"docs":{},"近":{"docs":{},"与":{"docs":{},"原":{"docs":{},"图":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"这":{"docs":{},"不":{"docs":{},"是":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"每":{"docs":{},"个":{"docs":{},"颜":{"docs":{},"色":{"docs":{},"代":{"docs":{},"表":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"字":{"docs":{},"在":{"docs":{},"降":{"docs":{},"维":{"docs":{},"到":{"docs":{},"二":{"docs":{},"维":{"docs":{},"空":{"docs":{},"间":{"docs":{},"中":{"docs":{},"的":{"docs":{},"分":{"docs":{},"布":{"docs":{},"情":{"docs":{},"况":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"之":{"docs":{},"前":{"docs":{},"使":{"docs":{},"用":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"例":{"docs":{},"子":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"到":{"docs":{},"这":{"docs":{},"条":{"docs":{},"模":{"docs":{},"型":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"弯":{"docs":{},"曲":{"docs":{},"，":{"docs":{},"而":{"docs":{},"且":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"陡":{"docs":{},"峭":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"想":{"docs":{},"象":{"docs":{},"这":{"docs":{},"条":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"的":{"docs":{},"一":{"docs":{},"些":{"docs":{},"θ":{"docs":{},"系":{"docs":{},"数":{"docs":{},"会":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"大":{"docs":{},"。":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"保":{"docs":{},"存":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"的":{"docs":{},"变":{"docs":{},"化":{"docs":{},"值":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576}}}}}}}}}}}}},"精":{"docs":{},"度":{"docs":{"梯度下降法/2.梯度下降法模拟.html":{"ref":"梯度下降法/2.梯度下降法模拟.html","tf":0.007575757575757576},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495},"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},"准":{"docs":{},"率":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":3.3405275779376495}},"=":{"4":{"0":{"docs":{},"%":{"docs":{},"：":{"docs":{},"我":{"docs":{},"们":{"docs":{},"没":{"docs":{},"做":{"1":{"0":{"0":{"docs":{},"次":{"docs":{},"患":{"docs":{},"病":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"会":{"docs":{},"有":{"4":{"0":{"docs":{},"次":{"docs":{},"是":{"docs":{},"对":{"docs":{},"的":{"docs":{"评价分类结果/9.2 精准率和召回率.html":{"ref":"评价分类结果/9.2 精准率和召回率.html","tf":0.1111111111111111}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}},"docs":{}},"docs":{}},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{"评价分类结果/9.2 精准率和召回率.html":{"ref":"评价分类结果/9.2 精准率和召回率.html","tf":5.111111111111111}},"是":{"docs":{},"两":{"docs":{},"个":{"docs":{},"指":{"docs":{},"标":{"docs":{},"，":{"docs":{},"有":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"高":{"docs":{},"一":{"docs":{},"些":{"docs":{},"，":{"docs":{},"有":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"高":{"docs":{},"一":{"docs":{},"些":{"docs":{},"，":{"docs":{},"在":{"docs":{},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"应":{"docs":{},"该":{"docs":{},"怎":{"docs":{},"么":{"docs":{},"解":{"docs":{},"读":{"docs":{},"这":{"docs":{},"个":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"呢":{"docs":{},"？":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"这":{"docs":{},"两":{"docs":{},"个":{"docs":{},"指":{"docs":{},"标":{"docs":{},"是":{"docs":{},"互":{"docs":{},"相":{"docs":{},"矛":{"docs":{},"盾":{"docs":{},"的":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"要":{"docs":{},"找":{"docs":{},"到":{"docs":{},"的":{"docs":{},"是":{"docs":{},"这":{"docs":{},"两":{"docs":{},"个":{"docs":{},"指":{"docs":{},"标":{"docs":{},"直":{"docs":{},"接":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"平":{"docs":{},"衡":{"docs":{},"。":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"：":{"docs":{},"预":{"docs":{},"测":{"docs":{},"数":{"docs":{},"据":{"docs":{},"为":{"1":{"docs":{},"，":{"docs":{},"预":{"docs":{},"测":{"docs":{},"对":{"docs":{},"了":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{"评价分类结果/9.2 精准率和召回率.html":{"ref":"评价分类结果/9.2 精准率和召回率.html","tf":0.1111111111111111}}}}}}}}}}},"docs":{}}}}}}}}}},"上":{"docs":{},"面":{"docs":{},"推":{"docs":{},"导":{"docs":{},"出":{"docs":{},"的":{"docs":{},"式":{"docs":{},"子":{"docs":{},"的":{"docs":{},"大":{"docs":{},"小":{"docs":{},"是":{"docs":{},"和":{"docs":{},"样":{"docs":{},"本":{"docs":{},"数":{"docs":{},"有":{"docs":{},"关":{"docs":{},"的":{"docs":{},"，":{"docs":{},"m":{"docs":{},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"结":{"docs":{},"果":{"docs":{},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"这":{"docs":{},"是":{"docs":{},"不":{"docs":{},"合":{"docs":{},"理":{"docs":{},"的":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"希":{"docs":{},"望":{"docs":{},"和":{"docs":{},"m":{"docs":{},"无":{"docs":{},"关":{"docs":{"梯度下降法/3.多元线性回归中的梯度下降法.html":{"ref":"梯度下降法/3.多元线性回归中的梯度下降法.html","tf":0.2}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"修":{"docs":{},"改":{"docs":{},"之":{"docs":{},"前":{"docs":{},"的":{"docs":{},"求":{"docs":{},"导":{"docs":{},"函":{"docs":{},"数":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}}}}}},"定":{"docs":{},"义":{"docs":{},"截":{"docs":{},"距":{"docs":{},"为":{"4":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}},"docs":{}}}},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"函":{"docs":{},"数":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}},"绘":{"docs":{},"图":{"docs":{},"模":{"docs":{},"型":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}},"斜":{"docs":{},"率":{"docs":{},"为":{"3":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}},"docs":{}}}},"比":{"docs":{},"较":{"docs":{},"笨":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"实":{"docs":{},"现":{"docs":{"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"ref":"梯度下降法/4.线性回归中的梯度下降法的实现.html","tf":0.005494505494505495}}}}}}}}},"如":{"docs":{},"如":{"docs":{},"果":{"docs":{},"只":{"docs":{},"是":{"docs":{},"区":{"docs":{},"分":{"docs":{},"蓝":{"docs":{},"色":{"docs":{},"的":{"docs":{},"数":{"docs":{},"字":{"docs":{},"和":{"docs":{},"紫":{"docs":{},"色":{"docs":{},"的":{"docs":{},"数":{"docs":{},"字":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"使":{"docs":{},"用":{"docs":{},"二":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"就":{"docs":{},"足":{"docs":{},"够":{"docs":{},"了":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"s":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.018691588785046728},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.017142857142857144},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02358490566037736},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}},"e":{"docs":{},"r":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.010869565217391304},"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.009708737864077669},"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.025974025974025976},"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.015503875968992248}},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},":":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.018691588785046728},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.017142857142857144},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.014285714285714285},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.014005602240896359},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.009852216748768473},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.02358490566037736},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.024752475247524754}}}}}}}}}}}}},"p":{"docs":{},"_":{"docs":{},"i":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}},"去":{"docs":{},"x":{"docs":{},"_":{"docs":{},"b":{"docs":{},",":{"docs":{},"i":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}},"批":{"docs":{},"量":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.016}},"带":{"docs":{},"来":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"问":{"docs":{},"题":{"docs":{},"是":{"docs":{},"η":{"docs":{},"的":{"docs":{},"值":{"docs":{},"需":{"docs":{},"要":{"docs":{},"设":{"docs":{},"置":{"docs":{},"的":{"docs":{},"比":{"docs":{},"较":{"docs":{},"小":{"docs":{},"，":{"docs":{},"在":{"docs":{},"样":{"docs":{},"本":{"docs":{},"数":{"docs":{},"比":{"docs":{},"较":{"docs":{},"多":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"导":{"docs":{},"致":{"docs":{},"不":{"docs":{},"是":{"docs":{},"速":{"docs":{},"度":{"docs":{},"特":{"docs":{},"别":{"docs":{},"慢":{"docs":{},"，":{"docs":{},"这":{"docs":{},"时":{"docs":{},"候":{"docs":{},"观":{"docs":{},"察":{"docs":{},"随":{"docs":{},"机":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"的":{"docs":{},"求":{"docs":{},"导":{"docs":{},"公":{"docs":{},"式":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"发":{"docs":{},"现":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"对":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"x":{"docs":{},"b":{"docs":{},"都":{"docs":{},"做":{"docs":{},"了":{"docs":{},"求":{"docs":{},"和":{"docs":{},"操":{"docs":{},"作":{"docs":{},"，":{"docs":{},"又":{"docs":{},"在":{"docs":{},"最":{"docs":{},"外":{"docs":{},"面":{"docs":{},"除":{"docs":{},"以":{"docs":{},"了":{"docs":{},"m":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"可":{"docs":{},"以":{"docs":{},"考":{"docs":{},"虑":{"docs":{},"将":{"docs":{},"求":{"docs":{},"和":{"docs":{},"和":{"docs":{},"除":{"docs":{},"以":{"docs":{},"m":{"docs":{},"的":{"docs":{},"两":{"docs":{},"个":{"docs":{},"运":{"docs":{},"算":{"docs":{},"约":{"docs":{},"掉":{"docs":{},"，":{"docs":{},"采":{"docs":{},"用":{"docs":{},"每":{"docs":{},"次":{"docs":{},"使":{"docs":{},"用":{"docs":{},"一":{"docs":{},"个":{"docs":{},"随":{"docs":{},"机":{"docs":{},"的":{"docs":{},"x":{"docs":{},"b":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"d":{"docs":{},"_":{"docs":{},"j":{"docs":{},"为":{"docs":{},"求":{"docs":{},"导":{"docs":{},"函":{"docs":{},"数":{"docs":{},"，":{"docs":{},"作":{"docs":{},"为":{"docs":{},"一":{"docs":{},"个":{"docs":{},"参":{"docs":{},"数":{"docs":{},"传":{"docs":{},"入":{"docs":{},"，":{"docs":{},"用":{"docs":{},"于":{"docs":{},"切":{"docs":{},"换":{"docs":{},"求":{"docs":{},"导":{"docs":{},"策":{"docs":{},"略":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"真":{"docs":{},"实":{"docs":{},"数":{"docs":{},"据":{"docs":{},"波":{"docs":{},"士":{"docs":{},"顿":{"docs":{},"房":{"docs":{},"价":{"docs":{},"进":{"docs":{},"行":{"docs":{},"测":{"docs":{},"试":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}},"的":{"docs":{},"θ":{"docs":{},"值":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}},"值":{"docs":{},"为":{"docs":{},"i":{"docs":{},"而":{"docs":{},"预":{"docs":{},"测":{"docs":{},"为":{"docs":{},"j":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"数":{"docs":{},"量":{"docs":{},"有":{"docs":{},"多":{"docs":{},"少":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}}}},"第":{"docs":{},"t":{"docs":{},"次":{"docs":{},"循":{"docs":{},"环":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112},"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}},"一":{"docs":{},"步":{"docs":{},":":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}},"个":{"docs":{},"系":{"docs":{},"数":{"docs":{},"是":{"docs":{},"x":{"docs":{},"前":{"docs":{},"面":{"docs":{},"的":{"docs":{},"系":{"docs":{},"数":{"docs":{},"，":{"docs":{},"第":{"docs":{},"二":{"docs":{},"个":{"docs":{},"系":{"docs":{},"数":{"docs":{},"是":{"docs":{},"x":{"docs":{},"平":{"docs":{},"方":{"docs":{},"前":{"docs":{},"面":{"docs":{},"的":{"docs":{},"系":{"docs":{},"数":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}}}}}}}}},"列":{"docs":{},"是":{"1":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}},"docs":{},"s":{"docs":{},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"为":{"docs":{},"我":{"docs":{},"们":{"docs":{},"添":{"docs":{},"加":{"docs":{},"的":{"docs":{},"x":{"docs":{},"的":{"docs":{},"零":{"docs":{},"次":{"docs":{},"方":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}}}}}}}}},"三":{"docs":{},"列":{"docs":{},"是":{"docs":{},"添":{"docs":{},"加":{"docs":{},"的":{"docs":{},"x":{"docs":{},"的":{"docs":{},"二":{"docs":{},"次":{"docs":{},"方":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}},"二":{"docs":{},"列":{"docs":{},"和":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"一":{"docs":{},"样":{"docs":{},"是":{"docs":{},"x":{"docs":{},"的":{"docs":{},"一":{"docs":{},"次":{"docs":{},"方":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}},"第":{"docs":{},"三":{"docs":{},"列":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"是":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"x":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"，":{"docs":{},"此":{"docs":{},"时":{"docs":{},"他":{"docs":{},"有":{"docs":{},"两":{"docs":{},"列":{"docs":{},"一":{"docs":{},"次":{"docs":{},"幂":{"docs":{},"的":{"docs":{},"项":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"五":{"docs":{},"列":{"docs":{},"是":{"docs":{},"原":{"docs":{},"来":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"两":{"docs":{},"列":{"docs":{},"相":{"docs":{},"乘":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}},"六":{"docs":{},"列":{"docs":{},"是":{"docs":{},"原":{"docs":{},"来":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"第":{"docs":{},"二":{"docs":{},"列":{"docs":{},"平":{"docs":{},"方":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}},"四":{"docs":{},"列":{"docs":{},"是":{"docs":{},"原":{"docs":{},"来":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"第":{"docs":{},"一":{"docs":{},"列":{"docs":{},"平":{"docs":{},"方":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}},"i":{"docs":{},"行":{"docs":{},"第":{"docs":{},"j":{"docs":{},"列":{"docs":{},"的":{"docs":{},"数":{"docs":{},"值":{"docs":{},"代":{"docs":{},"表":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}},"这":{"docs":{},"里":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"模":{"docs":{},"拟":{"docs":{},"退":{"docs":{},"火":{"docs":{},"的":{"docs":{},"思":{"docs":{},"想":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}},"穿":{"docs":{},"的":{"docs":{},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"起":{"docs":{},"始":{"docs":{},"值":{"docs":{},"比":{"docs":{},"岭":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"大":{"docs":{},"了":{"docs":{},"很":{"docs":{},"多":{"docs":{},"，":{"docs":{},"是":{"docs":{},"由":{"docs":{},"于":{"docs":{},"现":{"docs":{},"在":{"docs":{},"是":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"值":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"只":{"docs":{},"有":{"docs":{},"一":{"docs":{},"跟":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"放":{"docs":{},"在":{"docs":{},"了":{"docs":{},"二":{"docs":{},"维":{"docs":{},"数":{"docs":{},"组":{"docs":{},"的":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"元":{"docs":{},"素":{"docs":{},"中":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"分":{"docs":{},"数":{"docs":{},"不":{"docs":{},"是":{"docs":{},"很":{"docs":{},"高":{"docs":{},"。":{"docs":{},"但":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"s":{"docs":{},"v":{"docs":{},"r":{"docs":{},"有":{"docs":{},"很":{"docs":{},"多":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"可":{"docs":{},"以":{"docs":{},"调":{"docs":{},"节":{"docs":{},"，":{"docs":{},"比":{"docs":{},"如":{"docs":{},"c":{"docs":{},"。":{"docs":{},"如":{"docs":{},"果":{"docs":{},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"s":{"docs":{},"v":{"docs":{},"r":{"docs":{},"，":{"docs":{},"还":{"docs":{},"可":{"docs":{},"以":{"docs":{},"对":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"还":{"docs":{},"有":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"调":{"docs":{},"节":{"docs":{},"。":{"docs":{},"对":{"docs":{},"于":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"来":{"docs":{},"说":{"docs":{},"就":{"docs":{},"需":{"docs":{},"要":{"docs":{},"对":{"docs":{},"γ":{"docs":{},"进":{"docs":{},"行":{"docs":{},"调":{"docs":{},"节":{"docs":{},"，":{"docs":{},"对":{"docs":{},"于":{"docs":{},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"来":{"docs":{},"说":{"docs":{},"有":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"和":{"docs":{},"c":{"docs":{},"进":{"docs":{},"行":{"docs":{},"调":{"docs":{},"节":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"ref":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","tf":0.011363636363636364}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"向":{"docs":{},"量":{"docs":{},"空":{"docs":{},"间":{"docs":{},"是":{"docs":{},"由":{"docs":{},"能":{"docs":{},"够":{"docs":{},"被":{"docs":{},"拉":{"docs":{},"伸":{"docs":{},"最":{"docs":{},"长":{"docs":{},"的":{"docs":{},"前":{"docs":{},"k":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"向":{"docs":{},"量":{"docs":{},"组":{"docs":{},"成":{"docs":{},"的":{"docs":{},"；":{"docs":{},"而":{"docs":{},"且":{"docs":{},"这":{"docs":{},"k":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"向":{"docs":{},"量":{"docs":{},"相":{"docs":{},"互":{"docs":{},"正":{"docs":{},"交":{"docs":{},"；":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"轴":{"docs":{},"就":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"求":{"docs":{},"出":{"docs":{},"的":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}}}}}}}},"数":{"docs":{},"据":{"docs":{},"可":{"docs":{},"以":{"docs":{},"近":{"docs":{},"乎":{"docs":{},"表":{"docs":{},"示":{"docs":{},"每":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"轴":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"程":{"docs":{},"度":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}}}}},"噪":{"docs":{},"音":{"docs":{},"的":{"docs":{},"产":{"docs":{},"生":{"docs":{},"原":{"docs":{},"因":{"docs":{},"可":{"docs":{},"能":{"docs":{},"有":{"docs":{},"很":{"docs":{},"多":{"docs":{},"，":{"docs":{},"如":{"docs":{},"测":{"docs":{},"量":{"docs":{},"人":{"docs":{},"员":{"docs":{},"的":{"docs":{},"粗":{"docs":{},"心":{"docs":{},"，":{"docs":{},"测":{"docs":{},"量":{"docs":{},"手":{"docs":{},"段":{"docs":{},"有":{"docs":{},"问":{"docs":{},"题":{"docs":{},"等":{"docs":{},"等":{"docs":{},"原":{"docs":{},"因":{"docs":{},"，":{"docs":{},"都":{"docs":{},"会":{"docs":{},"使":{"docs":{},"得":{"docs":{},"我":{"docs":{},"们":{"docs":{},"在":{"docs":{},"现":{"docs":{},"实":{"docs":{},"世":{"docs":{},"界":{"docs":{},"中":{"docs":{},"采":{"docs":{},"集":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"答":{"docs":{},"案":{"docs":{},"，":{"docs":{},"和":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"大":{"docs":{},"多":{"docs":{},"数":{"docs":{},"的":{"docs":{},"取":{"docs":{},"舍":{"docs":{},"是":{"docs":{},"一":{"docs":{},"样":{"docs":{},"的":{"docs":{},"，":{"docs":{},"应":{"docs":{},"该":{"docs":{},"视":{"docs":{},"情":{"docs":{},"况":{"docs":{},"而":{"docs":{},"定":{"docs":{},"。":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"目":{"docs":{},"标":{"docs":{},"函":{"docs":{},"数":{"docs":{},"的":{"docs":{},"最":{"docs":{},"优":{"docs":{},"化":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"上":{"docs":{},"升":{"docs":{},"法":{"docs":{},"解":{"docs":{},"决":{"docs":{},"。":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}},"因":{"docs":{},"为":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"不":{"docs":{},"仅":{"docs":{},"仅":{"docs":{},"是":{"docs":{},"进":{"docs":{},"行":{"docs":{},"了":{"docs":{},"降":{"docs":{},"维":{"docs":{},"，":{"docs":{},"还":{"docs":{},"在":{"docs":{},"降":{"docs":{},"维":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"将":{"docs":{},"数":{"docs":{},"据":{"docs":{},"包":{"docs":{},"含":{"docs":{},"的":{"docs":{},"噪":{"docs":{},"音":{"docs":{},"给":{"docs":{},"消":{"docs":{},"除":{"docs":{},"了":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"趋":{"docs":{},"向":{"docs":{},"于":{"docs":{},"使":{"docs":{},"得":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"值":{"docs":{},"为":{"0":{"docs":{},"（":{"docs":{},"而":{"docs":{},"不":{"docs":{},"是":{"docs":{},"很":{"docs":{},"小":{"docs":{},"的":{"docs":{},"值":{"docs":{},"）":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"可":{"docs":{},"以":{"docs":{},"作":{"docs":{},"为":{"docs":{},"特":{"docs":{},"征":{"docs":{},"选":{"docs":{},"择":{"docs":{},"用":{"docs":{},"，":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"的":{"docs":{},"最":{"docs":{},"后":{"docs":{},"两":{"docs":{},"个":{"docs":{},"字":{"docs":{},"母":{"docs":{},"s":{"docs":{},"o":{"docs":{},"就":{"docs":{},"是":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"得":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"更":{"docs":{},"加":{"docs":{},"好":{"docs":{},"的":{"docs":{},"，":{"docs":{},"更":{"docs":{},"加":{"docs":{},"准":{"docs":{},"确":{"docs":{},"的":{"docs":{},"拿":{"docs":{},"到":{"docs":{},"我":{"docs":{},"们":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"，":{"docs":{},"从":{"docs":{},"而":{"docs":{},"使":{"docs":{},"得":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"大":{"docs":{},"大":{"docs":{},"提":{"docs":{},"高":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"条":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"只":{"docs":{},"是":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"点":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"y":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"值":{"docs":{},"连":{"docs":{},"接":{"docs":{},"起":{"docs":{},"来":{"docs":{},"的":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"，":{"docs":{},"不":{"docs":{},"过":{"docs":{},"有":{"docs":{},"很":{"docs":{},"多":{"docs":{},"地":{"docs":{},"方":{"docs":{},"可":{"docs":{},"能":{"docs":{},"没":{"docs":{},"有":{"docs":{},"那":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"连":{"docs":{},"接":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"和":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"不":{"docs":{},"一":{"docs":{},"样":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"成":{"docs":{},"为":{"docs":{},"欠":{"docs":{},"拟":{"docs":{},"合":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}},"下":{"docs":{},"，":{"docs":{},"很":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"存":{"docs":{},"在":{"docs":{},"欠":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"我":{"docs":{},"们":{"docs":{},"要":{"docs":{},"对":{"docs":{},"这":{"docs":{},"些":{"docs":{},"参":{"docs":{},"数":{"docs":{},"进":{"docs":{},"行":{"docs":{},"比":{"docs":{},"较":{"docs":{},"精":{"docs":{},"细":{"docs":{},"的":{"docs":{},"调":{"docs":{},"整":{"docs":{},"，":{"docs":{},"让":{"docs":{},"他":{"docs":{},"既":{"docs":{},"不":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"也":{"docs":{},"不":{"docs":{},"欠":{"docs":{},"拟":{"docs":{},"合":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"方":{"docs":{},"法":{"docs":{},"还":{"docs":{},"会":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"问":{"docs":{},"题":{"docs":{},"。":{"docs":{},"由":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"可":{"docs":{},"能":{"docs":{},"会":{"docs":{},"针":{"docs":{},"对":{"docs":{},"验":{"docs":{},"证":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"，":{"docs":{},"而":{"docs":{},"我":{"docs":{},"们":{"docs":{},"只":{"docs":{},"有":{"docs":{},"一":{"docs":{},"份":{"docs":{},"验":{"docs":{},"证":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"一":{"docs":{},"旦":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"里":{"docs":{},"有":{"docs":{},"比":{"docs":{},"较":{"docs":{},"极":{"docs":{},"端":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"就":{"docs":{},"会":{"docs":{},"下":{"docs":{},"降":{"docs":{},"很":{"docs":{},"多":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"为":{"docs":{},"了":{"docs":{},"解":{"docs":{},"决":{"docs":{},"这":{"docs":{},"个":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"就":{"docs":{},"有":{"docs":{},"了":{"docs":{},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{},"。":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"三":{"docs":{},"份":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"进":{"docs":{},"行":{"docs":{},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{},"后":{"docs":{},"产":{"docs":{},"生":{"docs":{},"了":{"docs":{},"这":{"docs":{},"三":{"docs":{},"个":{"docs":{},"结":{"docs":{},"果":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}},"样":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"达":{"docs":{},"到":{"docs":{},"最":{"docs":{},"优":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"再":{"docs":{},"讲":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"送":{"docs":{},"给":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"这":{"docs":{},"样":{"docs":{},"才":{"docs":{},"能":{"docs":{},"作":{"docs":{},"为":{"docs":{},"衡":{"docs":{},"量":{"docs":{},"模":{"docs":{},"型":{"docs":{},"最":{"docs":{},"终":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"。":{"docs":{},"换":{"docs":{},"句":{"docs":{},"话":{"docs":{},"说":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"是":{"docs":{},"不":{"docs":{},"参":{"docs":{},"与":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"创":{"docs":{},"建":{"docs":{},"的":{"docs":{},"，":{"docs":{},"而":{"docs":{},"其":{"docs":{},"他":{"docs":{},"两":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"都":{"docs":{},"参":{"docs":{},"与":{"docs":{},"了":{"docs":{},"训":{"docs":{},"练":{"docs":{},"。":{"docs":{},"但":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"对":{"docs":{},"于":{"docs":{},"模":{"docs":{},"型":{"docs":{},"是":{"docs":{},"完":{"docs":{},"全":{"docs":{},"不":{"docs":{},"可":{"docs":{},"知":{"docs":{},"的":{"docs":{},"，":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"在":{"docs":{},"模":{"docs":{},"型":{"docs":{},"这":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"完":{"docs":{},"全":{"docs":{},"不":{"docs":{},"知":{"docs":{},"道":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"就":{"docs":{},"将":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"函":{"docs":{},"数":{"docs":{},"从":{"docs":{},"一":{"docs":{},"个":{"docs":{},"一":{"docs":{},"维":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"映":{"docs":{},"射":{"docs":{},"成":{"docs":{},"了":{"docs":{},"二":{"docs":{},"维":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"这":{"docs":{},"里":{"docs":{},"，":{"docs":{},"很":{"docs":{},"显":{"docs":{},"然":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"通":{"docs":{},"过":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"来":{"docs":{},"区":{"docs":{},"分":{"docs":{},"两":{"docs":{},"种":{"docs":{},"类":{"docs":{},"别":{"docs":{},"，":{"docs":{},"原":{"docs":{},"来":{"docs":{},"在":{"docs":{},"一":{"docs":{},"维":{"docs":{},"空":{"docs":{},"间":{"docs":{},"中":{"docs":{},"线":{"docs":{},"性":{"docs":{},"不":{"docs":{},"可":{"docs":{},"分":{"docs":{},"的":{"docs":{},"空":{"docs":{},"间":{"docs":{},"，":{"docs":{},"在":{"docs":{},"二":{"docs":{},"维":{"docs":{},"空":{"docs":{},"间":{"docs":{},"中":{"docs":{},"变":{"docs":{},"的":{"docs":{},"线":{"docs":{},"性":{"docs":{},"可":{"docs":{},"分":{"docs":{},"了":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"只":{"docs":{},"要":{"docs":{},"引":{"docs":{},"入":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"项":{"docs":{},"就":{"docs":{},"好":{"docs":{},"了":{"docs":{},"。":{"docs":{},"把":{"docs":{},"x":{"1":{"2":{"docs":{},"和":{"docs":{},"x":{"2":{"2":{"docs":{},"分":{"docs":{},"别":{"docs":{},"作":{"docs":{},"为":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"项":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"学":{"docs":{},"习":{"docs":{},"到":{"docs":{},"的":{"docs":{},"他":{"docs":{},"们":{"docs":{},"前":{"docs":{},"面":{"docs":{},"的":{"docs":{},"系":{"docs":{},"数":{"docs":{},"都":{"docs":{},"是":{"1":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"招":{"docs":{},"聘":{"docs":{},"过":{"docs":{},"程":{"docs":{},"形":{"docs":{},"成":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"树":{"docs":{},"结":{"docs":{},"构":{"docs":{},"，":{"docs":{},"这":{"docs":{},"颗":{"docs":{},"树":{"docs":{},"的":{"docs":{},"所":{"docs":{},"有":{"docs":{},"叶":{"docs":{},"子":{"docs":{},"结":{"docs":{},"点":{"docs":{},"其":{"docs":{},"实":{"docs":{},"就":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"最":{"docs":{},"终":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"。":{"docs":{},"也":{"docs":{},"相":{"docs":{},"对":{"docs":{},"于":{"docs":{},"是":{"docs":{},"对":{"docs":{},"与":{"docs":{},"输":{"docs":{},"入":{"docs":{},"（":{"docs":{},"录":{"docs":{},"用":{"docs":{},"者":{"docs":{},"信":{"docs":{},"息":{"docs":{},"）":{"docs":{},"的":{"docs":{},"分":{"docs":{},"类":{"docs":{},"（":{"docs":{},"录":{"docs":{},"用":{"docs":{},"/":{"docs":{},"考":{"docs":{},"察":{"docs":{},"）":{"docs":{},"。":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"过":{"docs":{},"程":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"。":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"也":{"docs":{},"说":{"docs":{},"明":{"docs":{},"了":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"docs":{},"为":{"docs":{},"什":{"docs":{},"么":{"docs":{},"叫":{"docs":{},"岭":{"docs":{},"回":{"docs":{},"归":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"他":{"docs":{},"更":{"docs":{},"像":{"docs":{},"是":{"docs":{},"翻":{"docs":{},"山":{"docs":{},"越":{"docs":{},"岭":{"docs":{},"一":{"docs":{},"样":{"docs":{},"，":{"docs":{},"在":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"中":{"docs":{},"一":{"docs":{},"点":{"docs":{},"一":{"docs":{},"点":{"docs":{},"找":{"docs":{},"坡":{"docs":{},"度":{"docs":{},"缓":{"docs":{},"的":{"docs":{},"方":{"docs":{},"向":{"docs":{},"前":{"docs":{},"进":{"docs":{},"。":{"docs":{},"而":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"的":{"docs":{},"路":{"docs":{},"径":{"docs":{},"就":{"docs":{},"比":{"docs":{},"较":{"docs":{},"规":{"docs":{},"则":{"docs":{},"，":{"docs":{},"会":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"碰":{"docs":{},"到":{"docs":{},"一":{"docs":{},"些":{"docs":{},"轴":{"docs":{},"使":{"docs":{},"得":{"docs":{},"某":{"docs":{},"些":{"docs":{},"θ":{"docs":{},"为":{"0":{"docs":{},"。":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"就":{"docs":{},"是":{"docs":{},"一":{"docs":{},"种":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"的":{"docs":{},"表":{"docs":{},"现":{"docs":{},"。":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"到":{"docs":{},"上":{"docs":{},"面":{"docs":{},"的":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"l":{"docs":{},"f":{"docs":{},"_":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"的":{"docs":{},"n":{"docs":{},"_":{"docs":{},"n":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{},"s":{"docs":{},"（":{"docs":{},"k":{"docs":{},"）":{"docs":{},"为":{"5":{"docs":{},"。":{"docs":{},"之":{"docs":{},"前":{"docs":{},"的":{"docs":{},"讨":{"docs":{},"论":{"docs":{},"曾":{"docs":{},"经":{"docs":{},"说":{"docs":{},"过":{"docs":{},"，":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"在":{"docs":{},"面":{"docs":{},"对":{"docs":{},"属":{"docs":{},"性":{"docs":{},"是":{"docs":{},"数":{"docs":{},"值":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"是":{"docs":{},"怎":{"docs":{},"么":{"docs":{},"处":{"docs":{},"理":{"docs":{},"的":{"docs":{},"。":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}}}}}}}}}}}}}}}}}}},"时":{"docs":{},"候":{"docs":{},"回":{"docs":{},"过":{"docs":{},"头":{"docs":{},"来":{"docs":{},"看":{"docs":{},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"=":{"1":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"其":{"docs":{},"实":{"docs":{},"就":{"docs":{},"是":{"docs":{},"蓝":{"docs":{},"色":{"docs":{},"的":{"docs":{},"点":{"docs":{},"周":{"docs":{},"围":{"docs":{},"的":{"docs":{},"中":{"docs":{},"型":{"docs":{},"图":{"docs":{},"案":{"docs":{},"变":{"docs":{},"的":{"docs":{},"更":{"docs":{},"宽":{"docs":{},"了":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}},"需":{"docs":{},"要":{"docs":{},"注":{"docs":{},"意":{"docs":{},"的":{"docs":{},"是":{"docs":{},"s":{"docs":{},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"中":{"docs":{},"的":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"比":{"docs":{},"我":{"docs":{},"们":{"docs":{},"自":{"docs":{},"己":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"要":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"的":{"docs":{},"多":{"docs":{},"，":{"docs":{},"性":{"docs":{},"能":{"docs":{},"和":{"docs":{},"计":{"docs":{},"算":{"docs":{},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{},"上":{"docs":{},"都":{"docs":{},"比":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"要":{"docs":{},"好":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"只":{"docs":{},"是":{"docs":{},"用":{"docs":{},"来":{"docs":{},"演":{"docs":{},"示":{"docs":{},"过":{"docs":{},"程":{"docs":{},"，":{"docs":{},"具":{"docs":{},"体":{"docs":{},"生":{"docs":{},"产":{"docs":{},"上":{"docs":{},"的":{"docs":{},"使":{"docs":{},"用":{"docs":{},"还":{"docs":{},"是":{"docs":{},"应":{"docs":{},"该":{"docs":{},"使":{"docs":{},"用":{"docs":{},"s":{"docs":{},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"提":{"docs":{},"供":{"docs":{},"的":{"docs":{"梯度下降法/5.随机梯度下降法.html":{"ref":"梯度下降法/5.随机梯度下降法.html","tf":0.004016064257028112}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"事":{"docs":{},"实":{"docs":{},"上":{"docs":{},"，":{"docs":{},"这":{"docs":{},"也":{"docs":{},"正":{"docs":{},"是":{"docs":{},"导":{"docs":{},"数":{"docs":{},"的":{"docs":{},"定":{"docs":{},"义":{"docs":{},"，":{"docs":{},"当":{"docs":{},"函":{"docs":{},"数":{"docs":{},"y":{"docs":{},"=":{"docs":{},"f":{"docs":{},"(":{"docs":{},"x":{"docs":{},")":{"docs":{},"的":{"docs":{},"自":{"docs":{},"变":{"docs":{},"量":{"docs":{},"x":{"docs":{},"在":{"docs":{},"一":{"docs":{},"点":{"docs":{},"x":{"0":{"docs":{},"上":{"docs":{},"产":{"docs":{},"生":{"docs":{},"一":{"docs":{},"个":{"docs":{},"增":{"docs":{},"量":{"docs":{},"δ":{"docs":{},"x":{"docs":{},"时":{"docs":{},"，":{"docs":{},"函":{"docs":{},"数":{"docs":{},"输":{"docs":{},"出":{"docs":{},"值":{"docs":{},"的":{"docs":{},"增":{"docs":{},"量":{"docs":{},"δ":{"docs":{},"y":{"docs":{},"与":{"docs":{},"自":{"docs":{},"变":{"docs":{},"量":{"docs":{},"增":{"docs":{},"量":{"docs":{},"δ":{"docs":{},"x":{"docs":{},"的":{"docs":{},"比":{"docs":{},"值":{"docs":{},"在":{"docs":{},"δ":{"docs":{},"x":{"docs":{},"趋":{"docs":{},"于":{"0":{"docs":{},"时":{"docs":{},"的":{"docs":{},"极":{"docs":{},"限":{"docs":{},"a":{"docs":{},"如":{"docs":{},"果":{"docs":{},"存":{"docs":{},"在":{"docs":{},"，":{"docs":{},"a":{"docs":{},"即":{"docs":{},"为":{"docs":{},"在":{"docs":{},"x":{"0":{"docs":{},"处":{"docs":{},"的":{"docs":{},"导":{"docs":{},"数":{"docs":{},"，":{"docs":{},"记":{"docs":{},"作":{"docs":{},"f":{"docs":{},"'":{"docs":{},"(":{"docs":{},"x":{"0":{"docs":{},")":{"docs":{},"或":{"docs":{},"d":{"docs":{},"f":{"docs":{},"(":{"docs":{},"x":{"0":{"docs":{},")":{"docs":{},"/":{"docs":{},"d":{"docs":{},"x":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"指":{"docs":{},"标":{"docs":{},"可":{"docs":{},"以":{"docs":{},"之":{"docs":{},"间":{"docs":{},"定":{"docs":{},"义":{"docs":{},"样":{"docs":{},"本":{"docs":{},"间":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"方":{"docs":{},"差":{"docs":{},"（":{"docs":{},"v":{"docs":{},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"）":{"docs":{},"（":{"docs":{},"方":{"docs":{},"差":{"docs":{},"：":{"docs":{},"描":{"docs":{},"述":{"docs":{},"样":{"docs":{},"本":{"docs":{},"整":{"docs":{},"体":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"疏":{"docs":{},"密":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"指":{"docs":{},"标":{"docs":{},"，":{"docs":{},"方":{"docs":{},"差":{"docs":{},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"代":{"docs":{},"表":{"docs":{},"样":{"docs":{},"本":{"docs":{},"之":{"docs":{},"间":{"docs":{},"越":{"docs":{},"稀":{"docs":{},"疏":{"docs":{},"，":{"docs":{},"方":{"docs":{},"差":{"docs":{},"越":{"docs":{},"小":{"docs":{},"，":{"docs":{},"代":{"docs":{},"表":{"docs":{},"样":{"docs":{},"本":{"docs":{},"之":{"docs":{},"间":{"docs":{},"越":{"docs":{},"紧":{"docs":{},"密":{"docs":{},"）":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"正":{"docs":{},"是":{"docs":{},"这":{"docs":{},"样":{"docs":{},"。":{"docs":{},"只":{"docs":{},"不":{"docs":{},"过":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"函":{"docs":{},"数":{"docs":{},"表":{"docs":{},"达":{"docs":{},"出":{"docs":{},"的":{"docs":{},"这":{"docs":{},"种":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"映":{"docs":{},"射":{"docs":{},"是":{"docs":{},"非":{"docs":{},"常":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"的":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"在":{"docs":{},"真":{"docs":{},"正":{"docs":{},"的":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"之":{"docs":{},"前":{"docs":{},"，":{"docs":{},"先":{"docs":{},"使":{"docs":{},"用":{"docs":{},"d":{"docs":{},"_":{"docs":{},"j":{"docs":{},"_":{"docs":{},"d":{"docs":{},"e":{"docs":{},"b":{"docs":{},"u":{"docs":{},"g":{"docs":{},"这":{"docs":{},"种":{"docs":{},"调":{"docs":{},"试":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"验":{"docs":{},"证":{"docs":{},"一":{"docs":{},"下":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"d":{"docs":{},"_":{"docs":{},"j":{"docs":{},"_":{"docs":{},"m":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"是":{"docs":{},"否":{"docs":{},"正":{"docs":{},"确":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"再":{"docs":{},"进":{"docs":{},"行":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"。":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"基":{"docs":{},"础":{"docs":{},"上":{"docs":{},"，":{"docs":{},"添":{"docs":{},"加":{"docs":{},"一":{"docs":{},"个":{"docs":{},"σ":{"docs":{},"函":{"docs":{},"数":{"docs":{},"，":{"docs":{},"将":{"docs":{},"结":{"docs":{},"果":{"docs":{},"转":{"docs":{},"换":{"docs":{},"成":{"0":{"docs":{},"到":{"1":{"docs":{},"之":{"docs":{},"间":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"看":{"docs":{},"到":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"又":{"docs":{},"把":{"docs":{},"他":{"docs":{},"规":{"docs":{},"约":{"docs":{},"成":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"现":{"docs":{},"在":{"docs":{},"的":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"很":{"docs":{},"容":{"docs":{},"易":{"docs":{},"混":{"docs":{},"淆":{"1":{"docs":{},"和":{"9":{"docs":{},"以":{"docs":{},"及":{"1":{"docs":{},"和":{"8":{"docs":{},"，":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"微":{"docs":{},"调":{"1":{"docs":{},"和":{"9":{"docs":{},"和":{"1":{"docs":{},"和":{"8":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}}}}}}}}}}},"docs":{}}},"docs":{}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"希":{"docs":{},"望":{"docs":{},"有":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"是":{"docs":{},"斜":{"docs":{},"着":{"docs":{},"的":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"希":{"docs":{},"望":{"docs":{},"将":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"点":{"docs":{},"都":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"这":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{},"上":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"这":{"docs":{},"个":{"docs":{},"时":{"docs":{},"候":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"成":{"docs":{},"功":{"docs":{},"的":{"docs":{},"将":{"docs":{},"二":{"docs":{},"维":{"docs":{},"降":{"docs":{},"到":{"docs":{},"了":{"docs":{},"一":{"docs":{},"维":{"docs":{},"，":{"docs":{},"与":{"docs":{},"此":{"docs":{},"同":{"docs":{},"时":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"点":{"docs":{},"更":{"docs":{},"加":{"docs":{},"趋":{"docs":{},"近":{"docs":{},"与":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"点":{"docs":{},"的":{"docs":{},"分":{"docs":{},"布":{"docs":{},"情":{"docs":{},"况":{"docs":{},"，":{"docs":{},"换":{"docs":{},"句":{"docs":{},"话":{"docs":{},"说":{"docs":{},"，":{"docs":{},"点":{"docs":{},"和":{"docs":{},"点":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"比":{"docs":{},"无":{"docs":{},"论":{"docs":{},"是":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"x":{"docs":{},"还":{"docs":{},"是":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"y":{"docs":{},"周":{"docs":{},"，":{"docs":{},"他":{"docs":{},"们":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"区":{"docs":{},"分":{"docs":{},"度":{"docs":{},"都":{"docs":{},"更":{"docs":{},"加":{"docs":{},"的":{"docs":{},"大":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"更":{"docs":{},"加":{"docs":{},"容":{"docs":{},"易":{"docs":{},"区":{"docs":{},"分":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"让":{"docs":{},"θ":{"docs":{},"的":{"docs":{},"个":{"docs":{},"数":{"docs":{},"尽":{"docs":{},"量":{"docs":{},"小":{"docs":{},"，":{"docs":{},"描":{"docs":{},"述":{"docs":{},"的":{"docs":{},"是":{"docs":{},"非":{"docs":{},"零":{"docs":{},"θ":{"docs":{},"元":{"docs":{},"素":{"docs":{},"的":{"docs":{},"个":{"docs":{},"数":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"用":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"限":{"docs":{},"制":{"docs":{},"θ":{"docs":{},"的":{"docs":{},"数":{"docs":{},"量":{"docs":{},"尽":{"docs":{},"可":{"docs":{},"能":{"docs":{},"的":{"docs":{},"小":{"docs":{},"，":{"docs":{},"进":{"docs":{},"而":{"docs":{},"来":{"docs":{},"限":{"docs":{},"制":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"不":{"docs":{},"要":{"docs":{},"太":{"docs":{},"抖":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"这":{"docs":{},"个":{"docs":{},"比":{"docs":{},"例":{"docs":{},"越":{"docs":{},"高":{"docs":{},"越":{"docs":{},"好":{"docs":{},"。":{"docs":{},"如":{"docs":{},"果":{"docs":{},"我":{"docs":{},"们":{"docs":{},"预":{"docs":{},"测":{"docs":{},"股":{"docs":{},"票":{"docs":{},"升":{"docs":{},"了":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"要":{"docs":{},"购":{"docs":{},"买":{"docs":{},"这":{"docs":{},"个":{"docs":{},"股":{"docs":{},"票":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"我":{"docs":{},"们":{"docs":{},"犯":{"docs":{},"了":{"docs":{},"f":{"docs":{},"p":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{},"（":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"股":{"docs":{},"票":{"docs":{},"将":{"docs":{},"下":{"docs":{},"来":{"docs":{},"了":{"docs":{},"，":{"docs":{},"而":{"docs":{},"我":{"docs":{},"们":{"docs":{},"预":{"docs":{},"测":{"docs":{},"升":{"docs":{},"上":{"docs":{},"来":{"docs":{},"了":{"docs":{},"）":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"就":{"docs":{},"亏":{"docs":{},"钱":{"docs":{},"了":{"docs":{},"。":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"得":{"docs":{},"到":{"docs":{},"新":{"docs":{},"的":{"docs":{},"降":{"docs":{},"维":{"docs":{},"后":{"docs":{},"的":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"x":{"docs":{},"k":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"是":{"docs":{},"可":{"docs":{},"以":{"docs":{},"通":{"docs":{},"过":{"docs":{},"和":{"docs":{},"w":{"docs":{},"k":{"docs":{},"想":{"docs":{},"乘":{"docs":{},"回":{"docs":{},"复":{"docs":{},"回":{"docs":{},"来":{"docs":{},"的":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"由":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"在":{"docs":{},"降":{"docs":{},"维":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"丢":{"docs":{},"失":{"docs":{},"了":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"信":{"docs":{},"息":{"docs":{},"，":{"docs":{},"这":{"docs":{},"时":{"docs":{},"及":{"docs":{},"时":{"docs":{},"回":{"docs":{},"复":{"docs":{},"回":{"docs":{},"来":{"docs":{},"也":{"docs":{},"和":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"不":{"docs":{},"一":{"docs":{},"样":{"docs":{},"了":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"这":{"docs":{},"个":{"docs":{},"从":{"docs":{},"数":{"docs":{},"据":{"docs":{},"角":{"docs":{},"度":{"docs":{},"成":{"docs":{},"立":{"docs":{},"的":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"进":{"docs":{},"行":{"docs":{},"降":{"docs":{},"维":{"docs":{},"然":{"docs":{},"后":{"docs":{},"在":{"docs":{},"反":{"docs":{},"转":{"docs":{},"回":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"维":{"docs":{},"度":{"docs":{},"，":{"docs":{},"经":{"docs":{},"过":{"docs":{},"这":{"docs":{},"样":{"docs":{},"一":{"docs":{},"个":{"docs":{},"操":{"docs":{},"作":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"发":{"docs":{},"现":{"docs":{},"此":{"docs":{},"时":{"docs":{},"这":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"训":{"docs":{},"练":{"docs":{},"好":{"docs":{},"模":{"docs":{},"型":{"docs":{},"之":{"docs":{},"后":{"docs":{},"，":{"docs":{},"将":{"docs":{},"验":{"docs":{},"证":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"送":{"docs":{},"给":{"docs":{},"这":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"看":{"docs":{},"看":{"docs":{},"这":{"docs":{},"个":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"是":{"docs":{},"怎":{"docs":{},"么":{"docs":{},"样":{"docs":{},"的":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"效":{"docs":{},"果":{"docs":{},"不":{"docs":{},"好":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"重":{"docs":{},"新":{"docs":{},"换":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"重":{"docs":{},"新":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{},"直":{"docs":{},"到":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"针":{"docs":{},"对":{"docs":{},"验":{"docs":{},"证":{"docs":{},"数":{"docs":{},"据":{"docs":{},"来":{"docs":{},"说":{"docs":{},"已":{"docs":{},"经":{"docs":{},"达":{"docs":{},"到":{"docs":{},"最":{"docs":{},"优":{"docs":{},"了":{"docs":{},"。":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"如":{"docs":{},"何":{"docs":{},"使":{"docs":{},"用":{"docs":{},"管":{"docs":{},"道":{"docs":{},"呢":{"docs":{},"，":{"docs":{},"先":{"docs":{},"考":{"docs":{},"虑":{"docs":{},"我":{"docs":{},"们":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}}}}}},"真":{"docs":{},"正":{"docs":{},"需":{"docs":{},"要":{"docs":{},"的":{"docs":{},"是":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"更":{"docs":{},"高":{"docs":{},"，":{"docs":{},"解":{"docs":{},"决":{"docs":{},"这":{"docs":{},"个":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"分":{"docs":{},"离":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"目":{"docs":{},"的":{"docs":{},"是":{"docs":{},"为":{"docs":{},"了":{"docs":{},"使":{"docs":{},"得":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"能":{"docs":{},"够":{"docs":{},"尽":{"docs":{},"肯":{"docs":{},"能":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"，":{"docs":{},"在":{"docs":{},"这":{"docs":{},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"观":{"docs":{},"察":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"程":{"docs":{},"度":{"docs":{},"是":{"docs":{},"没":{"docs":{},"有":{"docs":{},"意":{"docs":{},"义":{"docs":{},"的":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"上":{"docs":{},"面":{"docs":{},"的":{"docs":{},"操":{"docs":{},"作":{"docs":{},"，":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"在":{"docs":{},"网":{"docs":{},"格":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"已":{"docs":{},"经":{"docs":{},"进":{"docs":{},"行":{"docs":{},"了":{"docs":{},"，":{"docs":{},"只":{"docs":{},"不":{"docs":{},"过":{"docs":{},"这":{"docs":{},"个":{"docs":{},"过":{"docs":{},"程":{"docs":{},"是":{"docs":{},"s":{"docs":{},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"的":{"docs":{},"网":{"docs":{},"格":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"自":{"docs":{},"带":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"过":{"docs":{},"程":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"每":{"docs":{},"次":{"docs":{},"使":{"docs":{},"用":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"来":{"docs":{},"分":{"docs":{},"析":{"docs":{},"性":{"docs":{},"能":{"docs":{},"的":{"docs":{},"好":{"docs":{},"坏":{"docs":{},"。":{"docs":{},"一":{"docs":{},"旦":{"docs":{},"发":{"docs":{},"现":{"docs":{},"结":{"docs":{},"果":{"docs":{},"不":{"docs":{},"好":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"换":{"docs":{},"一":{"docs":{},"个":{"docs":{},"参":{"docs":{},"数":{"docs":{},"（":{"docs":{},"可":{"docs":{},"能":{"docs":{},"是":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"也":{"docs":{},"可":{"docs":{},"能":{"docs":{},"是":{"docs":{},"其":{"docs":{},"他":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"）":{"docs":{},"重":{"docs":{},"新":{"docs":{},"进":{"docs":{},"行":{"docs":{},"训":{"docs":{},"练":{"docs":{},"。":{"docs":{},"这":{"docs":{},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"在":{"docs":{},"一":{"docs":{},"定":{"docs":{},"程":{"docs":{},"度":{"docs":{},"上":{"docs":{},"围":{"docs":{},"绕":{"docs":{},"着":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"打":{"docs":{},"转":{"docs":{},"。":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"说":{"docs":{},"我":{"docs":{},"们":{"docs":{},"在":{"docs":{},"寻":{"docs":{},"找":{"docs":{},"一":{"docs":{},"组":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"使":{"docs":{},"得":{"docs":{},"这":{"docs":{},"组":{"docs":{},"参":{"docs":{},"数":{"docs":{},"训":{"docs":{},"练":{"docs":{},"出":{"docs":{},"来":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"在":{"docs":{},"测":{"docs":{},"试":{"docs":{},"结":{"docs":{},"果":{"docs":{},"集":{"docs":{},"上":{"docs":{},"表":{"docs":{},"现":{"docs":{},"的":{"docs":{},"最":{"docs":{},"好":{"docs":{},"。":{"docs":{},"但":{"docs":{},"是":{"docs":{},"由":{"docs":{},"于":{"docs":{},"这":{"docs":{},"组":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"是":{"docs":{},"已":{"docs":{},"知":{"docs":{},"的":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"在":{"docs":{},"针":{"docs":{},"对":{"docs":{},"这":{"docs":{},"组":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"进":{"docs":{},"行":{"docs":{},"调":{"docs":{},"参":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"他":{"docs":{},"也":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"产":{"docs":{},"生":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"针":{"docs":{},"对":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"了":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"用":{"docs":{},"两":{"docs":{},"个":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"是":{"docs":{},"不":{"docs":{},"太":{"docs":{},"方":{"docs":{},"便":{"docs":{},"的":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"将":{"docs":{},"他":{"docs":{},"们":{"docs":{},"合":{"docs":{},"成":{"docs":{},"一":{"docs":{},"个":{"docs":{},"式":{"docs":{},"子":{"docs":{},"如":{"docs":{},"下":{"docs":{},"。":{"docs":{},"当":{"docs":{},"y":{"docs":{},"=":{"1":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"后":{"docs":{},"半":{"docs":{},"部":{"docs":{},"分":{"docs":{},"是":{"0":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"就":{"docs":{},"只":{"docs":{},"剩":{"docs":{},"下":{"docs":{"逻辑回归/2.逻辑回归的损失函数.html":{"ref":"逻辑回归/2.逻辑回归的损失函数.html","tf":0.09090909090909091}}}}}}}}}},"docs":{}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"训":{"docs":{},"练":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"每":{"docs":{},"得":{"docs":{},"到":{"docs":{},"一":{"docs":{},"个":{"docs":{},"新":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"得":{"docs":{},"到":{"docs":{},"一":{"docs":{},"根":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"p":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"通":{"docs":{},"过":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"，":{"docs":{},"判":{"docs":{},"断":{"docs":{},"出":{"docs":{},"了":{"docs":{},"这":{"docs":{},"样":{"docs":{},"做":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"算":{"docs":{},"法":{"docs":{},"是":{"docs":{},"完":{"docs":{},"全":{"docs":{},"没":{"docs":{},"有":{"docs":{},"用":{"docs":{},"的":{"docs":{},"。":{"docs":{"评价分类结果/9.2 精准率和召回率.html":{"ref":"评价分类结果/9.2 精准率和召回率.html","tf":0.1111111111111111}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"只":{"docs":{},"要":{"docs":{},"找":{"docs":{},"到":{"docs":{},"d":{"docs":{},"的":{"docs":{},"表":{"docs":{},"达":{"docs":{},"式":{"docs":{},"，":{"docs":{},"也":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"能":{"docs":{},"够":{"docs":{},"求":{"docs":{},"解":{"docs":{},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.2 SVM背后的最优化问题.html":{"ref":"支撑向量机SVM/11.2 SVM背后的最优化问题.html","tf":0.16666666666666666}}}}}}}}}}}}}}}}}}}}}}}}}}},"之":{"docs":{},"前":{"docs":{},"说":{"docs":{},"，":{"docs":{},"对":{"docs":{},"于":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"他":{"docs":{},"的":{"docs":{},"本":{"docs":{},"质":{"docs":{},"就":{"docs":{},"是":{"docs":{},"对":{"docs":{},"于":{"docs":{},"没":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"添":{"docs":{},"加":{"docs":{},"了":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"项":{"docs":{},"，":{"docs":{},"在":{"docs":{},"将":{"docs":{},"这":{"docs":{},"些":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"的":{"docs":{},"新":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"特":{"docs":{},"征":{"docs":{},"进":{"docs":{},"行":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"就":{"docs":{},"形":{"docs":{},"成":{"docs":{},"了":{"docs":{},"新":{"docs":{},"的":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"扩":{"docs":{},"展":{"docs":{},"到":{"docs":{},"多":{"docs":{},"维":{"docs":{},"维":{"docs":{},"度":{"docs":{},"则":{"docs":{},"如":{"docs":{},"下":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}}}}}}}},"添":{"docs":{},"加":{"docs":{},"噪":{"docs":{},"音":{"docs":{"梯度下降法/6.梯度下降法的调试.html":{"ref":"梯度下降法/6.梯度下降法的调试.html","tf":0.005434782608695652}}}}}},")":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857},"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"ref":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","tf":0.0049261083743842365},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}},")":{"docs":{},"*":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}},"低":{"docs":{},"，":{"docs":{},"每":{"docs":{},"一":{"docs":{},"次":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"不":{"docs":{},"确":{"docs":{},"定":{"docs":{},"，":{"docs":{},"甚":{"docs":{},"至":{"docs":{},"向":{"docs":{},"反":{"docs":{},"方":{"docs":{},"向":{"docs":{},"前":{"docs":{},"进":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}},"小":{"docs":{},"批":{"docs":{},"量":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"：":{"docs":{},"即":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"每":{"docs":{},"一":{"docs":{},"次":{"docs":{},"不":{"docs":{},"看":{"docs":{},"全":{"docs":{},"部":{"docs":{},"样":{"docs":{},"本":{"docs":{},"那":{"docs":{},"么":{"docs":{},"多":{"docs":{},"，":{"docs":{},"也":{"docs":{},"不":{"docs":{},"是":{"docs":{},"只":{"docs":{},"看":{"docs":{},"一":{"docs":{},"次":{"docs":{},"样":{"docs":{},"本":{"docs":{},"那":{"docs":{},"么":{"docs":{},"少":{"docs":{},"，":{"docs":{},"每":{"docs":{},"次":{"docs":{},"只":{"docs":{},"看":{"docs":{},"k":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"随":{"docs":{},"机":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"的":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"k":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}},"快":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"慢":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}},"每":{"docs":{},"一":{"docs":{},"次":{"docs":{},"只":{"docs":{},"需":{"docs":{},"观":{"docs":{},"察":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}},"行":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"，":{"docs":{},"他":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"表":{"docs":{},"达":{"docs":{},"了":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"人":{"docs":{},"脸":{"docs":{},"数":{"docs":{},"据":{"docs":{},"中":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"有":{"docs":{},"多":{"docs":{},"少":{"docs":{},"样":{"docs":{},"本":{"docs":{},",":{"docs":{},"在":{"docs":{},"列":{"docs":{},"方":{"docs":{},"向":{"docs":{},"上":{"docs":{},"求":{"docs":{},"和":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}},"次":{"docs":{},"对":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"看":{"docs":{},"一":{"docs":{},"遍":{"docs":{},"才":{"docs":{},"可":{"docs":{},"以":{"docs":{},"计":{"docs":{},"算":{"docs":{},"出":{"docs":{},"梯":{"docs":{},"度":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}}}}}},"看":{"docs":{},"k":{"docs":{},"个":{"docs":{},"元":{"docs":{},"素":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}},"就":{"docs":{},"从":{"docs":{},"n":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"挑":{"docs":{},"出":{"docs":{},"两":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"（":{"docs":{},"比":{"docs":{},"如":{"docs":{},"这":{"docs":{},"里":{"docs":{},"挑":{"docs":{},"出":{"docs":{},"红":{"docs":{},"蓝":{"docs":{},"两":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"）":{"docs":{},",":{"docs":{},"然":{"docs":{},"后":{"docs":{},"进":{"docs":{},"行":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"看":{"docs":{},"对":{"docs":{},"于":{"docs":{},"这":{"docs":{},"个":{"docs":{},"任":{"docs":{},"务":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"是":{"docs":{},"属":{"docs":{},"于":{"docs":{},"哪":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"。":{"docs":{},"然":{"docs":{},"后":{"docs":{},"依":{"docs":{},"次":{"docs":{},"类":{"docs":{},"推":{"docs":{},"进":{"docs":{},"行":{"docs":{},"扩":{"docs":{},"展":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"我":{"docs":{},"们":{"docs":{},"有":{"4":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"需":{"docs":{},"要":{"docs":{},"分":{"docs":{},"类":{"docs":{},"，":{"docs":{},"那":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"能":{"docs":{},"形":{"docs":{},"成":{"6":{"docs":{},"个":{"docs":{},"两":{"docs":{},"两":{"docs":{},"的":{"docs":{},"对":{"docs":{},"c":{"4":{"2":{"docs":{},"(":{"docs":{},"排":{"docs":{},"列":{"docs":{},"组":{"docs":{},"合":{"docs":{},"公":{"docs":{},"式":{"4":{"docs":{},"*":{"3":{"docs":{},"/":{"2":{"docs":{},"=":{"6":{"docs":{},")":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"6":{"docs":{},"个":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"。":{"docs":{},"对":{"docs":{},"于":{"docs":{},"这":{"6":{"docs":{},"个":{"docs":{},"分":{"docs":{},"类":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"判":{"docs":{},"定":{"docs":{},"他":{"docs":{},"在":{"docs":{},"哪":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"中":{"docs":{},"数":{"docs":{},"量":{"docs":{},"最":{"docs":{},"大":{"docs":{},"，":{"docs":{},"就":{"docs":{},"判":{"docs":{},"定":{"docs":{},"他":{"docs":{},"是":{"docs":{},"哪":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}},"docs":{}}}}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}}}}}}}},"docs":{}},"docs":{}}}}}}}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"张":{"docs":{},"脸":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"人":{"docs":{},"名":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"都":{"docs":{},"是":{"docs":{},"按":{"docs":{},"照":{"docs":{},"顺":{"docs":{},"序":{"docs":{},"排":{"docs":{},"列":{"docs":{},"的":{"docs":{},"。":{"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":0.08333333333333333}}}}}}}}}}}}}},"结":{"docs":{},"点":{"docs":{},"在":{"docs":{},"哪":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"做":{"docs":{},"划":{"docs":{},"分":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}}}},"模":{"docs":{},"型":{"docs":{},"都":{"docs":{},"在":{"docs":{},"尝":{"docs":{},"试":{"docs":{},"增":{"docs":{},"强":{"docs":{},"(":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},")":{"docs":{},"整":{"docs":{},"体":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}}}}}}}}}}}}}}}}}}}}},"稳":{"docs":{},"定":{"docs":{},"性":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}},"维":{"docs":{},"度":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}},"综":{"docs":{},"合":{"docs":{},"二":{"docs":{},"者":{"docs":{},"的":{"docs":{},"优":{"docs":{},"缺":{"docs":{},"点":{"docs":{},"，":{"docs":{},"有":{"docs":{},"一":{"docs":{},"种":{"docs":{},"新":{"docs":{},"的":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}}}}}}}},"速":{"docs":{},"度":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}},"高":{"docs":{},"，":{"docs":{},"一":{"docs":{},"定":{"docs":{},"可":{"docs":{},"以":{"docs":{},"先":{"docs":{},"向":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"下":{"docs":{},"降":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"前":{"docs":{},"进":{"docs":{"梯度下降法/7.梯度下降法的总结.html":{"ref":"梯度下降法/7.梯度下降法的总结.html","tf":0.008}}}}}}}}}}}}}}}}}}}},"维":{"docs":{},"数":{"docs":{},"据":{"docs":{},"向":{"docs":{},"低":{"docs":{},"维":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"映":{"docs":{},"射":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}}}}},"斯":{"docs":{},"函":{"docs":{},"数":{"docs":{},"是":{"docs":{},"将":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"了":{"docs":{},"无":{"docs":{},"穷":{"docs":{},"维":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"空":{"docs":{},"间":{"docs":{},"，":{"docs":{},"这":{"docs":{},"背":{"docs":{},"后":{"docs":{},"的":{"docs":{},"变":{"docs":{},"形":{"docs":{},"是":{"docs":{},"非":{"docs":{},"常":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"的":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"变":{"docs":{},"形":{"docs":{},"之":{"docs":{},"后":{"docs":{},"再":{"docs":{},"进":{"docs":{},"行":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"却":{"docs":{},"是":{"docs":{},"非":{"docs":{},"常":{"docs":{},"简":{"docs":{},"单":{"docs":{},"的":{"docs":{},"。":{"docs":{},"这":{"docs":{},"再":{"docs":{},"次":{"docs":{},"显":{"docs":{},"示":{"docs":{},"了":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"这":{"docs":{},"个":{"docs":{},"工":{"docs":{},"具":{"docs":{},"的":{"docs":{},"威":{"docs":{},"力":{"docs":{},"，":{"docs":{},"他":{"docs":{},"不":{"docs":{},"需":{"docs":{},"要":{"docs":{},"我":{"docs":{},"们":{"docs":{},"具":{"docs":{},"体":{"docs":{},"的":{"docs":{},"计":{"docs":{},"算":{"docs":{},"出":{"docs":{},"来":{"docs":{},"这":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"怎":{"docs":{},"么":{"docs":{},"映":{"docs":{},"射":{"docs":{},"成":{"docs":{},"新":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"只":{"docs":{},"需":{"docs":{},"要":{"docs":{},"关":{"docs":{},"注":{"docs":{},"最":{"docs":{},"终":{"docs":{},"的":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"运":{"docs":{},"算":{"docs":{},"结":{"docs":{},"果":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"了":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"本":{"docs":{},"质":{"docs":{},"也":{"docs":{},"是":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"也":{"docs":{},"叫":{"docs":{},"r":{"docs":{},"b":{"docs":{},"f":{"docs":{},"核":{"docs":{},"（":{"docs":{},"r":{"docs":{},"a":{"docs":{},"d":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}},"本":{"docs":{},"质":{"docs":{},"也":{"docs":{},"应":{"docs":{},"该":{"docs":{},"是":{"docs":{},"将":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"映":{"docs":{},"射":{"docs":{},"成":{"docs":{},"了":{"docs":{},"新":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"向":{"docs":{},"量":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"是":{"docs":{},"这":{"docs":{},"种":{"docs":{},"新":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"向":{"docs":{},"量":{"docs":{},"点":{"docs":{},"成":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"与":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"区":{"docs":{},"别":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}},"此":{"docs":{},"同":{"docs":{},"时":{"docs":{},"需":{"docs":{},"要":{"docs":{},"主":{"docs":{},"要":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"在":{"docs":{},"上":{"docs":{},"一":{"docs":{},"章":{"docs":{},"所":{"docs":{},"讲":{"docs":{},"的":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"是":{"docs":{},"对":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"降":{"docs":{},"维":{"docs":{},"处":{"docs":{},"理":{"docs":{},"，":{"docs":{},"而":{"docs":{},"我":{"docs":{},"们":{"docs":{},"这":{"docs":{},"一":{"docs":{},"章":{"docs":{},"所":{"docs":{},"讲":{"docs":{},"的":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"显":{"docs":{},"然":{"docs":{},"在":{"docs":{},"做":{"docs":{},"一":{"docs":{},"件":{"docs":{},"相":{"docs":{},"反":{"docs":{},"的":{"docs":{},"事":{"docs":{},"情":{"docs":{},"，":{"docs":{},"他":{"docs":{},"让":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"升":{"docs":{},"维":{"docs":{},"，":{"docs":{},"在":{"docs":{},"升":{"docs":{},"维":{"docs":{},"之":{"docs":{},"后":{"docs":{},"使":{"docs":{},"得":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"可":{"docs":{},"以":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"高":{"docs":{},"纬":{"docs":{},"度":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{},"中":{"docs":{},"的":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":5.0049504950495045}}}}}}}}}}},"举":{"docs":{},"个":{"docs":{},"栗":{"docs":{},"子":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}},"例":{"docs":{},"如":{"docs":{},"下":{"docs":{},"面":{"docs":{},"一":{"docs":{},"个":{"docs":{},"两":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"训":{"docs":{},"练":{"docs":{},"集":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"选":{"docs":{},"择":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"，":{"docs":{},"扔":{"docs":{},"掉":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},":":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}},"当":{"docs":{},"然":{"docs":{},"我":{"docs":{},"们":{"docs":{},"也":{"docs":{},"可":{"docs":{},"以":{"docs":{},"之":{"docs":{},"间":{"docs":{},"使":{"docs":{},"用":{"docs":{},"数":{"docs":{},"学":{"docs":{},"原":{"docs":{},"理":{"docs":{},"推":{"docs":{},"导":{"docs":{},"出":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"这":{"docs":{},"里":{"docs":{},"我":{"docs":{},"们":{"docs":{},"主":{"docs":{},"要":{"docs":{},"关":{"docs":{},"注":{"docs":{},"使":{"docs":{},"用":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"的":{"docs":{},"策":{"docs":{},"略":{"docs":{},"来":{"docs":{},"求":{"docs":{},"解":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"分":{"docs":{},"析":{"docs":{},"法":{"docs":{},"，":{"docs":{},"这":{"docs":{},"样":{"docs":{},"我":{"docs":{},"们":{"docs":{},"对":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"上":{"docs":{},"升":{"docs":{},"发":{"docs":{},"和":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"也":{"docs":{},"可":{"docs":{},"以":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"更":{"docs":{},"深":{"docs":{},"刻":{"docs":{},"的":{"docs":{},"认":{"docs":{},"识":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"在":{"docs":{},"实":{"docs":{},"际":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"不":{"docs":{},"好":{"docs":{},"说":{"docs":{},"x":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"就":{"docs":{},"是":{"docs":{},"一":{"docs":{},"点":{"docs":{},"噪":{"docs":{},"音":{"docs":{},"都":{"docs":{},"没":{"docs":{},"有":{"docs":{},"，":{"docs":{},"也":{"docs":{},"不":{"docs":{},"好":{"docs":{},"说":{"docs":{},"原":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"抖":{"docs":{},"动":{"docs":{},"全":{"docs":{},"都":{"docs":{},"是":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"这":{"docs":{},"显":{"docs":{},"然":{"docs":{},"是":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"了":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{},"p":{"docs":{},"h":{"docs":{},"a":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"就":{"docs":{},"是":{"docs":{},"在":{"docs":{},"优":{"docs":{},"化":{"docs":{},"θ":{"docs":{},"的":{"docs":{},"平":{"docs":{},"方":{"docs":{},"和":{"docs":{},"这":{"docs":{},"一":{"docs":{},"项":{"docs":{},"，":{"docs":{},"使":{"docs":{},"得":{"docs":{},"其":{"docs":{},"最":{"docs":{},"小":{"docs":{},"（":{"docs":{},"因":{"docs":{},"为":{"docs":{},"m":{"docs":{},"s":{"docs":{},"e":{"docs":{},"的":{"docs":{},"部":{"docs":{},"分":{"docs":{},"相":{"docs":{},"对":{"docs":{},"非":{"docs":{},"常":{"docs":{},"小":{"docs":{},"）":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"当":{"docs":{},"α":{"docs":{},"趋":{"docs":{},"近":{"docs":{},"与":{"docs":{},"无":{"docs":{},"穷":{"docs":{},"大":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"使":{"docs":{},"用":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"下":{"docs":{},"降":{"docs":{},"法":{"docs":{},"的":{"docs":{},"j":{"docs":{},"(":{"docs":{},"θ":{"docs":{},")":{"docs":{},"的":{"docs":{},"导":{"docs":{},"数":{"docs":{},"如":{"docs":{},"下":{"docs":{},"图":{"docs":{},"，":{"docs":{},"j":{"docs":{},"(":{"docs":{},"θ":{"docs":{},")":{"docs":{},"向":{"0":{"docs":{},"趋":{"docs":{},"近":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"每":{"docs":{},"个":{"docs":{},"θ":{"docs":{},"都":{"docs":{},"是":{"docs":{},"有":{"docs":{},"值":{"docs":{},"的":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"=":{"0":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"按":{"docs":{},"照":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"分":{"docs":{},"类":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"应":{"docs":{},"该":{"docs":{},"把":{"docs":{},"样":{"docs":{},"本":{"docs":{},"分":{"docs":{},"为":{"docs":{},"y":{"docs":{},"^":{"docs":{},"=":{"0":{"docs":{},"(":{"docs":{},"注":{"docs":{},"意":{"docs":{},"这":{"docs":{},"里":{"docs":{},"是":{"docs":{},"预":{"docs":{},"测":{"docs":{},"值":{"docs":{},")":{"docs":{},"这":{"docs":{},"一":{"docs":{},"类":{"docs":{},"。":{"docs":{},"但":{"docs":{},"是":{"docs":{},"这":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"实":{"docs":{},"际":{"docs":{},"是":{"docs":{},"y":{"docs":{},"=":{"1":{"docs":{},"(":{"docs":{},"注":{"docs":{},"意":{"docs":{},"这":{"docs":{},"里":{"docs":{},"是":{"docs":{},"真":{"docs":{},"值":{"docs":{},")":{"docs":{},"，":{"docs":{},"显":{"docs":{},"然":{"docs":{},"我":{"docs":{},"们":{"docs":{},"分":{"docs":{},"错":{"docs":{},"了":{"docs":{},"，":{"docs":{},"此":{"docs":{},"时":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"对":{"docs":{},"他":{"docs":{},"进":{"docs":{},"行":{"docs":{},"惩":{"docs":{},"罚":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"惩":{"docs":{},"罚":{"docs":{},"是":{"docs":{},"正":{"docs":{},"无":{"docs":{},"穷":{"docs":{},"的":{"docs":{},"；":{"docs":{},"随":{"docs":{},"着":{"docs":{},"p":{"docs":{},"逐":{"docs":{},"渐":{"docs":{},"变":{"docs":{},"大":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"损":{"docs":{},"失":{"docs":{},"越":{"docs":{},"来":{"docs":{},"越":{"docs":{},"小":{"docs":{},"。":{"docs":{},"当":{"docs":{},"p":{"docs":{},"=":{"1":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"会":{"docs":{},"将":{"docs":{},"样":{"docs":{},"本":{"docs":{},"分":{"docs":{},"类":{"docs":{},"为":{"docs":{},"y":{"docs":{},"=":{"1":{"docs":{},"，":{"docs":{},"此":{"docs":{},"时":{"docs":{},"和":{"docs":{},"这":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"真":{"docs":{},"实":{"docs":{},"的":{"docs":{},"y":{"docs":{},"=":{"1":{"docs":{},"是":{"docs":{},"一":{"docs":{},"致":{"docs":{},"的":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"此":{"docs":{},"时":{"docs":{"逻辑回归/2.逻辑回归的损失函数.html":{"ref":"逻辑回归/2.逻辑回归的损失函数.html","tf":0.09090909090909091}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}},"1":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"就":{"docs":{},"是":{"docs":{},"l":{"1":{"docs":{},"范":{"docs":{},"数":{"docs":{},"（":{"docs":{},"曼":{"docs":{},"哈":{"docs":{},"顿":{"docs":{},"距":{"docs":{},"离":{"docs":{},"|":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}}}}}}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}},"二":{"docs":{},"者":{"docs":{},"相":{"docs":{},"同":{"docs":{},"，":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"就":{"docs":{},"是":{"docs":{},"这":{"docs":{},"个":{"docs":{},"相":{"docs":{},"同":{"docs":{},"的":{"docs":{},"值":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}}}}}},"c":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{},"，":{"docs":{},"更":{"docs":{},"趋":{"docs":{},"近":{"docs":{},"于":{"docs":{},"h":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"其":{"docs":{},"实":{"docs":{},"这":{"docs":{},"个":{"docs":{},"计":{"docs":{},"算":{"docs":{},"开":{"docs":{},"销":{"docs":{},"是":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{},"的":{"docs":{},"，":{"docs":{},"也":{"docs":{},"正":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"这":{"docs":{},"样":{"docs":{},"，":{"docs":{},"在":{"docs":{},"使":{"docs":{},"用":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"进":{"docs":{},"行":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"训":{"docs":{},"练":{"docs":{},"时":{"docs":{},"间":{"docs":{},"会":{"docs":{},"比":{"docs":{},"较":{"docs":{},"长":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{},"算":{"docs":{},"法":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"选":{"docs":{},"用":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"值":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"在":{"docs":{},"调":{"docs":{},"整":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"度":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"越":{"docs":{},"来":{"docs":{},"越":{"docs":{},"小":{"docs":{},"，":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"越":{"docs":{},"来":{"docs":{},"越":{"docs":{},"像":{"docs":{},"一":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"开":{"docs":{},"始":{"docs":{},"变":{"docs":{},"的":{"docs":{},"欠":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"x":{"docs":{},"=":{"0":{"docs":{},".":{"5":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}},"docs":{}}},"docs":{}}},"系":{"docs":{},"统":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"都":{"docs":{},"是":{"docs":{},"等":{"docs":{},"概":{"docs":{},"率":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"其":{"docs":{},"实":{"docs":{},"是":{"docs":{},"他":{"docs":{},"最":{"docs":{},"不":{"docs":{},"确":{"docs":{},"定":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"此":{"docs":{},"时":{"docs":{},"他":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"是":{"docs":{},"最":{"docs":{},"高":{"docs":{},"的":{"docs":{},"。":{"docs":{},"如":{"docs":{},"果":{"docs":{},"系":{"docs":{},"统":{"docs":{},"偏":{"docs":{},"向":{"docs":{},"于":{"docs":{},"某":{"docs":{},"一":{"docs":{},"类":{"docs":{},"，":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"有":{"docs":{},"了":{"docs":{},"约":{"docs":{},"定":{"docs":{},"向":{"docs":{},"，":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"逐":{"docs":{},"渐":{"docs":{},"降":{"docs":{},"低":{"docs":{},"，":{"docs":{},"知":{"docs":{},"道":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"占":{"docs":{},"到":{"docs":{},"了":{"docs":{},"百":{"docs":{},"分":{"docs":{},"之":{"docs":{},"百":{"docs":{},"，":{"docs":{},"此":{"docs":{},"时":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"达":{"docs":{},"到":{"docs":{},"最":{"docs":{},"低":{"docs":{},"值":{"0":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"总":{"docs":{},"结":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776},"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}},"一":{"docs":{},"句":{"docs":{},"话":{"docs":{},"就":{"docs":{},"是":{"docs":{},"：":{"docs":{},"降":{"docs":{},"低":{"docs":{},"了":{"docs":{},"维":{"docs":{},"度":{"docs":{},"，":{"docs":{},"丢":{"docs":{},"失":{"docs":{},"了":{"docs":{},"信":{"docs":{},"息":{"docs":{},"，":{"docs":{},"同":{"docs":{},"时":{"docs":{},"也":{"docs":{},"去":{"docs":{},"除":{"docs":{},"了":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"噪":{"docs":{},"音":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"docs":{},"和":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}},"此":{"docs":{},"时":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"目":{"docs":{},"标":{"docs":{},"函":{"docs":{},"数":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"化":{"docs":{},"简":{"docs":{},"成":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}},"进":{"docs":{},"行":{"docs":{},"均":{"docs":{},"值":{"docs":{},"归":{"0":{"docs":{},"操":{"docs":{},"作":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"下":{"docs":{},"面":{"docs":{},"的":{"docs":{},"式":{"docs":{},"子":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}},"docs":{}}}}}},"那":{"docs":{},"么":{"docs":{},"如":{"docs":{},"何":{"docs":{},"找":{"docs":{},"到":{"docs":{},"这":{"docs":{},"个":{"docs":{},"让":{"docs":{},"样":{"docs":{},"本":{"docs":{},"间":{"docs":{},"间":{"docs":{},"距":{"docs":{},"最":{"docs":{},"大":{"docs":{},"的":{"docs":{},"轴":{"docs":{},"?":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}},"能":{"docs":{},"够":{"docs":{},"拉":{"docs":{},"伸":{"docs":{},"最":{"docs":{},"长":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"向":{"docs":{},"量":{"docs":{},"，":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"就":{"docs":{},"是":{"docs":{},"这":{"docs":{},"个":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"的":{"docs":{},"第":{"docs":{},"一":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"；":{"docs":{"PCA/1.PCA简介.html":{"ref":"PCA/1.PCA简介.html","tf":0.027777777777777776}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"怎":{"docs":{},"么":{"docs":{},"解":{"docs":{},"决":{"docs":{},"这":{"docs":{},"个":{"docs":{},"问":{"docs":{},"题":{"docs":{},"呢":{"docs":{},"？":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}},"初":{"docs":{},"始":{"docs":{},"值":{"docs":{},"不":{"docs":{},"能":{"docs":{},"为":{"0":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"将":{"0":{"docs":{},"带":{"docs":{},"入":{"docs":{},"求":{"docs":{},"导":{"docs":{},"公":{"docs":{},"式":{"docs":{},"，":{"docs":{},"会":{"docs":{},"发":{"docs":{},"现":{"docs":{},"得":{"0":{"docs":{},"，":{"docs":{},"没":{"docs":{},"有":{"docs":{},"任":{"docs":{},"何":{"docs":{},"方":{"docs":{},"向":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}},"docs":{}}}}}}}}}}}}},"docs":{}}}}}},"docs":{}}}}}}},"因":{"docs":{},"为":{"docs":{},"对":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"目":{"docs":{},"标":{"docs":{},"函":{"docs":{},"数":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"w":{"docs":{},"=":{"0":{"docs":{},"本":{"docs":{},"身":{"docs":{},"就":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"最":{"docs":{},"小":{"docs":{},"值":{"docs":{},"点":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}},"前":{"1":{"0":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"分":{"docs":{},"数":{"docs":{},"值":{"docs":{},"都":{"docs":{},"是":{"docs":{},"小":{"docs":{},"于":{"0":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}},"docs":{}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}},"我":{"docs":{},"们":{"docs":{},"本":{"docs":{},"来":{"docs":{},"就":{"docs":{},"是":{"docs":{},"要":{"docs":{},"使":{"docs":{},"得":{"docs":{},"方":{"docs":{},"差":{"docs":{},"最":{"docs":{},"大":{"docs":{},"，":{"docs":{},"而":{"docs":{},"标":{"docs":{},"准":{"docs":{},"化":{"docs":{},"的":{"docs":{},"目":{"docs":{},"的":{"docs":{},"是":{"docs":{},"使":{"docs":{},"得":{"docs":{},"方":{"docs":{},"差":{"docs":{},"为":{"1":{"docs":{"PCA/3.主成分PCA的实现.html":{"ref":"PCA/3.主成分PCA的实现.html","tf":0.008}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}},"生":{"docs":{},"成":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"是":{"docs":{},"具":{"docs":{},"有":{"docs":{},"二":{"docs":{},"次":{"docs":{},"项":{"docs":{},"的":{"docs":{},"。":{"docs":{},"但":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"这":{"docs":{},"里":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"是":{"docs":{},"线":{"docs":{},"性":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"w":{"docs":{},"和":{"docs":{},"w":{"2":{"docs":{},"都":{"docs":{},"是":{"docs":{},"单":{"docs":{},"位":{"docs":{},"向":{"docs":{},"量":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"他":{"docs":{},"们":{"docs":{},"两":{"docs":{},"个":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"就":{"docs":{},"是":{"docs":{},"他":{"docs":{},"们":{"docs":{},"夹":{"docs":{},"角":{"docs":{},"的":{"docs":{},"c":{"docs":{},"o":{"docs":{},"s":{"docs":{},"值":{"docs":{},"，":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}},"使":{"docs":{},"用":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"_":{"docs":{},"s":{"docs":{},"p":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"很":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"只":{"docs":{},"是":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"了":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"得":{"docs":{},"出":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"·":{"docs":{},"w":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}},"（":{"docs":{},"方":{"docs":{},"向":{"docs":{},"）":{"docs":{},"就":{"docs":{},"是":{"docs":{},"x":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"在":{"docs":{},"w":{"docs":{},"上":{"docs":{},"的":{"docs":{},"分":{"docs":{},"向":{"docs":{},"量":{"docs":{},"记":{"docs":{},"为":{"docs":{},"x":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"j":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"=":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"几":{"docs":{},"乎":{"docs":{},"为":{"0":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}},"docs":{}}}},"即":{"docs":{},"x":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"w":{"docs":{},"上":{"docs":{},"的":{"docs":{},"值":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"|":{"docs":{},"|":{"docs":{},"x":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"j":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"(":{"docs":{},"i":{"docs":{},")":{"docs":{},"|":{"docs":{},"|":{"docs":{},"（":{"docs":{},"大":{"docs":{},"小":{"docs":{},"）":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"θ":{"docs":{},"t":{"docs":{},"·":{"docs":{},"x":{"docs":{},"b":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}},"又":{"docs":{},"因":{"docs":{},"为":{"docs":{},"w":{"docs":{},"和":{"docs":{},"w":{"2":{"docs":{},"应":{"docs":{},"该":{"docs":{},"是":{"docs":{},"互":{"docs":{},"相":{"docs":{},"垂":{"docs":{},"直":{"docs":{},"的":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"他":{"docs":{},"们":{"docs":{},"夹":{"docs":{},"角":{"docs":{},"的":{"docs":{},"c":{"docs":{},"o":{"docs":{},"s":{"docs":{},"值":{"docs":{},"等":{"docs":{},"于":{"0":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"x":{"docs":{},"`":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}},"均":{"docs":{},"方":{"docs":{},"误":{"docs":{},"差":{"docs":{},"的":{"docs":{},"指":{"docs":{},"标":{"docs":{},"是":{"docs":{},"具":{"docs":{},"有":{"docs":{},"可":{"docs":{},"比":{"docs":{},"性":{"docs":{},"的":{"docs":{},"，":{"docs":{},"（":{"docs":{},"但":{"docs":{},"是":{"docs":{},"对":{"docs":{},"于":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"r":{"2":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"进":{"docs":{},"行":{"docs":{},"衡":{"docs":{},"量":{"docs":{},"是":{"docs":{},"没":{"docs":{},"有":{"docs":{},"问":{"docs":{},"题":{"docs":{},"是":{"docs":{},"）":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"误":{"docs":{},"差":{"docs":{},"依":{"docs":{},"然":{"docs":{},"是":{"docs":{},"比":{"docs":{},"较":{"docs":{},"小":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"比":{"docs":{},"之":{"docs":{},"前":{"docs":{},"的":{"1":{"docs":{},".":{"1":{"8":{"docs":{},"大":{"docs":{},"了":{"docs":{},"些":{"docs":{},"，":{"docs":{},"说":{"docs":{},"明":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"做":{"docs":{},"的":{"docs":{},"有":{"docs":{},"些":{"docs":{},"过":{"docs":{},"头":{"docs":{},"了":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}},"相":{"docs":{},"减":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"分":{"docs":{},"布":{"docs":{},"几":{"docs":{},"乎":{"docs":{},"垂":{"docs":{},"直":{"docs":{},"于":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"分":{"docs":{},"布":{"docs":{"PCA/4.求数据的前N个主成分.html":{"ref":"PCA/4.求数据的前N个主成分.html","tf":0.006993006993006993}}}}}}}}}}}}}}}}}}}}}},"比":{"docs":{},"之":{"docs":{},"前":{"docs":{},"，":{"docs":{},"数":{"docs":{},"字":{"docs":{},"清":{"docs":{},"楚":{"docs":{},"了":{"docs":{},"很":{"docs":{},"多":{"docs":{},"，":{"docs":{},"平":{"docs":{},"滑":{"docs":{},"了":{"docs":{},"很":{"docs":{},"多":{"docs":{},"，":{"docs":{},"说":{"docs":{},"明":{"docs":{},"使":{"docs":{},"用":{"docs":{},"p":{"docs":{},"c":{"docs":{},"a":{"docs":{},"进":{"docs":{},"行":{"docs":{},"降":{"docs":{},"噪":{"docs":{},"是":{"docs":{},"可":{"docs":{},"行":{"docs":{},"的":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"当":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"为":{"docs":{},"样":{"docs":{},"本":{"docs":{},"多":{"docs":{},"添":{"docs":{},"加":{"docs":{},"了":{"docs":{},"一":{"docs":{},"些":{"docs":{},"特":{"docs":{},"征":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"特":{"docs":{},"征":{"docs":{},"是":{"docs":{},"原":{"docs":{},"来":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"项":{"docs":{},"，":{"docs":{},"增":{"docs":{},"加":{"docs":{},"了":{"docs":{},"这":{"docs":{},"些":{"docs":{},"特":{"docs":{},"征":{"docs":{},"之":{"docs":{},"后":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"使":{"docs":{},"用":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"思":{"docs":{},"路":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"让":{"docs":{},"模":{"docs":{},"型":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"那":{"docs":{},"一":{"docs":{},"项":{"docs":{},"起":{"docs":{},"更":{"docs":{},"大":{"docs":{},"的":{"docs":{},"作":{"docs":{},"用":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}},"反":{"docs":{},"，":{"docs":{},"在":{"docs":{},"最":{"docs":{},"开":{"docs":{},"始":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"直":{"docs":{},"接":{"docs":{},"使":{"docs":{},"用":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"来":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"也":{"docs":{},"没":{"docs":{},"有":{"docs":{},"很":{"docs":{},"好":{"docs":{},"的":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"特":{"docs":{},"征":{"docs":{},"，":{"docs":{},"当":{"docs":{},"然":{"docs":{},"他":{"docs":{},"犯":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{},"不":{"docs":{},"是":{"docs":{},"太":{"docs":{},"过":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"了":{"docs":{},"，":{"docs":{},"而":{"docs":{},"是":{"docs":{},"太":{"docs":{},"过":{"docs":{},"简":{"docs":{},"单":{"docs":{},"了":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"从":{"6":{"4":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"降":{"docs":{},"到":{"docs":{},"两":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"虽":{"docs":{},"然":{"docs":{},"运":{"docs":{},"行":{"docs":{},"速":{"docs":{},"度":{"docs":{},"提":{"docs":{},"高":{"docs":{},"了":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"识":{"docs":{},"别":{"docs":{},"精":{"docs":{},"度":{"docs":{},"大":{"docs":{},"大":{"docs":{},"降":{"docs":{},"低":{"docs":{},"了":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"7":{"8":{"4":{"docs":{},"维":{"docs":{},"降":{"docs":{},"到":{"docs":{},"了":{"8":{"7":{"docs":{},"维":{"docs":{},"，":{"docs":{},"只":{"docs":{},"用":{"8":{"7":{"docs":{},"维":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"解":{"docs":{},"释":{"docs":{},"百":{"docs":{},"分":{"docs":{},"之":{"9":{"0":{"docs":{},"的":{"docs":{},"原":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}},"docs":{}},"docs":{}}}}}},"docs":{}},"docs":{}}}}}},"docs":{}},"docs":{}},"docs":{},"高":{"docs":{},"维":{"docs":{},"数":{"docs":{},"据":{"docs":{},"向":{"docs":{},"地":{"docs":{},"维":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"映":{"docs":{},"射":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}}}}},"上":{"docs":{},"图":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"出":{"docs":{},"，":{"docs":{},"当":{"docs":{},"我":{"docs":{},"们":{"docs":{},"添":{"docs":{},"加":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"（":{"docs":{},"原":{"docs":{},"来":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"平":{"docs":{},"方":{"docs":{},"）":{"docs":{},"之":{"docs":{},"后":{"docs":{},"，":{"docs":{},"再":{"docs":{},"从":{"docs":{},"x":{"docs":{},"的":{"docs":{},"维":{"docs":{},"度":{"docs":{},"来":{"docs":{},"看":{"docs":{},"，":{"docs":{},"就":{"docs":{},"形":{"docs":{},"成":{"docs":{},"了":{"docs":{},"一":{"docs":{},"条":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"，":{"docs":{},"显":{"docs":{},"然":{"docs":{},"这":{"docs":{},"个":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"对":{"docs":{},"原":{"docs":{},"来":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"程":{"docs":{},"度":{"docs":{},"是":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"趋":{"docs":{},"势":{"docs":{},"上":{"docs":{},"看":{"docs":{},"：":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}},"直":{"docs":{},"观":{"docs":{},"的":{"docs":{},"角":{"docs":{},"度":{"docs":{},"看":{"docs":{},"，":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"也":{"docs":{},"准":{"docs":{},"确":{"docs":{},"了":{"docs":{},"很":{"docs":{},"多":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}},"降":{"docs":{},"维":{"docs":{},"的":{"docs":{},"基":{"docs":{},"本":{"docs":{},"原":{"docs":{},"理":{"docs":{},":":{"docs":{},"找":{"docs":{},"到":{"docs":{},"另":{"docs":{},"外":{"docs":{},"一":{"docs":{},"个":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"坐":{"docs":{},"标":{"docs":{},"系":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"轴":{"docs":{},"依":{"docs":{},"次":{"docs":{},"可":{"docs":{},"以":{"docs":{},"表":{"docs":{},"达":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"他":{"docs":{},"们":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"程":{"docs":{},"度":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"称":{"docs":{},"为":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"取":{"docs":{},"得":{"docs":{},"前":{"docs":{},"k":{"docs":{},"个":{"docs":{},"最":{"docs":{},"重":{"docs":{},"要":{"docs":{},"的":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"，":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"将":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"映":{"docs":{},"射":{"docs":{},"到":{"docs":{},"这":{"docs":{},"k":{"docs":{},"个":{"docs":{},"轴":{"docs":{},"上":{"docs":{},"，":{"docs":{},"获":{"docs":{},"得":{"docs":{},"一":{"docs":{},"个":{"docs":{},"低":{"docs":{},"维":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"信":{"docs":{},"息":{"docs":{"PCA/5.高维数据向低维数据进行映射.html":{"ref":"PCA/5.高维数据向低维数据进行映射.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"仔":{"docs":{},"细":{"docs":{},"观":{"docs":{},"察":{"docs":{},"后":{"docs":{},"可":{"docs":{},"以":{"docs":{},"发":{"docs":{},"现":{"docs":{},"，":{"docs":{},"很":{"docs":{},"多":{"docs":{},"数":{"docs":{},"字":{"docs":{},"的":{"docs":{},"区":{"docs":{},"分":{"docs":{},"还":{"docs":{},"是":{"docs":{},"比":{"docs":{},"较":{"docs":{},"明":{"docs":{},"细":{"docs":{},"的":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"和":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"的":{"docs":{},"不":{"docs":{},"同":{"docs":{},"在":{"docs":{},"于":{"docs":{},"，":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"学":{"docs":{},"习":{"docs":{},"曲":{"docs":{},"线":{"1":{"docs":{},".":{"5":{"docs":{},"，":{"1":{"docs":{},".":{"8":{"docs":{},"左":{"docs":{},"右":{"docs":{},"；":{"2":{"docs":{},"阶":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"稳":{"docs":{},"定":{"docs":{},"在":{"docs":{},"了":{"1":{"docs":{},".":{"0":{"docs":{},"，":{"0":{"docs":{},".":{"9":{"docs":{},"左":{"docs":{},"右":{"docs":{},",":{"2":{"docs":{},"阶":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"稳":{"docs":{},"定":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"比":{"docs":{},"较":{"docs":{},"低":{"docs":{},"，":{"docs":{},"说":{"docs":{},"明":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}}}}}}}},"docs":{}}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}}}}}}}}}}},"docs":{}}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}},"代":{"docs":{},"表":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"可":{"docs":{},"以":{"docs":{},"解":{"docs":{},"释":{"1":{"4":{"docs":{},"%":{"docs":{},"的":{"docs":{},"原":{"docs":{},"数":{"docs":{},"据":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}},"docs":{}},"docs":{}}}}}}}}}},"二":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"可":{"docs":{},"以":{"docs":{},"解":{"docs":{},"释":{"1":{"3":{"docs":{},"%":{"docs":{},"的":{"docs":{},"原":{"docs":{},"数":{"docs":{},"据":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}},"虽":{"docs":{},"然":{"docs":{},"训":{"docs":{},"练":{"docs":{},"出":{"docs":{},"来":{"docs":{},"的":{"docs":{},"精":{"docs":{},"度":{"docs":{},"丢":{"docs":{},"失":{"docs":{},"了":{"docs":{},"一":{"docs":{},"些":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"效":{"docs":{},"率":{"docs":{},"却":{"docs":{},"大":{"docs":{},"大":{"docs":{},"提":{"docs":{},"高":{"docs":{},"了":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}}}}}}}}}}}}}}}}}}}}},"整":{"docs":{},"体":{"docs":{},"速":{"docs":{},"度":{"docs":{},"慢":{"docs":{},"了":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"这":{"docs":{},"个":{"docs":{},"结":{"docs":{},"果":{"docs":{},"却":{"docs":{},"是":{"docs":{},"可":{"docs":{},"信":{"docs":{},"赖":{"docs":{},"的":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}},"边":{"docs":{},"界":{"docs":{},"还":{"docs":{},"是":{"docs":{},"比":{"docs":{},"较":{"docs":{},"奇":{"docs":{},"怪":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"比":{"docs":{},"之":{"docs":{},"前":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"=":{"2":{"0":{"docs":{},"要":{"docs":{},"好":{"docs":{},"很":{"docs":{},"多":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}},"有":{"docs":{},"很":{"docs":{},"多":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"方":{"docs":{},"法":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}}}}},"说":{"docs":{},"明":{"docs":{},"前":{"2":{"8":{"docs":{},"个":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{},"表":{"docs":{},"示":{"docs":{},"了":{"docs":{},"百":{"docs":{},"分":{"docs":{},"之":{"9":{"5":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{"PCA/6.sklearn中的PCA.html":{"ref":"PCA/6.sklearn中的PCA.html","tf":0.003236245954692557}}}}}},"docs":{}},"docs":{}}}}}}}}}}}},"docs":{}},"docs":{}},"一":{"docs":{},"共":{"docs":{},"包":{"docs":{},"含":{"5":{"7":{"4":{"9":{"docs":{},"个":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"人":{"docs":{},"的":{"docs":{},"脸":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}},"总":{"docs":{},"有":{"docs":{},"一":{"docs":{},"条":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"，":{"docs":{},"他":{"docs":{},"能":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"，":{"docs":{},"使":{"docs":{},"得":{"docs":{},"均":{"docs":{},"方":{"docs":{},"误":{"docs":{},"差":{"docs":{},"的":{"docs":{},"值":{"docs":{},"为":{"0":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}},"默":{"docs":{},"认":{"docs":{},"的":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"c":{"docs":{},"=":{"1":{"docs":{},".":{"0":{"docs":{},";":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"t":{"docs":{},"y":{"docs":{},"=":{"docs":{},"l":{"2":{"docs":{},",":{"docs":{},"说":{"docs":{},"明":{"docs":{},"默":{"docs":{},"认":{"docs":{},"使":{"docs":{},"用":{"docs":{},"l":{"2":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}},"docs":{}}}}}}}}}},"docs":{}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}},"没":{"docs":{},"有":{"docs":{},"进":{"docs":{},"行":{"docs":{},"数":{"docs":{},"据":{"docs":{},"归":{"docs":{},"一":{"docs":{},"化":{"docs":{},"，":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"这":{"docs":{},"里":{"docs":{},"的":{"docs":{},"每":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"都":{"docs":{},"标":{"docs":{},"示":{"docs":{},"的":{"docs":{},"是":{"docs":{},"每":{"docs":{},"个":{"docs":{},"像":{"docs":{},"素":{"docs":{},"点":{"docs":{},"的":{"docs":{},"亮":{"docs":{},"度":{"docs":{},"，":{"docs":{},"他":{"docs":{},"们":{"docs":{},"的":{"docs":{},"尺":{"docs":{},"度":{"docs":{},"是":{"docs":{},"相":{"docs":{},"同":{"docs":{},"的":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"时":{"docs":{},"候":{"docs":{},"比":{"docs":{},"较":{"docs":{},"两":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"之":{"docs":{},"间":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"是":{"docs":{},"有":{"docs":{},"意":{"docs":{},"义":{"docs":{},"的":{"docs":{"PCA/试手MNIST数据集.html":{"ref":"PCA/试手MNIST数据集.html","tf":0.006493506493506494}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"指":{"docs":{},"定":{"docs":{},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"t":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}},"从":{"docs":{},"最":{"docs":{},"小":{"docs":{},"值":{"docs":{},"开":{"docs":{},"始":{"docs":{},"取":{"docs":{},"，":{"docs":{},"在":{"docs":{},"s":{"docs":{},"k":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"的":{"docs":{},"封":{"docs":{},"装":{"docs":{},"中":{"docs":{},"，":{"docs":{},"他":{"docs":{},"自":{"docs":{},"动":{"docs":{},"寻":{"docs":{},"找":{"docs":{},"了":{"docs":{},"他":{"docs":{},"认":{"docs":{},"为":{"docs":{},"最":{"docs":{},"重":{"docs":{},"要":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"噪":{"docs":{},"音":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"我":{"docs":{},"们":{"docs":{},"还":{"docs":{},"是":{"docs":{},"倾":{"docs":{},"向":{"docs":{},"于":{"docs":{},"说":{"docs":{},"从":{"docs":{},"x":{"docs":{},"到":{"docs":{},"x":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"丢":{"docs":{},"失":{"docs":{},"了":{"docs":{},"一":{"docs":{},"些":{"docs":{},"信":{"docs":{},"息":{"docs":{},"，":{"docs":{},"不":{"docs":{},"过":{"docs":{},"我":{"docs":{},"们":{"docs":{},"丢":{"docs":{},"失":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"很":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"有":{"docs":{},"很":{"docs":{},"大":{"docs":{},"的":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"就":{"docs":{},"成":{"docs":{},"为":{"docs":{},"了":{"docs":{},"一":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"比":{"docs":{},"较":{"docs":{},"一":{"docs":{},"下":{"docs":{},"这":{"docs":{},"两":{"docs":{},"个":{"docs":{},"图":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"说":{"docs":{},"，":{"docs":{},"经":{"docs":{},"过":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"操":{"docs":{},"作":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"将":{"docs":{},"原":{"docs":{},"有":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"噪":{"docs":{},"音":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"模":{"docs":{},"型":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"不":{"docs":{},"规":{"docs":{},"整":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}},"换":{"docs":{},"句":{"docs":{},"话":{"docs":{},"说":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"展":{"docs":{},"现":{"docs":{},"的":{"docs":{},"是":{"docs":{},"在":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"上":{"docs":{},"下":{"docs":{},"进":{"docs":{},"行":{"docs":{},"抖":{"docs":{},"动":{"docs":{},"式":{"docs":{},"的":{"docs":{},"分":{"docs":{},"布":{"docs":{},"，":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"这":{"docs":{},"种":{"docs":{},"抖":{"docs":{},"动":{"docs":{},"和":{"docs":{},"这":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"本":{"docs":{},"身":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"是":{"docs":{},"噪":{"docs":{},"音":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"非":{"docs":{},"常":{"docs":{},"高":{"docs":{},"维":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"虽":{"docs":{},"然":{"docs":{},"使":{"docs":{},"得":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"获":{"docs":{},"得":{"docs":{},"了":{"docs":{},"更":{"docs":{},"小":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"这":{"docs":{},"根":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"完":{"docs":{},"全":{"docs":{},"不":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"想":{"docs":{},"要":{"docs":{},"的":{"docs":{},"样":{"docs":{},"子":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"现":{"docs":{},"在":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"问":{"docs":{},"题":{"docs":{},"：":{"docs":{},"这":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"展":{"docs":{},"现":{"docs":{},"出":{"docs":{},"来":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"可":{"docs":{},"是":{"docs":{},"是":{"docs":{},"不":{"docs":{},"是":{"docs":{},"有":{"docs":{},"这":{"docs":{},"样":{"docs":{},"一":{"docs":{},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"就":{"docs":{},"应":{"docs":{},"该":{"docs":{},"是":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"呢":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"画":{"docs":{},"出":{"docs":{},"带":{"docs":{},"噪":{"docs":{},"音":{"docs":{},"的":{"docs":{},"图":{"docs":{},"像":{"docs":{"PCA/使用PCA对数据进行降噪.html":{"ref":"PCA/使用PCA对数据进行降噪.html","tf":0.00909090909090909}}}}}}}}}},"另":{"docs":{},"一":{"docs":{},"方":{"docs":{},"面":{"docs":{},"，":{"docs":{},"也":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"出":{"docs":{},"来":{"docs":{},"，":{"docs":{},"其":{"docs":{},"实":{"docs":{},"每":{"docs":{},"一":{"docs":{},"张":{"docs":{},"脸":{"docs":{},"都":{"docs":{},"是":{"docs":{},"这":{"docs":{},"些":{"docs":{},"人":{"docs":{},"脸":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"线":{"docs":{},"性":{"docs":{},"组":{"docs":{},"合":{"docs":{},"，":{"docs":{},"而":{"docs":{},"特":{"docs":{},"征":{"docs":{},"脸":{"docs":{},"依":{"docs":{},"据":{"docs":{},"重":{"docs":{},"要":{"docs":{},"程":{"docs":{},"度":{"docs":{},"顺":{"docs":{},"序":{"docs":{},"的":{"docs":{},"排":{"docs":{},"在":{"docs":{},"了":{"docs":{},"这":{"docs":{},"里":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"半":{"docs":{},"式":{"docs":{},"子":{"docs":{},"反":{"docs":{},"之":{"docs":{},"亦":{"docs":{},"然":{"docs":{},"。":{"docs":{"逻辑回归/2.逻辑回归的损失函数.html":{"ref":"逻辑回归/2.逻辑回归的损失函数.html","tf":0.09090909090909091}}}}}}}}}},"种":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}},"越":{"docs":{},"往":{"docs":{},"后":{"docs":{},"，":{"docs":{},"鼻":{"docs":{},"子":{"docs":{},"眼":{"docs":{},"睛":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"就":{"docs":{},"清":{"docs":{},"晰":{"docs":{},"了":{"docs":{},"起":{"docs":{},"来":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"说":{"docs":{},"想":{"docs":{},"求":{"docs":{},"出":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"主":{"docs":{},"成":{"docs":{},"分":{"docs":{"PCA/特征脸.html":{"ref":"PCA/特征脸.html","tf":0.006802721088435374}}}}}}}}}}}}}}},"依":{"docs":{},"然":{"docs":{},"是":{"docs":{},"在":{"docs":{},"特":{"docs":{},"征":{"docs":{},"平":{"docs":{},"面":{"docs":{},"分":{"docs":{},"成":{"docs":{},"了":{"docs":{},"两":{"docs":{},"部":{"docs":{},"分":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"对":{"docs":{},"于":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"分":{"docs":{},"布":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"是":{"docs":{},"不":{"docs":{},"可":{"docs":{},"能":{"docs":{},"使":{"docs":{},"用":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"来":{"docs":{},"分":{"docs":{},"割":{"docs":{},"的":{"docs":{},"。":{"docs":{},"但":{"docs":{},"是":{"docs":{},"可":{"docs":{},"以":{"docs":{},"使":{"docs":{},"用":{"docs":{},"一":{"docs":{},"个":{"docs":{},"圆":{"docs":{},"形":{"docs":{},"。":{"docs":{},"但":{"docs":{},"是":{"docs":{},"对":{"docs":{},"于":{"docs":{},"一":{"docs":{},"个":{"docs":{},"圆":{"docs":{},"形":{"docs":{},"，":{"docs":{},"他":{"docs":{},"的":{"docs":{},"数":{"docs":{},"学":{"docs":{},"表":{"docs":{},"达":{"docs":{},"式":{"docs":{},"是":{"docs":{},"x":{"1":{"2":{"docs":{},"+":{"docs":{},"x":{"2":{"2":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"他":{"docs":{},"的":{"docs":{},"关":{"docs":{},"键":{"docs":{},"在":{"docs":{},"于":{"docs":{},"为":{"docs":{},"原":{"docs":{},"来":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"，":{"docs":{},"添":{"docs":{},"加":{"docs":{},"新":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"。":{"docs":{},"而":{"docs":{},"我":{"docs":{},"们":{"docs":{},"得":{"docs":{},"到":{"docs":{},"新":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"是":{"docs":{},"原":{"docs":{},"有":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"的":{"docs":{},"组":{"docs":{},"合":{"docs":{},"。":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"面":{"docs":{},"积":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"也":{"docs":{},"会":{"docs":{},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"这":{"docs":{},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"分":{"docs":{},"类":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"就":{"docs":{},"更":{"docs":{},"好":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}}}}}}}}},"为":{"docs":{},"了":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"我":{"docs":{},"们":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"，":{"docs":{},"变":{"docs":{},"的":{"docs":{},"太":{"docs":{},"过":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"了":{"docs":{},"，":{"docs":{},"这":{"docs":{},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"就":{"docs":{},"是":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"【":{"docs":{},"o":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"离":{"docs":{},"散":{"docs":{},"的":{"docs":{},"值":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"需":{"docs":{},"要":{"docs":{},"穷":{"docs":{},"举":{"docs":{},"所":{"docs":{},"有":{"docs":{},"θ":{"docs":{},"的":{"docs":{},"值":{"docs":{},"来":{"docs":{},"找":{"docs":{},"出":{"docs":{},"哪":{"docs":{},"些":{"docs":{},"θ":{"docs":{},"需":{"docs":{},"要":{"docs":{},"，":{"docs":{},"哪":{"docs":{},"些":{"docs":{},"不":{"docs":{},"需":{"docs":{},"要":{"docs":{},"。":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"可":{"docs":{},"以":{"docs":{},"用":{"docs":{},"l":{"1":{"docs":{},"正":{"docs":{},"则":{"docs":{},"（":{"docs":{},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"0":{"docs":{"多项式回归/L1,L2和弹性网络.html":{"ref":"多项式回归/L1,L2和弹性网络.html","tf":0.043478260869565216}}},"docs":{}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"讲":{"docs":{},"原":{"docs":{},"本":{"docs":{},"m":{"docs":{},"n":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}},"很":{"docs":{},"明":{"docs":{},"显":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"用":{"docs":{},"一":{"docs":{},"跟":{"docs":{},"直":{"docs":{},"线":{"docs":{},"来":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"一":{"docs":{},"根":{"docs":{},"有":{"docs":{},"弧":{"docs":{},"度":{"docs":{},"的":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"，":{"docs":{},"效":{"docs":{},"果":{"docs":{},"是":{"docs":{},"不":{"docs":{},"好":{"docs":{},"的":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"显":{"docs":{},"然":{"docs":{},"，":{"docs":{},"现":{"docs":{},"在":{"docs":{},"这":{"docs":{},"个":{"docs":{},"样":{"docs":{},"子":{"docs":{},"相":{"docs":{},"比":{"docs":{},"上":{"docs":{},"面":{"docs":{},"的":{"docs":{},"形":{"docs":{},"状":{"docs":{},"不":{"docs":{},"在":{"docs":{},"有":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"，":{"docs":{},"有":{"docs":{},"了":{"docs":{},"非":{"docs":{},"常":{"docs":{},"清":{"docs":{},"晰":{"docs":{},"的":{"docs":{},"边":{"docs":{},"界":{"docs":{},"（":{"docs":{},"不":{"docs":{},"会":{"docs":{},"针":{"docs":{},"对":{"docs":{},"某":{"docs":{},"几":{"docs":{},"个":{"docs":{},"特":{"docs":{},"别":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"进":{"docs":{},"行":{"docs":{},"特":{"docs":{},"殊":{"docs":{},"的":{"docs":{},"变":{"docs":{},"化":{"docs":{},"）":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"采":{"docs":{},"用":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"解":{"docs":{},"决":{"docs":{},"一":{"docs":{},"些":{"docs":{},"非":{"docs":{},"线":{"docs":{},"性":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{"多项式回归/1.什么是多项式回归.html":{"ref":"多项式回归/1.什么是多项式回归.html","tf":0.012345679012345678}}}}}}}}}}}}}}}}}}}}}}}}},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"说":{"docs":{},"p":{"docs":{},"o":{"docs":{},"l":{"docs":{},"y":{"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"f":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"会":{"docs":{},"穷":{"docs":{},"举":{"docs":{},"出":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"组":{"docs":{},"合":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"将":{"docs":{},"特":{"docs":{},"别":{"docs":{},"的":{"docs":{},"低":{"docs":{},"，":{"docs":{},"只":{"docs":{},"有":{"docs":{},"二":{"docs":{},"者":{"docs":{},"都":{"docs":{},"非":{"docs":{},"常":{"docs":{},"高":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"值":{"docs":{},"才":{"docs":{},"会":{"docs":{},"特":{"docs":{},"别":{"docs":{},"高":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"传":{"docs":{},"入":{"docs":{},"每":{"docs":{},"一":{"docs":{},"步":{"docs":{},"的":{"docs":{},"对":{"docs":{},"象":{"docs":{},"名":{"docs":{},"和":{"docs":{},"类":{"docs":{},"的":{"docs":{},"实":{"docs":{},"例":{"docs":{},"化":{"docs":{"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"ref":"多项式回归/scikit-learn中的多项式回归于pipeline.html","tf":0.005050505050505051}}}}}}}}}}}}}}}}},"【":{"docs":{},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}},"刚":{"docs":{},"刚":{"docs":{},"我":{"docs":{},"们":{"docs":{},"进":{"docs":{},"行":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"在":{"docs":{},"实":{"docs":{},"验":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"度":{"docs":{},"，":{"docs":{},"对":{"docs":{},"于":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"模":{"docs":{},"型":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"阶":{"docs":{},"数":{"docs":{},"越":{"docs":{},"高":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"会":{"docs":{},"越":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"，":{"docs":{},"在":{"docs":{},"这":{"docs":{},"种":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"对":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"算":{"docs":{},"法":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"通":{"docs":{},"常":{"docs":{},"是":{"docs":{},"有":{"docs":{},"下":{"docs":{},"面":{"docs":{},"一":{"docs":{},"张":{"docs":{},"图":{"docs":{},"的":{"docs":{},"。":{"docs":{},"横":{"docs":{},"轴":{"docs":{},"是":{"docs":{},"模":{"docs":{},"型":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"度":{"docs":{},"（":{"docs":{},"对":{"docs":{},"于":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"代":{"docs":{},"表":{"docs":{},"的":{"docs":{},"是":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"意":{"docs":{},"思":{"docs":{},"，":{"docs":{},"比":{"docs":{},"如":{"docs":{},"对":{"docs":{},"于":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"是":{"docs":{},"阶":{"docs":{},"数":{"docs":{},"越":{"docs":{},"高":{"docs":{},"，":{"docs":{},"越":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"；":{"docs":{},"对":{"docs":{},"于":{"docs":{},"k":{"docs":{},"n":{"docs":{},"n":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"是":{"docs":{},"k":{"docs":{},"越":{"docs":{},"小":{"docs":{},"，":{"docs":{},"模":{"docs":{},"型":{"docs":{},"越":{"docs":{},"复":{"docs":{},"杂":{"docs":{},"，":{"docs":{},"k":{"docs":{},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"模":{"docs":{},"型":{"docs":{},"最":{"docs":{},"简":{"docs":{},"单":{"docs":{},"，":{"docs":{},"当":{"docs":{},"k":{"docs":{},"=":{"docs":{},"n":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"模":{"docs":{},"型":{"docs":{},"就":{"docs":{},"简":{"docs":{},"化":{"docs":{},"成":{"docs":{},"了":{"docs":{},"看":{"docs":{},"整":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"里":{"docs":{},"，":{"docs":{},"哪":{"docs":{},"种":{"docs":{},"样":{"docs":{},"本":{"docs":{},"最":{"docs":{},"多":{"docs":{},"，":{"docs":{},"当":{"docs":{},"k":{"docs":{},"=":{"1":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"对":{"docs":{},"于":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"点":{"docs":{},"，":{"docs":{},"都":{"docs":{},"要":{"docs":{},"找":{"docs":{},"到":{"docs":{},"离":{"docs":{},"他":{"docs":{},"最":{"docs":{},"近":{"docs":{},"的":{"docs":{},"那":{"docs":{},"个":{"docs":{},"点":{"docs":{},"）":{"docs":{},"，":{"docs":{},"另":{"docs":{},"一":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"是":{"docs":{},"模":{"docs":{},"型":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"（":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"他":{"docs":{},"能":{"docs":{},"够":{"docs":{},"多":{"docs":{},"好":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"）":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"显":{"docs":{},"然":{"docs":{},"使":{"docs":{},"用":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"是":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.009345794392523364}}}}}}}}}}}}}}}}}},"有":{"docs":{},"非":{"docs":{},"常":{"docs":{},"多":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{},"分":{"docs":{},"类":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"导":{"docs":{},"致":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"分":{"docs":{},"类":{"docs":{},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{},"只":{"docs":{},"有":{"0":{"docs":{},".":{"6":{"0":{"5":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}},"没":{"docs":{},"有":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"高":{"docs":{},"，":{"docs":{},"这":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"首":{"docs":{},"先":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"是":{"docs":{},"有":{"docs":{},"偏":{"docs":{},"的":{"docs":{},"，":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"都":{"docs":{},"比":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"要":{"docs":{},"低":{"docs":{},"一":{"docs":{},"些":{"docs":{},"，":{"docs":{},"在":{"docs":{},"这":{"docs":{},"里":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"和":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"能":{"docs":{},"够":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"反":{"docs":{},"应":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{},"其":{"docs":{},"次":{"docs":{},"使":{"docs":{},"用":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"进":{"docs":{},"行":{"docs":{},"预":{"docs":{},"测":{"docs":{},"，":{"docs":{},"明":{"docs":{},"显":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"比":{"docs":{},"较":{"docs":{},"低":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"f":{"1":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"欠":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"和":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"的":{"docs":{},"标":{"docs":{},"准":{"docs":{},"定":{"docs":{},"义":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}},"：":{"docs":{},"算":{"docs":{},"法":{"docs":{},"所":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"不":{"docs":{},"能":{"docs":{},"完":{"docs":{},"整":{"docs":{},"表":{"docs":{},"述":{"docs":{},"数":{"docs":{},"据":{"docs":{},"关":{"docs":{},"系":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}},"直":{"docs":{},"接":{"docs":{},"使":{"docs":{},"用":{"docs":{},"线":{"docs":{},"性":{"docs":{},"回":{"docs":{},"归":{"docs":{},"，":{"docs":{},"显":{"docs":{},"然":{"docs":{},"分":{"docs":{},"数":{"docs":{},"太":{"docs":{},"低":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"。":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}}}},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":0.014705882352941176}},"：":{"docs":{},"算":{"docs":{},"法":{"docs":{},"所":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"过":{"docs":{},"多":{"docs":{},"的":{"docs":{},"表":{"docs":{},"达":{"docs":{},"了":{"docs":{},"数":{"docs":{},"据":{"docs":{},"间":{"docs":{},"的":{"docs":{},"噪":{"docs":{},"音":{"docs":{},"关":{"docs":{},"系":{"docs":{"多项式回归/过拟合与前拟合.html":{"ref":"多项式回归/过拟合与前拟合.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}},"（":{"docs":{},"非":{"docs":{},"常":{"docs":{},"的":{"docs":{},"完":{"docs":{},"全":{"docs":{},"，":{"docs":{},"两":{"docs":{},"段":{"docs":{},"有":{"docs":{},"极":{"docs":{},"端":{"docs":{},"的":{"docs":{},"情":{"docs":{},"况":{"docs":{},"）":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"通":{"docs":{},"常":{"docs":{},"不":{"docs":{},"会":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"某":{"docs":{},"一":{"docs":{},"组":{"docs":{},"的":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"平":{"docs":{},"均":{"docs":{},"来":{"docs":{},"讲":{"docs":{},"这":{"docs":{},"个":{"docs":{},"分":{"docs":{},"数":{"docs":{},"会":{"docs":{},"稍":{"docs":{},"微":{"docs":{},"低":{"docs":{},"一":{"docs":{},"些":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"滤":{"docs":{},"超":{"docs":{},"出":{"docs":{},"绘":{"docs":{},"制":{"docs":{},"范":{"docs":{},"围":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}},"观":{"docs":{},"察":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{},"的":{"docs":{},"学":{"docs":{},"习":{"docs":{},"曲":{"docs":{},"线":{"docs":{"多项式回归/学习曲线.html":{"ref":"多项式回归/学习曲线.html","tf":0.008}}}}}}}}}}}},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"值":{"docs":{},"p":{"docs":{},"与":{"docs":{},"y":{"docs":{},"_":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"的":{"docs":{},"关":{"docs":{},"系":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015},"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}}}}}}}}}}}}}}}}}}}}},"交":{"docs":{},"叉":{"docs":{},"验":{"docs":{},"证":{"docs":{},"相":{"docs":{},"对":{"docs":{},"来":{"docs":{},"说":{"docs":{},"是":{"docs":{},"比":{"docs":{},"较":{"docs":{},"正":{"docs":{},"规":{"docs":{},"的":{"docs":{},"、":{"docs":{},"比":{"docs":{},"较":{"docs":{},"标":{"docs":{},"准":{"docs":{},"的":{"docs":{},"在":{"docs":{},"我":{"docs":{},"们":{"docs":{},"调":{"docs":{},"整":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"参":{"docs":{},"数":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"看":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"：":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"通":{"docs":{},"常":{"docs":{},"把":{"docs":{},"数":{"docs":{},"据":{"docs":{},"分":{"docs":{},"成":{"docs":{},"k":{"docs":{},"份":{"docs":{},"，":{"docs":{},"例":{"docs":{},"如":{"docs":{},"分":{"docs":{},"成":{"3":{"docs":{},"份":{"docs":{},"（":{"docs":{},"a":{"docs":{},"b":{"docs":{},"c":{"docs":{},"）":{"docs":{},"（":{"docs":{},"分":{"docs":{},"成":{"docs":{},"k":{"docs":{},"分":{"docs":{},"，":{"docs":{},"k":{"docs":{},"属":{"docs":{},"于":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"）":{"docs":{},"，":{"docs":{},"这":{"docs":{},"三":{"docs":{},"份":{"docs":{},"分":{"docs":{},"别":{"docs":{},"作":{"docs":{},"为":{"docs":{},"验":{"docs":{},"证":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"和":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"。":{"docs":{},"这":{"docs":{},"样":{"docs":{},"组":{"docs":{},"合":{"docs":{},"后":{"docs":{},"可":{"docs":{},"以":{"docs":{},"分":{"docs":{},"别":{"docs":{},"产":{"docs":{},"生":{"docs":{},"三":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"这":{"docs":{},"三":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"每":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"在":{"docs":{},"测":{"docs":{},"试":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"上":{"docs":{},"都":{"docs":{},"会":{"docs":{},"产":{"docs":{},"生":{"docs":{},"一":{"docs":{},"个":{"docs":{},"性":{"docs":{},"能":{"docs":{},"的":{"docs":{},"指":{"docs":{},"标":{"docs":{},"，":{"docs":{},"这":{"docs":{},"三":{"docs":{},"个":{"docs":{},"指":{"docs":{},"标":{"docs":{},"的":{"docs":{},"平":{"docs":{},"均":{"docs":{},"值":{"docs":{},"作":{"docs":{},"为":{"docs":{},"当":{"docs":{},"前":{"docs":{},"这":{"docs":{},"个":{"docs":{},"算":{"docs":{},"法":{"docs":{},"训":{"docs":{},"练":{"docs":{},"处":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"衡":{"docs":{},"量":{"docs":{},"的":{"docs":{},"标":{"docs":{},"准":{"docs":{},"是":{"docs":{},"怎":{"docs":{},"样":{"docs":{},"的":{"docs":{},"。":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"极":{"docs":{},"端":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"k":{"docs":{"多项式回归/验证数据集与交叉验证.html":{"ref":"多项式回归/验证数据集与交叉验证.html","tf":0.0035587188612099642}}}}}}}}},"偏":{"docs":{},"差":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}},"和":{"docs":{},"方":{"docs":{},"差":{"docs":{},"是":{"docs":{},"互":{"docs":{},"相":{"docs":{},"矛":{"docs":{},"盾":{"docs":{},"的":{"docs":{},"。":{"docs":{},"降":{"docs":{},"低":{"docs":{},"方":{"docs":{},"差":{"docs":{},"会":{"docs":{},"提":{"docs":{},"高":{"docs":{},"偏":{"docs":{},"差":{"docs":{},"，":{"docs":{},"降":{"docs":{},"低":{"docs":{},"偏":{"docs":{},"差":{"docs":{},"会":{"docs":{},"提":{"docs":{},"高":{"docs":{},"方":{"docs":{},"差":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"参":{"docs":{},"数":{"docs":{},"学":{"docs":{},"习":{"docs":{},"通":{"docs":{},"常":{"docs":{},"都":{"docs":{},"是":{"docs":{},"高":{"docs":{},"偏":{"docs":{},"差":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"。":{"docs":{},"因":{"docs":{},"为":{"docs":{},"对":{"docs":{},"数":{"docs":{},"据":{"docs":{},"具":{"docs":{},"有":{"docs":{},"极":{"docs":{},"强":{"docs":{},"的":{"docs":{},"假":{"docs":{},"设":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"大":{"docs":{},"多":{"docs":{},"数":{"docs":{},"算":{"docs":{},"法":{"docs":{},"具":{"docs":{},"有":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"调":{"docs":{},"整":{"docs":{},"偏":{"docs":{},"差":{"docs":{},"和":{"docs":{},"方":{"docs":{},"差":{"docs":{"多项式回归/偏差方差均衡.html":{"ref":"多项式回归/偏差方差均衡.html","tf":0.05263157894736842}}}}}}}}}}}}}}}}}}}}},"时":{"docs":{},"候":{"docs":{},"二":{"docs":{},"者":{"docs":{},"没":{"docs":{},"有":{"docs":{},"特":{"docs":{},"别":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"优":{"docs":{},"劣":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}}}}}},"α":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"代":{"docs":{},"表":{"docs":{},"在":{"docs":{},"我":{"docs":{},"们":{"docs":{},"模":{"docs":{},"型":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"下":{"docs":{},"新":{"docs":{},"的":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"中":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"要":{"docs":{},"让":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"θ":{"docs":{},"尽":{"docs":{},"可":{"docs":{},"能":{"docs":{},"的":{"docs":{},"小":{"docs":{},"，":{"docs":{},"小":{"docs":{},"的":{"docs":{},"程":{"docs":{},"度":{"docs":{},"占":{"docs":{},"我":{"docs":{},"们":{"docs":{},"整":{"docs":{},"个":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"的":{"docs":{},"多":{"docs":{},"少":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"α":{"docs":{},"等":{"docs":{},"于":{"0":{"docs":{},"，":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"没":{"docs":{},"有":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"；":{"docs":{},"如":{"docs":{},"果":{"docs":{},"α":{"docs":{},"是":{"docs":{},"正":{"docs":{},"无":{"docs":{},"穷":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"那":{"docs":{},"么":{"docs":{},"我":{"docs":{},"们":{"docs":{},"主":{"docs":{},"要":{"docs":{},"的":{"docs":{},"优":{"docs":{},"化":{"docs":{},"任":{"docs":{},"务":{"docs":{},"就":{"docs":{},"是":{"docs":{},"让":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"θ":{"docs":{},"尽":{"docs":{},"可":{"docs":{},"能":{"docs":{},"的":{"docs":{},"小":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"=":{"1":{"0":{"0":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"docs":{},"的":{"docs":{},"得":{"docs":{},"到":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"依":{"docs":{},"旧":{"docs":{},"是":{"docs":{},"一":{"docs":{},"根":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"，":{"docs":{},"事":{"docs":{},"实":{"docs":{},"上":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"docs":{},"很":{"docs":{},"难":{"docs":{},"得":{"docs":{},"到":{"docs":{},"一":{"docs":{},"根":{"docs":{},"倾":{"docs":{},"斜":{"docs":{},"的":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"他":{"docs":{},"一":{"docs":{},"直":{"docs":{},"是":{"docs":{},"弯":{"docs":{},"曲":{"docs":{},"的":{"docs":{},"形":{"docs":{},"状":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"岭":{"docs":{},"回":{"docs":{},"归":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}},"而":{"docs":{},"使":{"docs":{},"得":{"docs":{},"θ":{"docs":{},"的":{"docs":{},"平":{"docs":{},"方":{"docs":{},"和":{"docs":{},"最":{"docs":{},"小":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"使":{"docs":{},"得":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"θ":{"docs":{},"都":{"docs":{},"趋":{"docs":{},"近":{"docs":{},"于":{"0":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"时":{"docs":{},"候":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"就":{"docs":{},"趋":{"docs":{},"近":{"docs":{},"于":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"了":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"对":{"docs":{},"于":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"要":{"docs":{},"得":{"docs":{},"到":{"docs":{},"一":{"docs":{},"个":{"docs":{},"函":{"docs":{},"数":{"docs":{},"f":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"将":{"docs":{},"样":{"docs":{},"本":{"docs":{},"x":{"docs":{},"输":{"docs":{},"入":{"docs":{},"f":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"f":{"docs":{},"会":{"docs":{},"计":{"docs":{},"算":{"docs":{},"出":{"docs":{},"y":{"docs":{},"一":{"docs":{},"个":{"docs":{},"概":{"docs":{},"率":{"docs":{},"值":{"docs":{},"p":{"docs":{},"，":{"docs":{},"之":{"docs":{},"后":{"docs":{},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"这":{"docs":{},"个":{"docs":{},"概":{"docs":{},"率":{"docs":{},"值":{"docs":{},"p":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"分":{"docs":{},"类":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"p":{"docs":{},">":{"docs":{},"=":{"0":{"docs":{},".":{"5":{"docs":{},",":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"有":{"docs":{},"百":{"docs":{},"分":{"docs":{},"之":{"5":{"0":{"docs":{},"以":{"docs":{},"上":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"发":{"docs":{},"生":{"docs":{},"的":{"docs":{},"话":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"让":{"docs":{},"这":{"docs":{},"个":{"docs":{},"概":{"docs":{},"率":{"docs":{},"的":{"docs":{},"值":{"docs":{},"为":{"1":{"docs":{},"，":{"docs":{},"否":{"docs":{},"则":{"docs":{},"让":{"docs":{},"他":{"docs":{},"为":{"1":{"docs":{},"，":{"docs":{},"当":{"docs":{},"然":{"1":{"docs":{},"和":{"0":{"docs":{},"在":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"场":{"docs":{},"景":{"docs":{},"下":{"docs":{},"代":{"docs":{},"表":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"意":{"docs":{},"思":{"docs":{},"。":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"的":{"docs":{},"应":{"docs":{},"用":{"docs":{},"场":{"docs":{},"景":{"docs":{},"是":{"docs":{},"比":{"docs":{},"较":{"docs":{},"两":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"优":{"docs":{},"劣":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}}}}}}}}}}}},"让":{"docs":{},"r":{"docs":{},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{},"e":{"2":{"docs":{},"_":{"docs":{},"r":{"docs":{},"e":{"docs":{},"g":{"docs":{"多项式回归/模型正则化.html":{"ref":"多项式回归/模型正则化.html","tf":0.005714285714285714}}}}}}},"docs":{}}}}}}},"选":{"docs":{},"择":{"docs":{},"运":{"docs":{},"算":{"docs":{},"符":{"docs":{"多项式回归/LASSO.html":{"ref":"多项式回归/LASSO.html","tf":0.013888888888888888}}}}}}},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"：":{"docs":{},"解":{"docs":{},"决":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{"逻辑回归/1.什么是逻辑回归.html":{"ref":"逻辑回归/1.什么是逻辑回归.html","tf":0.03571428571428571}}}}}}}}},"中":{"docs":{},"添":{"docs":{},"加":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"项":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}},"，":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"是":{"docs":{},"在":{"docs":{},"决":{"docs":{},"策":{"docs":{},"平":{"docs":{},"面":{"docs":{},"中":{"docs":{},"找":{"docs":{},"到":{"docs":{},"一":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"使":{"docs":{},"用":{"docs":{},"这":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"。":{"docs":{},"用":{"docs":{},"这":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{},"来":{"docs":{},"分":{"docs":{},"割":{"docs":{},"所":{"docs":{},"有":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"分":{"docs":{},"类":{"docs":{},"，":{"docs":{},"用":{"docs":{},"这":{"docs":{},"样":{"docs":{},"一":{"docs":{},"个":{"docs":{},"例":{"docs":{},"子":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"到":{"docs":{},"。":{"docs":{},"为":{"docs":{},"什":{"docs":{},"么":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"只":{"docs":{},"能":{"docs":{},"解":{"docs":{},"决":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"这":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"只":{"docs":{},"能":{"docs":{},"将":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"平":{"docs":{},"面":{"docs":{},"分":{"docs":{},"成":{"docs":{},"两":{"docs":{},"部":{"docs":{},"分":{"docs":{},"。":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"如":{"docs":{},"果":{"docs":{},"我":{"docs":{},"们":{"docs":{},"传":{"docs":{},"进":{"docs":{},"来":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"有":{"docs":{},"多":{"docs":{},"个":{"docs":{},"分":{"docs":{},"类":{"docs":{},"，":{"docs":{},"他":{"docs":{},"讲":{"docs":{},"使":{"docs":{},"用":{"docs":{},"o":{"docs":{},"v":{"docs":{},"r":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"解":{"docs":{},"决":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"ref":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","tf":0.0026666666666666666}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"只":{"docs":{},"可":{"docs":{},"以":{"docs":{},"解":{"docs":{},"决":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"对":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"回":{"docs":{},"归":{"docs":{},"稍":{"docs":{},"加":{"docs":{},"改":{"docs":{},"造":{"docs":{},"，":{"docs":{},"让":{"docs":{},"他":{"docs":{},"解":{"docs":{},"决":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"：":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"可":{"docs":{},"以":{"docs":{},"估":{"docs":{},"算":{"docs":{},"概":{"docs":{},"率":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}},"（":{"docs":{},"注":{"docs":{},"意":{"docs":{},"区":{"docs":{},"别":{"docs":{},"i":{"docs":{},"f":{"docs":{"逻辑回归/2.逻辑回归的损失函数.html":{"ref":"逻辑回归/2.逻辑回归的损失函数.html","tf":0.09090909090909091}}}}}}}},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0091324200913242}}}}}},"σ":{"docs":{},"(":{"docs":{},"t":{"docs":{},")":{"docs":{},")":{"docs":{},"进":{"docs":{},"行":{"docs":{},"求":{"docs":{},"导":{"docs":{"逻辑回归/3.逻辑回归函数损失的梯度.html":{"ref":"逻辑回归/3.逻辑回归函数损失的梯度.html","tf":0.09090909090909091}}}}}}}}}}},"只":{"docs":{},"取":{"docs":{},"前":{"docs":{},"两":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"；":{"docs":{},"只":{"docs":{},"取":{"docs":{},"y":{"docs":{},"=":{"0":{"docs":{},"，":{"1":{"docs":{"逻辑回归/4.实现逻辑回归算法.html":{"ref":"逻辑回归/4.实现逻辑回归算法.html","tf":0.0043859649122807015}}},"docs":{}}},"docs":{}}}}}}}}}}}},"有":{"docs":{},"都":{"docs":{},"非":{"docs":{},"常":{"docs":{},"大":{"docs":{},"，":{"docs":{},"结":{"docs":{},"果":{"docs":{},"才":{"docs":{},"会":{"docs":{},"打":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}},"至":{"docs":{},"少":{"docs":{},"两":{"docs":{},"个":{"docs":{},"算":{"docs":{},"法":{"docs":{},"预":{"docs":{},"测":{"docs":{},"结":{"docs":{},"果":{"docs":{},"为":{"1":{"docs":{},"，":{"docs":{},"三":{"docs":{},"个":{"docs":{},"相":{"docs":{},"加":{"docs":{},"才":{"docs":{},"会":{"docs":{},"大":{"docs":{},"于":{"2":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}},"docs":{}}}}}}}}}}},"docs":{}}}}}}}}}}}}},"需":{"docs":{},"要":{"docs":{},"把":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"的":{"docs":{},"函":{"docs":{},"数":{"docs":{},"改":{"docs":{},"成":{"docs":{},"基":{"docs":{},"尼":{"docs":{},"系":{"docs":{},"数":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}}}}},"看":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"θ":{"docs":{},"t":{"docs":{},"·":{"docs":{},"x":{"docs":{},"b":{"docs":{},"=":{"0":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}},"docs":{}}}}}}},"是":{"docs":{},"一":{"docs":{},"条":{"docs":{},"抛":{"docs":{},"物":{"docs":{},"线":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}},"上":{"docs":{},"面":{"docs":{},"的":{"docs":{},"直":{"docs":{},"线":{"docs":{},"方":{"docs":{},"程":{"docs":{},"：":{"docs":{},"x":{"1":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}},"docs":{}}}}}}}}}},"下":{"docs":{},"面":{"docs":{},"的":{"docs":{},"直":{"docs":{},"线":{"docs":{},"方":{"docs":{},"程":{"docs":{},"：":{"docs":{},"x":{"1":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}},"docs":{}}}}}}}}}},"的":{"docs":{},"直":{"docs":{},"线":{"docs":{},"方":{"docs":{},"程":{"docs":{},"：":{"docs":{},"w":{"0":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}},"docs":{}}}}}}}},"非":{"docs":{},"常":{"docs":{},"不":{"docs":{},"规":{"docs":{},"则":{"docs":{},"，":{"docs":{},"产":{"docs":{},"生":{"docs":{},"了":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}}}}}}},"必":{"docs":{},"须":{"docs":{},"是":{"docs":{},"横":{"docs":{},"平":{"docs":{},"竖":{"docs":{},"直":{"docs":{},"的":{"docs":{"127-jue-ce-shu-de-ju-xian-xing.html":{"ref":"127-jue-ce-shu-de-ju-xian-xing.html","tf":0.2}}}}}}}}}}}},"树":{"docs":{},"可":{"docs":{},"以":{"docs":{},"解":{"docs":{},"决":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"非":{"docs":{},"参":{"docs":{},"数":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}}}}},"最":{"docs":{},"多":{"docs":{},"的":{"docs":{},"叶":{"docs":{},"子":{"docs":{},"节":{"docs":{},"点":{"docs":{"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"ref":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","tf":0.0049504950495049506}}}}}}}}},"解":{"docs":{},"决":{"docs":{},"回":{"docs":{},"归":{"docs":{},"问":{"docs":{},"题":{"docs":{"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"ref":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","tf":5.014705882352941}}}}}}}},"的":{"docs":{},"局":{"docs":{},"限":{"docs":{},"性":{"docs":{"127-jue-ce-shu-de-ju-xian-xing.html":{"ref":"127-jue-ce-shu-de-ju-xian-xing.html","tf":5.2}}}}}},"，":{"docs":{},"叶":{"docs":{},"子":{"docs":{},"结":{"docs":{},"点":{"docs":{},"中":{"docs":{},"占":{"docs":{},"比":{"docs":{},"例":{"docs":{},"最":{"docs":{},"大":{"docs":{},"的":{"docs":{},"类":{"docs":{},"别":{"docs":{},"数":{"docs":{},"据":{"docs":{},"占":{"docs":{},"整":{"docs":{},"个":{"docs":{},"叶":{"docs":{},"子":{"docs":{},"结":{"docs":{},"点":{"docs":{},"量":{"docs":{},"的":{"docs":{},"比":{"docs":{},"值":{"docs":{"132-softvoting-classifier.html":{"ref":"132-softvoting-classifier.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"在":{"docs":{},"节":{"docs":{},"点":{"docs":{},"划":{"docs":{},"分":{"docs":{},"上":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"随":{"docs":{},"机":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"和":{"docs":{},"随":{"docs":{},"机":{"docs":{},"的":{"docs":{},"阈":{"docs":{},"值":{"docs":{},"(":{"docs":{},"理":{"docs":{},"论":{"docs":{},"基":{"docs":{},"础":{"docs":{},"：":{"docs":{},"根":{"docs":{},"据":{"docs":{},"b":{"docs":{},"a":{"docs":{},"g":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}}}}}}}}}}}}}}}}}}},"在":{"docs":{},"随":{"docs":{},"机":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"子":{"docs":{},"集":{"docs":{},"上":{"docs":{},"寻":{"docs":{},"找":{"docs":{},"最":{"docs":{},"优":{"docs":{},"划":{"docs":{},"分":{"docs":{},"特":{"docs":{},"征":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"表":{"docs":{},"示":{"docs":{},"的":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"是":{"docs":{},"一":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"有":{"docs":{},"两":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"，":{"docs":{},"那":{"docs":{},"就":{"docs":{},"是":{"docs":{},"在":{"docs":{},"二":{"docs":{},"维":{"docs":{},"平":{"docs":{},"面":{"docs":{},"上":{"docs":{},"的":{"docs":{},"一":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{},"x":{"1":{"docs":{},"是":{"docs":{},"横":{"docs":{},"轴":{"docs":{},"，":{"docs":{},"x":{"2":{"docs":{},"是":{"docs":{},"纵":{"docs":{},"轴":{"docs":{"逻辑回归/5.决策边界.html":{"ref":"逻辑回归/5.决策边界.html","tf":0.004672897196261682}}}}}},"docs":{}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"。":{"docs":{},"针":{"docs":{},"对":{"docs":{},"x":{"1":{"2":{"docs":{},"和":{"docs":{},"x":{"2":{"2":{"docs":{},"依":{"docs":{},"然":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"线":{"docs":{},"性":{"docs":{},"关":{"docs":{},"系":{"docs":{},"。":{"docs":{},"但":{"docs":{},"是":{"docs":{},"对":{"docs":{},"于":{"docs":{},"x":{"1":{"docs":{},"，":{"docs":{},"和":{"docs":{},"x":{"2":{"docs":{},"就":{"docs":{},"是":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"了":{"docs":{"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"ref":"逻辑回归/6.在逻辑回归中使用多项式回归.html","tf":0.005714285714285714}}}}}}}},"docs":{}}}}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}}}}},"尝":{"docs":{},"试":{"docs":{},"使":{"docs":{},"用":{"docs":{},"l":{"1":{"docs":{},"正":{"docs":{},"则":{"docs":{},"项":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}},"docs":{}},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"回":{"docs":{},"归":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"特":{"docs":{},"征":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}},"增":{"docs":{},"大":{"docs":{},"多":{"docs":{},"项":{"docs":{},"式":{"docs":{},"项":{"docs":{},"d":{"docs":{},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"的":{"docs":{},"值":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}},"调":{"docs":{},"整":{"docs":{},"正":{"docs":{},"则":{"docs":{},"化":{"docs":{},"超":{"docs":{},"参":{"docs":{},"数":{"docs":{},"c":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}},"已":{"docs":{},"经":{"docs":{},"非":{"docs":{},"常":{"docs":{},"接":{"docs":{},"近":{"docs":{},"之":{"docs":{},"前":{"docs":{},"的":{"docs":{},"正":{"docs":{},"常":{"docs":{},"的":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"了":{"docs":{"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"ref":"逻辑回归/7.scikit-learn中的逻辑会回归.html","tf":0.002857142857142857}}}}}}}}}}}}}}}}}}},"复":{"docs":{},"杂":{"docs":{},"度":{"docs":{},"是":{"docs":{},"o":{"docs":{},"(":{"docs":{},"n":{"2":{"docs":{},")":{"docs":{},",":{"docs":{},"但":{"docs":{},"是":{"docs":{},"分":{"docs":{},"类":{"docs":{},"结":{"docs":{},"果":{"docs":{},"相":{"docs":{},"比":{"docs":{},"o":{"docs":{},"v":{"docs":{},"r":{"docs":{},"是":{"docs":{},"更":{"docs":{},"加":{"docs":{},"准":{"docs":{},"确":{"docs":{},"的":{"docs":{},"，":{"docs":{},"这":{"docs":{},"是":{"docs":{},"因":{"docs":{},"为":{"docs":{},"每":{"docs":{},"次":{"docs":{},"只":{"docs":{},"用":{"docs":{},"真":{"docs":{},"实":{"docs":{},"的":{"docs":{},"两":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"进":{"docs":{},"行":{"docs":{},"比":{"docs":{},"较":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"他":{"docs":{},"更":{"docs":{},"倾":{"docs":{},"向":{"docs":{},"于":{"docs":{},"真":{"docs":{},"实":{"docs":{},"的":{"docs":{},"样":{"docs":{},"本":{"docs":{},"属":{"docs":{},"于":{"docs":{},"哪":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}},"由":{"docs":{},"t":{"docs":{},"变":{"docs":{},"成":{"docs":{},"了":{"docs":{},"n":{"docs":{},"*":{"docs":{},"t":{"docs":{"逻辑回归/8.OvR与OvO.html":{"ref":"逻辑回归/8.OvR与OvO.html","tf":0.0045662100456621}}}}}}}}}}}}},"准":{"docs":{},"确":{"docs":{},"度":{"docs":{},"的":{"docs":{},"陷":{"docs":{},"阱":{"docs":{},"和":{"docs":{},"混":{"docs":{},"淆":{"docs":{},"矩":{"docs":{},"阵":{"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":5.083333333333333}}}}}}}}}}}}},"混":{"docs":{},"淆":{"docs":{},"矩":{"docs":{},"阵":{"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":0.08333333333333333}}}}}},"行":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"二":{"docs":{},"维":{"docs":{},"数":{"docs":{},"组":{"docs":{},"的":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"，":{"docs":{},"列":{"docs":{},"相":{"docs":{},"当":{"docs":{},"于":{"docs":{},"二":{"docs":{},"维":{"docs":{},"数":{"docs":{},"组":{"docs":{},"的":{"docs":{},"第":{"docs":{},"二":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"，":{"docs":{},"相":{"docs":{},"对":{"docs":{},"来":{"docs":{},"将":{"docs":{},"，":{"docs":{},"真":{"docs":{},"实":{"docs":{},"值":{"docs":{},"（":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"）":{"docs":{},"要":{"docs":{},"在":{"docs":{},"预":{"docs":{},"测":{"docs":{},"值":{"docs":{},"（":{"docs":{},"第":{"docs":{},"二":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"）":{"docs":{},"的":{"docs":{},"前":{"docs":{},"面":{"docs":{"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"ref":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"：":{"docs":{},"我":{"docs":{},"们":{"docs":{},"关":{"docs":{},"注":{"docs":{},"的":{"docs":{},"那":{"docs":{},"个":{"docs":{},"事":{"docs":{},"件":{"docs":{},"，":{"docs":{},"真":{"docs":{},"实":{"docs":{},"的":{"docs":{},"发":{"docs":{},"生":{"docs":{},"了":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"(":{"docs":{},"分":{"docs":{},"母":{"docs":{},")":{"docs":{},"中":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"成":{"docs":{},"功":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"了":{"docs":{},"多":{"docs":{},"少":{"docs":{},"。":{"docs":{"评价分类结果/9.2 精准率和召回率.html":{"ref":"评价分类结果/9.2 精准率和召回率.html","tf":0.1111111111111111}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"低":{"docs":{},"意":{"docs":{},"味":{"docs":{},"着":{"docs":{},":":{"docs":{},"本":{"docs":{},"来":{"docs":{},"一":{"docs":{},"个":{"docs":{},"病":{"docs":{},"人":{"docs":{},"得":{"docs":{},"病":{"docs":{},"了":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"我":{"docs":{},"们":{"docs":{},"没":{"docs":{},"有":{"docs":{},"把":{"docs":{},"他":{"docs":{},"预":{"docs":{},"测":{"docs":{},"出":{"docs":{},"来":{"docs":{},"，":{"docs":{},"这":{"docs":{},"就":{"docs":{},"意":{"docs":{},"味":{"docs":{},"着":{"docs":{},"这":{"docs":{},"个":{"docs":{},"病":{"docs":{},"人":{"docs":{},"的":{"docs":{},"病":{"docs":{},"情":{"docs":{},"会":{"docs":{},"继":{"docs":{},"续":{"docs":{},"恶":{"docs":{},"化":{"docs":{},"下":{"docs":{},"去":{"docs":{},"。":{"docs":{},"所":{"docs":{},"以":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"更":{"docs":{},"加":{"docs":{},"重":{"docs":{},"要":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"希":{"docs":{},"望":{"docs":{},"把":{"docs":{},"所":{"docs":{},"有":{"docs":{},"有":{"docs":{},"病":{"docs":{},"的":{"docs":{},"患":{"docs":{},"者":{"docs":{},"都":{"docs":{},"预":{"docs":{},"测":{"docs":{},"出":{"docs":{},"来":{"docs":{},"。":{"docs":{},"但":{"docs":{},"是":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"却":{"docs":{},"不":{"docs":{},"是":{"docs":{},"特":{"docs":{},"别":{"docs":{},"重":{"docs":{},"要":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"本":{"docs":{},"来":{"docs":{},"一":{"docs":{},"个":{"docs":{},"人":{"docs":{},"没":{"docs":{},"病":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"预":{"docs":{},"测":{"docs":{},"他":{"docs":{},"有":{"docs":{},"病":{"docs":{},"，":{"docs":{},"这":{"docs":{},"时":{"docs":{},"候":{"docs":{},"让":{"docs":{},"他":{"docs":{},"去":{"docs":{},"做":{"docs":{},"进":{"docs":{},"一":{"docs":{},"步":{"docs":{},"的":{"docs":{},"检":{"docs":{},"查":{"docs":{},"，":{"docs":{},"进":{"docs":{},"行":{"docs":{},"确":{"docs":{},"诊":{"docs":{},"就":{"docs":{},"好":{"docs":{},"了":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"犯":{"docs":{},"了":{"docs":{},"f":{"docs":{},"p":{"docs":{},"的":{"docs":{},"错":{"docs":{},"误":{"docs":{},"，":{"docs":{},"只":{"docs":{},"是":{"docs":{},"让":{"docs":{},"他":{"docs":{},"多":{"docs":{},"做":{"docs":{},"了":{"docs":{},"一":{"docs":{},"次":{"docs":{},"检":{"docs":{},"查":{"docs":{},"而":{"docs":{},"已":{"docs":{},"。":{"docs":{},"这":{"docs":{},"个":{"docs":{},"时":{"docs":{},"候":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"比":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"重":{"docs":{},"要":{"docs":{},"。":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"曲":{"docs":{},"线":{"docs":{"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":3.347721822541966}}}}}}},"手":{"docs":{},"动":{"docs":{},"将":{"docs":{},"手":{"docs":{},"写":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"变":{"docs":{},"成":{"docs":{},"及":{"docs":{},"其":{"docs":{},"偏":{"docs":{},"斜":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"。":{"docs":{},"不":{"docs":{},"是":{"9":{"docs":{},"的":{"docs":{},"y":{"docs":{},"=":{"0":{"docs":{},"，":{"docs":{},"是":{"9":{"docs":{},"的":{"docs":{},"y":{"docs":{},"=":{"1":{"docs":{"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"ref":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","tf":0.005555555555555556},"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008},"评价分类结果/9.6 精准率-召回率曲线.html":{"ref":"评价分类结果/9.6 精准率-召回率曲线.html","tf":0.007194244604316547}}},"docs":{}}}}},"docs":{}}}},"docs":{}}}}},"docs":{}}}}}}}}}}}}}}}}}}},"完":{"docs":{},"成":{"docs":{},"集":{"docs":{},"成":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}}}}},"能":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"反":{"docs":{},"应":{"docs":{},"算":{"docs":{},"法":{"docs":{},"的":{"docs":{},"水":{"docs":{},"平":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}}}}}},"被":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"拉":{"docs":{},"低":{"docs":{},"了":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"时":{"docs":{},"候":{"docs":{},"对":{"docs":{},"于":{"docs":{},"这":{"docs":{},"个":{"docs":{},"有":{"docs":{},"偏":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"更":{"docs":{},"倾":{"docs":{},"向":{"docs":{},"认":{"docs":{},"为":{"docs":{},"f":{"1":{"docs":{},"_":{"docs":{},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{"评价分类结果/F1 Score.html":{"ref":"评价分类结果/F1 Score.html","tf":0.008}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"升":{"docs":{},"高":{"docs":{},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"后":{"docs":{},",":{"docs":{},"精":{"docs":{},"准":{"docs":{},"率":{"docs":{},"升":{"docs":{},"高":{"docs":{},"，":{"docs":{},"召":{"docs":{},"回":{"docs":{},"率":{"docs":{},"降":{"docs":{},"低":{"docs":{"评价分类结果/9.5 Precision-Recall的平衡.html":{"ref":"评价分类结果/9.5 Precision-Recall的平衡.html","tf":0.012987012987012988}}}}}}}}}}}}}}}}}}}}}}}}}},"编":{"docs":{},"程":{"docs":{},"实":{"docs":{},"现":{"docs":{},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"曲":{"docs":{},"线":{"docs":{"评价分类结果/9.7 ROC曲线.html":{"ref":"评价分类结果/9.7 ROC曲线.html","tf":0.0125}}}}}}}}}}},"支":{"docs":{},"持":{"docs":{},"向":{"docs":{},"量":{"docs":{},"机":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05}},"的":{"docs":{},"思":{"docs":{},"想":{"docs":{},"是":{"docs":{},"找":{"docs":{},"到":{"docs":{},"一":{"docs":{},"条":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"，":{"docs":{},"让":{"docs":{},"这":{"docs":{},"条":{"docs":{},"边":{"docs":{},"界":{"docs":{},"理":{"docs":{},"两":{"docs":{},"部":{"docs":{},"分":{"docs":{},"数":{"docs":{},"据":{"docs":{},"都":{"docs":{},"尽":{"docs":{},"可":{"docs":{},"能":{"docs":{},"的":{"docs":{},"远":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"能":{"docs":{},"很":{"docs":{},"好":{"docs":{},"的":{"docs":{},"分":{"docs":{},"别":{"docs":{},"这":{"docs":{},"两":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"点":{"docs":{},"。":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"说":{"docs":{},"这":{"docs":{},"种":{"docs":{},"方":{"docs":{},"式":{"docs":{},"不":{"docs":{},"仅":{"docs":{},"要":{"docs":{},"将":{"docs":{},"现":{"docs":{},"在":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"进":{"docs":{},"行":{"docs":{},"一":{"docs":{},"个":{"docs":{},"很":{"docs":{},"好":{"docs":{},"的":{"docs":{},"划":{"docs":{},"分":{"docs":{},"，":{"docs":{},"同":{"docs":{},"时":{"docs":{},"还":{"docs":{},"考":{"docs":{},"虑":{"docs":{},"了":{"docs":{},"未":{"docs":{},"来":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"能":{"docs":{},"力":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.1 什么是SVM.html":{"ref":"支撑向量机SVM/11.1 什么是SVM.html","tf":0.05}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"&":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0056022408963585435},"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}},"天":{"docs":{},"然":{"docs":{},"的":{"docs":{},"支":{"docs":{},"持":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"如":{"docs":{},"果":{"docs":{},"是":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"在":{"docs":{},"特":{"docs":{},"征":{"docs":{},"平":{"docs":{},"面":{"docs":{},"内":{"docs":{},"就":{"docs":{},"会":{"docs":{},"有":{"docs":{},"多":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{},"，":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"可":{"docs":{},"以":{"docs":{},"解":{"docs":{},"决":{"docs":{},"多":{"docs":{},"分":{"docs":{},"类":{"docs":{},"问":{"docs":{},"题":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}}}}}},"落":{"docs":{},"在":{"docs":{},"两":{"docs":{},"根":{"docs":{},"直":{"docs":{},"线":{"docs":{},"上":{"docs":{},"的":{"docs":{},"这":{"docs":{},"些":{"docs":{},"点":{"docs":{},"即":{"docs":{},"为":{"docs":{},"支":{"docs":{},"撑":{"docs":{},"向":{"docs":{},"量":{"docs":{},"，":{"docs":{},"这":{"docs":{},"两":{"docs":{},"条":{"docs":{},"直":{"docs":{},"线":{"docs":{},"中":{"docs":{},"间":{"docs":{},"的":{"docs":{},"距":{"docs":{},"离":{"docs":{},"即":{"docs":{},"为":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"ref":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","tf":0.0028011204481792717}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"并":{"docs":{},"不":{"docs":{},"是":{"docs":{},"s":{"docs":{},"v":{"docs":{},"m":{"docs":{},"特":{"docs":{},"有":{"docs":{},"的":{"docs":{},"技":{"docs":{},"巧":{"docs":{},"，":{"docs":{},"只":{"docs":{},"要":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"算":{"docs":{},"法":{"docs":{},"转":{"docs":{},"换":{"docs":{},"成":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"最":{"docs":{},"优":{"docs":{},"化":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"在":{"docs":{},"求":{"docs":{},"解":{"docs":{},"这":{"docs":{},"个":{"docs":{},"最":{"docs":{},"优":{"docs":{},"化":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"存":{"docs":{},"在":{"docs":{},"x":{"docs":{},"i":{"docs":{},"点":{"docs":{},"乘":{"docs":{},"x":{"docs":{},"j":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"式":{"docs":{},"子":{"docs":{},"或":{"docs":{},"者":{"docs":{},"类":{"docs":{},"似":{"docs":{},"这":{"docs":{},"样":{"docs":{},"的":{"docs":{},"式":{"docs":{},"子":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"都":{"docs":{},"可":{"docs":{},"以":{"docs":{},"使":{"docs":{},"用":{"docs":{},"这":{"docs":{},"种":{"docs":{},"技":{"docs":{},"巧":{"docs":{"支撑向量机SVM/11.6 到底什么是核函数.html":{"ref":"支撑向量机SVM/11.6 到底什么是核函数.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"尽":{"docs":{},"管":{"docs":{},"如":{"docs":{},"此":{"docs":{},"，":{"docs":{},"还":{"docs":{},"是":{"docs":{},"有":{"docs":{},"很":{"docs":{},"多":{"docs":{},"的":{"docs":{},"场":{"docs":{},"景":{"docs":{},"是":{"docs":{},"比":{"docs":{},"较":{"docs":{},"适":{"docs":{},"合":{"docs":{},"使":{"docs":{},"用":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"函":{"docs":{},"数":{"docs":{},"的":{"docs":{},"，":{"docs":{},"比":{"docs":{},"如":{"docs":{},"样":{"docs":{},"本":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"非":{"docs":{},"常":{"docs":{},"多":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"的":{"docs":{},"数":{"docs":{},"量":{"docs":{},"可":{"docs":{},"能":{"docs":{},"并":{"docs":{},"不":{"docs":{},"多":{"docs":{},"，":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"m":{"docs":{"支撑向量机SVM/11.7 RBF核函数.html":{"ref":"支撑向量机SVM/11.7 RBF核函数.html","tf":0.01}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"γ":{"docs":{},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"正":{"docs":{},"态":{"docs":{},"分":{"docs":{},"布":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"中":{"docs":{},"型":{"docs":{},"图":{"docs":{},"案":{"docs":{},"越":{"docs":{},"窄":{"docs":{},"。":{"docs":{},"在":{"docs":{},"这":{"docs":{},"里":{"docs":{},"r":{"docs":{},"b":{"docs":{},"f":{"docs":{},"的":{"docs":{},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"γ":{"docs":{},"变":{"docs":{},"大":{"docs":{},"了":{"docs":{},"以":{"docs":{},"后":{"docs":{},"，":{"docs":{},"这":{"docs":{},"个":{"docs":{},"决":{"docs":{},"策":{"docs":{},"边":{"docs":{},"界":{"docs":{},"针":{"docs":{},"对":{"docs":{},"其":{"docs":{},"中":{"docs":{},"的":{"docs":{},"某":{"docs":{},"一":{"docs":{},"类":{"docs":{},"，":{"docs":{},"对":{"docs":{},"于":{"docs":{},"这":{"docs":{},"一":{"docs":{},"类":{"docs":{},"他":{"docs":{},"的":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"点":{"docs":{},"的":{"docs":{},"周":{"docs":{},"围":{"docs":{},"都":{"docs":{},"形":{"docs":{},"成":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"中":{"docs":{},"型":{"docs":{},"的":{"docs":{},"图":{"docs":{},"案":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"可":{"docs":{},"以":{"docs":{},"把":{"docs":{},"这":{"docs":{},"个":{"docs":{},"图":{"docs":{},"看":{"docs":{},"成":{"docs":{},"我":{"docs":{},"们":{"docs":{},"是":{"docs":{},"在":{"docs":{},"俯":{"docs":{},"视":{"docs":{},"这":{"docs":{},"个":{"docs":{},"中":{"docs":{},"型":{"docs":{},"图":{"docs":{},"案":{"docs":{},"，":{"docs":{},"每":{"docs":{},"个":{"docs":{},"蓝":{"docs":{},"色":{"docs":{},"的":{"docs":{},"点":{"docs":{},"就":{"docs":{},"是":{"docs":{},"图":{"docs":{},"案":{"docs":{},"的":{"docs":{},"尖":{"docs":{},"。":{"docs":{},"由":{"docs":{},"于":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"γ":{"docs":{},"值":{"docs":{},"比":{"docs":{},"较":{"docs":{},"大":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"中":{"docs":{},"型":{"docs":{},"图":{"docs":{},"案":{"docs":{},"比":{"docs":{},"较":{"docs":{},"窄":{"docs":{},"。":{"docs":{},"在":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"蓝":{"docs":{},"色":{"docs":{},"的":{"docs":{},"点":{"docs":{},"周":{"docs":{},"围":{"docs":{},"都":{"docs":{},"围":{"docs":{},"绕":{"docs":{},"了":{"docs":{},"一":{"docs":{},"定":{"docs":{},"的":{"docs":{},"区":{"docs":{},"域":{"docs":{},"，":{"docs":{},"只":{"docs":{},"有":{"docs":{},"在":{"docs":{},"这":{"docs":{},"个":{"docs":{},"区":{"docs":{},"域":{"docs":{},"内":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"才":{"docs":{},"判":{"docs":{},"断":{"docs":{},"成":{"docs":{},"蓝":{"docs":{},"色":{"docs":{},"的":{"docs":{},"点":{"docs":{},"，":{"docs":{},"否":{"docs":{},"则":{"docs":{},"判":{"docs":{},"断":{"docs":{},"为":{"docs":{},"红":{"docs":{},"色":{"docs":{},"的":{"docs":{},"点":{"docs":{},"。":{"docs":{},"这":{"docs":{},"也":{"docs":{},"是":{"docs":{},"高":{"docs":{},"斯":{"docs":{},"核":{"docs":{},"函":{"docs":{},"数":{"docs":{},"的":{"docs":{},"几":{"docs":{},"何":{"docs":{},"意":{"docs":{},"义":{"docs":{},"。":{"docs":{"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"ref":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","tf":0.0047169811320754715}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"引":{"docs":{},"入":{"docs":{},"决":{"docs":{},"策":{"docs":{},"树":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}},"某":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"在":{"docs":{},"哪":{"docs":{},"个":{"docs":{},"值":{"docs":{},"上":{"docs":{},"做":{"docs":{},"划":{"docs":{},"分":{"docs":{"121-shi-yao-shi-jue-ce-shu.html":{"ref":"121-shi-yao-shi-jue-ce-shu.html","tf":0.008928571428571428}}}}}}}}}}}}}},"了":{"docs":{},"解":{"docs":{},"了":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"的":{"docs":{},"概":{"docs":{},"念":{"docs":{},"，":{"docs":{},"解":{"docs":{},"决":{"docs":{},"上":{"docs":{},"面":{"docs":{},"的":{"docs":{},"两":{"docs":{},"个":{"docs":{},"问":{"docs":{},"题":{"docs":{},"就":{"docs":{},"好":{"docs":{},"说":{"docs":{},"了":{"docs":{},"。":{"docs":{},"在":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"结":{"docs":{},"点":{"docs":{},"上":{"docs":{},"，":{"docs":{},"都":{"docs":{},"希":{"docs":{},"望":{"docs":{},"在":{"docs":{},"某":{"docs":{},"一":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"上":{"docs":{},"基":{"docs":{},"于":{"docs":{},"某":{"docs":{},"一":{"docs":{},"个":{"docs":{},"阈":{"docs":{},"值":{"docs":{},"进":{"docs":{},"行":{"docs":{},"划":{"docs":{},"分":{"docs":{},"，":{"docs":{},"在":{"docs":{},"划":{"docs":{},"分":{"docs":{},"以":{"docs":{},"后":{"docs":{},"要":{"docs":{},"做":{"docs":{},"的":{"docs":{},"事":{"docs":{},"情":{"docs":{},"就":{"docs":{},"是":{"docs":{},"要":{"docs":{},"让":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"划":{"docs":{},"分":{"docs":{},"成":{"docs":{},"两":{"docs":{},"部":{"docs":{},"分":{"docs":{},"之":{"docs":{},"后":{"docs":{},"，":{"docs":{},"相":{"docs":{},"应":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"系":{"docs":{},"统":{"docs":{},"整":{"docs":{},"体":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"降":{"docs":{},"低":{"docs":{},"（":{"docs":{},"也":{"docs":{},"就":{"docs":{},"是":{"docs":{},"让":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"系":{"docs":{},"统":{"docs":{},"变":{"docs":{},"的":{"docs":{},"更":{"docs":{},"加":{"docs":{},"确":{"docs":{},"定":{"docs":{},"）":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":5.0285714285714285}}}}},"接":{"docs":{},"下":{"docs":{},"来":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"就":{"docs":{},"是":{"docs":{},"找":{"docs":{},"每":{"docs":{},"个":{"docs":{},"节":{"docs":{},"点":{"docs":{},"上":{"docs":{},"游":{"docs":{},"一":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"，":{"docs":{},"在":{"docs":{},"这":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{},"上":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"取":{"docs":{},"值":{"docs":{},"，":{"docs":{},"根":{"docs":{},"据":{"docs":{},"这":{"docs":{},"个":{"docs":{},"取":{"docs":{},"值":{"docs":{},"进":{"docs":{},"行":{"docs":{},"划":{"docs":{},"分":{"docs":{},"，":{"docs":{},"划":{"docs":{},"分":{"docs":{},"后":{"docs":{},"是":{"docs":{},"所":{"docs":{},"有":{"docs":{},"其":{"docs":{},"他":{"docs":{},"划":{"docs":{},"分":{"docs":{},"方":{"docs":{},"式":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"中":{"docs":{},"的":{"docs":{},"最":{"docs":{},"小":{"docs":{},"值":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"就":{"docs":{},"成":{"docs":{},"当":{"docs":{},"前":{"docs":{},"的":{"docs":{},"划":{"docs":{},"分":{"docs":{},"方":{"docs":{},"式":{"docs":{},"就":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"划":{"docs":{},"分":{"docs":{},"。":{"docs":{},"找":{"docs":{},"到":{"docs":{},"这":{"docs":{},"个":{"docs":{},"划":{"docs":{},"分":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"就":{"docs":{},"是":{"docs":{},"对":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"可":{"docs":{},"能":{"docs":{},"性":{"docs":{},"进":{"docs":{},"行":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"。":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"熵":{"docs":{},"在":{"docs":{},"信":{"docs":{},"息":{"docs":{},"论":{"docs":{},"中":{"docs":{},"代":{"docs":{},"表":{"docs":{},"随":{"docs":{},"机":{"docs":{},"变":{"docs":{},"量":{"docs":{},"中":{"docs":{},"不":{"docs":{},"确":{"docs":{},"定":{"docs":{},"度":{"docs":{},"的":{"docs":{},"度":{"docs":{},"量":{"docs":{},"。":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}}}}}}}}}}}}}}}}}}}},"越":{"docs":{},"大":{"docs":{},"，":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"不":{"docs":{},"确":{"docs":{},"定":{"docs":{},"性":{"docs":{},"越":{"docs":{},"高":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}}}}}}}}}}},"小":{"docs":{},"，":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"不":{"docs":{},"确":{"docs":{},"定":{"docs":{},"性":{"docs":{},"越":{"docs":{},"低":{"docs":{"122-xin-xi-shang.html":{"ref":"122-xin-xi-shang.html","tf":0.02857142857142857}}}}}}}}}}}}}},"信":{"docs":{},"息":{"docs":{},"的":{"docs":{},"计":{"docs":{},"算":{"docs":{},"比":{"docs":{},"基":{"docs":{},"尼":{"docs":{},"系":{"docs":{},"数":{"docs":{},"稍":{"docs":{},"慢":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}}},"!":{"docs":{},"=":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}},"遍":{"docs":{},"历":{"docs":{},"每":{"docs":{},"一":{"docs":{},"个":{"docs":{},"维":{"docs":{},"度":{"docs":{"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"ref":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","tf":0.0041841004184100415},"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}},"基":{"docs":{},"尼":{"docs":{},"系":{"docs":{},"数":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":5.004098360655738}},"的":{"docs":{},"意":{"docs":{},"义":{"docs":{},"和":{"docs":{},"信":{"docs":{},"息":{"docs":{},"熵":{"docs":{},"相":{"docs":{},"同":{"docs":{"124-ji-ni-xi-shu.html":{"ref":"124-ji-ni-xi-shu.html","tf":0.004098360655737705}}}}}}}}}}}}}}},"案":{"docs":{},"例":{"docs":{"127-jue-ce-shu-de-ju-xian-xing.html":{"ref":"127-jue-ce-shu-de-ju-xian-xing.html","tf":0.2}}}},"`":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"a":{"docs":{},"y":{"docs":{},".":{"docs":{},"s":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}}}},"少":{"docs":{},"数":{"docs":{},"服":{"docs":{},"从":{"docs":{},"多":{"docs":{},"数":{"docs":{"131-shi-yao-shi-ji-cheng-xue-xi.html":{"ref":"131-shi-yao-shi-ji-cheng-xue-xi.html","tf":0.006756756756756757}}}}}}}},"共":{"docs":{},"有":{"5":{"0":{"0":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"数":{"docs":{},"据":{"docs":{},";":{"docs":{},"每":{"docs":{},"个":{"docs":{},"子":{"docs":{},"模":{"docs":{},"型":{"docs":{},"只":{"docs":{},"看":{"1":{"0":{"0":{"docs":{},"个":{"docs":{},"样":{"docs":{},"本":{"docs":{},"数":{"docs":{},"据":{"docs":{},"，":{"docs":{},"每":{"docs":{},"个":{"docs":{},"子":{"docs":{},"模":{"docs":{},"型":{"docs":{},"不":{"docs":{},"需":{"docs":{},"要":{"docs":{},"太":{"docs":{},"高":{"docs":{},"的":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"取":{"docs":{},"样":{"docs":{},"：":{"docs":{},"放":{"docs":{},"回":{"docs":{},"取":{"docs":{},"样":{"docs":{},"（":{"docs":{},"b":{"docs":{},"a":{"docs":{},"g":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"）":{"docs":{},"，":{"docs":{},"不":{"docs":{},"放":{"docs":{},"回":{"docs":{},"取":{"docs":{},"样":{"docs":{},"（":{"docs":{},"p":{"docs":{},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"）":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"子":{"docs":{},"模":{"docs":{},"型":{"docs":{},"之":{"docs":{},"间":{"docs":{},"不":{"docs":{},"能":{"docs":{},"一":{"docs":{},"直":{"docs":{},"！":{"docs":{},"模":{"docs":{},"型":{"docs":{},"之":{"docs":{},"间":{"docs":{},"要":{"docs":{},"有":{"docs":{},"差":{"docs":{},"异":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}}}}}}}}}}}},"统":{"docs":{},"计":{"docs":{},"学":{"docs":{},"中":{"docs":{},"，":{"docs":{},"放":{"docs":{},"回":{"docs":{},"取":{"docs":{},"样":{"docs":{},"：":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"t":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"p":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}}}}}}}}}}}}}}}}},"集":{"docs":{},"成":{"docs":{},"几":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{"133-bagging-and-pasting.html":{"ref":"133-bagging-and-pasting.html","tf":0.019230769230769232}}}}}},"多":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}}}}},"放":{"docs":{},"回":{"docs":{},"取":{"docs":{},"样":{"docs":{},"导":{"docs":{},"致":{"docs":{},"一":{"docs":{},"部":{"docs":{},"分":{"docs":{},"样":{"docs":{},"本":{"docs":{},"很":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"没":{"docs":{},"有":{"docs":{},"取":{"docs":{},"到":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}}}}}}}}}}}}},"既":{"docs":{},"针":{"docs":{},"对":{"docs":{},"样":{"docs":{},"本":{"docs":{},"，":{"docs":{},"又":{"docs":{},"针":{"docs":{},"对":{"docs":{},"特":{"docs":{},"征":{"docs":{},"进":{"docs":{},"行":{"docs":{},"随":{"docs":{},"机":{"docs":{},"采":{"docs":{},"样":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}}}}}}}}}}},"针":{"docs":{},"对":{"docs":{},"特":{"docs":{},"征":{"docs":{},"进":{"docs":{},"行":{"docs":{},"随":{"docs":{},"机":{"docs":{},"采":{"docs":{},"样":{"docs":{"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"ref":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","tf":0.007751937984496124}}}}}}}}}},"e":{"2":{"docs":{},"训":{"docs":{},"练":{"docs":{},"第":{"docs":{},"三":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"m":{"3":{"docs":{},",":{"docs":{},"产":{"docs":{},"生":{"docs":{},"错":{"docs":{},"误":{"docs":{},"e":{"3":{"docs":{},".":{"docs":{},".":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}},"docs":{}}}},"提":{"docs":{},"供":{"docs":{},"额":{"docs":{},"外":{"docs":{},"的":{"docs":{},"随":{"docs":{},"机":{"docs":{},"性":{"docs":{},"，":{"docs":{},"抑":{"docs":{},"制":{"docs":{},"过":{"docs":{},"拟":{"docs":{},"合":{"docs":{},"，":{"docs":{},"但":{"docs":{},"增":{"docs":{},"大":{"docs":{},"了":{"docs":{},"b":{"docs":{},"i":{"docs":{},"a":{"docs":{"135-sui-ji-sen-lin.html":{"ref":"135-sui-ji-sen-lin.html","tf":0.01098901098901099}}}}}}}}}}}}}}}}}}}}}}}},"針":{"docs":{},"対":{"docs":{},"e":{"1":{"docs":{},"訓":{"docs":{},"繚":{"docs":{},"第":{"docs":{},"二":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"m":{"2":{"docs":{},",":{"docs":{},"产":{"docs":{},"生":{"docs":{},"錯":{"docs":{},"俣":{"docs":{},"e":{"2":{"docs":{"136-ada-boosting-he-gradient-boosting.html":{"ref":"136-ada-boosting-he-gradient-boosting.html","tf":0.011627906976744186}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}},"docs":{}}}},"层":{"docs":{},"次":{"docs":{},"继":{"docs":{},"续":{"docs":{},"增":{"docs":{},"多":{"docs":{},"，":{"docs":{},"就":{"docs":{},"越":{"docs":{},"来":{"docs":{},"越":{"docs":{},"像":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"神":{"docs":{},"经":{"docs":{},"网":{"docs":{},"络":{"docs":{"137-stacking.html":{"ref":"137-stacking.html","tf":0.3333333333333333}}}}}}}}}}}}}}}}}}}}}},"length":6924},"corpusTokens":["!=","\"\"","\"\"\"","\"\"\"初始化knn分类器\"\"\"","\"\"\"初始化linear","\"\"\"初始化logist","\"\"\"初始化pca\"\"\"","\"\"\"初始化simpl","\"\"\"将x根据这个standardscaler进行均值方差归一化处理\"\"\"","\"\"\"将数据x和y按照test_radio分割成x_train,y_train,x_test,y_test\"\"\"","\"\"\"损失函数\"\"\"","\"\"\"损失函数的导数\"\"\"","\"\"\"根据测试数据集","\"\"\"根据测试数据集x获得数据的均值和方差\"\"\"","\"\"\"根据训练数据集x_train,y_train,","\"\"\"根据训练数据集x_train和y_train训练knn分类器\"\"\"","\"\"\"根据训练数据集x_train，y_train","\"\"\"根据训练集x_train，y_train","\"\"\"给定单个待预测数据x_single，返回x_single对应的预测结果值\"\"\"","\"\"\"给定待预测数据集x_predict，返回表示x_predict的结果向量\"\"\"","\"\"\"给定待预测集x_predict，返回x_predict对应的预测结果值\"\"\"","\"\"\"获得数据集x的前n个元素\"\"\"","\"\"\"计算y_true和y_predict之间的mse\"\"\"","\"\"\"计算y_true和y_predict之间的r","\"\"\"计算y_true和y_predict之间的rmse\"\"\"","\"\"\"计算单位向量\"\"\"","\",best_k)","\",best_p)","\",best_score)","\"k","\"linearregression()\"","\"must","\"n_compon","\"n_neighbors\":[i","\"p\":[i","\"simpl","\"simplelinearregression1()\"","\"the","\"weights\"","\"weights\":['distance'],","#","##","#在每一个特征维度上排序","%%time","%%timeit","%run","%time","%timeit","&","'aaron","'aj","'distance'}","'linspace'","'n_neighbors':","'p':","'test')","'train')","'weights':","'weights':['distance'],","'weights':['uniform'],","'zumrati","'zurab","'zydruna","(","(\"dt_clf\",","(\"kernelsvc\",","(\"lasso_reg\",lasso(alpha=alpha))","(\"lin_reg\",linearregression())","(\"linear_svr\",","(\"linearsvc\",","(\"log_clf\",","(\"poly\",","(\"poly\",polynomialfeatures(degree=degree)),","(\"ridge_reg\",ridge(alpha=alpha))","(\"std_scaler\",","(\"std_scaler\",standardscaler()),","(\"std_scatter\",standardscaler()),","(\"svc\",svc(kernel=\"rbf\",","(\"svm_clf\",","('lin_reg',linearregression())","('linear_svr',","('linearsvc',","('log_reg',","('log_reg',logisticregression())","('log_reg',logisticregression(c=c))","('log_reg',logisticregression(c=c,penalty=penalty))","('poly',polynomialfeatures(degree=2)),","('poly',polynomialfeatures(degree=degree)),","('std_scaler',","('std_scaler',standardscaler()),","('svc',","((ab)t=btat)转换成最后的结果","((x[:,1]","(1","(1.+np.exp(","(100,","(100,)","(112,","(112,)","(113,","(113,)","(13233,","(1347,","(144,)","(145,)","(150,","(150,)","(1797,)","(2*epsilon)","(2914,","(36,","(37,","(37,)","(38,","(38,)","(5,","(60000,","(75,","(demean)（归0：所有样本都减去他们的均值），使得均值为0，这样可以简化方差的公式","(down_i","(f(w_1,x)","(j(theta_1,x_b,y)","(mean","(out","(plot_x","(precis","(t","(theta","(tp","(up_i","(x","(x**2).shape","(x2[:,0]","(x2[:,1]","(x[:,0]","(x[:,d]","(x[:col]","(x[sorted_index[i","(x_b_i.dot(theta)","(x_b_k.dot(theta)","(x_i","(x_train","(~200mb):","(θ0)",")","))*","*","**","**2","**kwds)","*w","+","+=",",",",3>0,所以判断这个新病人幻的事恶性肿瘤",",best_d,",",y(i)）,那么我们期望寻找的直线就是y=ax+b，当给出一个新的点x(j)的时候，我们希望预测的y^(j)=ax(j)+b","...,","./hello.pi",".metric","/","/anaconda3/lib/python3.6/importlib/_bootstrap.py:219:","/anaconda3/lib/python3.6/sit","/len(x)","/len(x_b)","0","0)","0,","0,0,0","0.","0.0","0.001","0.00436497,","0.00519961,","0.00649351,","0.00657895,","0.00671141,","0.00680272,","0.00699301,","0.00704225,","0.00729927,","0.00740741,","0.00763359],","0.00987992,","0.01","0.01,n_iter","0.01298701,","0.01342282,","0.01360544,","0.01398601,","0.01408451,","0.01526718],","0.02013423,","0.02020202],","0.02040816,","0.02097902,","0.02112676,","0.02189781,","0.03053435],","0.03496503,","0.03649635,","0.03970336,","0.04026846,","0.04087684],","0.04217092,","0.04253308128544431","0.04378511,","0.04580153],","0.05109489,","0.05152343,","0.053248","0.06060606,","0.06192266,","0.07070707,","0.07152432])","0.07482993,","0.08080808,","0.08125616,","0.09090909,","0.0916967","0.1","0.1)","0.1):","0.1010101","0.10411811,","0.10473243910508653","0.10824648,","0.1186457","0.12121212,","0.13735469","0.13735469])","0.14566817","0.15151515,","0.15151515],","0.15855562,","0.16161616,","0.16496953,","0.16559799,","0.1680384087791495","0.17345038],","0.18000000000000002","0.18181818,","0.18717298,","0.18983267,","0.19191919,","0.2","0.2020202","0.20468323,","0.2105714900645938","0.21212121,","0.22169257,","0.22427024],","0.2320918","0.2665797","0.27272727,","0.27841562,","0.2805277286657274","0.28282828,","0.28902506,","0.30602392],","0.3064319992975,","0.30849545083110386","0.31313131],","0.3156554505030807","0.32323232,","0.3281103","0.33333333,","0.33513866,","0.34343434,","0.35353535,","0.37373737,","0.39186171,","0.39393939,","0.39999999999999947","0.40226093,","0.4040404","0.41245214,","0.4132278899361904","0.41404933,","0.42424242,","0.42424242],","0.43434343,","0.43859746],","0.44076874,","0.44444444,","0.44858475,","0.45153738,","0.45454545,","0.46464646,","0.46848484848484845","0.47474747,","0.48484848,","0.48484848],","0.4917171717171717","0.49345605,","0.49494949,","0.5","0.5*x**2","0.5053078","0.52423752])","0.52525253,","0.5333333333333333","0.53535354,","0.53587921,","0.5406237455773699","0.5431979125088253","0.54545455,","0.55555556,","0.55555556])]","0.56203085,","0.56565657,","0.571171","0.57496445,","0.57575758,","0.57575758],","0.5782292347434448","0.58585859,","0.5959596","0.602674505080953","0.605","0.6060327991735741","0.60606061,","0.6066666666666667","0.61875389,","0.62626263],","0.6360330630824074","0.63636364,","0.64092567,","0.64646465,","0.65068927,","0.65068972,","0.65069316]),","0.65069317])","0.65069321])","0.6578947368421053","0.65930628,","0.66666667,","0.67547694,","0.67676768,","0.68686869,","0.6931471805599453","0.6969697","0.69839152,","0.70374454],","0.70707071,","0.72033239,","0.7235198625270994","0.72452652,","0.72727273,","0.73219998,","0.7354244906092771","0.73737374,","0.74747475,","0.75*x[:,0]+3.+np.random.normal(0.,10.,size=100)","0.75429833])","0.7545644","0.75934372])]","0.75934411])","0.75倍的x[:,0]加上3加上一个噪音","0.76767677,","0.77777778,","0.78273335,","0.78892303,","0.7894736842105263","0.7922037464116539","0.7933333333333333","0.8","0.8008916199519077","0.80808081,","0.81818182,","0.82","0.82181859,","0.824","0.8266666666666667","0.82828283]])","0.83631808],","0.83838384,","0.84","0.84564608,","0.84848485,","0.8533333333333334","0.85858586,","0.86","0.864","0.8674698795180723","0.86780201,","0.86868687,","0.87878788,","0.88","0.88028026,","0.88713794],","0.892","0.896","0.8989899","0.9","0.90236912,","0.904","0.90616043,","0.912","0.9133333333333333","0.91446368,","0.91919192,","0.92","0.921","0.92348443,","0.9243243243243242","0.92929293,","0.9304589707927677","0.93939394],","0.9394112675409493","0.94","0.94524567,","0.94575233","0.94720873,","0.9473684210526315","0.94949495,","0.95","0.95194827,","0.95909217,","0.95959596,","0.96","0.96629213])","0.9688","0.96889162],","0.96889162]])","0.96969697,","0.9728","0.9755555555555555","0.97777778,","0.97831023,","0.97979798,","0.98","0.98002369,","0.980528511821975","0.98183066,","0.9823599874006478","0.9823747680890538","0.9830452674897119","0.9833333333333333","0.9853862212943633","0.98551291,","0.9860917941585535","0.9866666666666667","0.98773424,","0.99,","0.99639662,","01,","01])","02,","03,","04,","05,","06","06,","07,","0]","0])","0],","0],x[y==0,","0],x[y==1,","0`","1","1)","1))","1)),","1)]:","1,","1,1)","1,1)*w","1,1),","1,10","1,6,141)","1,\\","1,verbose=1)","1,verbose=2)","1,x1,x2","1.","1.%run","1.,","1.0","1.0,","1.0,1.5])","1.000151338154146","1.01581521,","1.01728635e","1.01971148],","1.02621444,","1.0295875","1.05783059e","1.06270960e+00,","1.06867274,","1.07253826,","1.07377463e","1.08043759,","1.08077650e","1.09467835,","1.09940036e+01,","1.0min","1.1","1.10146516],","1.1213911351818648","1.12926131,","1.13966053,","1.14235739e","1.1496080843259968","1.15228502],","1.15301457,","1.16916667])","1.17408507,","1.17572392e","1.17777287e","1.18598435,","1.1888759304218461","1.19209981,","1.19638358,","1.2","1.21","1.23179738e","1.23186515e","1.2340387","1.2427074","1.25310662,","1.26617002e","1.27644165,","1.28485856],","1.29943044,","1.29961033e","1.3","1.30982967,","1.31552689,","1.31964561130862","1.3215547","1.336192585265726","1.3420996","1.35615349,","1.36655271,","1.36661224],","1.37354688e","1.3761132675144652,","1.38003340e","1.387437803144217","1.39986872,","1.4","1.40096142,","1.40778005e","1.4174321","1.42044841e","1.42327576,","1.42823692e","1.43895396,","1.45798298e","1.46","1.46959958,","1.47999881,","1.4900114024329525,","1.49918578],","1.49918578]])","1.5","1.5,","1.5,2.5,","1.52329579,","1.52632263,","1.52769283e","1.53672185,","1.54843104,","1.56238103,","1.5])","1.62783776,","1.63175932],","1.63976872,","1.64672287,","1.67","1.68580811,","1.69649176,","1.71368535e","1.74999096,","1.75","1.76295187,","1.76433286],","1.77014994,","1.77530738,","1.7763568394002505e","1.79346003e","1.80123135]])","1.80304649e","1.80923518,","1.80993786,","1.8408939659515595","1.8547141","1.88195578e","1.89357701,","1.93266225,","1.9427736300237914","1.98028746e","1.98082712","1.f1","1.knn算法的原理介绍","1.l1,l2正则","1.pca简介","1.两组调参得出的参数结果是不同的，通常这时候我们更愿意详细使用交叉验证的方式得出的结果。","1.主成分分析法的两个轴都是特征，线性回归y轴是目标结果值","1.交叉验证","1.什么是ovr与ovo","1.什么是roc","1.什么是核函数","1.什么是模型正则化","1.什么是过拟合和欠拟合","1.什么是逻辑回归","1.使用```polynomialfeatures```生成多项式特征的数据集","1.使用scikit","1.创建“0”数组","1.加载mnist数据集","1.加载numpy与查看版本","1.加载人脸数据库","1.回忆我们之前的例子","1.基本属性","1.多项式回归简介","1.对σ(t)求导","1.将数据集分割成测试数据集合训练数据集","1.将数据集分成训练数据集合测试数据集","1.将这个数据映射到0~xmax","1.数据进行改变，将数据在第一个主成分上的分量去掉","1.机器学习基本概念","1.梯度下降法简介","1.模拟多项式回归的数据集","1.注意上面式子里的每一个(x1(i)·w1+x2(i)·w2+......xn(i)·wn)都是一个x(i)和w的点乘，所以式子可以进一步化解，","1.理解为什么二者是矛盾的","1.线性回归算法简介","1.逻辑回归算法的封装实现","1.降低模型复杂度","1.高斯核函数","1.高维数据向低维数据映射","1/(1+np.exp(","1/w1","1/w[1]","10)","10,","10,10,100)","10.,","10.99758484]","10.支撑向量机","100","100)","100,","100.]])","1000000","101,","102,","103,","104,","105,","106,","107,","108,","109,","10]","10],","10]])","10]},","10min","11,","11.1","11.2","11.3","11.4","11.5","11.6","11.7","11.8","11.9","11.决策树","110,","111,","112,","113,","114,","115,","116,","116]])","117,","118,","119,","11，分别拿每一个k去调用算法，得出分数，取得分最高的那个k","12","12,","12.,","12.1","12.2","12.3","12.4","12.5","12.6","12.7","120","120,","121,","122,","123,","124,","125,","126,","127,","128,","129,","13,","13.1","13.2","13.3","13.4","13.5","13.6","13.7","13.集成学习和随机森林","130,","131,","132,","133,","134,","135,","136,","137,","138,","139,","14,","140,","141,","142,","143,","144,","145,","146,","147,","148,","149,","15,","15.],","15],","16,","16.21331661,","16.],","166","167.94010860151894","17","17,","18,","180","19,","19.7173878","19.889528301315465","197","1:","1]","1])","1],","1],d]","1e","1e4):","1e4,epsilon","1e4,epsilon=1","1min","1为使用所有的核","1之间","1之间）显然不符合我们的数据","1之间，那么正规式子将以优化前半部分为主，如果c特别大，那么这个式子主要优化的目标就是后半部分","1的一个随机排列","2","2\"","2)","2))","2*(theta","2*np.random.random(s","2+np.random.normal(0,1,size=100)","2,","2.","2.%timeit","2.,","2.0","2.00218372,","2.07484052,","2.08159044,","2.1","2.1795164","2.1观察线性回归的学习曲线：观察线性回归模型，随着训练数据集增加，性能的变化","2.2","2.20030857e","2.25417403e","2.28700570e","2.30294347,","2.30846679,","2.354574897431513,","2.38328586e","2.42337671e","2.43","2.45","2.45307516e","2.45593641,","2.49296044]])","2.5)","2.5)**2","2.5,","2.50362666e","2.53534313,","2.54","2.54979762,","2.55","2.5786840957478887]","2.5980174","2.61","2.6112077267395803","2.65","2.73","2.74141103,","2.7714817137686794","2.87619649e","2.98909581,","2.],","2.f1","2.jupyt","2.knn算法的一个简单实现","2.l0正则项","2.python","2.scikit","2.sklearn中的ovr与ovo","2.为什么要使用训练数据集和测试数据集","2.主成分分析法的点是垂直于方差轴直线的，线性回归的点事垂直于x轴的","2.以多项式核函数为例，看核函数时怎么运作的","2.使用knn","2.使用交叉验证得出的最好分数0.982是小于使用分割训练测试数据集得出的0.986，因为在交叉验证的","2.使用梯度上升法解决pca问题","2.使用线性回归拟合","2.减少数据维度；降噪","2.创建全\"1\"矩阵和创建全\"n\"矩阵","2.化简过后可以进行向量化，即每一个∑(x(i)·w1)·x1(i)","2.在新的数据上求第一主成分","2.如果生成数据幂特别的大，那么特征直接的差距就会很大，导致我们的搜索非常慢，这时候可以进行**数据归一化**","2.实际编程实现学习曲线","2.实际编程用可视化的方式观察特征脸","2.对log(σ(t))进行求导","2.将数据集进行归一化处理","2.将训练数据集进行归一化","2.手写识别的例子","2.梯度下降法模拟","2.测试我们的逻辑回归算法","2.然后对于每个x相比于整个范围所占的比例","2.特征脸","2.用一个简单的例子模拟高斯函数到底在座什么事情","2.直观的理解","2.简单线性回归的实现","2.编程实现","2.编程实现岭回归","2.编程实验多项式回归","2.调用linearregression对x2进行预测","2.逻辑回归的损失函数","2/len(x_b_k).","20,","200)","209","21,","214","22.05701166,","220","22])","23,","24,","24.54003391,","24]])","25,","25.,","25.04286766,","26,","27,","27.6","28","28,","29,","2914","2914)","2])","2],","2s","2s,","3","3,","3,3,0,6])","3,3,100).reshape(100,1)","3,3,size=100)","3.","3.%time","3.,","3.0","3.0,3.0,size=100)","3.02265606,","3.05407804e","3.06246837],","3.06882065","3.0825","3.09269839e","3.1","3.1086244689504386e","3.12783163e","3.14917937e","3.2","3.26989470e","3.3","3.41105814e","3.49155727e+00,","3.57755036e","3.60119663e","3.63","3.70916667,","3.72917505e","3.82793717e","3.93118623])","3.94835863","3.99667727e","3.knn算法的学习与使用","3.numpy.array","3.pca进行降维","3.piplin","3.reshap","3.主成分pca的实现","3.使用程序直观理解高斯函数","3.使用训练数据集的均值和方差将测试数据集归一化","3.关于polynomialfeatur","3.创建一个kneighborsclassifi","3.判断机器学习算法的性能","3.增加样本数（模型太过复杂，模型中的参数非常多，而样本数不足以支撑计算出这么复杂的参数）","3.多元线性回归中的梯度下降法","3.对j(θ)的前一部分求导","3.弹性网","3.总结","3.最后根据转置法则","3.生成等步长数组","3.编码观察精准率和召回率的平衡","3.衡量线性回归算法的指标","3.解决方案，添加一个特征","3.过拟合与前拟合","3.进行线性回归","3.逻辑回归函数损失的梯度","30","30,","30.,","30.8","31,","31.3","31.5","31.7","31.],","318","31],","32,","32.64566083965224","323","33,","33.02939187,","337","34,","34])","35,","352","355","356","36","36,","36.],","36]])","37,","38,","39,","3])","3组数据集，30种组合，一共要进行120次的训练","3：0","4","4)","4,","4,4,","4,4])","4,5,1)","4.","4.,","4.01354717e","4.021457858204859","4.03237972,","4.1","4.1.1","4.1.2","4.11","4.192433747323001e+21","4.2","4.26605279e","4.3","4.30496317],","4.30926281e","4.34888085e","4.4","4.64","4.6986266144110695,","4.89020776e","4.97139932","4.],","4.random","4.使用kneighborsclassifi","4.使用训练数集训练处模型","4.使用验证集","4.其他","4.学习曲线","4.实现逻辑回归算法","4.对log(1","4.最好的衡量线性回归法的指标","4.求数据的前n个主成分","4.线性回归中的梯度下降法的实现","4.线性回归算法","40","40,","403","41,","41s,","42,","42.],","42],","43,","44","44,","44.39167886,","45,","46,","47","47)","47,","48,","48.25125463,","48.],","480","48],","49,","49.,","4])","4],","5","5)]","5,","5.","5.,","5.07877569])","5.08255707e","5.1","5.11542945e","5.2","5.22","5.3","5.4","5.48538930e","5.60545588e","5.7","5.71688020e","5.76411920e","5.79162563e","5.83460014556857,","5.86018996e","5.86686040e","5.9859077","5.使用归一化后的测试数据集测试分类的准确度（accuracy）","5.使用训练数据集得出分类准确度","5.决策边界","5.多元线性回归","5.对j(θ)的后一部分求导","5.数据归一化","5.梯度下降法","5.模型正则化","5.随机梯度下降法","5.验证数据集与交叉验证","5.高维数据向低维数据进行映射","50","50,","51,","52,","53,","531","54","54,","54s,","55,","55])","56,","56.,","57","57,","57.],","5749","575","578","57],","58,","580","59,","5]}],","5})","6","6,","6.,","6.06659094e","6.1","6.189696362066091,","6.74969443],","6.749798999160064,","6.85977267e","6.93","6.sklearn","6.sklearn中的pca","6.主成分分析法","6.使用我们的模型预测新的数据","6.使用网格搜索寻找最好的超参数，然后回到1","6.偏差方差均衡","6.在逻辑回归中使用多项式回归","6.整合两部分的求导结果","6.梯度下降法的调试","6.线性回归的可解性和更多思考","60","60,","61,","613","62*47","62,","62.],","62],","63,","64)","64,","64.],","65,","66,","666)","67","67,","68,","69,","6]","6],","7","7,","7.,","7.01077392","7.1","7.11864860e","7.15189459e","7.2","7.3","7.44075955e","7.5,","7.51533441],","7.60516219e","7.73828332e","7.99250414","7.scikit","7.多项式回归","7.探索超参数","7.整合对j(θ)所有θ的求导结果","7.梯度下降法的总结","7.模型正则化","7.试手mnist数据集","70,","71,","72,","73,","74,","745","75,","76,","76.7","77,","78,","784)","79,","8):","8,","8.,","8.40132221e","8.43243544e","8.49968861e","8.80618320e","8.85220461e","8.99151383","8.lasso","8.ovr与ovo","8.使用pca对数据进行降噪","8.对比之前的线性回归的损失函数的导数形式，找到相似点","8.逻辑回归","80,","80.37908046,","81,","81.,","82,","82.]])","826","82]])","83,","84,","84.6","85,","85.68606536672523","86,","87,","88","88,","89,","8])","8],","9","9,","9.,","9.00017642e","9.09314698e","9.1","9.16425531e","9.2","9.3","9.4","9.5","9.55152460e","9.6","9.7","9.8","9.97525811","9.l1,l2和弹性网络","9.人脸识别与特征脸","9.向量化","9.评价分类结果","90,","90.,","91,","92,","93,","93.],","935","93],","94,","94.6","95,","96,","97,","98,","99,","9],",":",":param",":return:","=","=0","=666)","==","=>","=[",">",">=",">o(n)",">正合适",">过拟合）","[","[\"distance\"],","[\"uniform\",\"distance\"]:","[\"uniform\"],","['distance'],","['uniform'],","[(1,","[(x","[0,","[0,0,0,0,0,1,1,1,1,1]","[0.","[0.00675676,","[0.06060606,","[0.15151515,","[0.25252525,","[0.27272727,","[0.33333333,","[0.39393939,","[0.49494949,","[0.73737374,","[0.98989899,","[0]*len(x[y==0]))","[0]*len(x[y==1]))","[1,","[1.343808831,3.368360954],","[15,","[15.,","[2,","[2.110073483,1.781539638],","[2.280362439,2.866990263],","[25,","[25.,","[27,","[27.,","[3.582294042,4.679179110],","[33,","[33.,","[39,","[39.,","[4.812566907609877,","[49,","[49.,","[5.745051997,3.533989803],","[7.423436942,4.696522875],","[7.792783481,3.424088941],","[7.939820817,0.791637231]","[73,","[73.,","[9.172168622,2.511101045],","[98,","[98.,","[[3.393533211,2.331273381],","[]","[array([0.75934077,","[array([0.95959596,","[fn(y_true,","[i","[np.sum(pca.explained_variance_ratio_[:i+1])","[parallel(n_jobs=","[parallel(n_jobs=1)]:","[sqrt(np.sum((x_train","[tn(y_true,","[y_train[i]","\\","]","])","],","]])","__init__(self):","__init__(self,k):","__init__(self,n_components):","__repr__(self):","_predict(self,","_sigmoid(self,","_theta","_theta:","`array.s","a,b公式","accuracy_scor","accuracy_score(y_test,","ada","ada_clf","ada_clf.fit(x_train,","ada_clf.score(x_test,","adaboostclassifi","adaboostclassifier(algorithm='samme.r',","adaboostclassifier(base_estimator=decisiontreeclassifier(max_depth=2),","algo.fit(x_train[:i],y_train[:i])","algo.predict(x_test)","algo.predict(x_train[:i])","algorithm","alpha=1的时候正则化已经过头了","ambiguous.","analysis）：也是一个梯度分析的应用，不仅是机器学习的算法，也是统计学的经典算法","arang","array","array([","array(['aj","array([0,","array([0.","array([0.02945732,","array([0.14566817,","array([0.75934073,","array([0.75934077,","array([0.81019502,","array([0.95365304])","array([0.98895028,","array([1,","array([1.08043759,","array([1.45668166","array([139,","array([2,","array([3.00706277])","array([3.03182269,","array([5.2])","array([5.83416667,","array([95,","array([[","array([[0.","array([[0.52525253,","array([[147,","array([[403,","array([[404,","array([[52,","array([[52.,","array>","assert","average=\"micro\")","ax.imshow(data[i].reshape(62,47),cmap='bone')","ax.imshow(data[i].reshape(8,8),","axis.","axis:坐标轴的范围；0123对应的就是x轴和y轴的范围","axis=1)","axis=[","axis=[0.5,","axis[0])*100)).reshape(","axis[1],","axis[2])","axis[2])*100)).reshape(","a的每一行与b的每一列相乘再相加，等到结果是m行n列的）","b","bag","bag)","bag.","bagging_clf","bagging_clf.fit(x,","bagging_clf.fit(x_train,","bagging_clf.oob_score_","bagging_clf.score(x_test,","bagging_clf2","bagging_clf2.fit(x_train,","bagging_clf2.score(x_test,","baggingclassifi","baggingclassifier(decisiontreeclassifier(),","bagging更常用","bagging的思路极易并行化处理","base","base_estimator=decisiontreeclassifier(class_weight=none,","basi","befor","best_d,","best_d2,","best_entropi","best_entropy,","best_entropy2,","best_g","best_g,","best_g2,","best_k","best_knn_clf","best_knn_clf.fit(x_train,y_train)","best_knn_clf.score(x_test,y_test)","best_method","best_p","best_scor","best_score,best_k,best_p","best_score:","best_v","best_v)","best_v2","big_i","big_x","binari","boolean,","boost","bootstrap","bootstrap=true)","bootstrap=true,","bootstrap_featur","bootstrap_features=true)","boston","boston.data","boston.target","c=0.1","c=0.1):","c=1.0","cache_size=200,","candidates,","cfm","cg","cg\")","cg',","changed,","characterist","check","class","class_weight=none,","classfif","classif","classifi","clim=(0,16))","cmap='binary',","cmap=plt.cm.gray)","coef0=0.0,","col","collect","collections的counter方法可以求出一个数组的相同元素的个数，返回一个dict【key=元素名，value=元素个数】","compon","confus","confusion_matrix","confusion_matrix(y_test,","confusion_matrix(y_test,y_log_predict)","confusion_matrix(y_true,","confusion_matrix,","contour:","cook',","correspond","counter","counter(topk_y)","counter(y)","counter({0:","counter.values():","cpu","crat","crat：","criterion=\"entropy\")","criterion=\"gini\")","criterion='entropy',","criterion='gini',","cross","cross_val_scor","cross_val_score(knn_clf,x_train,y_train)","cross_val_score(knn_clf,x_train,y_train,cv=3)","cur_it","curve，描述tpr","custom_cmap","cv默认为3，可以修改改参数，修改修改不同分数的数据集","c·j(θ)","c放小之后，容错空间变大，所以犯了一个错误","c越大，越偏向于是一个hard","d","d,","d_j_debug(theta,x_b,y,epsilon=0.01):","d_j_debug是通用的，可以放在任何求导的debug过程中，所以可以作为我们机器学习的工具箱来使用","d_j_main(theta,x_b,y):","data","data\"","dataset","datasets.load_boston()","datasets.load_digits()","datasets.load_iris()","datasets.make_moons()","datasets.make_moons(n_samples=500,","datasets.make_moons(noise=0.15,","datasets.make_moons(noise=0.25,","datasets可以用来加载真实数据进行模型训练的测试","data），只使用分类准确度是远远不够的","deamean(x)","decis","decision_function_shape='ovr',","decision_scor","decision_scores)","decisiontreeclassifi","decisiontreeclassifier()","decisiontreeclassifier())","decisiontreeclassifier(class_weight=none,","decisiontreeclassifier(max_depth=2)","decisiontreeclassifier(max_depth=2,","decisiontreeclassifier(max_leaf_nodes=4)","decisiontreeclassifier(min_samples_leaf=6)","decisiontreeclassifier(min_samples_split=10)","decisiontreeclassifier(random_state=666))","decisiontreeregressor","decisiontreeregressor()","decisiontreeregressor(criterion='mse',","decomposition提供了降维相关算法的实现","def","degree=3,","degree从2到10到100的过程中，虽然均方误差是越来越小的，从均方误差的角度来看是更加小的","demean(x):","demean(x_pca)","deprecationwarning:","dev.","df(w,x):","df_debug(w,x,epsilon=0.0001):","df_math(w,x):","diff:","digit","digits.data","digits.data,digits.target","digits.target","digits.target.copy()","dimens","direction(inital_w)","direction(w):","distanc","distances.append(d)","dj(theta):","dj(theta,","dj_sgd(theta,","dj_sgd(theta,x_b[rand_i],y[rand_i])","dj_sgd(theta,x_b_i,y_i):","done","dot()","down_i","download","dscision_scor","dt_cfl","dt_cfl.fit(x,","dt_clf","dt_clf.fit(x,","dt_clf.fit(x_train,","dt_clf.predict(x_test)","dt_clf.score(x_test,","dt_clf2","dt_clf2.fit(x,","dt_clf3","dt_clf3.fit(x,","dt_clf4","dt_clf4.fit(x,","dt_clf5","dt_clf5.fit(x,","dt_reg","dt_reg.fit(x_train,","dt_reg.score(x_test,","dt_reg.score(x_train,","dtype='#","dtype='int')","dual=false,","dual=true,","e","each","each)","eckhart',","elapsed:","elast","empti","empty.","ensur","entropy(p):","entropy(x))","entropy(y):","entropy(y1_l)","entropy(y1_r)","entropy(y2_l)","entropy(y2_r)","entropy(y_l)","entropy(y_r)","enumerate(axes.flat):","enumerate(x):","epsilon","epsilon))","epsilon:","epsilon=0.1,","epsilon=1","equal","err_matrix","error.","error_score='raise',","estimator:","estimator=kneighborsclassifier(algorithm='auto',","estimator=kneighborsregressor(algorithm='auto',","et_clf","et_clf.fit(x,","et_clf.oob_score_","eta","eta)","eta,","eta:学习率η","eta=0.01,","example_digit","example_digits.shap","example_fac","example_faces.shap","except:","expect","explainedvariance_ratio","extra","extratreesclassifi","extratreesclassifier(bootstrap=true,","extratreesclassifier(n_estimators=500,","f(*args,","f(w,x):","f(w_2,x))","f1","f1_score","f1_score(precision,","f1_score(y_test,","face","faces.data,faces.target","faces.data.shap","faces.data[random_indexs]","faces.images.shap","faces.target_nam","faces2","faces2.data,faces2.target","fals","false,","featur","fetch_lfw_peopl","fetch_lfw_people()","fetch_lfw_people(min_faces_per_person=60)","fetch_lfw_people用于加载人脸数据集","fetch_mldata","fetch_mldata(\"mnist","fetch_mldata用于加载mnist数据集","fig,ax","filtered_digit","finish","first_componet(","first_componet(x2,inital_w,eta)","first_componet(x_demean,inital_w,eta)","first_componet(x_pca,initial_w,eta)","first_n_componet(2,x)","first_n_componet(n,x,eta","fit","fit(self,","fit(self,x,eta=0.01,n_iters=1e4):","fit_gd(self,","fit_intercept=true,","fit_lit_sgd(self,","fit_normal(self,","fit_params=none,","fit_sgd(self,","fitting】","float('inf')","fn","fn(y_test,y_log_predict)","fn(y_true,","fn)","fold","follow","for循环方式实现","fp","fp(y_test,y_log_predict)","fp(y_true,","fp(y_true,y_predict)],","fp)","fpr","fprs,","fprs.append(fpr(y_test,","fpr之间的关系","fpr都是逐渐升高的，换句话说，tpr和fpr之间是存在着相一致的关系的","fpr：预测为1，但是预测错了的数量占真实值为0的百分比是多少","function","function），也叫核函数技巧","futur","gamma","gamma=1.0,","gamma=100,","gamma=gamma))","gamma越低，模型复杂度越低，欠拟合程度越低","gamma越高，模型复杂度越高，过拟合程度越高","gaussian(data,l1)","gaussian(data,l2)","gaussian(x,","gini(y):","gini(y1_l)","gini(y1_r)","gini(y2_l)","gini(y2_r)","gini(y_l)","gini(y_r)","grad_clf","grad_clf.fit(x_train,","grad_clf.score(x_test,","gradient","gradient_ascent(df,x,inital_w,eta,n_it","gradient_ascent(df_debug,x_demean,inital_w,eta)","gradient_ascent(df_math,x_demean,inital_w,eta)","gradient_descent(0.,eta)","gradient_descent(0.,eta,n_it","gradient_descent(d_j,x_b,","gradient_descent(d_j_debug,x_b,","gradient_descent(d_j_main,x_b,","gradient_descent(initial_theta,eta,n_it","gradient_descent(x_b,","gradientboostingclassifi","gradientboostingclassifier(criterion='friedman_mse',","gradientboostingclassifier(max_depth=2,","graph","grid","grid_search","grid_search.best_estimator_","grid_search.best_estimator_.score(x_test,y_test)","grid_search.best_params_","grid_search.best_score_","grid_search.fit(x_train,y_train)","gridsearchcv","gridsearchcv(cv=none,","gridsearchcv(knn_clf,param_grid)","gridsearchcv(knn_clf,param_grid,n_jobs=","gridsearchcv(knn_clf,param_grid,verbose=1,cv=3)","gridsearchcv(knn_reg,param_grid,n_jobs=","gridsearchcv用于进行参数搜索，寻找合适的超参数","gridsearchcv里的cv实际上就是交叉验证的方式","gridspec_kw=dict(hspace=0.1,wspace=0.1))","guide是对每一个算法的概述介绍。api是每一个算法的使用文档（也可以在首页大搜中搜索）","hard","http://scikit","https://blog.csdn.net/nomadlx53/article/details/50849941","https://ndownloader.figshare.com/files/5976006","https://ndownloader.figshare.com/files/5976009","https://ndownloader.figshare.com/files/5976012","https://ndownloader.figshare.com/files/5976015","i)","i,","i,ax","i]))","i_it","i_iters2.2","id3，c4.5，c5.0等是使用其他的方式实现决策树","iid=true,","ilgauskas'],","images是将我们的数据集以一个二维平面可视化的角度展现出来","import","include_bias=true,","incompatibility.","index","index_a","indic","init=none,","inital_w","initial_theta","initial_theta,","initial_theta:初始化的theta值","initial_w","int(len(x)","interaction_only=false)),","intercept_scaling=1,","intercept_scaling=1.0,","interpolation='nearest',","iri","iris.data","iris.data[:,2:]","iris.data[:,:2]","iris.target","j(theta):","j(theta,","j(theta,x_b,y):","j(theta_2,x_b,y))/(2*epsilon)","juma',","jupyt","k","k:","kernel='rbf',","kernel）","kneighborsclassifi","kneighborsclassifier()","kneighborsclassifier(algorithm='auto',","kneighborsclassifier(n_neighbors=3)","kneighborsclassifier(n_neighbors=50)","kneighborsclassifier(n_neighbors=6)","kneighborsclassifier(n_neighbors=k)","kneighborsclassifier(n_neighbors=k,weights='distance',p=p)","kneighborsclassifier(n_neighbors=k,weights=method)","kneighborsclassifier(weights='distance',n_neighbors=2,p=2)","kneighborsclassifier(weights='distance',n_neighbors=k,p=p)","kneighborsclassifier是knn算法解决分类问题的实现","kneighborsclassifier是knn算法解决回归问题的实现","kneighborsregressor","kneighborsregressor()","knn","knn_clf","knn_clf.fit(x_train,y_train)","knn_clf.fit(x_train_reduction,y_train)","knn_clf.score(x_test,y_test)","knn_clf.score(x_test_reduction,y_test)","knn_clf_all","knn_clf_all.fit(iris.data[:,:2],iris.target)","knn_reg","knn_reg.fit(x_train,y_train)","knn_reg.score(x_test,y_test)","knnclassifi","knnclassifier(k=6)","knnclassifier:","knn可以估算规律，结果占离他最近的k个点","knn的决策边界","knn的另外一个超参数：距离的权重","knn算法的封装","kwarg","k为k近邻中的寻找k个最近元素","k越小，那么我们的模型就越复杂，在这里，我们可视化的看到了复杂的含义","k近邻算法是非常特殊的，可以被认为是没有模型的算法","l)**2)","l):","l1","l1,","l2","l2范数（欧拉距离|lasso","lamas',","lasso","lasso1_predict","lasso1_reg","lasso1_reg.fit(x_train,y_train)","lasso1_reg.predict(x_test)","lasso2_predict","lasso2_reg","lasso2_reg.fit(x_train,y_train)","lasso2_reg.predict(x_test)","lasso3_predict","lasso3_reg","lasso3_reg.fit(x_train,y_train)","lasso3_reg.predict(x_test)","lassoregression(degree,alpha):","lassoregression(degree=20,alpha=0.01)","lassoregression(degree=20,alpha=0.1)","lassoregression(degree=20,alpha=1)","lasso回归有一些选择的功能","last","leaf_size=30,","learn","learn.org/stable/","learn.org/stable/documentation.html","learn.org/stable/modules/generated/sklearn.svm.svc.html","learning_rate(cur_it","learning_rate(cur_iter)","learning_rate(t):","learning_rate=0.1,","learning_rate=1.0,","learn中使用scal","learn中对于并行处理的算法，可以传入n_jobs来调整使用几个核来处理","learn中提供的算法","learn中的precis","learn中的roc曲线","learn中的svm","learn中的回归问题","learn中的多项式回归于pipelin","learn中的多项式对数据进行预处理","learn中的混淆矩阵，精准率和召回率","learn中的混淆矩阵，精准率和召回率,f1_scor","learn中的逻辑会回归","learn中默认使用基尼系数","learn和numpy为技术栈，学习了机器学习入门的基本算法，并自己实现了部分sikit","learn官网：http://scikit","learn封装的决策树解决回归问题","learn的决策实现：crat","learn算法","learn：oob_score_","len(faces.target_names)","len(self.coef_),\\","len(self.mean_),","len(theta)):","len(x_b)","len(x_train)","len(y)","len(y_predict)","len(y_predict),","len(y_train),\\","len(y_true)","lfw","lin_reg","lin_reg.coef_","lin_reg.fit(x,y)","lin_reg.fit(x_train,y_train)","lin_reg.fit_gd(x,y)","lin_reg.intercept_","lin_reg.interception_","lin_reg.predict(x)","lin_reg.predict(x_test)","lin_reg.score(x,y)","lin_reg.score(x_test,y_test)","lin_reg2","lin_reg2.coef_","lin_reg2.fit(x2,y)","lin_reg2.intercept_","lin_reg2.predict(x2)","linear","linear_model提供了线性模型相关算法的实现","linearregress","linearregression()","linearregression())])","linearregression(copy_x=true,","linearregression:","linearregression是线性回归算法的实现","linearsvc","linearsvc(c=0.01)","linearsvc(c=0.01,","linearsvc(c=0.1)","linearsvc(c=0.1,","linearsvc(c=1000000000.0,","linearsvc(c=1e9)","linearsvc(c=c))","linearsvr","linearsvr(c=1.0,","linearsvr(epsilon=","linspac","list","listedcolormap","listedcolormap(['#ef9a9a','#fff59d','#90caf9'])","list的元素可以存任何类型，在灵活度提升的同时，也导致性能下降了","log","log(0)也就是没有任何损失。","log(1","log(p^).当y=0的时候，前半部分是0，就只剩下","log_clf","log_clf.fit(x_train,","log_clf.predict(x_test)","log_clf.score(x_test,","log_reg","log_reg.coef_","log_reg.coef_[0]","log_reg.coef_[1]","log_reg.decision_function(x_test)","log_reg.decision_function(x_test)[:10]","log_reg.fit(x,y)","log_reg.fit(x_train,","log_reg.fit(x_train,y_train)","log_reg.interception_","log_reg.interception_)","log_reg.predict(x_test)","log_reg.predict(x_test)[:10]","log_reg.predict_proba(x_test)","log_reg.score(x,y)","log_reg.score(x_test,","log_reg.score(x_test,y_test)","log_reg2","log_reg2.fit(x_train,y_train)","log_reg2.score(x_test,y_test)","logisticregress","logisticregression()","logisticregression()),","logisticregression(c=0.1,","logisticregression(c=1.0,","logisticregression(multi_class='multinomial',solver=\"newton","logisticregression:","logisticregression是逻辑回归的实现，默认使用了l2正则化","loop","loss='deviance',","loss='epsilon_insensitive',","loss='squared_hinge',","m","m2","m3","machin","machine_learn","machine_learning.knn","machine_learning.linearregress","machine_learning.logisticregress","machine_learning.metr","machine_learning.module_select","machine_learning.simplelinearregression1","mae的实现","mae的局限性","margin","math","matplotlib","matplotlib.color","matplotlib.pyplot","matrix","max_depth=2,","max_depth=none,","max_featur","max_features='auto',","max_features=1,","max_features=none,","max_iter=","max_iter=100,","max_iter=1000,","max_leaf_nodes=10,","max_leaf_nodes=none,","max_samples=100,","max_samples=500,","max_samples每个子模型看几个样本数据","mean_","mean_absolute_error","mean_absolute_error(y_true,","mean_squared_error","mean_squared_error(lasso1_predict,y_test)","mean_squared_error(lasso2_predict,y_test)","mean_squared_error(lasso3_predict,y_test)","mean_squared_error(y,y100_predict)","mean_squared_error(y,y10_predict)","mean_squared_error(y,y2_predict)","mean_squared_error(y,y_predict)","mean_squared_error(y_test,ridge1_predict)","mean_squared_error(y_test,ridge2_predict)","mean_squared_error(y_test,ridge3_predict)","mean_squared_error(y_test,ridge4_predict)","mean_squared_error(y_test,y100_predict)","mean_squared_error(y_test,y10_predict)","mean_squared_error(y_test,y20_predict)","mean_squared_error(y_test,y2_predict)","mean_squared_error(y_test,y_predict)","mean_squared_error(y_true,","metadata:","method","metric='minkowski',","metric_params=none,","metrics模块提供了数据之间的度量相关运算","min_impurity_decrease=0.0,","min_impurity_split=none,","min_samples_leaf=1,","min_samples_split=2,","min_weight_fraction_leaf=0.0,","mnist","mnist.data,mnist.target","model.coef_[0]","model.intercept_[0]","model.predict(x_new)","model_selection模块提供了模型选择的相关操作","model产生的错误会很大，使用我们的模型预测产生的错误会相对少些（因为我们的模型充分的考虑了y和x之间的关系），用这两者相减，结果就是拟合了我们的错误指标，用1减去这个商结果就是我们的模型没有产生错误的指标","model：模型","module.predict(x_plot)","most_common方法求出最多的元素对应的那个键值对","ms","ms,","mse","mse,rmse,mae的实现","mse的实现","multi_class='multinomial',","multi_class='ovr'","multi_class='ovr',","multiclass模块提供了多分类问题的相关实现","my_knn_clf","my_knn_clf.fit(x_train,y_train)","my_knn_clf.predict(x_test)","mymodule.firstml","n_compon","n_components>=1,","n_estim","n_estimators=30,","n_estimators=500,","n_estimators=5000,","n_iter","n_iters,","n_iters:","n_iters=1e4):","n_iters=1e4,","n_iters=5,","n_iters=len(x_b)//3)","n_iters=n_iters,","n_job","n_jobs=","n_jobs=1,","n_neighbors=3,","n_neighbors=5,","n_neighbors=50,","n_neighbors=6,","nan（not","nearset","nearset[:k]]","neighbors模块提供了近邻相关的算法实现","net","noise=0.3,","noisy_digit","noisy_digits[y==0,:][:10]","noisy_digits[y==num,:][:10]","none","none,\\","normal","normalization：把所有数据映射到0","normalize=false)","notebook与numpy的使用","notebook自动决定的）","np","np.arange(","np.arange(1,11).reshape(5,2)","np.arange(1,12,dtype=float)","np.arange(np.min(decision_scores),","np.argsort(distances)","np.argsort(x[:,d])","np.array((x>=","np.array((y_predict1+y_predict2+y_predict3)","np.array([","np.array([1.,2.,3.,4.,5.])","np.array([1.,3.,2.,3.,5.])","np.array([8.093607318,3.365731514])","np.array([np.mean(x[:i])","np.array([np.std(x[:i])","np.array([self._predict(x)","np.array(decision_scor","np.array(dscision_scor","np.array(raw_data_x)","np.array(raw_data_y)","np.array(x,dtype=float)","np.array(x2,dtype=float)","np.array(x[60000:],dtype=float)","np.array(x[:,0]**2","np.array(x[:,0]**2+x[:,1]","np.array(x[:60000],dtype=float)","np.array(y[60000:],dtype=float)","np.array(y[:60000],dtype=float)","np.c_[x0.ravel(),x1.ravel()]","np.empty((100,2))","np.empty((len(x),","np.empty(len(theta))","np.empty(len(w))","np.empty(shape=x.shape,dtype=float)","np.empty(x.shape)","np.exp(","np.fill_diagonal(err_matrix,","np.hstack","np.hstack([np.ones((len(x),","np.hstack([np.ones((len(x),1)),x])","np.hstack([np.ones((len(x_predict),","np.hstack([np.ones((len(x_train),","np.hstack([x,x**2])","np.linalg.inv()","np.linalg.inv(x_b.t.dot(x_b)).dot(x_b.t).dot(y_train)","np.linalg.norm(w)","np.linspace(","np.linspace(0.01,","np.linspace(4,8,1000)","np.linspace(axis[0],","np.linspace(axis[0],axis[1],int((axis[1]","np.linspace(axis[2],axis[3],int((axis[3]","np.log(1","np.log(p)","np.max(decision_scores))","np.max(decision_scores),","np.mean(scores)","np.mean(x,axis=0)","np.mean(x2[:,0])","np.mean(x2[:,0]))/np.std(x2[:,0])","np.mean(x2[:,1])","np.mean(x2[:,1]))/np.std(x2[:,1])","np.mean(x[:,0])","np.mean(x[:,1])","np.meshgrid(","np.min(x))/np.max(x)","np.min(x)]","np.min(x[:,0]))","np.min(x[:,0]))/(np.max(x[:,0])","np.min(x[:,1])))","np.min(x[:,1]))/(np.max(x[:,1])","np.ones((len(x_train),","np.random.normal(0,1,size=(200,2))","np.random.normal(0,1,size=100)","np.random.normal(0,4,size=x.shape)","np.random.normal(size=(1000,10))","np.random.normal(size=100)","np.random.normal(size=1000)","np.random.normal(size=m)","np.random.permutation(len(faces.data))","np.random.permutation(len(x))","np.random.permutation(m)","np.random.randint(0,100,(50,2))","np.random.randint(0,100,size=100)","np.random.randint(len(x_b))","np.random.randn(x_b.shape[1])","np.random.random(size=m)","np.random.random(x.shape[1])","np.random.random(x_pca.shape[1])","np.random.seed(42)","np.random.seed(666)","np.random.uniform(","np.random.uniform(0.,100.,size=100)","np.std(x2[:,0])","np.std(x2[:,1])","np.std(x[:,0])","np.std(x[:,1])","np.sum((i","np.sum((x.dot(w)**2))/len(x)","np.sum((x_b.dot(theta)","np.sum((x_b_k","np.sum((y_tru","np.sum((y_true==0)&(y_predict==0))","np.sum((y_true==0)&(y_predict==1))","np.sum((y_true==1)&(y_predict==0))","np.sum((y_true==1)&(y_predict==1))","np.sum(cfm,","np.sum(np.absolute(y_tru","np.sum(x_b.dot(theta)","np.sum(y*np.log(y_hat)","np.vstack([example_digits,x_num])","np.zeros(x_b.shape[1])","num","num/d","number","number）","numpi","numpy.array","numpy.dtyp","on","onevsoneclassifi","onevsoneclassifier(log_reg)","onevsoneclassifier是ovo的实现","onevsrestclassifi","onevsrestclassifier(log_reg)","onevsrestclassifier是ovr的实现","oob","oob_score=true)","oob_score=true,","oper","operator的首字母缩写","optional(default=false)","original\")","out","ovo","ovo.fit(x_train,y_train)","ovo.score(x_test,y_test)","ovr","ovr.fit(x_train,y_train)","ovr.score(x_test,y_test)","p","p)","p**2","p*log(p)","p=2,","p=3,","p^)。","packages/matplotlib/contour.py:967:","packages/sklearn/preprocessing/label.py:151:","param_grid","param_grid=[{'weights':","past","patch","pca","pca(0.5)","pca(0.9)","pca(0.95)","pca(n_components=1)","pca(n_components=2)","pca(n_components=64)","pca(svd_solver='randomized')","pca.components_.shap","pca.explained_variance_ratio_","pca.fit(noisy_digits)","pca.fit(x)","pca.fit(x_train)","pca.inverse_transform(components)","pca.inverse_transform(x_reduction)","pca.n_components_","pca.transform(noisy_digits)","pca.transform(x)","pca.transform(x_test)","pca.transform(x_train)","pca:","pca给出了主成分分析法的相关实现","pca降维到两维的意义","pca（princip","penalty='l1',","penalty='l2',","per","permutation(n)","pipelin","pipeline([","pipeline(memory=none,","piplin","pipline的英文名字是管道，那么","plot","plot_decision_boundary(dt_cfl,","plot_decision_boundary(dt_clf,","plot_decision_boundary(dt_clf2,","plot_decision_boundary(dt_clf3,","plot_decision_boundary(dt_clf4,","plot_decision_boundary(dt_clf5,","plot_decision_boundary(knn_clf,axis=[4,7.5,1.5,4.5])","plot_decision_boundary(knn_clf_all,axis=[4,8,1.5,4.5])","plot_decision_boundary(log_reg,axis=[","plot_decision_boundary(log_reg,axis=[4,7.5,1.5,4.5])","plot_decision_boundary(log_reg,axis=[4,8.5,1.5,4.5])","plot_decision_boundary(log_reg2,axis=[4,8.5,1.5,4.5])","plot_decision_boundary(model,axis):","plot_decision_boundary(poly_kernel_svc,","plot_decision_boundary(poly_log_reg,axis=[","plot_decision_boundary(poly_log_reg2,axis=[","plot_decision_boundary(poly_log_reg3,axis=[","plot_decision_boundary(poly_log_reg4,axis=[","plot_decision_boundary(poly_svc,","plot_decision_boundary(svc,axis=[","plot_decision_boundary(svc2,axis=[","plot_decision_boundary(svc_gamma01,axis=[","plot_decision_boundary(svc_gamma05,axis=[","plot_decision_boundary(svc_gamma10,axis=[","plot_decision_boundary(svc_gamma100,axis=[","plot_digits(data):","plot_digits(example_digits)","plot_digits(example_faces)","plot_digits(filtered_digits)","plot_digits(pca.components_[:36])","plot_i","plot_learning_curve(algo,x_train,x_test,y_train,y_test):","plot_learning_curve(linearregression(),x_train,x_test,y_train,y_test)","plot_learning_curve(poly20_reg,x_train,x_test,y_train,y_test)","plot_learning_curve(poly2_reg,x_train,x_test,y_train,y_test)","plot_module(lasso1_reg)","plot_module(lasso2_reg)","plot_module(lasso3_reg)","plot_module(module):","plot_module(poly20_reg)","plot_module(ridge1_reg)","plot_module(ridge2_reg)","plot_module(ridge3_reg)","plot_module(ridge4_reg)","plot_svc_decision_boundary(model,axis):","plot_svc_decision_boundary(svc,axis=[","plot_svc_decision_boundary(svc2,axis=[","plot_svc_decision_boundary(svc3,axis=[","plot_theta_history()","plot_x","plt","plt.axis([","plt.axis([0,6,0,6])","plt.axis([0,len(x_train)+1,0,4])","plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)","plt.legend()","plt.matshow(cfm,","plt.matshow(err_matrix,","plt.plot([0,w[0]*30],[0,w[1]*30],color='r')","plt.plot([i","plt.plot(fprs,","plt.plot(fprs,tprs)","plt.plot(np.sort(x),y100_predict[np.argsort(x)],color='r')","plt.plot(np.sort(x),y10_predict[np.argsort(x)],color='r')","plt.plot(np.sort(x),y2_predict[np.argsort(x)],color='r')","plt.plot(np.sort(x),y_predict2[np.argsort(x)],color='r')","plt.plot(np.sort(x),y_predict[np.argsort(x)],color='r')","plt.plot(plot_x,plot_y)","plt.plot(precisions,","plt.plot(thresholds,","plt.plot(x,y)","plt.plot(x,y_hat,color='r')","plt.plot(x,y_predict,color='r')","plt.plot(x1_plot,x2_plot)","plt.plot(x_plot,y_plot,color='r')","plt.plot(x_plot[:,0],y_plot,color='r')","plt.scatter(iris.data[iris.target==0,0],iris.data[iris.target==0,1])","plt.scatter(iris.data[iris.target==1,0],iris.data[iris.target==1,1])","plt.scatter(iris.data[iris.target==2,0],iris.data[iris.target==2,1])","plt.scatter(x,","plt.scatter(x,y)","plt.scatter(x2[:,0],x2[:,1])","plt.scatter(x[0],x[1],color='b')","plt.scatter(x[:,0],x[:,1])","plt.scatter(x[:,0],x[:,1],color='b',alpha=0.5)","plt.scatter(x[:,0],x[:,1],color='b',alpha=0.8)","plt.scatter(x[y==0,","plt.scatter(x[y==0,0],x[y==0,1])","plt.scatter(x[y==0,0],x[y==0,1],color='red')","plt.scatter(x[y==0],","plt.scatter(x[y==1,","plt.scatter(x[y==1,0],x[y==1,1])","plt.scatter(x[y==1,0],x[y==1,1],color='blue')","plt.scatter(x[y==1],","plt.scatter(x[y==2,0],x[y==2,1])","plt.scatter(x_demean[:,0],x_demean[:,1])","plt.scatter(x_new[y==0,0],x_new[y==0,1])","plt.scatter(x_new[y==1,0],x_new[y==1,1])","plt.scatter(x_reduction[y==i,0],x_reduction[y==i,1],alpha=0.8)","plt.scatter(x_restore[:,0],x_restore[:,1],color='b',alpha=0.8)","plt.scatter(x_restore[:,0],x_restore[:,1],color='r',alpha=0.5)","plt.scatter(x_standard[y==0,0],x_standard[y==0,1],color='red')","plt.scatter(x_standard[y==1,0],x_standard[y==1,1],color='blue')","plt.scatter(x_train[y_train==0,0],x_train[y_train==0,1],color='g')","plt.scatter(x_train[y_train==1,0],x_train[y_train==1,1],color='r')","plt.show()","plt.subplots(10,10,figsize=(10,10),","plt.subplots(6,6,figsize=(10,10),","poli","polnomialsvc(degree,","polnomialsvc(degree=3)","poly.fit(x)","poly.transform(x)","poly20_reg","poly20_reg.fit(x_train,y_train)","poly20_reg.predict(x_test)","poly2_reg","poly_kernel_svc","poly_kernel_svc.fit(x,y)","poly_log_reg","poly_log_reg.fit(x,y)","poly_log_reg.fit(x_train,y_train)","poly_log_reg.score(x,y)","poly_log_reg.score(x_test,y_test)","poly_log_reg.score(x_train,y_train)","poly_log_reg2","poly_log_reg2.fit(x,y)","poly_log_reg2.fit(x_train,y_train)","poly_log_reg2.score(x_test,y_test)","poly_log_reg2.score(x_train,y_train)","poly_log_reg3","poly_log_reg3.fit(x_train,y_train)","poly_log_reg3.score(x_test,y_test)","poly_log_reg3.score(x_train,y_train)","poly_log_reg4","poly_log_reg4.fit(x_train,y_train)","poly_log_reg4.score(x_test,y_test)","poly_log_reg4.score(x_train,y_train)","poly_reg","poly_reg.fit(x,y)","poly_reg.predict(x)","poly_reg10","poly_reg10.fit(x,y)","poly_reg10.fit(x_train,y_train)，","poly_reg10.predict(x)","poly_reg10.predict(x_test)","poly_reg100","poly_reg100.fit(x,y)","poly_reg100.fit(x_train,y_train)","poly_reg100.predict(x)","poly_reg100.predict(x_plot)","poly_reg100.predict(x_test)","poly_reg2","poly_reg2.fit(x,y)","poly_reg2.fit(x_train,y_train)","poly_reg2.predict(x)","poly_reg2.predict(x_test)","poly_svc","poly_svc.fit(x,","polynomialfeatur","polynomialfeatures()","polynomialfeatures(degree=2)","polynomialfeatures(degree=2,","polynomialfeatures(degree=20,","polynomialfeatures(degree=3,","polynomialfeatures(degree=degree)),","polynomialfeatures,","polynomialfeatures进行多项式曾维处理，使用线性回归的方法解决非线性问题","polynomialkernelsvc(3)","polynomialkernelsvc(degree,c=1.0):","polynomiallogisticregression(degree):","polynomiallogisticregression(degree,c):","polynomiallogisticregression(degree,c,penalty='l2'):","polynomiallogisticregression(degree=2)","polynomiallogisticregression(degree=20)","polynomiallogisticregression(degree=20,c=0.1)","polynomiallogisticregression(degree=20,c=0.1,penalty='l1')","polynomialregression(10)","polynomialregression(100)","polynomialregression(2)","polynomialregression(20)","polynomialregression(degree):","posit","pre_dispatch='2*n_jobs',","precis","precision_recall_curv","precision_recall_curve(y_test,","precision_scor","precision_score(y_test,","precision_score(y_true,","precision_score,","precisions)","precisions,","precisions.append(precision_score(y_test,","precisions.shap","precisions[:","predict!\"","predict\"","predict(self,","predict_i","preprocessing模块提供了数据预处理的相关操作","presort='auto',","presort=false,","print(\"best_k","print(\"best_k=\",best_k)","print(\"best_method=\",best_method)","print(\"best_p","print(\"best_p=\",best_p)","print(\"best_scor","print(\"best_score=\",best_score)","print(\"best_score=0.0.\",best_score)","print(best_d)","print(best_d2)","print(best_entropy)","print(best_entropy2)","print(best_g)","print(best_g2)","print(best_v)","print(best_v2)","print(len(theta_history))","print(log_reg.score(x_test,y_test))","print(log_reg.score(x_train,y_train))","print(np.max(dscision_scores))","print(np.min(dscision_scores))","print(poly_log_reg.score(x,y))","print(precision_score(y_test,","print(recall_score(y_test,","print(theta)","print(x_test.shape)","print(x_train.shape)","print(y_test.shape)","print(y_train.shape)","probability=false,","pyplot","python3入门机器学习","p为明科夫斯基距离的p","p越趋近于0，逻辑回归算法越愿意将数据预测为0","p越趋近于1，逻辑回归算法越愿意将数据预测为1","r","r2=0。","r2_score","r2_score(y_test,","r2_score(y_test,y_predict)","r2_score(y_true,","r2_score的实现","rand_i","random","random_index","random_patches_clf","random_patches_clf.fit(x,","random_patches_clf.oob_score_","random_state=42)","random_state=666)","random_state=666,","random_state=none,","random_subspaces_clf","random_subspaces_clf.fit(x,","random_subspaces_clf.oob_score_","randomforestclassifi","randomforestclassifier(bootstrap=true,","randomforestclassifier(n_estimators=500,","range(1,","range(1,10):","range(1,11):","range(1,11)]","range(1,11)],","range(1,5):","range(1,6):","range(1,6)]","range(1,len(x)):","range(1,len(x_train)+1):","range(1,len(x_train)+1)],np.sqrt(test_score),label","range(1,len(x_train)+1)],np.sqrt(train_score),label","range(10):","range(2,10):","range(2,10)],","range(len(theta)):","range(len(w)):","range(len(x)):","range(m):","range(n):","range(n_iters):","range(x.shape(1))])","range(x.shape[1]):","range(x_train.shape[1])])","range(x_train.shape[1])],","rate","rate，fpr","raw_data_i","raw_data_x=","rbfkernalsvc()","rbfkernalsvc(gamma=0.1)","rbfkernalsvc(gamma=0.5)","rbfkernalsvc(gamma=1.0):","rbfkernalsvc(gamma=10)","rbfkernalsvc(gamma=100)","rbf核函数","rbf核函数中的gamma","re","recal","recall)","recall):","recall_scor","recall_score(y_test,","recall_score(y_true,","recall_score,f1_scor","recalls)","recalls,","recalls.append(recall_score(y_test,","recalls.shap","recalls[:","recall的平衡","refit=true,","reg1","reg1.a_","reg1.b_","reg1.fit(big_x,big_y)","reg1.fit(x,y)","reg1.predict(np.array([6.]))","reg1.predict(x)","reg2.fit(big_x,big_y)","regress","regression模型","regression模型\"\"\"","regression认为对应的这些特征有用，所以他可以当做特征选择用","regression认为这个θ对应的特征是没有用的，剩下的那些不等于0的θ就说明lasso","regression）","regression）来替代，从而达到选择去掉一些θ的过程","regression），当p=2的时候就是","regressor","regular","res.append(w)","res[0]","res[i]","respect","rest","result","resx","resx[:col]","return","return_train_score='warn',","rf_clf","rf_clf.fit(x,","rf_clf.oob_score_","rf_clf2","rf_clf2.fit(x,","rf_clf2.oob_score_","ridg","ridge1_predict","ridge1_reg","ridge1_reg.fit(x_train,y_train)","ridge1_reg.predict(x_test)","ridge2_predict","ridge2_reg","ridge2_reg.fit(x_train,y_train)","ridge2_reg.predict(x_test)","ridge3_predict","ridge3_reg","ridge3_reg.fit(x_train,y_train)","ridge3_reg.predict(x_test)","ridge4_predict","ridge4_reg","ridge4_reg.fit(x_train,y_train)","ridge4_reg.predict(x_test)","ridgeregression(degree,alpha):","ridgeregression(degree=20,alpha=0.00001)","ridgeregression(degree=20,alpha=1)","ridgeregression(degree=20,alpha=100)","ridgeregression(degree=20,alpha=100000)","ridge和lasso都是在损失函数中添加一项，来调节θ的值使其尽可能的小，使得我们的模型泛化能力更好一些","ridge是岭回归的实现","rmse","roc_auc_scor","roc_auc_score(y_test,","roc_curv","roc_curve(y_test,","roc_curve,roc_auc_scor","roc曲线","roc：receiv","root_mean_squared_error(y_true,","row_sum","runs,","runtimewarning:","r曲线更靠外（也就是与x轴，y轴所包围的面积越大），那么说明这根曲线对应的模型越好，因为对应这个曲线上的每一个点，他的precision_score和recall_score都比另一个曲线要大","r曲线，如果这个p","s","s)","s,","scikit","score","score(self,","score,k,p","score变低是由于数据比较简单。主要看决策边界","score的含义","score，让我们兼顾精准率和召回率","scoring=none,","scr","search","seed=none):","select","self","self._sigmoid(x_b.dot(theta))","self._theta","self._theta[0]","self._theta[1:]","self._x_train","self._y_train","self.a_","self.a_*x_single+self.b_","self.b_","self.coef_","self.components_","self.interception_","self.k","self.mean_","self.mean_[col])","self.n_compon","self.n_componentspca","self.predict(x_test)","self.scale_","self.scale_[col]","sgd(x_b,","sgd(x_b,y,initial_theta,n_iters):","sgdregressor","sgdregressor是梯度下降法相关的实现","shrinking=true,","shuffle_index","shuffle_indexes[:tets_size]","shuffle_indexes[tets_size:]","sigmoid(t):","sigmoid(x)","sikit","simpl","simplelinearregression1","simplelinearregression1()","simplelinearregression1:","size","sklearn","sklearn.dataset","sklearn.decomposit","sklearn.ensembl","sklearn.linear_model","sklearn.liner_model","sklearn.metr","sklearn.model_select","sklearn.multiclass","sklearn.neighbor","sklearn.pipelin","sklearn.preprocess","sklearn.svm","sklearn.tre","sklearn_knn_clf","sklearn_knn_clf.fit(x_train,y_train)","sklearn_knn_clf.predict(x_test)","sklearn_knn_clf.score(x_test,y_test)","sklearn中会根据我们顶一个的decision_scores的值来取最合适的步长","sklearn中对数据进行预处理的函数都封装在preprocessing模块下，包括之前学的归一化standardscal","sklearn中的pca算法支持传入一个小于1的数来表示我们希望能解释多少比例的主成分","sklearn中计算logistic不是简单的使用梯度下降法，他是使用更快的一种方法，所以需要修改solver参数","soft","softvot","solv","solver='liblinear',","solver='newton","sorted_index","spilt(x,","spilt(x1_r,y1_r,best_d2,best_v2)","split","splitter='best')","splitter='best'),","sqrt","sqrt(mean_squared_error(y_true,","sqrt(np.sum((x_train","squar","square\"\"\"","squared简介","stack","standard","standardlinearsvr()","standardlinearsvr(epsilon","standardscal","standardscaler()","standardscaler()),","standardscaler(copy=true,","standardscaler.fit(x)","standardscaler.fit(x_train)","standardscaler.mean_","standardscaler.scale_","standardscaler.transform(x)","standardscaler.transform(x_test)","standardscaler.transform(x_train)","standardscaler:","standardscaler提供数据归一化运算","start","std.","std_\"","steps=[('poly',","steps=[('std_scaler',","subplot_kw={'xticks':[],'yticks':[]},","subsample=1.0,","subspac","sum(y_predict==y_test)/len(y_test)","support","svc","svc()","svc()),","svc(c=1.0,","svc(kernel=\"poly\",degree=degree,c=c))","svc(probability=true)),","svc.coef_","svc.fit(x,y)","svc.fit(x_standard,","svc.intercept_","svc2","svc2.fit(x_standard,","svc3","svc3.fit(x_standard,","svc_gamma01","svc_gamma01.fit(x,","svc_gamma05","svc_gamma05.fit(x,","svc_gamma10","svc_gamma10.fit(x,","svc_gamma100","svc_gamma100.fit(x,","svm","svm_clf","svm_clf.fit(x_train,","svm_clf.predict(x_test)","svm_clf.score(x_test,","svm中使用多项式特征和核函数","svm对应的表达式为","svm就退化成了hard","svm思想解决回归问题","svm提供了支持向量机相关算法的实现","svm的决策边界的特点：理两个类别的数据点都尽可能的远，也就是这两个类别离决策边界最近的这些点离决策边界也尽肯能的远","svm的最终数学推导","svm算法：probability:","svm背后的最优化问题","svm这种思想对于未来的泛化能力的考量没有集中在数据预处理阶段，或者是找到了这个模型之后在进行正则化。而是将这种考量放在了算法内部。也就是要找到一条决策边界离两类样本都尽可能的远，这样的决策边界，他的泛化能力就是好的","svm进行改变得来的","svm，其是在hard","svm，取值越小，相当于他的容错空间更大一些","svr","svr.fit(x_train,","svr.score(x_test,","sys:","t))","t):","t0","t0,","t0:","t0=5,","t1","t1)","t1:","t1=50):","t1=50,k=10):","t:","test","test_index","test_radio=0.2,","test_ratio","test_ratio)","test_scor","test_score.append(mean_squared_error(y_test,y_test_predict))","test_size=0.8)","tets_siz","theta","theta.copy()","theta:","theta_1","theta_1[i]","theta_2","theta_2[i]","theta_histori","theta_history.append(initial_theta)","theta_history[","threshold","threshold,","threshold,dtype='int')","threshold.","thresholds.shap","thresholds:","threshold。可不可以让这个threshold作为决策边界呢","threshold。这样，相当于我们为我们的逻辑回归算法添加了一个新的超参数threshold，通过指定threshold，相对于可以平移决策边界的直线，进而影响逻辑回归的结果。","time:","times:","tn(y_test,y_log_predict)","tn(y_true,","tol=0.0001,","tol=0.001,","topk_i","total","total:","tp","tp(y_test,y_log_predict)","tp(y_true,","tp(y_true,y_predict)]","tpr","tpr,fpr","tprs)","tprs,","tprs.append(tpr(y_test,","tpr和fpr之间的关系","tpr：预测为1，并且预测对了的数量占真实值为1的百分比是多少","train","train_index","train_scor","train_score.append(mean_squared_error(y_train[:i],y_train_predict))","train_test_spilt","train_test_split","train_test_split(x,","train_test_split(x,y)","train_test_split(x,y,random_st","train_test_split(x,y,random_state=10)","train_test_split(x,y,random_state=666)","train_test_split(x,y,test_radio=0.25)","train_test_split(x,y,test_size=0.2,random_state=666)","train_test_split(x,y,test_size=0.4,random_st","train_test_split用于分割测试数据集和训练数据集","transform!\"","transform(self,","tree","true","true_theta","truth","try:","try_spilt(x,","try_spilt(x1_r,","tsereteli',","up_i","up_index","us","user","userwarning:","v","v)","vaild\"","valid","valid\"","validation可以叫做留一法","valu","value)","value):","vector","verbos","verbose=0)","verbose=0))])","verbose=0,","verbose=1)","verbose=false))])","vote","votes.most_common(1)","votes.most_common(1)[0][0]","voting=\"hard\")","voting=\"soft\")","voting_clf","voting_clf.fit(x_train,","voting_clf.score(x_test,","voting_clf2","voting_clf2.fit(x_train,","voting_clf2.score(x_test,","votingclassifi","votingclassifier(estimators=[","vs","w","w.copy()","w.dot(w2)","w0/w1","w1","w2","w[0]/w[1]","w[1]","w_1","w_1[i]","w_2","w_2[i]","wall","warm_start=false)","warm_start=false))])","weights='distance')","weights='uniform')","weights='uniform'),","with_mean=true,","with_std=true)","with_std=true)),","x","x(i)","x(i)·w","x(i)映射到w的距离实际上就是x(i)与w的点乘（蓝色的线），根据定义推导，其值实际上就是xproject","x)**2))","x):","x**2","x,","x,i","x,inital_w,eta,n_it","x.copy()","x.dot(w).reshape(","x.ndim","x.reshape(","x.shape","x.shape[0]","x.shape[1]","x.t.dot(x.dot(w))","x0","x0,x1","x1","x13,x23,x12x2,x22x1","x1_l,","x1_plot","x1_r,","x1分别乘以w1到wn，得到的k个数组成的向量，就是样本1映射到wk这个坐标系上得到的k维的向量，由于kt**(为什么是转置呢，因为我们是拿x的每一行去和w的每一行做点乘的，但是矩阵乘法规定是拿x的每一行和w的每一列做乘法)","x2","x2(x1):","x2(x1_plot)","x21,x22,x1*x2","x2[:,0]","x2[:,1]","x2[:5]","x2[i]","x2_l,","x2_plot","x2_r,","x[:,0]","x[:,1]","x[:,1]**2","x[:10,:]","x[:36,:]","x[i","x[i]","x[i].dot(w)*w","x[index_a],","x[index_b],","x[sorted_index[i","x[sorted_index[i],d])","x[sorted_index[i],d]:","x[test_indexes]","x[train_indexes]","x[y4.3","x])","x_b","x_b,","x_b.dot(self._theta)","x_b.dot(theta))**2)","x_b.dot(theta))**2)/len(x_b)","x_b.dot(true_theta)","x_b.t","x_b.t.dot(self._sigmoid(x_b.dot(theta))","x_b.t.dot(x_b.dot(theta)","x_b:","x_b[indexes]","x_b_i","x_b_i,","x_b_i.t.dot(x_b_i.dot(theta)","x_b_i:","x_b_k,","x_b_new","x_b_new[i],","x_demean","x_i,","x_l,","x_mean","x_mean)","x_mean)*(y_i","x_mean).dot(x_train","x_mean).dot(y_train","x_new","x_new[i,","x_num","x_pca","x_pca.dot(w).reshape(","x_plot","x_predict","x_predict):","x_predict.ndim","x_predict.shape[1]","x_predict])","x_r,","x_reduct","x_restor","x_single):","x_standard","x_test","x_test,","x_test_reduct","x_train","x_train\"","x_train,","x_train,x_test,y_train,y_test","x_train,y_train,x_test,y_test","x_train.mean()","x_train.ndim","x_train.shap","x_train.shape[0]","x_train:","x_train]","x_train])","x_train_reduct","x_train_reduction.shap","xmin","xproject(i)","xproject(i)就可以实现将x样本在xproject相应上的分量去掉，相减之后的集合意义就是讲x样本映射到了xproject向量相垂直的一个轴上，记为x`(i)","x特征矩阵","y","y\"","y)","y)*np.log(1","y).dot(x_b[:,","y):","y,","y.shape","y.shape[0],\\","y100_predict","y10_predict","y1_l,","y1_r","y1_r)","y20_predict","y2_l,","y2_predict","y2_r","y:","y[digits.target!=9]","y[digits.target==9]","y[index_a],","y[index_b]","y[indexes]","y[test_indexes]","y[train_indexes]","y_hat","y_hat))","y_i","y_i)","y_i):","y_i:","y_k)","y_k):","y_l,","y_log_predict","y_log_predict)","y_mean","y_mean)","y_new","y_new[i])","y_plot","y_predict","y_predict\"","y_predict)","y_predict))","y_predict)**2)","y_predict),","y_predict)/np.var(y_true)","y_predict):","y_predict,","y_predict.reshape(x0.shape)","y_predict1","y_predict2","y_predict2)","y_predict2))","y_predict3","y_predict=np.array(decision_scor","y_predict[:10]","y_r","y_test","y_test)","y_test):","y_test_predict","y_train","y_train\"","y_train)","y_train):","y_train,","y_train.mean()","y_train.shape[0],","y_train:","y_train_predict","y_true","zip(x_train,","zz","{","{'n_neighbors':","{'weights':","|","||xproject(i)||","}","},","±","µs","µs,","·w","·w（方向）就是x(i)在w上的分向量记为xproject(i)=","α=100的时候，使用ridge的得到的模型曲线依旧是一根曲线，事实上，使用ridge很难得到一根倾斜的直线，他一直是弯曲的形状","α实际上是一个超参数，代表在我们模型正则化下新的损失函数中，我们要让每一个θ尽可能的小，小的程度占我们整个损失函数的多少，如果α等于0，相当于没有正则化；如果α是正无穷的话，那么我们主要的优化任务就是让每一个θ尽可能的小","γ越大，正态分布对应的中型图案越窄。在这里rbf的kernal对应的γ变大了以后，这个决策边界针对其中的某一类，对于这一类他的每一个样本点的周围都形成了一个中型的图案，我们可以把这个图看成我们是在俯视这个中型图案，每个蓝色的点就是图案的尖。由于我们的γ值比较大，所以中型图案比较窄。在每一个蓝色的点周围都围绕了一定的区域，只有在这个区域内，我们才判断成蓝色的点，否则判断为红色的点。这也是高斯核函数的几何意义。","η太大，甚至导致不收敛","η太小，会减慢收敛学习速度","θ向量","θ求和的系数二分之一是一个惯例，加不加都可以，加上的原因是因为，将来对θ2>求导的时候可以抵消系数2，方便计算。不要也是可以的","μs（有多少次循环是由jupyt","σ(t))进行求导","。针对x12和x22依然是一个线性关系。但是对于x1，和x2就是曲线了","【under","一个三维空间中的梯度下降法（x,y为系数，z为损失函数）","一个是对于训练数据集来说的，模型越复杂，模型准确率越高，因为模型越复杂，对训练数据集的拟合就越好，相应的模型准确率就越高","一个矩阵可以把一个向量拉伸或者缩短λ倍，这个向量就是特征向量，λ是特征值；","一个陷阱","一些情况下，回归任务可以简化成分类任务，比如学生的具体成绩预测转换成评级，无人车驾驶，转换成油门，刹车，方向盘的程度","一些算法只支持完成二分类的任务","一些需要注意的细节：","一元二次方程","一共2914个维度，所以求出了2914个主成分","一对一的进行比较）","一方面我们可以方便直观的看出在人脸识别的过程中我们是怎么看到每一张脸相应的特征的","一直替代方法是是使用array可以在构造数组的时候限定类型，但是由于array只是把元素当成一个一维或者多维数组，而并没有当做矩阵，向量，所以也没有提供相应的方法函数，使得在机器学习中非常的不方便","一组线性不相关的特征向量可以组成一个特征向量空间；","一组超参数而已，拿到这组超参数后我们就可以训练处我们的最佳模型","一般情况下使用距离的导数作为权证","一部分数据有“标记”或者“答案”，另一部分数据没有","一针对剩余）","上面推导出的式子的大小是和样本数有关的，m越大，结果越大，这是不合理的，我们希望和m无关","下图分别是扔掉了特征一和特征二的两种方案，很明显右边这种的效果会更好一些，因为访问二扔掉特征二以后，点之间的分布情况更接近与原图，但是这不是更好的","下图是我们之前使用多项式回归过拟合一个样本的例子，可以看到这条模型曲线非常的弯曲，而且非常的陡峭，可以想象这条曲线的一些θ系数会非常的大。","下图每个颜色代表一个数字在降维到二维空间中的分布情况","下面尝试真正还原原来的曲线（构造均匀分布的原数据集）","下面来看一下，如果eta取较大值1.1，会出现什么情况","下面来看下二者的对比","不传max_depth会一直划分直到基尼系数为0为止","不使用测试数据集，而使用这部分没有取到的样本做测试/验证。","不使用直接相减的方式，由于差值有正有负，会抵消","不是用户传入的参数，而是根据用户传入的参数计算出来的结果，以_结尾","不规则的决策边界的绘制方法","不过即使如此还是存在一个，就是直线太简单了，比如如下的情况","不过实际上我们是很少使用l0正则的，因为l0正则的优化是一个np难的问题，我们不能使用诸如梯度下降法甚至数学公式来找到一个最优解。","不过无论是knn，还是逻辑回归算法，我们依然可以加入多项式项，使得他的决策边界不再是一根直线，对于这种情况，我们就不能简单的求出这根直线的方程。然后将整根直线画出来来看到这个决策的边界。这个时候我们需要一个绘制不规则的决策边界的方法","不适用绝对值的方式，由于绝对值函数存在不可导的点","与决策树中的超参数","与此同时需要主要，我们在上一章所讲的pca是对我们的数据进行降维处理，而我们这一章所讲的多项式回归显然在做一件相反的事情，他让我们的数据升维，在升维之后使得我们的算法可以更好的拟合高纬度的数据","与线性回归的区别","两个主成分加起来可以解释百分之27的原数据，而其他的信息丢失了","两个特征的样本x","两种距离的整理对比","中使用knn算法的总结整理","中的scaler","中的standardscal","中的user","中的y=1是真值；我们使用σ函数求出来的是预测值）","中的线性回归","中的阈值，来相应的调整多分类问题的准确度","中的随机一个元素进行导数公式的计算","中的随机选择k个元素进行导数公式的计算","为了减慢变化速度，t0为了增加随机性","为了可视化，先使用两个类别","为了和其他算法统一，可以认为训练数据集就是模型","为了提高tpr，我们就要将threshold拉低，但是在拉低的过程中，犯fp的错误的概率也会增高","为数据集添加一些噪音","主成分分析法","主要以sikit","举个栗子","之前学习的逻辑回归（通过最小化损失函数找到一个决策边界，通过决策边界来分类数据）有一个非常大的不足，就是他的模型泛化能力非常弱，因为我们通过已知的数据求出了决策边界，而并没有考虑未知的数据。","之间","也将特别的低，只有二者都非常高，我们得到的值才会特别高","也就是说polynomialfeatures会穷举出所有的多项式组合","了解了信息熵的概念，解决上面的两个问题就好说了。在每一个结点上，都希望在某一个维度上基于某一个阈值进行划分，在划分以后要做的事情就是要让我们的数据划分成两部分之后，相应我们的系统整体的信息熵降低（也就是让我们的系统变的更加确定）","事实上有一个指标可以之间定义样本间的距离，就是方差（variance）（方差：描述样本整体之间的疏密的一个指标，方差越大，代表样本之间越稀疏，方差越小，代表样本之间越紧密）","事实上正是这样。只不过高斯函数表达出的这种数据的映射是非常复杂的。","事实上，这也正是导数的定义，当函数y=f(x)的自变量x在一点x0上产生一个增量δx时，函数输出值的增量δy与自变量增量δx的比值在δx趋于0时的极限a如果存在，a即为在x0处的导数，记作f'(x0)或df(x0)/dx","二分类任务","二分类基尼系数函数","二分类的信息熵函数","交叉验证相对来说是比较正规的、比较标准的在我们调整我们的模型参数的时候看我们的性能的方式","交叉验证：在训练模型的时候，通常把数据分成k份，例如分成3份（abc）（分成k分，k属于超参数），这三份分别作为验证数据集和训练数据集。这样组合后可以分别产生三个模型，这三个模型，每个模型在测试数据集上都会产生一个性能的指标，这三个指标的平均值作为当前这个算法训练处的模型衡量的标准是怎样的。","什么是svm","什么是决策树","什么是学习曲线","什么是距离","什么是集成学习","从64个维度降到两个维度以后，虽然运行速度提高了，但是识别精度大大降低了","从784维降到了87维，只用87维就可以解释百分之90的原数据集","从上图可以看出，当我们添加了一个特征（原来特征的平方）之后，再从x的维度来看，就形成了一条曲线，显然这个曲线对原来数据集的拟合程度是更好的","从直观的角度看，决策边界也准确了很多","从趋势上看：","从高维数据向地维数据的映射","仔细观察后可以发现，很多数字的区分还是比较明细的","仔细观察，和线性回归曲线的不同在于，线性回归的学习曲线1.5，1.8左右；2阶多项式回归稳定在了1.0，0.9左右,2阶多项式稳定的误差比较低，说明","他为了拟合我们所有的样本点，变的太过复杂了，这种情况就是过拟合【over","他是一个离散的值，我们需要穷举所有θ的值来找出哪些θ需要，哪些不需要。实际上可以用l1正则（lass0","他的关键在于为原来的样本，添加新的特征。而我们得到新的特征的方式是原有特征的多项式的组合。","他的面积相应的也会越大，这种情况下分类算法的效果就更好","他讲原本mn","代表第一个主成分可以解释14%的原数据","代表第二个主成分可以解释13%的原数据","以上这样的方式，就是所谓的多项式回归","以下列出本笔记（课程）学习使用到的sikit","以下是定义了一个损失函数以后，参数theta对应的损失函数j的值对应的示例图，我们需要找到使得损失函数值j取得最小值对应的theta（这里是二维平面，也就是我们的参数只有一个）","以此类推，我们分别进行四次分类，哪次获得的类别得分最高，我们就任务他属于哪一个类别。对于逻辑回归来说，就是我们的概率p","优点","传入每一步的对象名和类的实例化","但是lasso不同,在lasso的损失函数中，如果我们让α趋近于无穷，只看后面一部分的话，那么后面一部分的绝对值实际上是不可导的，我们可以使用一种sign函数刻画一下绝对值导数，如下图。那么这个时候，同样在j(θ)向0趋近的过程中，他会先走到θ等于0的y轴位置，然后再沿着y轴往下向零点的方向走","但是一旦来了新的样本点，他就不能很好的预测了，在这种情况下，我们就称我们得到的这条弯弯曲曲的曲线，他的泛化能力（由此及彼的能力）非常弱","但是从投票的角度看，还是不够多","但是他真的能更好的预测我们数据的走势吗，例如我们选择2.5到3的一个x，使用上图预测出来的y的大小（0或者","但是使用lasso的时候，当α=0.1，虽然得到的依然是一根曲线，但是他显然比radge的程度更低，更像一根直线","但是使用交叉验证得到的最好参数best_score并不是真正的最好的结果，我们使用这种方式只是为了拿到","但是多分类的任务可以转换成二分类的任务","但是如果我们添加上多项式特征的话，相当于我们在座的事情就是升维。让我们的数据点不但有横轴的值，还有第二个维度的值，也就是x2.一点我们这样做了，我们原来的数据点就变成了线性可分的。这就是升维的意义","但是对于这个应用来说，很有可能我们对召回率不是特别关注。可能有很多上升周期，但是我们落掉了一些上升的周期（本来为1，我们错误的判断为0），这对我们没有太多的损失，因为我们漏掉了他，也不会投钱进去。","但是我们依然可以使用上面的方式绘制决策边界","低，每一次的方式不确定，甚至向反方向前进","使模型产生差异化","使用%lsmagic查看所有的魔法命令","使用%timeit","使用%time让测试只执行一次","使用10个维度","使用20阶多项式回归","使用64个维度的数据集，训练knn算法","使用baselin","使用d_j_debug调试模式求出theta","使用debug模式","使用degree=10的时候得到的均方误差要大于degree=2的时候，说明当degree等于10的时候，他的模型泛化能力变弱了","使用degree=20得出的决策边界的外面变的很奇怪。出现这样的情况就是因为degree=20太大了。导致边界的形状非常的不规则。此时显然发生了过拟合","使用lasso的过程如果某一项θ等于0了，就说明lasso","使用linspace将x轴，y轴划分成无数的小点","使用math数学解","使用n_job","使用numpy创建数组(和python的array中几乎一样)","使用oob","使用ovo","使用ovo的方式预测结果达到了百分之百；","使用pca进行降维后的数据集进行训练，不光时间变短了，准确度也变高了","使用pca进行降维，然后再训练knn算法","使用pca降噪","使用pipeline构建多项式回归","使用pipline构建多项式回归模型","使用sklearn中的","使用sklearn中的ovo","使用sklearn中的决策树直观的感受","使用sklearn提供的交叉验证","使用svm之前，和knn一样，也是要做数据标准化处理的，因为svm是涉及到距离的","使用vote","使用|θ|代替θ2来标示θ的大小","使用上小节的过拟合结果，我们可以得知，虽然我们训练出的曲线将原来的样本点拟合的非常好，总体的误差非常的小，","使用不同η学习率测试并观察我们的梯度下降法的结果","使用二阶多项式回归","使用二阶线性回归的性能是比较好的","使用交叉验证","使用交叉验证的方式来进行调参的过程","使用信息熵寻找最优划分","使用决策树方式进行集成学习的方式也叫随机森林","使用分割训练数据集和测试数据集来判断我们的机器学习性能的好坏，虽然是一个非常好的方案，但是会产生一个问题：针对特定测试数据集过拟合","使用可以解释百分之90原数据集的主成分","使用向量化点乘计算分子和分母","使用均方误差来看拟合的结果，这是因为我们同样都是对一组数据进行拟合，所以使用不同的方法对数据进行拟合","使用多项式回归","使用多项式回归后分数达到了百分之95.结果非常好","使用多项式核函数的svm","使用多项式特征为什么能处理非线性的数据问题：他的基本原理是依靠升维使得原本线性不可分的数据线性可分。","使用多项式特征的svm","使用岭回归","使用我们自己封装的测试分割函数分割训练集","使用我们自己的simplelinearregression1","使用我们自己的逻辑回归算法","使用所有的主成分绘制特征脸，","使用支持向量机思想解决分类问题","使用支持向量机思想解决回归问题","使用数学解求出theta","使用更多的维度进行多项式回归","使用核函数的支撑向量机","使用核函数的解决回归问题的支撑向量机","使用梯度上升法求解主成分","使用梯度下降法训练linear","使用梯度下降法训练logist","使用每次增强训练出来的子模型进行投票得出最终的结果","使用真实的数据测试","使用真实的数据，调整eta和iters，要么由于eta太小导致无法得出真实的结果，要么由于eta太大导致训练时间加长，这是由于数据的规模在不同的特征上不同，所以我们需要对数据进行归一化","使用这种正则化。如果c很小。那我们的任务就是集中精力调整l1或者l2的大小；如果c很大，那我们的任务就是集中精力调整原损失函数j(θ)的大小","使用这种正则化的好处是。我们不得不进行正则化；因为l1/l2前面的系数不能为0","使用逻辑回归","使用随机梯度下降法训练linear","使用随机的方式来求解出pca","例如:","例如下面一个两个特征的一个训练集，我们可以选择一个特征，扔掉一个特征","保存theta的变化值","信息熵","修改之前的求导函数","假设一共有4个类别，选取其中的某一个类别（假设红色）（one），而对于剩下的类别，把他们融合在一起，把他称之为其他的类别（rest），这样就把一个四分类问题转换成了二分类问题，转换成了使红色的概率是多少，是非红色的概率是多少","假设现在设计一个程序判断一个新的肿瘤病人是良性肿瘤还是恶性肿瘤","偏差","偏差和方差是互相矛盾的。降低方差会提高偏差，降低偏差会提高方差","先new一个默认的classifier对象","先基于原有的肿瘤病人的发现时间和肿瘤大小（特征）对应的良性/恶性（值）建立了一张散点图，横坐标是肿瘤大小，纵坐标是发现时间，红色代表良性，蓝色代表恶性，现在要预测的病人的颜色为绿色","共有500个样本数据;每个子模型只看100个样本数据，每个子模型不需要太高的准确率","其中衡量标准是和m有关的，因为越多的数据量产生的误差和可能会更大，但是毫无疑问越多的数据量训练出来的模型更好，为此需要一个取消误差的方法，如下","其他创建numpy.array","其他注意事项","再将得到的数据集与原数据集进行拼接，","再看机器学习","决策树可以解决分类问题","决策树在节点划分上，使用随机的特征和随机的阈值(理论基础：根据bag","决策树在节点划分上，在随机的特征子集上寻找最优划分特征","决策树是一个非参数学习的算法","决策树最多的叶子节点","决策树的局限性","决策树解决回归问题","决策树，叶子结点中占比例最大的类别数据占整个叶子结点量的比值","决策边界θt·xb=0","决策边界上面的直线方程：x1","决策边界下面的直线方程：x1","决策边界必须是横平竖直的","决策边界是一条抛物线","决策边界的直线方程：w0","决策边界非常不规则，产生了过拟合","准确度的陷阱和混淆矩阵","几乎为0","分别计算两部分的信息熵然后相加","分别计算两部分的基尼系数然后相加","分割数据集","分子","分数不好使由于我们只使用了两个维度","分母","分类任务","分类准确度比使用ovr的时候要高了很多","刚刚我们进行的实验实际上在实验模型的复杂度，对于多项式模型来说，我们回归的阶数越高，我们的模型会越复杂，在这种情况下对于我们的机器学习算法来说，通常是有下面一张图的。横轴是模型复杂度（对于不同的算法来说，代表的是不同的意思，比如对于多项式回归来说，是阶数越高，越复杂；对于knn来说，是k越小，模型越复杂，k越大，模型最简单，当k=n的时候，模型就简化成了看整个样本里，哪种样本最多，当k=1来说，对于每一个点，都要找到离他最近的那个点），另一个维度是模型准确率（也就是他能够多好的预测我们的曲线）","创建n维0数组,第一个参数shape是数组维度，第二个参数是类型","创建一维0数组","创建更多的子模型！集成更多的子模型的意见","初始值不能为0，因为将0带入求导公式，会发现得0，没有任何方向","判断发放给客户信用卡有风险；没有风险","判断发放给客户信用卡的风险评级","判断某支股票涨；跌","判断病患良性肿瘤；恶性肿瘤","判断邮件是垃圾邮件；不是垃圾邮件","到底什么是核函数","到现在我们所介绍的svm，都必须在样本点中能求出一根确确实实的直线，满足以上的条件，也就是线性可分的","到这里，我们获得了一个新的超参数","加载书写识别数据集","加载波士顿房价数据","加载鸢尾花数据集","医院已经积累了一定的病人信息和他们最终确诊是否患病的情况","升高threshold后,精准率升高，召回率降低","半监督学习","即x(i)映射到w上的值，那么||xproject(i)||（大小）","即θt·xb","原始集合","原来所有的数据都在x中，现在对x中每一个数据都进行平方，","原理案例介绍","去x_b,i","参数学习通常都是高偏差的算法。因为对数据具有极强的假设","又因为w和w2应该是互相垂直的，所以他们夹角的cos值等于0","取样：放回取样（bagging），不放回取样（pasting）","另一半式子反之亦然。","另一方面，也可以看出来，其实每一张脸都是这些人脸的一个线性组合，而特征脸依据重要程度顺序的排在了这里","另一种正则化","只取前两个特征；只取y=0，1","只有至少两个算法预测结果为1，三个相加才会大于2","只有都非常大，结果才会打","只看样本的一部分","只需要把信息熵的函数改成基尼系数","召回率低意味着:本来一个病人得病了，但是我们没有把他预测出来，这就意味着这个病人的病情会继续恶化下去。所以召回率更加重要，我们希望把所有有病的患者都预测出来。但是精准率却不是特别重要，因为本来一个人没病，我们预测他有病，这时候让他去做进一步的检查，进行确诊就好了。我们犯了fp的错误，只是让他多做了一次检查而已。这个时候召回率比精准率重要。","召回率曲线","召回率：我们关注的那个事件，真实的发生了的数据(分母)中，我们成功的预测了多少。","可以使用explainedvariance_ratio这个参数来查看每个主成分所解释的原数据，来判断要取多少个主成分","可以发现，只要η不超过一个限度，我们编写的函数都可以在有限次数之后找到最优解，并且η越小，学习的次数越多","可以想象如果将degree设置为3，那么将产生一下10个元素","可以方便可视化展示，帮助人们理解","可以直接使用import命令导入本机目录下的包","可以看出当我们的eta取1.1，函数会循环直至终止，这是由于，我们的η设置过大，导致每次循环过后，损失函数j的值都向大的方向变化","可以看出，向量化的运行速度比循环的形式速度要快80倍","可以看到排在前面的这些脸相应的比较笼统，排名第一的这张脸，告诉我们人脸大概就是这个位置，大概有这样一个轮廓","可以看成是(x·w)这个向量的转置（本来是个行向量，转置后是1行m列的列向量）与x这个矩阵（m行n列）做点乘等到的其中一项的相乘相加的结果","可以解决回归问题（将最终预测数据点落在叶子节点所有数据点的平均值作为预测值）","可以说knn是一个不需要训练过程的算法","可能我们计算出梯度下降法的公式，并使用python编程实现，预测的过程中程序并没有报错，但是可能我们需要求的梯度的结果是错误的，这个时候需要怎么样去调试发现错误呢。","可能预测房源准确度，rmse或者mae的值为5，预测学生的分数，结果的误差是10，这个5和10没有判断性，因为5和10对应不同的单位和量纲，无法比较","同样行数的，只有一列的全是1的矩阵","向量化","向量化实现的性能测试","向量化改进num,d的计算方法","向量化，x.dot(w)为m*1的向量，reshape后变成了1*m的列向量，再乘以w（方向）就是x的每一个值在w上","和","和关于bagging的更多讨论","和我们上面得到的best_scor","噪音，所以我们还是倾向于说从x到x_restore丢失了一些信息，不过我们丢失的信息很有可能有很大的一部分","回归任务","回归算法的本质就是找到一根直线或者一个曲线能够最大程度上的拟合我们的数据点，如何定义拟合，就是不同的回归算法的关键。比如线性回归的算法定义拟合的方式就是让数据点到直线的mse的值最小。","回归问题怎么解决分类问题？","回忆小批量梯度下降法也是将随机梯度下降法和批量梯度下降法结合到了一起。在机器学习领域中，经常使用这种方式来创造出一些新的方法，这些方法虽然名词非常的酷，但是他们背后的意义是非常简单的","回忆我们上一章学习的逻辑回归，通过一系列的推导，我们得出了决策边界:θt·xb","回忆解析几何，点到直线的距离","回顾网格搜素","因为w和w2都是单位向量，所以他们两个点乘得到的结果就是他们夹角的cos值，","因为使用train_test_split很有可能只是过拟合了测试数据集得出的结果","因为对于前10个样本来说，分数值都是小于0","因为对于我们的目标函数来说，w=0本身就是一个最小值点","因为我们本来就是要使得方差最大，而标准化的目的是使得方差为1","因为我们生成的数据是具有二次项的。但是我们这里使用的是线性逻辑回归","图像已经拥有了标记信息","图像识别","在10000个癌症患者中，一共有10个癌症患者，我们成功的预测的预测出了8个","在fpr越低的时候（犯fp错误的次数越少的时候），相应的fpr越高的时候，这根曲线整体就会被抬的越高，","在scikit","在svm中，可以不使用现使用polynomialfeatures再扔给linearsvc这种方式进行多项式回归","在人脸识别领域中，x的每一行都是人脸，而w中的每一行，相应的也可以理解为是一个人脸，就是特征脸。之所以叫特征脸就是因为，每一行都能反应原来的样本的一个重要的特征。","在使用20阶多项式回归训练模型的时候可以发现，在数据量偏多的时候，我们的训练数据集拟合的是比较好的，但是测试数据集的误差相对来说增大了很多，离训练数据集比较远，通常这就是过拟合的结果，他的泛化能力是不够的","在使用方法后面加?查看文档","在具体训练svm回归问题的结果的时候，我们对margin是有一个指定的，所以在这里引入了一个超参数ɛ，代表margin范围的两根直线的任意一跟到中间直线的距离。","在决策树建立之后，每个叶子结点都包含了一些数据。","在及其有偏的数据中，我们不看准确率，而看精准率和召回率，才能分析出算法的好坏","在多项式求解的时候需要对xi添加多项式项形成x`i,对xj添加多项式项形成x`j。然后再将二者进行点乘。","在我们之前推导的svm基础上，可以适当的将条件放宽松一些，允许我们的模型犯一些错误","在我们之前推导的最优化函数基础上，需要进行进一步的变形","在我们的决策平面上，分布着很多点，对于每一个点，我们都使用我们的模型来判断他分为哪一类。然后将这些颜色绘制出来得到的结果就是决策边界","在我们调小gamma以后，可以想象成每一个蓝色点周围的中型图案变宽了一些，由于这些蓝色的点离的比较近，所以图案连在了一起","在损失函数下，添加上一个l1正则项和一个l2正则项，并引入一个参数r来表示他们之间的比例。同时结合了岭回归和lasso回归的优势","在曲线方程中，导数代表切线斜率","在最终，测试误差和训练误差趋于相等，不过测试误差还是高于训练误差一些，这是因为，训练数据在数据非常多的情况下，可以将数据拟合的比较好，误差小一些，但是泛化到测试数据集的时候，还是有可能多一些误差","在有偏的数据中，我们将分类1作为我们关注的对象，例如在医疗中，这个精准率就是指我们预测癌症预测的成功率","在机器学习领域中，我们会发明不同的名词来描述不同的标准，比如用ridge和lasso来衡量正则化的这一项；mse和mae用来衡量回归结果的好坏，欧拉距离和曼哈顿距离用来衡量两点之间的距离。但是他们背后的数学思想是非常的类似的，表达出的数学含义也是一致的。只不过应用到不同的场景中产生了不同的效果","在每一个结点上，他首先选择一个维度以及这个维度上的一个阈值，分成两支，循环往复，来进行分类","在测试数据集上，在使用非常少的样本进行训练的时候，刚开始我们的测试误差非常的大，当训练样本大到一定程度以后，我们的测试误差就会逐渐减小，减小到一定程度后，也不会小太多，达到一种相对稳定的情况","在生成规则数据的基础上，增加标准差","在用新的数据集进行线性回归","在直线方程中，导数代表斜率","在第一个维度0.75的位置进行划分，划分的结果的信息熵为0.41左右","在训练数据集上预测的准确率是百分之百的","在训练数据集上，误差是逐渐升高的。这是因为我们的训练数据越来越多，我们的数据点越难得到全部的累积，不过整体而言，在刚开始的时候误差变化的比较快，后来就几乎不变了","在这种情况下，svm干的事，和解决分类算法是相反的过程。我们期望的是在margin范围里，包围的点越多越好。","在进行这一项变化之后，我们可以发现，在第二项中其中有一部分是xixj,也就是对于任意两个样本点的x值点乘。","在随机梯度下降法中，n_iters代表所有的样本会被看几圈","均值","均值归0","均值方差归一化","均值，可以看出现在的数据集是均匀分布的","基尼系数","基尼系数的意义和信息熵相同","增大alpha继续试验","增强学习","复杂度是o(n2),但是分类结果相比ovr是更加准确的，这是因为每次只用真实的两个类别进行比较，所以他更倾向于真实的样本属于哪个类别","复杂度由t变成了n*t","多元线性回归中的梯度下降法","多元线性回归实现","多元线性回归简介和正规方程解","多分类任务","多分类问题中的混淆矩阵","多分类问题中越亮的地方代表的就是犯错误越多的地方，并且通过横纵坐标可以看出具体的错误","多次运行，随机化初始点","多线性回归在机器学习算法上并没有新的地方，完全是使用线性回归的思路","多线程并行处理，占用几个核，","多项式回归对样本进行训练，使用20个维度","大多数时候二者没有特别的效果优劣","大多数算法具有相应的参数，可以调整偏差和方差","天然的可以解决多分类问题","天然的支持多分类的问题，如果是多分类问题，在特征平面内就会有多条直线，","如knn中的k，线性回归中使用多项式回归","如何创建差异性？","如何定义样本间间距?","如何将我们的样本x从n维转化成k维呢，回忆们之前学到的，对于一个x样本，与一个w进行点乘，其实就是讲一个样本映射到了w这个坐标轴，得到的模，如果讲这一个样本和这k个w分别做点乘，得到的就是这一个样本，在这k个方向上做映射后每一个方向上的大小，这k个元素合在一起，就代表这一个样本映射到新的k个轴所代表的坐标系上相应的这个样本的大小","如果c=1，代表两部分的重要程度是一样的，如果c在0","如果c越大，对应的我们就是在逼迫着所有的℥为0，此时意味着模型的容错空间更小，soft","如果k=10，则有必要对10以上的数字进行搜索","如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。","如果将w中的每一行都看作一个样本的话，那么我们也可以说，第一行所代表的样本是最重要的那个样本，最能反应x这个矩阵原来的那个特征的样本","如果将x2理解为一个特征，将x理解为另外一个特征,换句话说，本来我们的样本只有一个特征x，现在我们把他看成有两个特征的一个数据集。多了一个特征x2，那么从这个角度来看，这个式子依旧是一个线性回归的式子，但是从x的角度来看，他就是一个二次的方程","如果想让精准率变高的话，相应的其实就是我们只能讲那些特别有把握的数据分类为1。在这种情况下，实际上我们做的事情是让我们的算法做的事让其判断样本的概率是百分之90甚至百分之99的时候，我们才预测他为1，在这样的情况，很显然有很多真实为1的样本就被排除在了外面，所以召回率就会变低。","如果我们的数据输出的是数据的话，那就是回归问题所解决的问题，一个预测的数据来临之后，通过决策树到达了某一个叶子节点，我们就可以将这些叶子节点包含数据的平均值作为我们的预测值","如果我们的数据输出的是类别的话，我们就让这些叶子结点包含的数据进行投票，票数最多的即为我们输出的分类","如果我们的模型面对测试数据结果很差的话，那么他的泛化能力就很弱。事实上，这是训练数据集更大的意义","如果我们真的进行了一个机器学习算法进行训练，而准确度是99.9%的话，那么说明我们的机器学习算法是失败的，因为他比纯粹的预测一个人是监控的准确率更低。","如果我们设置任意一个常量","如果样本数非常多，那么即使使用梯度下降法也会导致速度比较慢，因为在梯度下降法中，每一个样本都要参与运算。这时候需要采用随机梯度下降法，我们将在下一小节进行介绍","如果每个子模型只有51%的准确率","如果每个子模型有60%的准确率","如果能找到一个函数k，将xi和xj分别作为参数传入，返回的就是x`i和x`j点乘的结果，那么我们就不需要分别运算再相乘。并且也节省了空间（因为我们不需要将低维的数据变形为高维的数据进行存储，可以利用k函数直接计算出结果）。这个k就是核函数（kernel","如果要让召回率升高，我们就要降低算法判断的概率的threshold，这时却是召回率提高了，但是精准率下降了","如第一个测试数据的预测概率值为0.02945732，对应的y_test为0","子模型之间不能一直！模型之间要有差异","存放了均值方差归一化所对应的信息","学生成绩","官方github代码：https://github.com/liuyubobobo/play","定义多项式回归函数","定义截距为4","定义绘图模型","实现","实现debug模式的dj(θ)","实现j(θ)函数","实现数学推导出的dj(θ)","实现混淆矩阵，精准率和召回率","实现线性回归","实现自己的standardscal","实现过程简单编码","实际应用中，通常应该先尝试一下岭回归（如果计算能力足够的话）。但是如果θ数量太大的话，消耗计算资源可能非常大，而lasso由于有的时候急于把一些θ化为0，可能会导致得到的偏差比价大。这个时候需要使用弹性网","实际情况下，应该多试一些数字，找到最合适的数字","实际编程（准备代码参考上一节岭回归）","实验搜索","对pca和线性代数中的特征向量的对比","对x_b进行一个乱序的排序","对于d次方的多项式","对于knn来说，这个决策边界是没有表达式的，因为我们不是使用数学求解的方式。而是使用距离投票的方式","对于roc曲线来说，他对于有偏的数据并不是那么敏感，所以在有偏的数据集里，看一下精准率和召回率是非常有必要的","对于roc曲线，我们通常关注的是这根曲线下面的面积的大小，面积越大，代表分类算法效果越好。","对于svm的算法来说。对于拟合的定义是，我们要定义一个mergin值，在margin范围内，我们能够包含的样本点最多。也就代表我们这个范围能够比较好的来表达我们的样本数据点，在这种情况下，我们取中间的直线作为我们真正的回归的结果，用他来预测其他点，位置点的值。","对于w这个矩阵来说，每一行代表一个方向，第一行是最重要的方向，第二行是次重要的方向","对于θ的求和i是从1到n,没有将θ0加进去，因为他不是任意一项的系数，他只是一个截距，决定了整个曲线的高低，但是不决定曲线每一部分的陡峭和缓和","对于一个数据集x来说，这个x有m行n列，代表有m个样本n个特征，通过我们前面学习的主成分分析法，假设我们已经求出了针对这个数据来说的前k个主成分，每一个主成分对应一个单位方向，用w来表示,w也是一个矩阵，他有k行，代表我们求出的前k个主成分，每一行有n列，代表每一个主成分的坐标轴应该是有n个元素的。这是因为我们的主成分分析法主要就是将数据从一个坐标系转化成了另外一个坐标系，原来这个坐标系有n个维度，现在这个坐标系也应该有n个维度，只不过对于转化的坐标系来说，我们取出来前k个，这k个方向更加重要。","对于一个结点来说，至少要有多少个样本数据，我们才对他继续进行拆分下去","对于二分类问题来说，混淆矩阵是一个2*2的矩阵。每一行代表的是对于预测的问题来说真实值是多少，相应的每一列是分类算法进行预测的预测值是多少。","对于叶子节点来说，至少要有几个样本","对于小批量梯度下降法，由多了一个超参数","对于所有的测试数据，我们的logisticregression全都正确的进行了分类","对于极度偏斜的数据（skeke","对于欠拟合比最佳的情况趋于稳定的那个位置要高一些，说明无论对于训练数据集还是测试数据集来说，误差都比较大。这是因为我们本身模型选的就不对，所以即使在训练数据集上，他的误差也是大的，所以才会呈现出这样的一种形态","对于每一个样本点，都应该有一个℥。也就是说对于每一个数据点，我们都应该求出他的一个容错空间","对于测试数据集来说，在模型很简单的时候，模型的准确率也比较低，随着模型逐渐变复杂，对测试数据集的准确率在逐渐的提升，提升到一定程度后，如果模型继续变复杂，那么我们的模型准确率将会进行下降（欠拟合","对于现在的数据（基于二次方程构造），我们使用低于2项的拟合结果，就是欠拟合；高于2项的拟合结果，就是过拟合","对于线性回归来说，我们得到一个函数f，将样本x输入f后，得到的值y就是要预测的值；","对于过拟合的情况，在训练数据集上，他的误差不大，和最佳的情况是差不多的，甚至在极端情况，如果degree取更高的话，那么训练数据集的误差会更低，但是问题在于，测试数据集的误差相对是比较大的，并且训练数据集的误差和测试数据集的误差相差比较大（表现在图上相差比较远），这就说明了此时我们的模型的泛化能力不够好，他的泛化能力是不够的","对于这样一个一维的数据，他是线性不可分的，我们很难画一跟直线，将这些样本点区分开。","对于这样一个系统，我们什么都没有做，就可以达到99.9%的准确率","对于这样的决策树来说，他具有数据结构利用树的所有性质。包括根结点，叶子结点，深度。在这里我们称决策树的深度为3","对于逻辑回归，定义他的损失函数比较困难。","对于那些线性不可分的情况，对应的解决办法是","对于高斯函数时将样本点映射成了无穷维空间的理解。如果样本点可以有无穷多个，那么就是将每一个样本点映射到了无穷维的空间","对于高斯函数，每一个数据点都是landmark，也就是对于每一个x，他都要尝试对于每一个样本y，进行核函数的计算，成为新的高维数据相对于的某一维的元素。","对任意一个维度x，我们都可以求这样一个值，他的每一个值x都对他的p次方进行求和，再开p次方根。通常将这个式子成为lp范数。","对应的是0次幂","对数据敏感","对数据进行降维处理","对整个数据集看一遍","对明克夫斯基距离进一步泛化","对比sklearn的划分结果，就是在横轴上（第0个维度），大约2.45的位置进行了划分","对没有“标记”的数据进行分类","对测试数据集如何归一化？","对特征进行随机取样的方式","对特征随机取样","对象","对象进行fit创建出模型","寻找好的超参数","寻找最好的k","导数代表theta单位变化时，j相应的变化","封装我们自己的","封装的kneighborsclassifier，在fit过程中如果数据集较大，会以树结构的过程进行存储，以加快knn的预测过程，但是会导致fit过程变慢","将52的矩阵进行多项式转换后变成了56","将两个数据点的平均值作为切分点","将对角线上的数字全都填成是0，剩下的其他的格子就是犯错误的百分比","将样例的均值归为0","将样本的特征和样本发生的概率联系起来，概率是一个数。","将横坐标作为x轴，纵坐标作为y轴，每一个点为（x(i)","将给定的数据进行分类，比如区分猫和狗","将计算分数方法封装到我们的simplelinearregression中","小批量","小批量梯度下降法：即，我们每一次不看全部样本那么多，也不是只看一次样本那么少，每次只看k个样本","小批量随机下降法的超参数k","少数服从多数","尝试使用l1正则项","尝试使用多项式回归","尝试使用所有的数据特征","尝试增大多项式项degree的值","尝试调整正则化超参数c","就成为了一条直线，比较一下这两个图，我们可以说，经过这样的操作，我们将原有数据集的噪音","就是模型非常的不规整","尽管如此，还是有很多的场景是比较适合使用高斯函数的，比如样本的特征非常多，但是样本点的数量可能并不多，也就是m","层次继续增多，就越来越像是一个神经网络","岭回归","已经非常接近之前的正常的决策边界了","市场分析","市场积累了房屋的基本信息和最终成交的金额","平均大约有37%的样本没有取到。这些样本就叫out","平方累加后再开根号，如果某些预测结果和真实结果相差非常大，那么rmse的结果会相对变大，所以rmse有放大误差的趋势，而mae没有，他直接就反应的是预测结果和真实结果直接的差距，正因如此，从某种程度上来说，想办法我们让rmse变的更小小对于我们来说比较有意义，因为这意味着整个样本的错误中，那个最值相对比较小，而且我们之前训练样本的目标，就是rmse根号里面1/m的这一部分，而这一部分的本质和优化rmse是一样的","并不是所有函数都有唯一的极值点","异常检测","引入决策树","当alpha非常大，我们的模型实际上相当于就是在优化θ的平方和这一项，使得其最小（因为mse的部分相对非常小）","当c非常大，更趋近于hard","当gamma越来越小，决策边界越来越像一条直线，开始变的欠拟合。","当p=0的时候，按照我们的分类，我们应该把样本分为y^=0(注意这里是预测值)这一类。但是这个样本实际是y=1(注意这里是真值)，显然我们分错了，此时，我们对他进行惩罚，这个惩罚是正无穷的；随着p逐渐变大，我们的损失越来越小。当p=1的时候，我们会将样本分类为y=1，此时和这个样本真实的y=1是一致的，那么此时","当p=1的时候就是l1范数（曼哈顿距离|ridg","当x=0.5","当二者相同，得到的就是这个相同的值","当使用ridge的时候，当α趋近与无穷大，那么使用梯度下降法的j(θ)的导数如下图，j(θ)向0趋近的过程中，每个θ都是有值的","当我们使用svm算法，我们的kernal选用高斯kernal，我们的gamma值相当于在调整模型的复杂度。","当我们使用高斯核函数的时候，其实这个计算开销是非常大的，也正是因为这样，在使用高斯核函数进行训练的时候，训练时间会比较长。","当然我们也可以之间使用数学原理推导出结果，这里我们主要关注使用搜索的策略来求解主成分分析法，这样我们对梯度上升发和梯度下降法也可以有一个更深刻的认识","当然这显然是过拟合了","当然，在实际情况下，我们不好说x_restore就是一点噪音都没有，也不好说原数据的所有的抖动全都是","当系统每一个类别都是等概率的时候，其实是他最不确定的时候，此时他的信息熵是最高的。如果系统偏向于某一类，相当于有了约定向，信息熵逐渐降低，知道有一个类别占到了百分之百，此时信息熵达到最低值0","很明显，我们用一跟直线来拟合一根有弧度的曲线，效果是不好的","很显然，现在这个样子相比上面的形状不在有过拟合，有了非常清晰的边界（不会针对某几个特别的样本点进行特殊的变化）","得到的x`","得到的均方误差的指标是具有可比性的，（但是对于多项式回归来说，使用r2score进行衡量是没有问题是）","得到的误差依然是比较小，但是比之前的1.18大了些，说明正则化做的有些过头了","快","思路，遍历1","总结","总结ridge和lasso","总结一句话就是：降低了维度，丢失了信息，同时也去除了一部分噪音","意义","慢","我们上面的操作，实际上在网格搜索的过程中已经进行了，只不过这个过程是sklean的网格搜索自带的一个过程","我们之前说，对于多项式核函数来说，他的本质就是对于没一个数据点添加了多项式项，在将这些多项式的新的数据特征进行点乘就形成了新的多项式核函数。","我们使用pca进行降维然后在反转回原来的维度，经过这样一个操作，可以发现此时这个数据","我们使用训练数据集训练好模型之后，将验证数据集送给这个模型，看看这个训练数据集训练的效果是怎么样的，如果效果不好的话，我们重新换参数，重新训练模型。直到我们的模型针对验证数据来说已经达到最优了。","我们只要找到d的表达式，也相应的能够求解svm的问题。","我们可以在真正的机器学习之前，先使用d_j_debug这种调试方式来验证一下我们的d_j_main的结果是否正确，然后再进行机器学习。","我们可以在线性回归的结果基础上，添加一个σ函数，将结果转换成0到1之间","我们可以看到，这个分类的结果，我们又把他规约成了一个二分类的问题，我们现在的分类的结果很容易混淆1和9以及1和8，相应的我们可以微调1和9和1和8分类问题","我们如何使用管道呢，先考虑我们多项式回归的过程","我们希望有一根直线，是斜着的，我们希望将所有的点都映射到这条直线上，那么这个时候我们就成功的将二维降到了一维，与此同时，这些点更加趋近与原来的点的分布情况，换句话说，点和点之间的距离比无论是映射到x还是映射到y周，他们之间的区分度都更加的大，也就更加容易区分","我们希望让θ的个数尽量小，描述的是非零θ元素的个数。我们用这样的方式来限制θ的数量尽可能的小，进而来限制我们的曲线不要太抖","我们希望这个比例越高越好。如果我们预测股票升了，我们就要购买这个股票，如果我们犯了fp的错误（实际上股票将下来了，而我们预测升上来了），那么我们就就亏钱了。","我们得到新的降维后的矩阵xk以后，是可以通过和wk想乘回复回来的，但是由于我们在降维的过程中丢失了一部分信息，这时及时回复回来也和原来的矩阵不一样了，但是这个从数据角度成立的","我们每次使用测试数据来分析性能的好坏。一旦发现结果不好，我们就换一个参数（可能是degree也可能是其他超参数）重新进行训练。这种情况下，我们的模型在一定程度上围绕着测试数据集打转。也就是说我们在寻找一组参数，使得这组参数训练出来的模型在测试结果集上表现的最好。但是由于这组测试数据集是已知的，我们相当于在针对这组测试数据集进行调参，那么他也有可能产生过拟合的情况，也就是我们得到的模型针对测试数据集过拟合了","我们用不同的超参数训练不同的模型，每得到一个新的模型，就可以得到一根不同的p","我们用两个损失函数训练模型是不太方便的，可以将他们合成一个式子如下。当y=1的时候，后半部分是0，所以就只剩下","我们真正需要的是，我们得到的模型的泛化能力更高，解决这个问题的方法也就是使用训练数据集，测试数据集的分离","我们训练的模型目的是为了使得预测的数据能够尽肯能的准确，在这种情况下，我们观察训练数据集的拟合程度是没有意义的","我们通过精准率和召回率，判断出了这样做的预测算法是完全没有用的。","截距","房屋价格","房屋面积","所以为了限制让他们比较小，我们前面系数可以取的小一些","所以主成分分析实际上就是把矩阵转换到了一个特殊的特征向量空间；","所以从计算准确度上来说，我们应该更加倾向于ridge，但是如果我们的维度比较多，样本非常大（比如多项式回归时degree=100）","所以完整的soft","所述类别","手动完成集成学习的过程","手动将手写数据集变成及其偏斜的数据。不是9的y=0，是9的y=1","执行python脚本，并将脚本中的函数加载","扩展到多维维度则如下","批量梯度下降法","批量梯度下降法带来的一个问题是η的值需要设置的比较小，在样本数比较多的时候导致不是速度特别慢，这时候观察随机梯度下降法损失函数的求导公式，可以发现，我们对每一个xb都做了求和操作，又在最外面除以了m，那么可以考虑将求和和除以m的两个运算约掉，采用每次使用一个随机的xb","批量梯度下降法，d_j为求导函数，作为一个参数传入，用于切换求导策略","把所有数据归一到均值为0方差为1的分布中","投票","拼接矩阵","换句话说，我们使用了一个非常高维的数据，虽然使得我们的样本点获得了更小的误差，但是这根曲线完全不是我们想要的样子","换句话说，这个数据集展现的是在一根直线上下进行抖动式的分布，实际上这种抖动和这根直线本身的距离是噪音","接下来的任务，就是找每个节点上游一个维度，在这个维度上有一个取值，根据这个取值进行划分，划分后是所有其他划分方式的信息熵中的最小值。我们就成当前的划分方式就是一个最好的划分。找到这个划分的方法就是对所有的可能性进行搜索。","推导","推导过程","推导过程参考","描绘三分类的决策边界","描绘线性的决策边界","描述决策边界","描述数据的分布范围（标准差）","提供额外的随机性，抑制过拟合，但增大了bia","搜索明可夫斯基距离相应的p","支持向量机","支持向量机的思想是找到一条决策边界，让这条边界理两部分数据都尽可能的远，并且能很好的分别这两个类别的数据点。也就是说这种方式不仅要将现在的数据进行一个很好的划分，同时还考虑了未来的泛化能力。","改变维度","放回取样导致一部分样本很有可能没有取到","数字识别","数据基础","数据归一化","数据归一化总过程","数据标准化","数据量太大会报错","斜率为3","方便可视化","方差","无人驾驶","既针对样本，又针对特征进行随机采样","明克夫斯基距离","是x中的所有样本都去除了第一主成分上的分量得到的结果，要求第二主成分，只要在新的数据上，重新求一下第一主成分","是否打印搜索信息,传入值越大，输出信息越详细","是否放回取样","是吻合的","是噪音，这也解释了为什么我们在上一节降维处理以后，反而识别率提高了","是有噪音的","显然使用多项式回归得到的结果是更好的","显然有非常多的错误分类，所以导致我们的分类准确度只有0.605","显然没有精准率和召回率高，这是因为首先我们的数据是有偏的，精准率和召回率都比准确率要低一些，在这里精准率和召回率能够更好的反应我们的结果。其次使用逻辑回归进行预测，明显召回率比较低，所以f1_score","曲线","更合理的投票，应该有权值","更多的距离定义","更常见：各种原因产生的标记缺失","更快的训练速度","曼哈顿距离","最值归一化","最大循环次数","最好的分数","最好的参数","最好的评估结果，返回的是kneighborsclassifier对象","最终预测结果是:m1","有一个非常小，整体就非常小","有一些算法只能解决分类问题","有一些算法只能解决回归问题","有一些算法天然可以完成多分类任务","有一些算法天生是高偏差算法。如线性回归（用一条直线去拟合一条曲线，导致整体预测结果都距离真实数据查很大，偏差非常大）","有一些算法天生是高方差算法。如knn（过于依赖数据，一点选取的数据点有多数是不正确的，那么预测的结果就是错误的。导致有的很准确，有的非常不准确，方差非常大）","有一些算法既能解决回归问题，也能解决分类问题","有了这个点到直线的距离，我们就可以相应的得出svm这个问题的表达式","有关数据的一些术语","有的时候我们希望同时关注精准率和召回率，这个时候我们可以使用f1","有的时候我们注重召回率，如病人诊断。","有的时候我们注重精准率，如股票预测。","本次测试时间比上面的测试时间会多，是因为只测试了一次。可能不够准确","本笔记记录笔者观看慕课网入门机器学习课程的笔记过程","本质","机器人","机器学习中监督学习的基本任务","机器学习总过程","机器学习流程回顾","机器学习的主要调整来源于方差（这是站在算法的角度上，而不是问题的角度上,比如对金融市场的理解，很多人尝试用历史的数据预测未来的金融走势，这样的尝试通常都不太理想。很有可能因为历史的金融趋势不能很好的反应未来的走向，这种预测方法本身带来的非常大的偏差）换句话说，我们很容易让模型变的很复杂，从而降低模型的偏差，但是由于这样的模型的方差非常的大，最终也没有很好的性能。","极端情况下，k","构建决策树的问题","构造一个两个样本之间有基本线性关系的数据集，可以使得我们降维的效果更加明显","构造一个和x_train","某个维度在哪个值上做划分","查看数组元素类型","样本特征只有一个的线性回归问题，为简单线性回归，如房屋价格","样本间的距离被一个字段所主导","核函数并不是svm特有的技巧，只要我们的算法转换成了一个最优化问题，并且在求解这个最优化问题的过程中，存在xi点乘xj这样的式子或者类似这样的式子，我们都可以使用这种技巧","根据周围环境的情况，采取行动，根据采取行动的结果，学习行动方式","根据某一个维度d和某一个阈值v进行二分","根据训练数据集x_train,","案例","梯度上升法","梯度下降法","梯度下降法封装","梯度下降法模拟","梯度下降法的初始点也是一个超参数","梯度下降法简介","梯度下降法调试的原理","梯度下降法调试的实现","模型\"\"\"","模型样本","模型正则化基本原理","模型正则化需要做的事情就是限制这些系数的大小","模型没有完全的学到数据的中心，而学习到了很多噪音","模型泛化的一个举例。我们在考试前会做很多练习题。我们做练习题不是为了把全部的练习题（训练数据集）都得到满分，而是为了在最后的那一场考试（真实数据）中得到满分","模型泛化能力并没有降低","模型的泛化能力","模型误差=偏差（bias）均差(variance)+不可避免的误差","模型返回能力变脆。因为出现了过拟合","模拟使用信息熵进行划分","模拟使用基尼系数进行划分","模拟数据进行测试","模拟测试用例","欠拟合和过拟合的标准定义","欠拟合：算法所训练的模型不能完整表述数据关系","欧拉","欧拉距离","此时我们的目标函数就可以化简成","每一次只需观察一个样本","每一行实际上是一个主成分，他相当于表达了一部分原来的人脸数据中对应的一个特征","每一行有多少样本,在列方向上求和","每个模型都在尝试增强(boosting)整体的效果","每个结点在哪个维度做划分","每个维度都是按照顺序排列的。","每张脸对应的人名","每次对所有的样本看一遍才可以计算出梯度","每次就从n个类别挑出两个类别（比如这里挑出红蓝两个类别）,然后进行二分类任务，看对于这个任务来说，我们的样本点是属于哪个类别。然后依次类推进行扩展，如果我们有4个类别需要分类，那我们就能形成6个两两的对c42(排列组合公式4*3/2=6)，也就是6个二分类问题。对于这6个分类结果，判定他在哪个类别中数量最大，就判定他是哪个类别","每次看k个元素","比如如果只是区分蓝色的数字和紫色的数字，那么使用二个维度就足够了","比较笨的方法实现","求出准确率","求出第一主成分以后，如何求出下一个主成分?","求出第二主成分","求出距离测试点最近的6个点的类别","求均值","求平方","没有从最小值开始取，在sklearn的封装中，他自动寻找了他认为最重要的数据","没有指定n_componet","没有进行数据归一化，是因为这里的每个维度都标示的是每个像素点的亮度，他们的尺度是相同的，这个时候比较两个样本之间的距离是有意义的","注意2：不能从0向量开始","注意3：不能使用standardscaler标准化数据","注意alpha后面的参数是所有theta的平方和，而对于多项式回归来说，岭回归之前得到的θ都非常大","注意full","注意这个x_test,y_test在交叉验证过程中是完全没有用过的，也就是说我们这样得出的结果是可信的","注意：这里由于数据集比较小，耗时的差别比较小","注：|xproject|的平均值也是一个向量","注：有了l1,l2正则项，我们就可以进一步得到ln正则项，虽然实际应用中我们的n不会超过2，但是在数学推导中是有意义的","测试","测试一个排序算法，由于第一次执行完毕后数组已经排好序，那么在后面执行的时候，如果使用插入排序等算法就会导致后面999次的时间非常短，导致测试值不准确","测试代码的性能","测试多次在每次测试的执行性能不一样的时候测试结果会不准确。","测试多维的数据集","测试我们的knn算法","测试我们的算法","测试数据对于我们的模型是全新的数据，如果使用训练数据获得的模型面对测试数据也能获得很好的结果，那么我们就说我们的模型泛化能力是很强的。","测试数据集的比例","测试整个代码块","测试结果表明，运行了一千次，取有价值的7次，平均每次耗时324+/","混淆矩阵","添加噪音","然后用第一步中取到的三个点进行投票，比如本例中投票结果就是蓝：红","熵信息的计算比基尼系数稍慢","熵在信息论中代表随机变量中不确定度的度量。","熵越大，数据的不确定性越高","熵越小，数据的不确定性越低","特征","特征压缩：pca","特征提取：信用卡的信用评级和人的胖瘦无关","现在有一个问题：这个数据集展现出来这样的结果，可是是不是有这样一种情况，这个数据集就应该是一根直线呢","生成一个一维向量进行归一化","生成一个二维矩阵进行归一化","生成不真实的非线性的数据集","生成表达式","用于加载手写识别的数据集","用于加载波士顿房价的数据集","用于加载鸢尾花数据集","用我们找到的k和p。来对x_train,y_train整体fit一下，来看他对x_test,y_test的测试结果","由上图可知，不同的threshold对应的精准率和召回率的关系。随着threshold增大，精准率在不断的变高，而召回率在不断的降低。由此说明精准率和召回率是相互矛盾的","由于fetch_lfw_people这个库的人脸是分布不均匀的，有的人可能只有一张图片，有的人有几十张","由于python","由于x是乱的，所以应该进行排序","由于我们使用的事随机梯度下降法，所以导致我们的最终结果不会像批量梯度下降法一样准确的朝着一个方向运算，而是曲线行下降，这时候我们就希望，越到下面，η值相应减小，事运算次数变多，从而精确计算结果","由于我们有一个求平均的过程，所以不会由于一份验证数据集中有比较极端的数据而导致模型有过大的偏差，这比我们只分成训练、验证、测试数据集要更加准确","由于训练数据集和测试数据集的分割和我们的稍有不同，所以结果会略有不同","由此可以看出，我们的d_j_debug和d_j_main的结果是相近的，所以我们的d_j_main的数学推导是没问题的。","画出带噪音的图像","的","的alpha值等于1，均差误差更加的缩小，并且曲线越来越趋近于一根倾斜的直线","的作用就是把上面的三个步骤合并，使得我们不用一直重复这三步","的分量矩阵","的原理，只要大多数的决策树的决策能力比扔硬币的能力好一点就够了)","的基本操作","的实现","的意思就是交叉验证中分割了三组数据集，而我们的参数组合为8*5=40中组合","的数据映射成了mm的数据，如果m非常的大，那么经过高斯核函数后，就映射成了一个非常非常高维空间的数据点。","的方法","的时候，信息熵达到了最大值（当两类样本各占一半的时候），也就是说这个时候的样本是最不稳定的","的混淆矩阵天然支持多分类问题","的特点","的缺点，量纲不准确，如果y的单位是万元，平方后就变成了万元的平方，这可能会给我们带来一些麻烦","的调试","的，所以预测值都是0","监督学习","监督学习和半监督学习是基础","直接使用线性回归，显然分数太低","直接使用逻辑回归。","相减得到的样本分布几乎垂直于原来的样本分布","相反，在最开始，我们直接使用一根直线来拟合我们的数据，也没有很好的拟合我们的样本特征，当然他犯的错误不是太过复杂了，而是太过简单了","相当于我们为样本多添加了一些特征，这些特征是原来样本的多项式项，增加了这些特征之后，我们们可以使用线性回归的思路更好的我们的数据","相当于让模型正则化那一项起更大的作用","相比之前，数字清楚了很多，平滑了很多，说明使用pca进行降噪是可行的","真值为i而预测为j的样本数量有多少","真实数据波士顿房价进行测试","真实的θ值","矩阵中的每一行的数字都会除以这一行的数字和得到的一个百分比矩阵","矩阵点乘","确定当前模型的准确度\"\"\"","稳定性","笔记整理","第i行第j列的数值代表","第t次循环","第一个系数是x前面的系数，第二个系数是x平方前面的系数","第一列是1","第一列是sklearn为我们添加的x的零次方的特征","第一步:","第三列是添加的x的二次方的特征","第二列和原来的特征一样是x的一次方的特征","第二列和第三列对应的是原来的x矩阵，此时他有两列一次幂的项","第五列是原来数据的两列相乘的结果","第六列是原来数据的第二列平方的结果","第四列是原来数据的第一列平方的结果","简介","简单模拟一个损失函数","简单自定义一个训练集并描绘","精准率","精准率=40%：我们没做100次患病的预测，其中会有40次是对的","精准率和召回率","精准率和召回率是两个指标，有的时候精准率高一些，有的时候召回率高一些，在我们使用的时候，我们应该怎么解读这个精准率和召回率呢？","精准率和召回率这两个指标是互相矛盾的。我们要找到的是这两个指标直接的一个平衡。","精准率：预测数据为1，预测对了的概率","精度","系数向量（θ1,θ2,.....θn）","线性回归中的梯度下降法的实现","线性回归添加了多项式项后。degree这个阶数越大，模型越复杂，就越容易发生过拟合","线性回归算法以一个坐标系里一个维度为结果，其他维度为特征（如二维平面坐标系中横轴为特征，纵轴为结果），无数的训练集放在坐标系中，发现他们是围绕着一条执行分布。线性回归算法的期望，就是寻找一条直线，最大程度的“拟合”样本特征和样本输出标记的关系","线性回归计算出来的值域是负无穷到正无穷，而我们使用逻辑回归得出来的p是只取0到之间的个值的。这使得我们不能直接使用线性回归的方法，单单从应用的角度来说，但是这样做不够好，因为我们的逻辑回归的值域是有限制的，使用线性回归或者多项式回归拟合出来的直线或者曲线肯定会比较差。","经验数值","结果即为对于每一个样本来说，在逻辑回归算法中对应的score值是什么","结果向量","结果是一个连续数字的值，而非一个类别","绘制knn算法的决策边界。可以看到是一根弯弯曲曲的曲线","绘制roc曲线","绘制sigmoid函数","绘制三个不同类别","绘制上下两条直线","绘制决策边界","绘制学习曲线","绘制我们模拟的损失函数","绘制数据集及要预测的点","绘制曲线观察取前i个主成分的时候，所能解释的原数据比例","绘制样本曲线","绘制模型曲线","绘制混淆矩阵，越亮的地方说明数值越大","给出从0到n","给机器的训练数据拥有“标记”或者“答案”，人类已经给机器对数据进行了正确答案的划分，这个答案的划分本身就是监督的信息","给机器的训练数据没有任何的“答案”和“标记”","给消除了","统计学中，放回取样：bootstrap","维度","综合二者的优缺点，有一种新的梯度下降法","编程实现roc曲线","缺点","网格搜索","网格搜索超参数","考虑下面的数据，虽然我们可以使用线性回归来拟合这些数据，但是这些数据更像是一条二次曲线,相应的方程是y=ax2+bx+c,这是式子虽然可以理解为二次方程，但是我们呢可以从另外一个角度来理解这个式子：","考虑用%timeit","考虑距离？不考虑距离","而roc的应用场景是比较两个模型的优劣","而使得θ的平方和最小，就是使得每一个θ都趋近于0，这个时候曲线就趋近于一根直线了","而对于逻辑回归来说，我们要得到一个函数f，我们将样本x输入f以后，f会计算出y一个概率值p，之后我们使用这个概率值p来进行分类，如果p>=0.5,也就是有百分之50以上的概率发生的话，我们就让这个概率的值为1，否则让他为1，当然1和0在不同的场景下代表不同的意思。","聚类分析","股票价格","能更好的反应算法的水平","获取测试数据集","获取矩阵的转置","获取矩阵的逆","获得前n个主成分实现","获得最好的评估结果，返回的是kneighborsclassifier对象，可以直接拿来做机器学习预测了","获得每个标记加了噪音的10个元素，一共10个标记，公100个元素","落在两根直线上的这些点即为支撑向量，这两条直线中间的距离即为margin。","虽然整体速度慢了，但是这个结果却是可信赖的","虽然有很多机器学习方法","虽然训练出来的精度丢失了一些，但是效率却大大提高了","虽然边界还是比较奇怪，但是比之前degree=20要好很多","行相当于二维数组的第一个维度，列相当于二维数组的第二个维度，相对来将，真实值（第一个维度）要在预测值（第二个维度）的前面","衡量标准","补充（一个1xm的行向量乘以一个mx1的列向量等于一个数）","补充（矩阵点乘：a（m行）·b（n列）","表示的实际上是一条直线，如果有两个特征，那就是在二维平面上的一条直线，x1是横轴，x2是纵轴","被召回率拉低了，这个时候对于这个有偏的数据来说，我们更倾向认为f1_score","要求集合的每一个模型都能估算概率","要预测的点","观察多项式回归的学习曲线","观察逻辑回归预测的概率值p与y_test的关系","解决办法","解决方案","解决方案：","解决的方式其实就是：我们需要将我们的问题分为三部分，这三部分分别是训练数据集，验证数据集，测试数据集。","解决高方差的通常手段：","解释方差的比例","计算roc曲线的面积","计算分子分母","计算参数a和b","计算学习曲线数据","计算学习率，t1","计算方式","计算每两个值之间的信息熵，所以从1开始","让ridge2_reg","训练linear","训练simpl","训练train_test_spilt","训练一个模型m1,产生错误e1","训练数据集和测试数据集的意义","训练模型使用的x_train，是预测的模型使用x_test，以计算模型的泛化能力","训练集合","访问数组","说明一共包含5749个不同的人的脸","说明前28个主成分表示了百分之95的信息","说明总有一条曲线，他能拟合所有的样本点，使得均方误差的值为0","说明默认的正则化超参数c=1.0;penalty=l2,说明默认使用l2正则化","调和平均值的特点：如果一个值特别高，一个值特别低，那么我们得到的f1","调整k为50","调整过后的样子已经比上面的决策边界规整了很多。整体分成了三大块，非常的清晰","调用fit方法执行网格搜索","调用gridsearchcv创建网格搜索对象，传入参数为classifier对象以及参数列表","调用sikit","超参数和模型参数","越往后，鼻子眼睛的信息就清晰了起来","输出","输出10001","输出5.1820671385094386e","输出结果","过拟合","过拟合（非常的完全，两段有极端的情况）","过拟合：算法所训练的模型过多的表达了数据间的噪音关系","过滤超出绘制范围的数据","过程中，通常不会过拟合某一组的测试数据，所以平均来讲这个分数会稍微低一些","运用了cv交叉验证的方式","返回排序后的结果的索引,也就是距离测试点距离最近的点的排序坐标数组","返回的是一个数组，有三个元素，说明cross_val_score方法默认将我们的数据集分成了三份","这三份数据集进行交叉验证后产生了这三个结果","这个噪音的产生原因可能有很多，如测量人员的粗心，测量手段有问题等等原因，都会使得我们在现实世界中采集的数据","这个数据可以近乎表示每个主成分轴的重要程度","这个特征向量空间是由能够被拉伸最长的前k个特征向量组成的；而且这k个特征向量相互正交；","这个轴就是我们求出的第一个主成分","这个问题的答案，和机器学习大多数的取舍是一样的，应该视情况而定。","这也说明了ridge为什么叫岭回归，因为他更像是翻山越岭一样，在梯度下降法中一点一点找坡度缓的方向前进。而lasso的路径就比较规则，会在训练的过程中碰到一些轴使得某些θ为0。","这使得我们可以更加好的，更加准确的拿到我们数据集对应的特征，从而使得准确率大大提高","这就是一种过拟合的表现。可以看到上面的knn_clf_all的n_neighbors（k）为5。之前的讨论曾经说过，","这就是决策树在面对属性是数值特征的时候是怎么处理的。","这时候回过头来看gamma=1的时候，其实就是蓝色的点周围的中型图案变的更宽了","这是一个目标函数的最优化问题，使用梯度上升法解决。","这是因为lasso趋向于使得一部分theta值为0（而不是很小的值），所以可以作为特征选择用，lasso的最后两个字母so就是select","这是因为pca的过程中，不仅仅是进行了降维，还在降维的过程中将数据包含的噪音给消除了","这条曲线只是原来的点对应的y的预测值连接起来的曲线，不过有很多地方可能没有那个数据点，所以连接的结果和原来的曲线不一样","这样我们就将高斯函数从一个一维的数据映射成了二维的数据，这里，很显然我们可以通过一根直线来区分两种类别，原来在一维空间中线性不可分的空间，在二维空间中变的线性可分了","这样我们的模型达到最优以后，再讲测试数据集送给模型，这样才能作为衡量模型最终的性能。换句话说，我们的测试数据集是不参与模型的创建的，而其他两个数据集都参与了训练。但是我们的测试数据集对于模型是完全不可知的，相当于我们在模型这个模型完全不知道的数据","这样的一个招聘过程形成了一个树结构，这颗树的所有叶子结点其实就是我们最终的决策。也相对于是对与输入（录用者信息）的分类（录用/考察）。这样的一个过程，就是决策树。","这样，我们只要引入多项式项就好了。把x12和x22分别作为一个特征项。我们学习到的他们前面的系数都是1","这种情况下，很有可能存在欠拟合，所以我们要对这些参数进行比较精细的调整，让他既不过拟合也不欠拟合","这种情况，我们成为欠拟合","这种方法还会有一个问题。由于我们的模型可能会针对验证数据集过拟合，而我们只有一份验证数据集，一旦我们的数据集里有比较极端的情况，那么模型的性能就会下降很多，那么为了解决这个问题，就有了交叉验证。","这里使用了模拟退火的思想","这里分数不是很高。但是使用svr有很多超参数可以调节，比如c。如果我们使用svr，还可以对不同的kernal还有不同的参数调节。对于高斯kernal来说就需要对γ进行调节，对于polynomial来说有degree和c进行调节。","这里是二分类，所以只有一跟直线，放在了二维数组的第一个元素中","这里穿的alpha起始值比岭回归的时候大了很多，是由于现在是绝对值","进行均值归0操作以后，就是下面的式子","适用于分布有明显边界的情况；受outlier影响较大","适用于数据分布没有明显边界；有可能存在极端情况值","选择运算符","通常对于这样一个图，会有两根曲线：","通过上面推导可以得出","通过上面的推导，我们可以归纳出一类机器学习算法的基本思路，如下图；其中损失函数是计算期望值和预测值的差值，期望其差值（也就是损失）越来越小，而效用函数则是描述拟合度，期望契合度越来越好","通过传入average参数可以让precision_score处理多分类问题","通过使用岭回归，使得我们的均方误差小了非常多,曲线也缓和了非常多","通过曲线再次印证了，精准率和召回率是相互制约的，这个曲线急剧下降的一个点，可能就是精准率和召回率平衡最好的一个位置","通过构建新的y_predict，来实现修改threshold为5","通过求特征脸","通过绘制的结果，可以看出，绘制出的结果是非常不规则的，甚至在黄色的部分还掺杂着一些蓝色的绿色的点。","通过观察上面的图可以发现，如果新来一个数据点，落在了直线（决策边界）的上方，对应的x1θ1+x2θ2+θ>0，也就是p>0.5,那么我们就将他分类为1；反之，则分类为0。这就是决策曲线","通过观察两组调参过程的结果可以发现","通过这个方法我们可以取出至少有60张脸的人的数据","通过这样一个例子，再次印证了对于knn算法来说k越大，模型越简单，对于决策边界的划分就是决策越规整，分块越明显","通过这样一个矩阵，我们就可以很清晰的发现分类的错误，并且更重要的是，可以看出具体的错误类型，比如有很多的8我们把他规约为了1，有很多1我们规约成了8，有了这些提示，我们就可以进一步改进我们的算法了。","速度","逻辑回归中添加多项式项","逻辑回归只可以解决二分类问题。我们可以对逻辑回归稍加改造，让他解决多分类问题：","逻辑回归可以估算概率","逻辑回归，如果我们传进来的数据集有多个分类，他讲使用ovr的方式来解决多分类的问题","逻辑回归，实际上是在决策平面中找到一根直线，通过使用这根直线。用这条直线来分割所有样本的分类，用这样一个例子可以看到。为什么逻辑回归只能解决二分类问题，因为这根直线只能将我们的特征平面分成两部分。","逻辑回归：解决分类问题","遍历每一个维度","那么如何找到这个让样本间间距最大的轴?","那么怎么解决这个问题呢？","那么能够拉伸最长的特征向量，实际上就是这个矩阵的第一主成分；","采用这样的方式，我们就可以解决一些非线性的问题","重点看http://scikit","針対e1訓繚第二个模型m2,产生錯俣e2","针对e2训练第三个模型m3,产生错误e3..","针对特征进行随机采样","银行已经积累了一定的客户信息和他们信息卡的信用情况","降维的基本原理:找到另外一个坐标系，这个坐标系每一个轴依次可以表达原来的样本他们的重要程度，也就是称为所有的主成分，我们取得前k个最重要的主成分，就可以将所有的样本映射到这k个轴上，获得一个低维的数据信息","随机","随机整数","随机梯度下降法","随机梯度下降法介绍","随机梯度下降法实现","随机梯度下降法的优点","随机梯度下降法的封装和测试","随机森林","随机森林包含决策树的参数","随机浮点数","随机的检查了3分之一个样本总量的样本","随机获取36张脸","随着threshold逐渐降低，tpr","随着训练样本的主键增多，算法训练出的模型的表现能力","集成几个模型","集成多个模型","需要注意的是sklearn中的梯度下降法比我们自己的算法要复杂的多，性能和计算准确度上都比我们的要好，我们的算法只是用来演示过程，具体生产上的使用还是应该使用sklearn提供的","非参数学习通常都是高方差的算法。因为不对数据做任何假设","非常好的可解释性","非常接近一根直线","非监督学习","预测波士顿房价的测试","预测结果","领域知识","首先以二维坐标平面为例，一个点（o）的导数就是曲线在这个点的切线的斜率，在这个点两侧各取一个点（ab），那么ab两点对应的直线的斜率应该大体等于o的切线的斜率，并且这a和b的距离越近，那么两条直线的斜率就越接近","首先对高斯核函数进行一下改变，我们把y的值不取样本点，而取固定的的点，取两个固定的点分别叫l1，l2（landmark）。高斯核函数做的升维过程，就是对于每一个x的值，如果他有两个地标的话，就把他们升维成一个二维的样本点。","首先整体从趋势上，和线性回归的学习曲线是类似的","首先讨论二次方的多项式项核函数","首先需要取一个k值（这个k值的取法后面会介绍），然后找到距离要预测的病人的点（绿点）距离最近的k个点","高斯函数是将每一个样本点映射到了无穷维的特征空间，这背后的变形是非常复杂的，但是变形之后再进行点乘的结果却是非常简单的。这再次显示了核函数这个工具的威力，他不需要我们具体的计算出来这个样本点怎么映射成新的样本点，我们只需要关注最终的点乘运算结果就可以了","高斯函数的本质也是这样的。","高斯核函数也叫rbf核（radial","高斯核函数本质也应该是将原来的数据点映射成了新的特征向量，然后是这种新的特征向量点成的结果。","高维数据向低维数据进行映射","高，一定可以先向损失函数下降的方式前进","默认solver='liblinear',为了正确的调用ovo，缓存newton","默认使用基尼系数划分数据","默认支持多分类任务，而且默认使用ovr方式","默认的数据类型是整形","（one","（注意区别if","，也就是说想求出所有的主成分","，依然是在特征平面分成了两部分，但是对于这样的分布来说，是不可能使用一根直线来分割的。但是可以使用一个圆形。但是对于一个圆形，他的数学表达式是x12+x22","：将所有的数据映射到同一尺度"],"pipeline":["stopWordFilter","stemmer"]},"store":{"./":{"url":"./","title":"简介","keywords":"","body":"Python3入门机器学习-笔记整理\n本笔记记录笔者观看慕课网入门机器学习课程的笔记过程\n主要以sikit-learn和numpy为技术栈，学习了机器学习入门的基本算法，并自己实现了部分sikit-learn中提供的算法\n官方Github代码：https://github.com/liuyubobobo/Play-with-Machine-Learning-Algorithms\nscikit-learn官网：http://scikit-learn.org/stable/\n重点看http://scikit-learn.org/stable/documentation.html 中的User-Guide是对每一个算法的概述介绍。API是每一个算法的使用文档（也可以在首页大搜中搜索）\n主要以sikit-learn和numpy为技术栈，学习了机器学习入门的基本算法，并自己实现了部分sikit-learn中提供的算法\n\n以下列出本笔记（课程）学习使用到的sikit-learn算法\n\ndatasets可以用来加载真实数据进行模型训练的测试\n\n\nimport sklearn.datasets\ndatasets.load_iris() # 用于加载鸢尾花数据集\ndatasets.load_digits() # 用于加载手写识别的数据集\ndatasets.load_boston() #  用于加载波士顿房价的数据集\n\n# fetch_mldata用于加载MNIST数据集\nfrom sklearn.datasets import fetch_mldata\n\n# fetch_lfw_people用于加载人脸数据集\nfrom sklearn.datasets import fetch_lfw_people\n\nmodel_selection模块提供了模型选择的相关操作\n\n\n# train_test_split用于分割测试数据集和训练数据集\nfrom sklearn.model_selection import train_test_split\n\n# GridSearchCV用于进行参数搜索，寻找合适的超参数\nfrom sklearn.model_selection import GridSearchCV\n\nmetrics模块提供了数据之间的度量相关运算\n\n\n# MSE的实现\nfrom sklearn.metrics import mean_squared_error\n# MAE的实现\nfrom sklearn.metrics import mean_absolute_error\n# r2_score的实现\nfrom sklearn.metrics import r2_score\n# scikit-learn中的混淆矩阵，精准率和召回率,f1_score\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score,f1_score\n# scikit-learn中的Precision-Recall 曲线\nfrom sklearn.metrics import precision_recall_curve\n# scikit-learn中的ROC曲线\nfrom sklearn.metrics import roc_curve,roc_auc_score\n\nmulticlass模块提供了多分类问题的相关实现\n\n\n# OneVsRestClassifier是OvR的实现\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# OneVsOneClassifier是OvO的实现\nfrom sklearn.multiclass import OneVsOneClassifier\n\npreprocessing模块提供了数据预处理的相关操作\n\n\n# PolynomialFeatures进行多项式曾维处理，使用线性回归的方法解决非线性问题\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# StandardScaler提供数据归一化运算\nfrom sklearn.preprocessing import StandardScaler\n\nneighbors模块提供了近邻相关的算法实现\n\n\n# KNeighborsClassifier是KNN算法解决分类问题的实现\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# KNeighborsClassifier是KNN算法解决回归问题的实现\nfrom sklearn.neighbors import KNeighborsRegressor\n\nlinear_model提供了线性模型相关算法的实现\n\n\n# LinearRegression是线性回归算法的实现\nfrom sklearn.linear_model import LinearRegression\n\n# SGDRegressor是梯度下降法相关的实现\nfrom sklearn.linear_model import SGDRegressor\n\n# Ridge是岭回归的实现\nfrom sklearn.liner_model import Ridge\n\n# LogisticRegression是逻辑回归的实现，默认使用了l2正则化\nfrom sklearn.linear_model import LogisticRegression\n\ndecomposition提供了降维相关算法的实现\n\n\n# PCA给出了主成分分析法的相关实现\nfrom sklearn.decomposition import PCA\n\nsvm提供了支持向量机相关算法的实现\n\n\n# SVC--Support Vector Classifier 使用支持向量机思想解决分类问题\nfrom sklearn.svm import LinearSVC\n\n# 使用核函数的支撑向量机\nfrom sklearn.svm import SVC\n\n# SCR--Support Vector Regression 使用支持向量机思想解决回归问题\nfrom sklearn.svm import LinearSVR \n\n# 使用核函数的解决回归问题的支撑向量机\nfrom sklearn.svm import SVR\n"},"ji-qi-xue-xi-ji-ben-gai-nian.html":{"url":"ji-qi-xue-xi-ji-ben-gai-nian.html","title":"1.机器学习基本概念","keywords":"","body":""},"chapter1/you-guan-shu-ju-de-yi-xie-zhu-yu.html":{"url":"chapter1/you-guan-shu-ju-de-yi-xie-zhu-yu.html","title":"有关数据的一些术语","keywords":"","body":"\n\n\n\n"},"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html":{"url":"chapter1/ji-qi-xue-xi-zhong-jian-du-xue-xi-de-ji-ben-ren-wu.html","title":"机器学习中监督学习的基本任务","keywords":"","body":"\n分类任务\n将给定的数据进行分类，比如区分猫和狗\n\n二分类任务\n\n判断邮件是垃圾邮件；不是垃圾邮件\n判断发放给客户信用卡有风险；没有风险\n判断病患良性肿瘤；恶性肿瘤\n判断某支股票涨；跌\n\n\n多分类任务\n\n数字识别\n图像识别\n判断发放给客户信用卡的风险评级\n\n\n\n\n\n一些算法只支持完成二分类的任务\n但是多分类的任务可以转换成二分类的任务\n有一些算法天然可以完成多分类任务\n\n\n\n回归任务\n\n\n结果是一个连续数字的值，而非一个类别\n\n房屋价格\n市场分析\n学生成绩\n股票价格\n\n\n\n有一些算法只能解决回归问题\n有一些算法只能解决分类问题\n有一些算法既能解决回归问题，也能解决分类问题\n一些情况下，回归任务可以简化成分类任务，比如学生的具体成绩预测转换成评级，无人车驾驶，转换成油门，刹车，方向盘的程度\n\n\n\n监督学习\n给机器的训练数据拥有“标记”或者“答案”，人类已经给机器对数据进行了正确答案的划分，这个答案的划分本身就是监督的信息\n图像已经拥有了标记信息\n银行已经积累了一定的客户信息和他们信息卡的信用情况\n医院已经积累了一定的病人信息和他们最终确诊是否患病的情况\n市场积累了房屋的基本信息和最终成交的金额\n\n\n非监督学习\n给机器的训练数据没有任何的“答案”和“标记”\n对没有“标记”的数据进行分类-聚类分析\n对数据进行降维处理\n特征提取：信用卡的信用评级和人的胖瘦无关\n特征压缩：PCA\n方便可视化\n\n\n\n异常检测\n\n半监督学习\n一部分数据有“标记”或者“答案”，另一部分数据没有\n更常见：各种原因产生的标记缺失\n增强学习\n根据周围环境的情况，采取行动，根据采取行动的结果，学习行动方式\n\n无人驾驶\n机器人\n\n监督学习和半监督学习是基础\n\n"},"jupyter-notebookyu-numpy-de-shi-yong.html":{"url":"jupyter-notebookyu-numpy-de-shi-yong.html","title":"2.Jupyter Notebook与Numpy的使用","keywords":"","body":""},"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html":{"url":"jupyter-notebookyu-numpy-de-shi-yong/jupyter-notebookyu-numpy-de-shi-yong.html","title":"Jupyter Notebook与Numpy的使用","keywords":"","body":"1.%run\n\n%run 执行python脚本，并将脚本中的函数加载\n%run ./hello.py\n\n可以直接使用import命令导入本机目录下的包\nimport mymodule.FirstML\n\n\n2.%timeit\n\n%timeit 测试代码的性能\n\n测试结果表明，运行了一千次，取有价值的7次，平均每次耗时324+/-5.7 μs（有多少次循环是由Jupyter Notebook自动决定的）\n%%timeit 测试整个代码块\n\n\n3.%time\n\n使用%time让测试只执行一次\n\n本次测试时间比上面的测试时间会多，是因为只测试了一次。可能不够准确\n\n一个陷阱\n使用%timeit 测试多次在每次测试的执行性能不一样的时候测试结果会不准确。\n考虑用%timeit 测试一个排序算法，由于第一次执行完毕后数组已经排好序，那么在后面执行的时候，如果使用插入排序等算法就会导致后面999次的时间非常短，导致测试值不准确\n\n\n\n4.其他\n\n使用%lsmagic查看所有的魔法命令\n\n在使用方法后面加?查看文档\n\n\n"},"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html":{"url":"jupyter-notebookyu-numpy-de-shi-yong/numpy-shu-ju-ji-chu.html","title":"Numpy 数据基础","keywords":"","body":"1.加载numpy与查看版本\n\n2.Python List 的特点\n\n由于Python List的元素可以存任何类型，在灵活度提升的同时，也导致性能下降了\n\n\n一直替代方法是是使用array可以在构造数组的时候限定类型，但是由于array只是把元素当成一个一维或者多维数组，而并没有当做矩阵，向量，所以也没有提供相应的方法函数，使得在机器学习中非常的不方便\n\n\n\n3.numpy.array\n\n使用numpy创建数组(和python的array中几乎一样)\n\n查看数组元素类型\n\n\n"},"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html":{"url":"jupyter-notebookyu-numpy-de-shi-yong/qi-ta-chuang-jian-numpy-array-de-fang-fa.html","title":"其他创建numpy.array 的方法","keywords":"","body":"1.创建“0”数组\n\n创建一维0数组\n\n创建N维0数组,第一个参数shape是数组维度，第二个参数是类型\n\n\n2.创建全\"1\"矩阵和创建全\"N\"矩阵\n\n注意full 默认的数据类型是整形\n3.生成等步长数组\n\narange\n\n\nlinspace\n\n\n\n4.random\n\n随机整数\n\n\n随机浮点数\n\n\n\n"},"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html":{"url":"jupyter-notebookyu-numpy-de-shi-yong/numpyarray-de-ji-ben-cao-zuo.html","title":"Numpy.array 的基本操作","keywords":"","body":"\n1.基本属性\n\n2. 访问数组\n\n\n\n\n3.reshape 改变维度\n\n\n"},"knnsuan-fa-de-xue-xi-yu-shi-yong.html":{"url":"knnsuan-fa-de-xue-xi-yu-shi-yong.html","title":"3.kNN算法的学习与使用","keywords":"","body":""},"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html":{"url":"knnsuan-fa-de-xue-xi-yu-shi-yong/1knnsuan-fa-de-yuan-li-jie-shao.html","title":"1.KNN算法的原理介绍","keywords":"","body":"\n优点\n\n缺点\n\n\n\n\n\n原理案例介绍\n\n\n假设现在设计一个程序判断一个新的肿瘤病人是良性肿瘤还是恶性肿瘤\n先基于原有的肿瘤病人的发现时间和肿瘤大小（特征）对应的良性/恶性（值）建立了一张散点图，横坐标是肿瘤大小，纵坐标是发现时间，红色代表良性，蓝色代表恶性，现在要预测的病人的颜色为绿色\n\n首先需要取一个k值（这个k值的取法后面会介绍），然后找到距离要预测的病人的点（绿点）距离最近的k个点\n然后用第一步中取到的三个点进行投票，比如本例中投票结果就是蓝：红 = 3：0 ,3>0,所以判断这个新病人幻的事恶性肿瘤\n\n本质\n如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。\n\n\n"},"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html":{"url":"knnsuan-fa-de-xue-xi-yu-shi-yong/2knnsuan-fa-de-yi-ge-jian-dan-shi-xian.html","title":"2.KNN算法的一个简单实现","keywords":"","body":"import numpy as np\nimport matplotlib.pyplot as plt\n\n原始集合\n# 特征\nraw_data_x= [[3.393533211,2.331273381],\n             [2.110073483,1.781539638],\n             [1.343808831,3.368360954],\n             [3.582294042,4.679179110],\n             [2.280362439,2.866990263],\n             [7.423436942,4.696522875],\n             [5.745051997,3.533989803],\n             [9.172168622,2.511101045],\n             [7.792783481,3.424088941],\n             [7.939820817,0.791637231]\n            ]\n# 所述类别\nraw_data_y = [0,0,0,0,0,1,1,1,1,1]\n训练集合\nX_train = np.array(raw_data_x)\ny_train = np.array(raw_data_y)\n# 要预测的点\nx = np.array([8.093607318,3.365731514])\n\n绘制数据集及要预测的点\nplt.scatter(X_train[y_train==0,0],X_train[y_train==0,1],color='g')\nplt.scatter(X_train[y_train==1,0],X_train[y_train==1,1],color='r')\nplt.scatter(x[0],x[1],color='b')\n\n\n\nKNN 实现过程简单编码\nfrom math import sqrt\ndistances = []\nfor x_train in X_train:\n    # 欧拉 \n    # **2 求平方\n    d = sqrt(np.sum((x_train - x)**2))\n    distances.append(d)\ndistances\n\n[4.812566907609877,\n 6.189696362066091,\n 6.749798999160064,\n 4.6986266144110695,\n 5.83460014556857,\n 1.4900114024329525,\n 2.354574897431513,\n 1.3761132675144652,\n 0.3064319992975,\n 2.5786840957478887]\n# 生成表达式\ndistances = [sqrt(np.sum((x_train - x)**2)) for x_train in X_train]\ndistances\n\n[4.812566907609877,\n 6.189696362066091,\n 6.749798999160064,\n 4.6986266144110695,\n 5.83460014556857,\n 1.4900114024329525,\n 2.354574897431513,\n 1.3761132675144652,\n 0.3064319992975,\n 2.5786840957478887]\n# 返回排序后的结果的索引,也就是距离测试点距离最近的点的排序坐标数组\nnearset = np.argsort(distances)\n\nk = 6\n\n投票\n# 求出距离测试点最近的6个点的类别\ntopK_y = [y_train[i] for i in nearset[:k]]\ntopK_y\n\n[1, 1, 1, 1, 1, 0]\n# collections的Counter方法可以求出一个数组的相同元素的个数，返回一个dict【key=元素名，value=元素个数】\nfrom collections import Counter\nCounter(topK_y)\n\nCounter({0: 1, 1: 5})\n# most_common方法求出最多的元素对应的那个键值对\nvotes = Counter(topK_y)\nvotes.most_common(1)\n\n[(1, 5)]\n votes.most_common(1)[0][0]\n\n1\npredict_y = votes.most_common(1)[0][0]\npredict_y\n\n1\n\nKNN算法的封装\nimport numpy as np\nfrom math import sqrt\nfrom collections import Counter\nclass KNNClassifier:\n\n    def __init__(self,k):\n        \"\"\"初始化kNN分类器\"\"\"\n        assert k >= 1, \"k must be valid\"\n        self.k = k\n        self._X_train = None\n        self._y_train = None\n\n    def fit(self, X_train, y_train):\n        \"\"\"根据训练数据集X_train和y_train训练kNN分类器\"\"\"\n        assert X_train.shape[0] == y_train.shape[0], \\\n            \"the size of X_train must equal to the size of y_train\"\n        assert self.k 再看机器学习\n\n  可以说kNN是一个不需要训练过程的算法\n  k近邻算法是非常特殊的，可以被认为是没有模型的算法\n  为了和其他算法统一，可以认为训练数据集就是模型\n"},"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html":{"url":"knnsuan-fa-de-xue-xi-yu-shi-yong/3pan-duan-ji-qi-xue-xi-suan-fa-de-xing-neng.html","title":"3.判断机器学习算法的性能","keywords":"","body":"\ntrain test split\n\n封装我们自己的 train test split\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n# 加载鸢尾花数据集\niris = datasets.load_iris()\n\nX = iris.data\ny = iris.target\n\nx.shape\n\n(150, 4)\ny.shape\n\n(150,)\ntrain_test_spilt\n# permutation(n) 给出从0到n-1的一个随机排列\nshuffle_indexes = np.random.permutation(len(X))\n\nshuffle_indexes\n\narray([139,  40,  63, 138,  88, 123, 101, 122,  89,   0, 132, 108, 120,\n       111, 140,  30,  47,   6, 128,  46,  49, 105,   3,  53,  85,   9,\n       147,  95, 116,  75,  20, 134,  34,  42, 144,   7,  10,  73,  90,\n        72, 141,  99,  57,  93,  74, 103,  39, 106,  86,  35,  15,  96,\n        78, 129,  19,  51, 117,  62, 113,  77, 100, 118,  83,  18,  70,\n        94,  26,  25,  12,  50,  28, 133, 145,  43,  33, 109,  44, 114,\n        92, 112,  82, 119, 115,  69,  27,  80,  41,  38,  98,  97,  61,\n        16,  56,  11,  64, 135,   1, 126, 137,  45,  32,  60, 124,  71,\n        58,  52,  84,  21,  81,  13, 142, 127,  55,  79,  14,  68, 146,\n        48,  23,  76,  17,   8, 136, 110,  87,   2, 143, 104,  24,  37,\n       107,  31,   4, 131,  66, 121, 149, 102,   5,  65,  54, 148,  59,\n       125,  29,  67,  36,  91, 130,  22])\n# 测试数据集的比例\ntest_ratio = 0.2\n# 获取测试数据集\ntets_size = int(len(X) * test_ratio)\ntets_size\n\n30\ntest_indexes = shuffle_indexes[:tets_size]\ntrain_indexes = shuffle_indexes[tets_size:]\n\nX_train = X[train_indexes]\ny_train = y[train_indexes]\n\nX_test = X[test_indexes]\ny_test = y[test_indexes]\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n\n(113, 4)\n(113,)\n(37, 4)\n(37,)\n使用我们自己封装的测试分割函数分割训练集\nimport numpy as np\n\n\ndef train_test_split(X, y, test_radio=0.2, seed=None):\n    \"\"\"将数据X和y按照test_radio分割成X_train,y_train,X_test,y_test\"\"\"\n    assert X.shape[0] == y.shape[0],\\\n        \"the size of X must be equal to the size of y\"\n    assert 0.0 import machine_learning\nfrom machine_learning.module_selection import train_test_split\n\nX_train,y_train,X_test,y_test = train_test_split(X,y,test_radio=0.25)\n\n测试我们的KNN算法\nfrom machine_learning.KNN import KNNClassifier\n\nmy_knn_clf = KNNClassifier(k=6)\n\nmy_knn_clf.fit(X_train,y_train)\n\n\n# 预测结果\ny_predict = my_knn_clf.predict(X_test)\n\ny_predict\n\narray([2, 2, 2, 1, 0, 0, 2, 2, 2, 1, 1, 0, 1, 1, 2, 2, 2, 2, 0, 0, 1, 2,\n       0, 2, 0, 2, 1, 1, 2, 1, 1, 1, 2, 0, 1, 2, 2, 2])\ny_test\n\narray([2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 0, 0, 1, 2,\n       0, 2, 0, 2, 1, 1, 2, 1, 1, 1, 2, 0, 1, 2, 1, 2])\n# 求出准确率\nsum(y_predict==y_test)/len(y_test)\n\n0.9473684210526315\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n\n(112, 4)\n(112,)\n(38, 4)\n(38,)\nfrom sklearn.neighbors import KNeighborsClassifier\nsklearn_knn_clf = KNeighborsClassifier(n_neighbors=6)\n\nsklearn_knn_clf.fit(X_train,y_train)\n\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=6, p=2,\n           weights='uniform')\ny_predict = sklearn_knn_clf.predict(X_test)\n\ny_predict\n\narray([2, 2, 2, 1, 0, 0, 2, 2, 2, 1, 1, 0, 1, 1, 2, 2, 2, 2, 0, 0, 1, 2,\n       0, 2, 0, 2, 1, 1, 2, 1, 1, 1, 2, 0, 1, 2, 2, 2])\ny_test\n\narray([2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 0, 0, 1, 2,\n       0, 2, 0, 2, 1, 1, 2, 1, 1, 1, 2, 0, 1, 2, 1, 2])\nsum(y_predict==y_test)/len(y_test)\n\n0.9473684210526315\n"},"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html":{"url":"knnsuan-fa-de-xue-xi-yu-shi-yong/4. 超参数和模型参数.html","title":"4. 超参数和模型参数","keywords":"","body":"\n\n寻找好的超参数\n\n领域知识\n经验数值\n实验搜索\n\n\n\n寻找最好的k\n# 思路，遍历1-11，分别拿每一个k去调用算法，得出分数，取得分最高的那个k\nbest_score = 0.0\nbest_k = -1\nfor k in range(1,11):\n    knn_clf = KNeighborsClassifier(n_neighbors=k)\n    knn_clf.fit(X_train,y_train)\n    score = knn_clf.score(X_test,y_test)\n    if score > best_score:\n        best_k = k\n        best_score = score\n# 如果k=10，则有必要对10以上的数字进行搜索\nprint(\"best_k=\",best_k)\nprint(\"best_score=0.0.\",best_score)\n\nkNN的另外一个超参数：距离的权重\n一般情况下使用距离的导数作为权证\n\n\n考虑距离？不考虑距离\nbest_method = \"\"\nbest_score = 0.0\nbest_k = -1\nfor method in [\"uniform\",\"distance\"]:\n    for k in range(1,11):\n        knn_clf = KNeighborsClassifier(n_neighbors=k,weights=method)\n        knn_clf.fit(X_train,y_train)\n        score = knn_clf.score(X_test,y_test)\n        if score > best_score:\n            best_k = k\n            best_score = score\n            best_method = method\nprint(\"best_k=\",best_k)\nprint(\"best_score=\",best_score)\nprint(\"best_method=\",best_method)\n\n什么是距离\n\n欧拉距离\n\n\n\n曼哈顿距离\n\n两种距离的整理对比\n\n明克夫斯基距离\n\n\n到这里，我们获得了一个新的超参数 p\n搜索明可夫斯基距离相应的p\nbest_p = -1\nbest_score = 0.0\nbest_k = -1\nfor k in range(1,11):\n    for p in range(1,6):\n        knn_clf = KNeighborsClassifier(n_neighbors=k,weights='distance',p=p)\n        knn_clf.fit(X_train,y_train)\n        score = knn_clf.score(X_test,y_test)\n        if score > best_score:\n            best_k = k\n            best_score = score\n            best_p = p\nprint(\"best_p=\",best_p)\nprint(\"best_k=\",best_k)\nprint(\"best_score=\",best_score)\n网格搜索\nGrid Search\n# array>\nparam_grid =[\n    {\n        'weights':['uniform'],\n        'n_neighbors': [i for i in range(1,11)]\n    },\n    {\n        'weights':['distance'],\n        'n_neighbors': [i for i in range(1,11)],\n        'p': [i for i in range(1,6)]\n    }\n]\n\n# 先new一个默认的Classifier对象\nknn_clf = KNeighborsClassifier()\n\n# 调用GridSearchCV创建网格搜索对象，传入参数为Classifier对象以及参数列表\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_search = GridSearchCV(knn_clf,param_grid)\n\n# 调用fit方法执行网格搜索\n%%time\ngrid_search.fit(X_train,y_train)\n\nGridSearchCV(cv=None, error_score='raise',\n       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n           weights='uniform'),\n       fit_params=None, iid=True, n_jobs=1,\n       param_grid=[{'weights': ['uniform'], 'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, {'weights': ['distance'], 'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'p': [1, 2, 3, 4, 5]}],\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=0)\n# 不是用户传入的参数，而是根据用户传入的参数计算出来的结果，以_结尾\n# 最好的评估结果，返回的是KNeighborsClassifier对象\ngrid_search.best_estimator_\n\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=3, p=3,\n           weights='distance')\n# 最好的分数\ngrid_search.best_score_\n\n0.9853862212943633\n# 最好的参数\ngrid_search.best_params_\n\n{'n_neighbors': 3, 'p': 3, 'weights': 'distance'}\nknn_clf = grid_search.best_estimator_\n\nknn_clf.score(X_test,y_test)\n\n0.9833333333333333\n%%time\n# n_jobs 多线程并行处理，占用几个核，-1为使用所有的核\n# verbose 是否打印搜索信息,传入值越大，输出信息越详细\ngrid_search = GridSearchCV(knn_clf,param_grid,n_jobs=-1,verbose=2)\ngrid_search.fit(X_train,y_train)\n\n更多的距离定义\n\n\n"},"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html":{"url":"knnsuan-fa-de-xue-xi-yu-shi-yong/5.数据归一化.html","title":"5.数据归一化","keywords":"","body":"样本间的距离被一个字段所主导\n\n\n解决方案 ：将所有的数据映射到同一尺度\n\n最值归一化 normalization：把所有数据映射到0-1之间\n\n1.将这个数据映射到0~Xmax-Xmin 之间\n2.然后对于每个x相比于整个范围所占的比例\n\n适用于分布有明显边界的情况；受outlier影响较大\n\n均值方差归一化 standardization\n把所有数据归一到均值为0方差为1的分布中\n\n适用于数据分布没有明显边界；有可能存在极端情况值\n\n最值归一化 normalization\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# 生成一个一维向量进行归一化\nx = np.random.randint(0,100,size=100)\n\nx\n\narray([95,  6, 47, 89, 87, 86, 72, 46, 45, 42, 44, 68, 89, 28, 99, 10, 58,\n       32, 96, 85, 69, 20, 84, 89,  6, 99, 74, 54,  6,  8, 66, 64, 52,  0,\n        7, 55, 35, 20, 33, 28, 40, 92, 70, 49, 21, 16, 68, 76, 91, 68, 48,\n       52, 19, 83, 34, 80, 15, 20, 60, 39, 56, 37, 27, 32, 12, 21, 54, 85,\n       54, 43, 20, 86, 95, 81,  0, 18, 63, 40, 40, 70, 53, 77, 57, 64, 70,\n       33,  9, 86, 72, 35, 97, 67, 55, 73, 99, 85, 94, 59, 80, 55])\n[(x-np.min(x))/np.max(x)-np.min(x)]\n\n[array([0.95959596, 0.06060606, 0.47474747, 0.8989899 , 0.87878788,\n        0.86868687, 0.72727273, 0.46464646, 0.45454545, 0.42424242,\n        0.44444444, 0.68686869, 0.8989899 , 0.28282828, 1.        ,\n        0.1010101 , 0.58585859, 0.32323232, 0.96969697, 0.85858586,\n        0.6969697 , 0.2020202 , 0.84848485, 0.8989899 , 0.06060606,\n        1.        , 0.74747475, 0.54545455, 0.06060606, 0.08080808,\n        0.66666667, 0.64646465, 0.52525253, 0.        , 0.07070707,\n        0.55555556, 0.35353535, 0.2020202 , 0.33333333, 0.28282828,\n        0.4040404 , 0.92929293, 0.70707071, 0.49494949, 0.21212121,\n        0.16161616, 0.68686869, 0.76767677, 0.91919192, 0.68686869,\n        0.48484848, 0.52525253, 0.19191919, 0.83838384, 0.34343434,\n        0.80808081, 0.15151515, 0.2020202 , 0.60606061, 0.39393939,\n        0.56565657, 0.37373737, 0.27272727, 0.32323232, 0.12121212,\n        0.21212121, 0.54545455, 0.85858586, 0.54545455, 0.43434343,\n        0.2020202 , 0.86868687, 0.95959596, 0.81818182, 0.        ,\n        0.18181818, 0.63636364, 0.4040404 , 0.4040404 , 0.70707071,\n        0.53535354, 0.77777778, 0.57575758, 0.64646465, 0.70707071,\n        0.33333333, 0.09090909, 0.86868687, 0.72727273, 0.35353535,\n        0.97979798, 0.67676768, 0.55555556, 0.73737374, 1.        ,\n        0.85858586, 0.94949495, 0.5959596 , 0.80808081, 0.55555556])]\n# 生成一个二维矩阵进行归一化\nX = np.random.randint(0,100,(50,2))\nX[:10,:]\n\narray([[52,  2],\n       [25, 93],\n       [73, 31],\n       [39, 48],\n       [15, 57],\n       [33, 42],\n       [27, 15],\n       [49, 48],\n       [ 6, 62],\n       [98, 82]])\nX = np.array(X,dtype=float)\n\nX[:10,:]\n\narray([[52.,  2.],\n       [25., 93.],\n       [73., 31.],\n       [39., 48.],\n       [15., 57.],\n       [33., 42.],\n       [27., 15.],\n       [49., 48.],\n       [ 6., 62.],\n       [98., 82.]])\nX[:,0] = (X[:,0]-np.min(X[:,0]))/(np.max(X[:,0])-np.min(X[:,0]))\n\nX[:,1] = ((X[:,1]-np.min(X[:,1]))/(np.max(X[:,1])-np.min(X[:,1])))\n\nX[:10,:]\n\narray([[0.52525253, 0.02020202],\n       [0.25252525, 0.93939394],\n       [0.73737374, 0.31313131],\n       [0.39393939, 0.48484848],\n       [0.15151515, 0.57575758],\n       [0.33333333, 0.42424242],\n       [0.27272727, 0.15151515],\n       [0.49494949, 0.48484848],\n       [0.06060606, 0.62626263],\n       [0.98989899, 0.82828283]])\n# 均值，可以看出现在的数据集是均匀分布的\nnp.mean(X[:,0])\n\n0.46848484848484845\n# 方差\nnp.std(X[:,0])\n\n0.3156554505030807\nnp.mean(X[:,1])\n\n0.4917171717171717\nnp.std(X[:,1])\n\n0.2805277286657274\n均值方差归一化 Standardization\nX2 = np.random.randint(0,100,(50,2))\n\nX2 = np.array(X2,dtype=float)\n\nX2[:,0] = (X2[:,0]-np.mean(X2[:,0]))/np.std(X2[:,0])\n\nX2[:,1] = (X2[:,1]-np.mean(X2[:,1]))/np.std(X2[:,1])\n\nplt.scatter(X2[:,0],X2[:,1])\n\n\n\nnp.mean(X2[:,0])\n\n3.1086244689504386e-17\nnp.std(X2[:,0])\n\n1.0\nnp.mean(X2[:,1])\n\n1.7763568394002505e-17\nnp.std(X2[:,1])\n\n1.0\n对测试数据集如何归一化？\n\n在scikit-learn中使用Scaler\n\nScikit-learn 中的Scaler\nimport numpy as np\nfrom sklearn import datasets\n\niris = datasets.load_iris()\n\nX = iris.data\ny = iris.target\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=666)\n\nscikit-learn 中的StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\nstandardScaler = StandardScaler()\n\n# 存放了均值方差归一化所对应的信息\nstandardScaler.fit(X_train)\n\nStandardScaler(copy=True, with_mean=True, with_std=True)\n## 均值\nstandardScaler.mean_\n\narray([5.83416667, 3.0825    , 3.70916667, 1.16916667])\n## 描述数据的分布范围（标准差）\nstandardScaler.scale_\n\narray([0.81019502, 0.44076874, 1.76295187, 0.75429833])\nX_train = standardScaler.transform(X_train)\nX_train\n\narray([[-0.90616043,  0.94720873, -1.30982967, -1.28485856],\n       [-1.15301457, -0.18717298, -1.30982967, -1.28485856],\n       [-0.16559799, -0.64092567,  0.22169257,  0.17345038],\n       [ 0.45153738,  0.72033239,  0.95909217,  1.49918578],\n       [-0.90616043, -1.3215547 , -0.40226093, -0.0916967 ],\n       [ 1.43895396,  0.2665797 ,  0.56203085,  0.30602392],\n       [ 0.3281103 , -1.09467835,  1.07253826,  0.30602392],\n       [ 2.1795164 , -0.18717298,  1.63976872,  1.2340387 ],\n       [-0.78273335,  2.30846679, -1.25310662, -1.4174321 ],\n       [ 0.45153738, -2.00218372,  0.44858475,  0.43859746],\n       [ 1.80923518, -0.41404933,  1.46959958,  0.83631808],\n       [ 0.69839152,  0.2665797 ,  0.90236912,  1.49918578],\n       [ 0.20468323,  0.72033239,  0.44858475,  0.571171  ],\n       [-0.78273335, -0.86780201,  0.10824648,  0.30602392],\n       [-0.53587921,  1.40096142, -1.25310662, -1.28485856],\n       [-0.65930628,  1.40096142, -1.25310662, -1.28485856],\n       [-1.0295875 ,  0.94720873, -1.19638358, -0.7545644 ],\n       [-1.77014994, -0.41404933, -1.30982967, -1.28485856],\n       [-0.04217092, -0.86780201,  0.10824648,  0.04087684],\n       [-0.78273335,  0.72033239, -1.30982967, -1.28485856],\n       [-1.52329579,  0.72033239, -1.30982967, -1.15228502],\n       [ 0.82181859,  0.2665797 ,  0.78892303,  1.10146516],\n       [-0.16559799, -0.41404933,  0.27841562,  0.17345038],\n       [ 0.94524567, -0.18717298,  0.39186171,  0.30602392],\n       [ 0.20468323, -0.41404933,  0.44858475,  0.43859746],\n       [-1.39986872,  0.2665797 , -1.19638358, -1.28485856],\n       [-1.15301457,  0.03970336, -1.25310662, -1.4174321 ],\n       [ 1.06867274,  0.03970336,  1.07253826,  1.63175932],\n       [ 0.57496445, -0.86780201,  0.67547694,  0.83631808],\n       [ 0.3281103 , -0.64092567,  0.56203085,  0.04087684],\n       [ 0.45153738, -0.64092567,  0.61875389,  0.83631808],\n       [-0.16559799,  2.98909581, -1.25310662, -1.01971148],\n       [ 0.57496445, -1.3215547 ,  0.67547694,  0.43859746],\n       [ 0.69839152, -0.41404933,  0.33513866,  0.17345038],\n       [-0.90616043,  1.62783776, -1.02621444, -1.01971148],\n       [ 1.19209981, -0.64092567,  0.61875389,  0.30602392],\n       [-0.90616043,  0.94720873, -1.30982967, -1.15228502],\n       [-1.89357701, -0.18717298, -1.47999881, -1.4174321 ],\n       [ 0.08125616, -0.18717298,  0.78892303,  0.83631808],\n       [ 0.69839152, -0.64092567,  1.07253826,  1.2340387 ],\n       [-0.28902506, -0.64092567,  0.67547694,  1.10146516],\n       [-0.41245214, -1.54843104, -0.00519961, -0.22427024],\n       [ 1.31552689,  0.03970336,  0.67547694,  0.43859746],\n       [ 0.57496445,  0.72033239,  1.07253826,  1.63175932],\n       [ 0.82181859, -0.18717298,  1.18598435,  1.36661224],\n       [-0.16559799,  1.62783776, -1.13966053, -1.15228502],\n       [ 0.94524567, -0.41404933,  0.5053078 ,  0.17345038],\n       [ 1.06867274,  0.49345605,  1.12926131,  1.76433286],\n       [-1.27644165, -0.18717298, -1.30982967, -1.4174321 ],\n       [-1.0295875 ,  1.17408507, -1.30982967, -1.28485856],\n       [ 0.20468323, -0.18717298,  0.61875389,  0.83631808],\n       [-1.0295875 , -0.18717298, -1.19638358, -1.28485856],\n       [ 0.3281103 , -0.18717298,  0.67547694,  0.83631808],\n       [ 0.69839152,  0.03970336,  1.01581521,  0.83631808],\n       [-0.90616043,  1.40096142, -1.25310662, -1.01971148],\n       [-0.16559799, -0.18717298,  0.27841562,  0.04087684],\n       [-1.0295875 ,  0.94720873, -1.36655271, -1.15228502],\n       [-0.90616043,  1.62783776, -1.25310662, -1.15228502],\n       [-1.52329579,  0.2665797 , -1.30982967, -1.28485856],\n       [-0.53587921, -0.18717298,  0.44858475,  0.43859746],\n       [ 0.82181859, -0.64092567,  0.5053078 ,  0.43859746],\n       [ 0.3281103 , -0.64092567,  0.16496953,  0.17345038],\n       [-1.27644165,  0.72033239, -1.19638358, -1.28485856],\n       [-0.90616043,  0.49345605, -1.13966053, -0.88713794],\n       [-0.04217092, -0.86780201,  0.78892303,  0.96889162],\n       [-0.28902506, -0.18717298,  0.22169257,  0.17345038],\n       [ 0.57496445, -0.64092567,  0.78892303,  0.43859746],\n       [ 1.06867274,  0.49345605,  1.12926131,  1.2340387 ],\n       [ 1.68580811, -0.18717298,  1.18598435,  0.571171  ],\n       [ 1.06867274, -0.18717298,  0.84564608,  1.49918578],\n       [-1.15301457,  0.03970336, -1.25310662, -1.4174321 ],\n       [-1.15301457, -1.3215547 ,  0.44858475,  0.70374454],\n       [-0.16559799, -1.3215547 ,  0.73219998,  1.10146516],\n       [-1.15301457, -1.54843104, -0.2320918 , -0.22427024],\n       [-0.41245214, -1.54843104,  0.05152343, -0.0916967 ],\n       [ 1.06867274, -1.3215547 ,  1.18598435,  0.83631808],\n       [ 0.82181859, -0.18717298,  1.01581521,  0.83631808],\n       [-0.16559799, -1.09467835, -0.1186457 , -0.22427024],\n       [ 0.20468323, -2.00218372,  0.73219998,  0.43859746],\n       [ 1.06867274,  0.03970336,  0.56203085,  0.43859746],\n       [-1.15301457,  0.03970336, -1.25310662, -1.4174321 ],\n       [ 0.57496445, -1.3215547 ,  0.73219998,  0.96889162],\n       [-1.39986872,  0.2665797 , -1.36655271, -1.28485856],\n       [ 0.20468323, -0.86780201,  0.78892303,  0.571171  ],\n       [-0.04217092, -1.09467835,  0.16496953,  0.04087684],\n       [ 1.31552689,  0.2665797 ,  1.12926131,  1.49918578],\n       [-1.77014994, -0.18717298, -1.36655271, -1.28485856],\n       [ 1.56238103, -0.18717298,  1.2427074 ,  1.2340387 ],\n       [ 1.19209981,  0.2665797 ,  1.2427074 ,  1.49918578],\n       [-0.78273335,  0.94720873, -1.25310662, -1.28485856],\n       [ 2.54979762,  1.62783776,  1.52632263,  1.10146516],\n       [ 0.69839152, -0.64092567,  1.07253826,  1.36661224],\n       [-0.28902506, -0.41404933, -0.06192266,  0.17345038],\n       [-0.41245214,  2.53534313, -1.30982967, -1.28485856],\n       [-1.27644165, -0.18717298, -1.30982967, -1.15228502],\n       [ 0.57496445, -0.41404933,  1.07253826,  0.83631808],\n       [-1.77014994,  0.2665797 , -1.36655271, -1.28485856],\n       [-0.53587921,  1.8547141 , -1.13966053, -1.01971148],\n       [-1.0295875 ,  0.72033239, -1.19638358, -1.01971148],\n       [ 1.06867274, -0.18717298,  0.73219998,  0.70374454],\n       [-0.53587921,  1.8547141 , -1.36655271, -1.01971148],\n       [ 2.30294347, -0.64092567,  1.69649176,  1.10146516],\n       [-0.28902506, -0.86780201,  0.27841562,  0.17345038],\n       [ 1.19209981, -0.18717298,  1.01581521,  1.2340387 ],\n       [-0.41245214,  0.94720873, -1.36655271, -1.28485856],\n       [-1.27644165,  0.72033239, -1.02621444, -1.28485856],\n       [-0.53587921,  0.72033239, -1.13966053, -1.28485856],\n       [ 2.30294347,  1.62783776,  1.69649176,  1.36661224],\n       [ 1.31552689,  0.03970336,  0.95909217,  1.2340387 ],\n       [-0.28902506, -1.3215547 ,  0.10824648, -0.0916967 ],\n       [-0.90616043,  0.72033239, -1.25310662, -1.28485856],\n       [-0.90616043,  1.62783776, -1.19638358, -1.28485856],\n       [ 0.3281103 , -0.41404933,  0.56203085,  0.30602392],\n       [-0.04217092,  2.08159044, -1.42327576, -1.28485856],\n       [-1.0295875 , -2.45593641, -0.1186457 , -0.22427024],\n       [ 0.69839152,  0.2665797 ,  0.44858475,  0.43859746],\n       [ 0.3281103 , -0.18717298,  0.5053078 ,  0.30602392],\n       [ 0.08125616,  0.2665797 ,  0.61875389,  0.83631808],\n       [ 0.20468323, -2.00218372,  0.16496953, -0.22427024],\n       [ 1.93266225, -0.64092567,  1.35615349,  0.96889162]])\nX_test = standardScaler.transform(X_test)\nX_test\n\narray([[-0.28902506, -0.18717298,  0.44858475,  0.43859746],\n       [-0.04217092, -0.64092567,  0.78892303,  1.63175932],\n       [-1.0295875 , -1.77530738, -0.2320918 , -0.22427024],\n       [-0.04217092, -0.86780201,  0.78892303,  0.96889162],\n       [-1.52329579,  0.03970336, -1.25310662, -1.28485856],\n       [-0.41245214, -1.3215547 ,  0.16496953,  0.17345038],\n       [-0.16559799, -0.64092567,  0.44858475,  0.17345038],\n       [ 0.82181859, -0.18717298,  0.84564608,  1.10146516],\n       [ 0.57496445, -1.77530738,  0.39186171,  0.17345038],\n       [-0.41245214, -1.09467835,  0.39186171,  0.04087684],\n       [ 1.06867274,  0.03970336,  0.39186171,  0.30602392],\n       [-1.64672287, -1.77530738, -1.36655271, -1.15228502],\n       [-1.27644165,  0.03970336, -1.19638358, -1.28485856],\n       [-0.53587921,  0.72033239, -1.25310662, -1.01971148],\n       [ 1.68580811,  1.17408507,  1.35615349,  1.76433286],\n       [-0.04217092, -0.86780201,  0.22169257, -0.22427024],\n       [-1.52329579,  1.17408507, -1.53672185, -1.28485856],\n       [ 1.68580811,  0.2665797 ,  1.29943044,  0.83631808],\n       [ 1.31552689,  0.03970336,  0.78892303,  1.49918578],\n       [ 0.69839152, -0.86780201,  0.90236912,  0.96889162],\n       [ 0.57496445,  0.49345605,  0.56203085,  0.571171  ],\n       [-1.0295875 ,  0.72033239, -1.25310662, -1.28485856],\n       [ 2.30294347, -1.09467835,  1.80993786,  1.49918578],\n       [-1.0295875 ,  0.49345605, -1.30982967, -1.28485856],\n       [ 0.45153738, -0.41404933,  0.33513866,  0.17345038],\n       [ 0.08125616, -0.18717298,  0.27841562,  0.43859746],\n       [-1.0295875 ,  0.2665797 , -1.42327576, -1.28485856],\n       [-0.41245214, -1.77530738,  0.16496953,  0.17345038],\n       [ 0.57496445,  0.49345605,  1.29943044,  1.76433286],\n       [ 2.30294347, -0.18717298,  1.35615349,  1.49918578]])\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(n_neighbors=3)\n\nknn_clf.fit(X_train,y_train)\n\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n           weights='uniform')\nknn_clf.score(X_test,y_test)\n\n1.0\n实现自己的StandardScaler\nimport numpy as np\n\nclass StandardScaler:\n\n    def __init__(self):\n        self.mean_ = None\n        self.scale_ = None\n\n    def fit(self, X):\n        \"\"\"根据测试数据集X获得数据的均值和方差\"\"\"\n        assert X.ndim == 2, \"The dimension of X must be 2\"\n\n        self.mean_ = np.array([np.mean(x[:i]) for i in range(X.shape(1))])\n        self.scale_ = np.array([np.std(X[:i]) for i in range(X.shape(1))])\n\n        return self\n\n\n    def transform(self, X):\n        \"\"\"将X根据这个StandardScaler进行均值方差归一化处理\"\"\"\n        assert X.ndim == 2, \"The dimension of X must be 2\"\n        assert self.mean_ is not None and self.scale_ is not None,\\\n            \"must fit before transform!\"\n        assert X.shape[1] == len(self.mean_), \\\n            \"the feature number of X must be equal to mean_ and std_\"\n\n        resX = np.empty(shape=X.shape,dtype=float)\n        for col in range(X.shape[1]):\n            resX[:col] = (X[:col]-self.mean_[col]) / self.scale_[col]\n        return resX\n"},"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html":{"url":"knnsuan-fa-de-xue-xi-yu-shi-yong/6.sklearn 中使用knn算法的总结整理.html","title":"6.sklearn 中使用knn算法的总结整理","keywords":"","body":"机器学习流程回顾\n\n1.将数据集分成训练数据集合测试数据集\n2.将训练数据集进行归一化\n3.使用训练数据集的均值和方差将测试数据集归一化\n4.使用训练数集训练处模型\n5.使用归一化后的测试数据集测试分类的准确度（accuracy）\n6.使用网格搜索寻找最好的超参数，然后回到1-5\n机器学习总过程\n\n数据归一化总过程\n\n\n1.将数据集分割成测试数据集合训练数据集\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y)\n2.将数据集进行归一化处理\nfrom sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\n# 存放了均值方差归一化所对应的信息\nstandardScaler.fit(X_train)\nX_train = standardScaler.transform(X_train)\nX_test = standardScaler.transform(X_test)\n3.创建一个KNeighborsClassifier 对象\nfrom sklearn.neighbors import KNeighborsClassifier\nsklearn_knn_clf = KNeighborsClassifier(n_neighbors=6)\n4.使用KNeighborsClassifier 对象进行fit创建出模型\nsklearn_knn_clf.fit(X_train,y_train)\n5.使用训练数据集得出分类准确度\nsklearn_knn_clf.score(X_test,y_test)\n6.使用我们的模型预测新的数据\ny_predict = sklearn_knn_clf.predict(X_test)\n7.探索超参数\n# array>\nparam_grid =[\n    {\n        'weights':['uniform'],\n        'n_neighbors': [i for i in range(1,11)]\n    },\n    {\n        'weights':['distance'],\n        'n_neighbors': [i for i in range(1,11)],\n        'p': [i for i in range(1,6)]\n    }\n]\n\n# 先new一个默认的Classifier对象\nknn_clf = KNeighborsClassifier()\n\n# 调用GridSearchCV创建网格搜索对象，传入参数为Classifier对象以及参数列表\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_search = GridSearchCV(knn_clf,param_grid)\n\n# 调用fit方法执行网格搜索\n%%time\ngrid_search.fit(X_train,y_train)\n\n# 获得最好的评估结果，返回的是KNeighborsClassifier对象，可以直接拿来做机器学习预测了\ngrid_search.best_estimator_\n\n# 最好的分数\ngrid_search.best_score_\n\n# 最好的参数\ngrid_search.best_params_\n"},"线性回归算法/":{"url":"线性回归算法/","title":"4.线性回归算法","keywords":"","body":""},"线性回归算法/1.线性回归算法简介.html":{"url":"线性回归算法/1.线性回归算法简介.html","title":"1.线性回归算法简介","keywords":"","body":"\n线性回归算法以一个坐标系里一个维度为结果，其他维度为特征（如二维平面坐标系中横轴为特征，纵轴为结果），无数的训练集放在坐标系中，发现他们是围绕着一条执行分布。线性回归算法的期望，就是寻找一条直线，最大程度的“拟合”样本特征和样本输出标记的关系\n\n样本特征只有一个的线性回归问题，为简单线性回归，如房屋价格-房屋面积\n将横坐标作为x轴，纵坐标作为y轴，每一个点为（X(i) ,y(i)）,那么我们期望寻找的直线就是y=ax+b，当给出一个新的点x(j)的时候，我们希望预测的y^(j)=ax(j)+b\n\n\n不使用直接相减的方式，由于差值有正有负，会抵消\n不适用绝对值的方式，由于绝对值函数存在不可导的点\n\n\n\n通过上面的推导，我们可以归纳出一类机器学习算法的基本思路，如下图；其中损失函数是计算期望值和预测值的差值，期望其差值（也就是损失）越来越小，而效用函数则是描述拟合度，期望契合度越来越好\n\n\n"},"线性回归算法/2.简单线性回归的实现.html":{"url":"线性回归算法/2.简单线性回归的实现.html","title":"2.简单线性回归的实现","keywords":"","body":"2.1 for循环方式实现\n\n实现\n\n\na,b公式\n\nclass SimpleLinearRegression1:\n\n    def __init__(self):\n        \"\"\"初始化Simple Linear Regression 模型\"\"\"\n        self.a_ = None\n        self.b_ = None\n\n    def fit(self, x_train, y_train):\n        \"\"\"根据训练集x_train，y_train 训练Simple Linear Regression 模型\"\"\"\n        assert x_train.ndim == 1,\\\n            \"Simple Linear Regression can only solve simple feature training data\"\n        assert len(x_train) == len(y_train),\\\n            \"the size of x_train must be equal to the size of y_train\"\n\n        # 求均值\n        x_mean = x_train.mean()\n        y_mean = y_train.mean()\n\n        # 分子\n        num = 0.0\n        # 分母\n        d = 0.0\n\n        # 计算分子分母\n        for x_i, y_i in zip(x_train, y_train):\n            num += (x_i-x_mean)*(y_i-y_mean)\n            d += (x_i-x_mean) ** 2\n\n        # 计算参数a和b\n        self.a_ = num/d\n        self.b_ = y_mean - self.a_ * x_mean\n\n        return self\n\n    def predict(self, x_predict):\n        \"\"\"给定待预测集x_predict，返回x_predict对应的预测结果值\"\"\"\n        assert x_predict.ndim == 1,\\\n            \"Simple Linear Regression can only solve simple feature training data\"\n        assert self.a_ is not None and self.b_ is not None,\\\n            \"must fit before predict!\"\n\n        return np.array([self._predict(x) for x in x_predict])\n\n    def _predict(self, x_single):\n        \"\"\"给定单个待预测数据x_single，返回x_single对应的预测结果值\"\"\"\n        return self.a_*x_single+self.b_\n\n    def __repr__(self):\n        return \"SimpleLinearRegression1()\"\n\n测试\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n简单自定义一个训练集并描绘\nx = np.array([1.,2.,3.,4.,5.])\ny = np.array([1.,3.,2.,3.,5.])\nplt.scatter(x,y)\nplt.axis([0,6,0,6])\n\n[0, 6, 0, 6]\n\n使用我们自己的SimpleLinearRegression1\nfrom machine_learning.SimpleLinearRegression1 import SimpleLinearRegression1\n\nreg1 = SimpleLinearRegression1()\nreg1.fit(x,y)\n# 输出  SimpleLinearRegression1()\n\ny_predict = reg1.predict(np.array([6.]))\ny_predict\n#   输出  array([5.2])\n\nreg1.a_\n #  0.8\nreg1.b_\n#     0.39999999999999947\n\ny_hat = reg1.predict(x)\nplt.scatter(x,y)\nplt.plot(x,y_hat,color='r')\nplt.axis([0,6,0,6])\n\n\n2.2 向量化\n\n\n向量化改进num,d的计算方法\n# 使用向量化点乘计算分子和分母\nnum = (x_train-x_mean).dot(y_train-y_mean)\nd = (x_train-x_mean).dot(x_train-x_mean)\n向量化实现的性能测试\nm = 1000000\nbig_x = np.random.random(size=m)\nbig_y = big_x * 2.0 + 3.0 + np.random.normal(size=m)\n\n%timeit reg1.fit(big_x,big_y)\n%timeit reg2.fit(big_x,big_y)\n\n    # 输出\n    826 ms ± 6.93 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n    11.3 ms ± 84.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n可以看出，向量化的运行速度比循环的形式速度要快80倍\n"},"线性回归算法/3.衡量线性回归算法的指标.html":{"url":"线性回归算法/3.衡量线性回归算法的指标.html","title":"3.衡量线性回归算法的指标","keywords":"","body":"3.衡量线性回归算法的指标\n3.1 衡量标准\n\n其中衡量标准是和m有关的，因为越多的数据量产生的误差和可能会更大，但是毫无疑问越多的数据量训练出来的模型更好，为此需要一个取消误差的方法，如下\n\nMSE 的缺点，量纲不准确，如果y的单位是万元，平方后就变成了万元的平方，这可能会给我们带来一些麻烦\n\n\nRMSE 平方累加后再开根号，如果某些预测结果和真实结果相差非常大，那么RMSE的结果会相对变大，所以RMSE有放大误差的趋势，而MAE没有，他直接就反应的是预测结果和真实结果直接的差距，正因如此，从某种程度上来说，想办法我们让RMSE变的更小小对于我们来说比较有意义，因为这意味着整个样本的错误中，那个最值相对比较小，而且我们之前训练样本的目标，就是RMSE根号里面1/m的这一部分，而这一部分的本质和优化RMSE是一样的\n3.2 MSE,RMSE,MAE的实现\ndef mean_squared_error(y_true, y_predict):\n    \"\"\"计算y_true和y_predict之间的MSE\"\"\"\n    assert len(y_true) == len(y_predict), \\\n        \"the size of y_true must be equal to the size of y_predict\"\n\n    return np.sum((y_true - y_predict)**2) / len(y_true)\n\n\ndef root_mean_squared_error(y_true, y_predict):\n    \"\"\"计算y_true和y_predict之间的RMSE\"\"\"\n\n    return sqrt(mean_squared_error(y_true, y_predict))\n\n\ndef mean_absolute_error(y_true, y_predict):\n    \"\"\"计算y_true和y_predict之间的RMSE\"\"\"\n    assert len(y_true) == len(y_predict), \\\n        \"the size of y_true must be equal to the size of y_predict\"\n\n    return np.sum(np.absolute(y_true - y_predict)) / len(y_true)\n3.3 调用sikit learn 的实现\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nmean_squared_error(y_test,y_predict)\n"},"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html":{"url":"线性回归算法/4.最好的衡量线性回归法的指标-R-Squared.html","title":"4.最好的衡量线性回归法的指标 R Squared","keywords":"","body":"4.最好的衡量线性回归法的指标 R Squared\nRMSE 和 MAE的局限性\n\n可能预测房源准确度，RMSE或者MAE的值为5，预测学生的分数，结果的误差是10，这个5和10没有判断性，因为5和10对应不同的单位和量纲，无法比较\n4.1 解决办法-R Squared简介\n\n4.1.1 R Squared 意义\n\n使用BaseLine Model产生的错误会很大，使用我们的模型预测产生的错误会相对少些（因为我们的模型充分的考虑了y和x之间的关系），用这两者相减，结果就是拟合了我们的错误指标，用1减去这个商结果就是我们的模型没有产生错误的指标\n \n\n4.1.2 实现\ndef r2_score(y_true, y_predict):\n    \"\"\"计算y_true和y_predict之间的R Square\"\"\"\n\n    return 1 - mean_squared_error(y_true, y_predict)/np.var(y_true)\nsikit learn\nfrom sklearn.metrics import r2_score\nr2_score(y_test,y_predict)\n将计算分数方法封装到我们的SimpleLinearRegression中\nfrom .metrics import r2_score\ndef score(self, x_test, y_test):\n        \"\"\"根据测试数据集 x_test 和 y_test 确定当前模型的准确度\"\"\"\n\n        y_predict = self.predict(x_test)\n        return r2_score(y_test, y_predict)\n"},"线性回归算法/5.多元线性回归.html":{"url":"线性回归算法/5.多元线性回归.html","title":"5.多元线性回归","keywords":"","body":"5.多元线性回归\n5.1 多元线性回归简介和正规方程解\n\n\n\n\n补充（矩阵点乘：A（m行）·B（n列） = A的每一行与B的每一列相乘再相加，等到结果是m行n列的）\n\n补充（一个1xm的行向量乘以一个mx1的列向量等于一个数）\n\n\n推导过程参考 https://blog.csdn.net/nomadlx53/article/details/50849941\n4.2 多元线性回归实现\n\nimport numpy as np\nfrom .metrics import r2_score\n\n\nclass LinearRegression:\n\n    def __init__(self):\n        \"\"\"初始化Linear Regression模型\"\"\"\n\n        # 系数向量（θ1,θ2,.....θn）\n        self.coef_ = None\n        # 截距 (θ0)\n        self.interception_ = None\n        # θ向量\n        self._theta = None\n\n    def fit_normal(self, X_train, y_train):\n        \"\"\"根据训练数据集X_train，y_train 训练Linear Regression模型\"\"\"\n        assert X_train.shape[0] == y_train.shape[0], \\\n            \"the size of X_train must be equal to the size of y_train\"\n\n        # np.ones((len(X_train), 1)) 构造一个和X_train 同样行数的，只有一列的全是1的矩阵\n        # np.hstack 拼接矩阵\n        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])\n        # X_b.T 获取矩阵的转置\n        # np.linalg.inv() 获取矩阵的逆\n        # dot() 矩阵点乘\n        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)\n\n        self.interception_ = self._theta[0]\n        self.coef_ = self._theta[1:]\n\n        return self\n\n    def predict(self, X_predict):\n        \"\"\"给定待预测数据集X_predict，返回表示X_predict的结果向量\"\"\"\n        assert self.coef_ is not None and self.interception_ is not None,\\\n            \"must fit before predict\"\n        assert X_predict.shape[1] == len(self.coef_),\\\n            \"the feature number of X_predict must be equal to X_train\"\n\n        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])\n        return X_b.dot(self._theta)\n\n    def score(self, X_test, y_test):\n        \"\"\"根据测试数据集 X_test 和 y_test 确定当前模型的准确度\"\"\"\n\n        y_predict = self.predict(X_test)\n        return r2_score(y_test, y_predict)\n\n    def __repr__(self):\n        return \"LinearRegression()\"\n预测波士顿房价的测试\nimport numpy as np\nimport matplotlib.pyplot as plot\nfrom sklearn import datasets\n\n# 加载波士顿房价数据\nboston = datasets.load_boston()\nX = boston.data\ny = boston.target\nX = X[y4.3 scikit-learn中的回归问题\nscikit-learn 中的线性回归\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=666)\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\n\nlin_reg.fit(X_train,y_train)\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\nlin_reg.coef_\n\narray([-1.14235739e-01,  3.12783163e-02, -4.30926281e-02, -9.16425531e-02,\n       -1.09940036e+01,  3.49155727e+00, -1.40778005e-02, -1.06270960e+00,\n        2.45307516e-01, -1.23179738e-02, -8.80618320e-01,  8.43243544e-03,\n       -3.99667727e-01])\n# 由于训练数据集和测试数据集的分割和我们的稍有不同，所以结果会略有不同\nlin_reg.intercept_\n\n32.64566083965224\nlin_reg.score(X_test,y_test)\n\n0.8008916199519077\nkNN Regressor 实现线性回归\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknn_reg = KNeighborsRegressor()\nknn_reg.fit(X_train,y_train)\nknn_reg.score(X_test,y_test)\n\n0.602674505080953\n# 网格搜索超参数\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {\n        \"weights\" : [\"uniform\"],\n        \"n_neighbors\":[i for i in range(1,11)]\n    },\n    {\n        \"weights\" : [\"distance\"],\n        \"n_neighbors\":[i for i in range(1,11)],\n        \"p\":[i for i in range(1,6)]\n    }\n]\n\nknn_reg = KNeighborsRegressor()\ngrid_search = GridSearchCV(knn_reg,param_grid,n_jobs=-1,verbose=1)\ngrid_search.fit(X_train,y_train)\n\nFitting 3 folds for each of 60 candidates, totalling 180 fits\n\n\n[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    0.5s finished\n\n\n\n\n\nGridSearchCV(cv=None, error_score='raise',\n       estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n          metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n          weights='uniform'),\n       fit_params=None, iid=True, n_jobs=-1,\n       param_grid=[{'weights': ['uniform'], 'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, {'weights': ['distance'], 'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'p': [1, 2, 3, 4, 5]}],\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=1)\ngrid_search.best_params_\n\n{'n_neighbors': 6, 'p': 1, 'weights': 'distance'}\n# 运用了CV交叉验证的方式\ngrid_search.best_score_\n\n0.6060327991735741\ngrid_search.best_estimator_.score(X_test,y_test)\n\n0.7354244906092771\n"},"线性回归法/6线性回归的可解性和更多思考.html":{"url":"线性回归法/6线性回归的可解性和更多思考.html","title":"6.线性回归的可解性和更多思考","keywords":"","body":"6.线性回归的可解性和更多思考\n"},"梯度下降法/":{"url":"梯度下降法/","title":"5.梯度下降法","keywords":"","body":"梯度下降法\n"},"梯度下降法/1.梯度下降法简介.html":{"url":"梯度下降法/1.梯度下降法简介.html","title":"1.梯度下降法简介","keywords":"","body":"1. 梯度下降法简介\n\n以下是定义了一个损失函数以后，参数theta对应的损失函数J的值对应的示例图，我们需要找到使得损失函数值J取得最小值对应的theta（这里是二维平面，也就是我们的参数只有一个）\n在直线方程中，导数代表斜率\n在曲线方程中，导数代表切线斜率\n导数代表theta单位变化时，J相应的变化\n\n\nη太小，会减慢收敛学习速度\n\nη太大，甚至导致不收敛\n\n其他注意事项\n\n并不是所有函数都有唯一的极值点\n\n解决方案：\n多次运行，随机化初始点\n梯度下降法的初始点也是一个超参数\n\n\n\n\n"},"梯度下降法/2.梯度下降法模拟.html":{"url":"梯度下降法/2.梯度下降法模拟.html","title":"2.梯度下降法模拟","keywords":"","body":"2 梯度下降法模拟\n2.1 实现\nimport numpy as np\nimport matplotlib.pyplot as plt\n# 简单模拟一个损失函数\nplot_x = np.linspace(-1,6,141)\nplot_y = (plot_x-2.5)**2-1\n# 绘制我们模拟的损失函数\nplt.plot(plot_x,plot_y)\n\ndef dJ(theta):\n    \"\"\"损失函数的导数\"\"\"\n    return 2*(theta-2.5)\n\ndef J(theta):\n    \"\"\"损失函数\"\"\"\n    try:\n        return (theta-2.5)**2-1\n    except:\n        return float('inf')\n\ndef gradient_descent(initial_theta,eta,n_iters = 1e4,epsilon=1e-8):\n    \"\"\"\n    梯度下降法封装\n    initial_theta:初始化的theta值\n    eta:学习率η\n    n_iters: 最大循环次数\n    epsilon: 精度\n    \"\"\"\n    theta = initial_theta\n    # theta_history 保存theta的变化值\n    theta_history.append(initial_theta)\n    i_iters = 0\n\n    while i_iters2.2 使用不同η学习率测试并观察我们的梯度下降法的结果\neta = 0.1\ntheta_history = []\ngradient_descent(0.,eta)\nplot_theta_history()\n\neta = 0.01\ntheta_history = []\ngradient_descent(0.,eta)\nplot_theta_history()\n\neta = 0.001\ntheta_history = []\ngradient_descent(0.,eta)\nplot_theta_history()\n\neta = 0.8\ntheta_history = []\ngradient_descent(0.,eta)\nplot_theta_history()\n\n可以发现，只要η不超过一个限度，我们编写的函数都可以在有限次数之后找到最优解，并且η越小，学习的次数越多\n下面来看一下，如果eta取较大值1.1，会出现什么情况\neta = 1.1\ntheta_history = []\ngradient_descent(0.,eta)\n# 数据量太大会报错\n# plot_theta_history()\nprint(len(theta_history))\n# 输出10001\ntheta_history[-1]\n# 输出 nan（not a number）\n可以看出当我们的eta取1.1，函数会循环直至终止，这是由于，我们的η设置过大，导致每次循环过后，损失函数j的值都向大的方向变化\neta = 1.1\ntheta_history = []\ngradient_descent(0.,eta,n_iters = 10)\nplot_theta_history()\n\n"},"梯度下降法/3.多元线性回归中的梯度下降法.html":{"url":"梯度下降法/3.多元线性回归中的梯度下降法.html","title":"3.多元线性回归中的梯度下降法","keywords":"","body":"3 多元线性回归中的梯度下降法\n\n一个三维空间中的梯度下降法（x,y为系数，z为损失函数）\n\n推导过程\n\n\n上面推导出的式子的大小是和样本数有关的，m越大，结果越大，这是不合理的，我们希望和m无关\n\n"},"梯度下降法/4.线性回归中的梯度下降法的实现.html":{"url":"梯度下降法/4.线性回归中的梯度下降法的实现.html","title":"4.线性回归中的梯度下降法的实现","keywords":"","body":"4 线性回归中的梯度下降法的实现\n4.1 比较笨的方法实现\ndef fit_gd(self, X_train, y_train, eta=0.01, n_iters = 1e4):\n        \"\"\"根据训练数据集X_train,y_train, 使用梯度下降法训练Linear Regression 模型\"\"\"\n        assert X_train.shape[0] == y_train.shape[0], \\\n            \"the size of X_train must be equal to the size of y_train\"\n\n        def J(theta, X_b, y):\n            try:\n                return np.sum((y - X_b.dot(theta))**2) / len(X_b)\n            except:\n                return float('inf')\n\n        def dJ(theta, X_b, y):\n            res = np.empty(len(theta))\n            res[0] = np.sum(X_b.dot(theta) - y)\n\n            for i in range(1, len(theta)):\n                res[i] = np.sum((X_b.dot(theta) - y).dot(X_b[:, i]))\n\n            return res * 2 / len(X_b)\n\n        def gradient_descent(X_b, y, initial_theta, eta, n_iters=n_iters, epsilon=1e-8):\n            \"\"\"\n            梯度下降法封装\n            X_b: X特征矩阵\n            y: 结果向量\n            initial_theta:初始化的theta值\n            eta:学习率η\n            n_iters: 最大循环次数\n            epsilon: 精度\n            \"\"\"\n            theta = initial_theta\n            i_iters = 0\n\n            while i_iters 4.2 测试我们的算法\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(666)\nx = 2*np.random.random(size = 100)\n# 定义截距为4 斜率为3\ny = x * 3. + 4. + np.random.normal(size=100)\nX = x.reshape(-1,1)\nplt.scatter(x,y)\n\nfrom machine_learning.LinearRegression import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit_gd(X,y)\n\nlin_reg.interception_\n# 4.021457858204859\nlin_reg.coef_\n# array([3.00706277])\n4.3 向量化\n\n\n\n修改之前的求导函数\ndef dJ(theta, X_b, y):\n            # res = np.empty(len(theta))\n            # res[0] = np.sum(X_b.dot(theta) - y)\n            #\n            # for i in range(1, len(theta)):\n            #     res[i] = np.sum((X_b.dot(theta) - y).dot(X_b[:, i]))\n            #\n            # return res * 2 / len(X_b)\n            return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(X_b)\n使用真实的数据测试\n\n使用真实的数据，调整eta和iters，要么由于eta太小导致无法得出真实的结果，要么由于eta太大导致训练时间加长，这是由于数据的规模在不同的特征上不同，所以我们需要对数据进行归一化\n4.4 数据归一化\n\n\n\n如果样本数非常多，那么即使使用梯度下降法也会导致速度比较慢，因为在梯度下降法中，每一个样本都要参与运算。这时候需要采用随机梯度下降法，我们将在下一小节进行介绍\n"},"梯度下降法/5.随机梯度下降法.html":{"url":"梯度下降法/5.随机梯度下降法.html","title":"5.随机梯度下降法","keywords":"","body":"5 随机梯度下降法\n5.1 随机梯度下降法介绍\n\n批量梯度下降法带来的一个问题是η的值需要设置的比较小，在样本数比较多的时候导致不是速度特别慢，这时候观察随机梯度下降法损失函数的求导公式，可以发现，我们对每一个Xb都做了求和操作，又在最外面除以了m，那么可以考虑将求和和除以m的两个运算约掉，采用每次使用一个随机的Xb\n\n\n由于我们使用的事随机梯度下降法，所以导致我们的最终结果不会像批量梯度下降法一样准确的朝着一个方向运算，而是曲线行下降，这时候我们就希望，越到下面，η值相应减小，事运算次数变多，从而精确计算结果\n\n这里使用了模拟退火的思想\n\n5.2 随机梯度下降法实现\ndef dJ_sgd(theta,X_b_i,y_i):\n    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2\n\ndef sgd(X_b,y,initial_theta,n_iters):\n\n    t0 = 5\n    t1 = 50\n\n    def learning_rate(t):\n        return t0 / (t + t1)\n\n    theta = initial_theta\n    for cur_iter in range(n_iters):\n        rand_i = np.random.randint(len(X_b))\n        gradient = dJ_sgd(theta,X_b[rand_i],y[rand_i])\n        theta = theta - learning_rate(cur_iter) * gradient\n    return theta\n        %%time\n        eta = 0.01\n        X_b = np.hstack([np.ones((len(X), 1)), X])\n        initial_theta = np.zeros(X_b.shape[1])\n        # 随机的检查了3分之一个样本总量的样本\n        _theta = sgd(X_b, y, initial_theta, n_iters=len(X_b)//3)\n        # 输出 CPU times: user 318 ms, sys: 5.22 ms, total: 323 ms\n        # Wall time: 337 ms\n        # _theta: array([3.03182269, 3.93118623])\n5.3 随机梯度下降法的封装和测试\n    def fit_sgd(self, X_train, y_train, n_iters=5, t0=5, t1=50):\n        \"\"\"\n        根据训练数据集X_train, y_train, 使用随机梯度下降法训练Linear Regression模型\n        :param X_train:\n        :param y_train:\n        :param n_iters: 在随机梯度下降法中，n_iters代表所有的样本会被看几圈\n        :param t0:\n        :param t1:\n        :return:\n        \"\"\"\n        assert X_train.shape[0] == y_train.shape[0], \\\n            \"the size of X_train must be equal to the size of y_train\"\n        assert n_iters >= 1\n\n        def dJ_sgd(theta, X_b_i, y_i):\n            \"\"\"\n            去X_b,y 中的随机一个元素进行导数公式的计算\n            :param theta:\n            :param X_b_i:\n            :param y_i:\n            :return:\n            \"\"\"\n            return X_b_i * (X_b_i.dot(theta) - y_i) * 2.\n\n        def sgd(X_b, y, initial_theta, n_iters, t0=5, t1=50):\n\n            def learning_rate(t):\n                \"\"\"\n                计算学习率，t1 为了减慢变化速度，t0为了增加随机性\n                :param t: 第t次循环\n                :return:\n                \"\"\"\n                return t0 / (t + t1)\n\n            theta = initial_theta\n            m = len(X_b)\n\n            for cur_iter in range(n_iters):\n                # 对X_b进行一个乱序的排序\n                indexes = np.random.permutation(m)\n                X_b_new = X_b[indexes]\n                y_new = y[indexes]\n\n                # 对整个数据集看一遍\n                for i in range(m):\n                    gradient = dJ_sgd(theta, X_b_new[i], y_new[i])\n                    theta = theta - learning_rate(cur_iter * m + i) * gradient\n\n            return theta\n\n        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])\n        initial_theta = np.random.randn(X_b.shape[1])\n        self._theta = sgd(X_b, y_train, initial_theta, n_iters, t0, t1)\n\n        self.interception_ = self._theta[0]\n        self.coef_ = self._theta[1:]\n\n        return self\n模拟数据进行测试\n\n真实数据波士顿房价进行测试\n\n5.4 使用Sklearn中的 随机梯度下降法\n\n需要注意的是sklearn中的梯度下降法比我们自己的算法要复杂的多，性能和计算准确度上都比我们的要好，我们的算法只是用来演示过程，具体生产上的使用还是应该使用Sklearn提供的\n"},"梯度下降法/6.梯度下降法的调试.html":{"url":"梯度下降法/6.梯度下降法的调试.html","title":"6.梯度下降法的调试","keywords":"","body":"6 梯度下降法 的调试\n6.1 梯度下降法调试的原理\n可能我们计算出梯度下降法的公式，并使用python编程实现，预测的过程中程序并没有报错，但是可能我们需要求的梯度的结果是错误的，这个时候需要怎么样去调试发现错误呢。\n首先以二维坐标平面为例，一个点（O）的导数就是曲线在这个点的切线的斜率，在这个点两侧各取一个点（AB），那么AB两点对应的直线的斜率应该大体等于O的切线的斜率，并且这A和B的距离越近，那么两条直线的斜率就越接近\n事实上，这也正是导数的定义，当函数y=f(x)的自变量x在一点x0上产生一个增量Δx时，函数输出值的增量Δy与自变量增量Δx的比值在Δx趋于0时的极限a如果存在，a即为在x0处的导数，记作f'(x0)或df(x0)/dx\n\n扩展到多维维度则如下\n\n梯度下降法调试的实现\nnp.random.seed(666)\nX = np.random.normal(size=(1000,10))\nX_b = np.hstack([np.ones((len(X),1)),X])\n\n# 真实的θ值\ntrue_theta = np.arange(1,12,dtype=float)\n# np.random.normal(size=1000) 添加噪音\ny = X_b.dot(true_theta) + np.random.normal(size=1000)\n\n# 实现J(θ)函数\ndef J(theta,X_b,y):\n    try:\n        return np.sum((y-X_b.dot(theta))**2)/len(X_b)\n    except:\n        return float('inf')\n\n# 实现数学推导出的dJ(θ)\ndef d_J_main(theta,X_b,y):\n    return X_b.T.dot(X_b.dot(theta) - y) * 2. /len(X_b)\n\n# 实现debug模式的dJ(θ)\ndef d_J_debug(theta,X_b,y,epsilon=0.01):\n    res = np.empty(len(theta))\n    for i in range(len(theta)):\n        theta_1 = theta.copy()\n        theta_1[i] += epsilon\n        theta_2 = theta.copy()\n        theta_2[i] -= epsilon\n        res[i] = (J(theta_1,X_b,y)-J(theta_2,X_b,y))/(2*epsilon)\n    return res\n\n# 批量梯度下降法，d_J为求导函数，作为一个参数传入，用于切换求导策略\ndef gradient_descent(d_J,X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):\n            theta = initial_theta\n            i_iters = 0\n\n            while i_iters \nX_b = np.hstack([np.ones((len(X), 1)), X])\ninitial_theta = np.zeros(X_b.shape[1])\neta = 0.01\n# 使用d_J_debug调试模式求出theta\n%time theta = gradient_descent(d_J_debug,X_b, y, initial_theta, eta)\nprint(theta)\n# 使用数学解求出theta\n%time theta = gradient_descent(d_J_main,X_b, y, initial_theta, eta)\nprint(theta)\n\n# 输出结果\nCPU times: user 531 ms, sys: 214 ms, total: 745 ms\nWall time: 613 ms\n[ 0.94575233  1.98082712  3.06882065  3.94835863  4.97139932  5.9859077\n  7.01077392  7.99250414  8.99151383  9.97525811 10.99758484]\nCPU times: user 67 ms, sys: 27.6 ms, total: 94.6 ms\nWall time: 76.7 ms\n[ 0.94575233  1.98082712  3.06882065  3.94835863  4.97139932  5.9859077\n  7.01077392  7.99250414  8.99151383  9.97525811 10.99758484]\n由此可以看出，我们的d_J_debug和d_J_main的结果是相近的，所以我们的d_J_main的数学推导是没问题的。\n我们可以在真正的机器学习之前，先使用d_J_debug这种调试方式来验证一下我们的d_J_main的结果是否正确，然后再进行机器学习。\nd_J_debug是通用的，可以放在任何求导的debug过程中，所以可以作为我们机器学习的工具箱来使用\n"},"梯度下降法/7.梯度下降法的总结.html":{"url":"梯度下降法/7.梯度下降法的总结.html","title":"7.梯度下降法的总结","keywords":"","body":"7.梯度下降法的总结\n7.1 小批量\n\n批量梯度下降法\n随机梯度下降法\n下面来看下二者的对比\n\n\n\n\n维度\n批量梯度下降法\n随机梯度下降法\n\n\n\n\n计算方式\n每次对所有的样本看一遍才可以计算出梯度\n每一次只需观察一个样本\n\n\n速度\n慢\n快\n\n\n稳定性\n高，一定可以先向损失函数下降的方式前进\n低，每一次的方式不确定，甚至向反方向前进\n\n\n\n综合二者的优缺点，有一种新的梯度下降法\n\n小批量梯度下降法：即，我们每一次不看全部样本那么多，也不是只看一次样本那么少，每次只看k个样本\n对于小批量梯度下降法，由多了一个超参数\ndef fit_lit_sgd(self, X_train, y_train, n_iters=5, t0=5, t1=50,k=10):\n        \"\"\"\n        根据训练数据集X_train, y_train, 使用随机梯度下降法训练Linear Regression模型\n        :param X_train:\n        :param y_train:\n        :param n_iters: 在随机梯度下降法中，n_iters代表所有的样本会被看几圈\n        :param t0:\n        :param t1:\n        :param k: 小批量随机下降法的超参数k\n        :return:\n        \"\"\"\n        assert X_train.shape[0] == y_train.shape[0], \\\n            \"the size of X_train must be equal to the size of y_train\"\n        assert n_iters >= 1\n\n        def dJ_sgd(theta, X_b_k, y_k):\n            \"\"\"\n            去X_b,y 中的随机选择k个元素进行导数公式的计算\n            :param theta:\n            :param X_b_i:\n            :param y_i:\n            :return:\n            \"\"\"\n            return np.sum((X_b_k * (X_b_k.dot(theta) - y_k) ))* 2/len(X_b_k).\n\n        def sgd(X_b, y, initial_theta, n_iters, t0=5, t1=50):\n\n            def learning_rate(t):\n                \"\"\"\n                计算学习率，t1 为了减慢变化速度，t0为了增加随机性\n                :param t: 第t次循环\n                :return:\n                \"\"\"\n                return t0 / (t + t1)\n\n            theta = initial_theta\n            m = len(X_b)\n\n            for cur_iter in range(n_iters):\n                # 每次看k个元素\n                i =0\n                while i 7.2 随机\n随机梯度下降法的优点\n\n7.3 梯度上升法\n\n\n\n"},"PCA/":{"url":"PCA/","title":"6.主成分分析法-PCA","keywords":"","body":"主成分分析法-PCA\n"},"PCA/1.PCA简介.html":{"url":"PCA/1.PCA简介.html","title":"1.PCA简介","keywords":"","body":"1.PCA简介\nPCA（Principal Component Analysis）：也是一个梯度分析的应用，不仅是机器学习的算法，也是统计学的经典算法\n\n1.1 举个栗子\n例如下面一个两个特征的一个训练集，我们可以选择一个特征，扔掉一个特征\n\n下图分别是扔掉了特征一和特征二的两种方案，很明显右边这种的效果会更好一些，因为访问二扔掉特征二以后，点之间的分布情况更接近与原图，但是这不是更好的\n\n我们希望有一根直线，是斜着的，我们希望将所有的点都映射到这条直线上，那么这个时候我们就成功的将二维降到了一维，与此同时，这些点更加趋近与原来的点的分布情况，换句话说，点和点之间的距离比无论是映射到x还是映射到y周，他们之间的区分度都更加的大，也就更加容易区分\n\n1.2 总结\n那么如何找到这个让样本间间距最大的轴?\n如何定义样本间间距?\n事实上有一个指标可以之间定义样本间的距离，就是方差（Variance）（方差：描述样本整体之间的疏密的一个指标，方差越大，代表样本之间越稀疏，方差越小，代表样本之间越紧密）\n\n\n第一步: 将样例的均值归为0 (demean)（归0：所有样本都减去他们的均值），使得均值为0，这样可以简化方差的公式\n\n1.3 推导\n\n进行均值归0操作以后，就是下面的式子\n\n注：|Xproject|的平均值也是一个向量\nX(i)映射到w的距离实际上就是X(i)与w的点乘（蓝色的线），根据定义推导，其值实际上就是Xproject\n\n此时我们的目标函数就可以化简成\n\n这是一个目标函数的最优化问题，使用梯度上升法解决。\n当然我们也可以之间使用数学原理推导出结果，这里我们主要关注使用搜索的策略来求解主成分分析法，这样我们对梯度上升发和梯度下降法也可以有一个更深刻的认识\n1.4 与线性回归的区别\n\n\n1.主成分分析法的两个轴都是特征，线性回归y轴是目标结果值\n2.主成分分析法的点是垂直于方差轴直线的，线性回归的点事垂直于x轴的\n1.5 对PCA和线性代数中的特征向量的对比\n\n一个矩阵可以把一个向量拉伸或者缩短λ倍，这个向量就是特征向量，λ是特征值；\n那么能够拉伸最长的特征向量，实际上就是这个矩阵的第一主成分；\n一组线性不相关的特征向量可以组成一个特征向量空间；\n所以主成分分析实际上就是把矩阵转换到了一个特殊的特征向量空间；\n这个特征向量空间是由能够被拉伸最长的前k个特征向量组成的；而且这k个特征向量相互正交；\n\n"},"PCA/2.使用梯度上升法解决PCA问题.html":{"url":"PCA/2.使用梯度上升法解决PCA问题.html","title":"2.使用梯度上升法解决PCA问题","keywords":"","body":"2.使用梯度上升法解决PCA问题\n\n1.注意上面式子里的每一个(X1(i)·w1+X2(i)·w2+......Xn(i)·wn)都是一个X(i)和w的点乘，所以式子可以进一步化解，\n2.化简过后可以进行向量化，即每一个∑(X(i)·w1)·X1(i) 可以看成是(X·w)这个向量的转置（本来是个行向量，转置后是1行m列的列向量）与X这个矩阵（m行n列）做点乘等到的其中一项的相乘相加的结果\n3.最后根据转置法则 ((AB)T=BTAT)转换成最后的结果\n\n\n"},"PCA/3.主成分PCA的实现.html":{"url":"PCA/3.主成分PCA的实现.html","title":"3.主成分PCA的实现","keywords":"","body":"3.主成分PCA的实现\n使用梯度上升法求解主成分\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n构造一个两个样本之间有基本线性关系的数据集，可以使得我们降维的效果更加明显\n\n\nX = np.empty((100,2))\nX[:,0] = np.random.uniform(0.,100.,size=100)\n# 0.75倍的X[:,0]加上3加上一个噪音\nX[:,1] = 0.75*X[:,0]+3.+np.random.normal(0.,10.,size=100)\n\nplt.scatter(X[:,0],X[:,1])\n\n\n\n\n均值归0\n\n\ndef demean(X):\n    return X-np.mean(X,axis=0)\n\nX_demean = deamean(X)\nplt.scatter(X_demean[:,0],X_demean[:,1])\n\n\n\n梯度上升法\ndef f(w,X):\n    return np.sum((X.dot(w)**2))/len(X)\n\ndef df_math(w,X):\n    return X.T.dot(X.dot(w)) * 2. /len(X)\n\ndef df_debug(w,X,epsilon=0.0001):\n    res = np.empty(len(w))\n    for i in range(len(w)):\n        w_1 = w.copy()\n        w_1[i] += epsilon\n        w_2 = w.copy()\n        w_2[i] -= epsilon\n        res[i] = (f(w_1,X)-f(w_2,X)) / (2*epsilon)\n    return res\n\ndef direction(w):\n    \"\"\"计算单位向量\"\"\"\n    return w / np.linalg.norm(w)\n\ndef gradient_ascent(df,X,inital_w,eta,n_iters = 1e4,epsilon=1e-8):\n\n    w = direction(inital_w)\n    cur_iter = 0\n\n    while cur_iter \n# 初始值不能为0，因为将0带入求导公式，会发现得0，没有任何方向\n# 因为对于我们的目标函数来说，w=0本身就是一个最小值点\n\n## 注意2：不能从0向量开始\ninital_w = np.random.random(X.shape[1])\neta = 0.01\n\n# 注意3：不能使用StandardScaler标准化数据\n# 因为我们本来就是要使得方差最大，而标准化的目的是使得方差为1\n\n# 使用debug模式\ngradient_ascent(df_debug,X_demean,inital_w,eta)\n# 输出  array([0.75934073, 0.65069321])\n\n# 使用math数学解\ngradient_ascent(df_math,X_demean,inital_w,eta)\n# 输出     array([0.75934073, 0.65069321])\n\nw = gradient_ascent(df_math,X_demean,inital_w,eta)\nplt.scatter(X_demean[:,0],X_demean[:,1])\n# 这个轴就是我们求出的第一个主成分\nplt.plot([0,w[0]*30],[0,w[1]*30],color='r')\n\n\n\n"},"PCA/4.求数据的前N个主成分.html":{"url":"PCA/4.求数据的前N个主成分.html","title":"4.求数据的前N个主成分","keywords":"","body":"4.求数据的前N个主成分\n求出第一主成分以后，如何求出下一个主成分?\n1.数据进行改变，将数据在第一个主成分上的分量去掉\nX(i)·w = ||Xproject(i)|| 即X(i)映射到w上的值，那么||Xproject(i)||（大小） ·w（方向）就是X(i)在w上的分向量记为Xproject(i)= ||Xproject(i)|| ·w\nX(i)-Xproject(i)就可以实现将X样本在Xproject相应上的分量去掉，相减之后的集合意义就是讲X样本映射到了Xproject向量相垂直的一个轴上，记为X`(i) = Xproject(i)\n\n2.在新的数据上求第一主成分\n得到的X` 是X中的所有样本都去除了第一主成分上的分量得到的结果，要求第二主成分，只要在新的数据上，重新求一下第一主成分\n4.1 获得前n个主成分实现\ndef f(w,X):\n    return np.sum((X.dot(w)**2))/len(X)\n\ndef df(w,X):\n    return X.T.dot(X.dot(w)) * 2. /len(X)\n\ndef direction(w):\n    \"\"\"计算单位向量\"\"\"\n    return w / np.linalg.norm(w)\n\ndef first_componet( X,inital_w,eta,n_iters = 1e4,epsilon=1e-8):\n    w = direction(inital_w)\n    cur_iter = 0\n\n    while cur_iter \ninital_w = np.random.random(X.shape[1])\neta = 0.01\n\nfirst_componet(X_demean,inital_w,eta)\n\narray([0.75934077, 0.65069317])\n# X2 = np.empty(X.shape)\n# for i in range(len(X)):\n#    X2[i] = X[i] - X[i].dot(w)*w\n# 向量化，X.dot(w)为m*1的向量，reshape后变成了1*m的列向量，再乘以w（方向）就是X的每一个值在w上 的分量矩阵\nX2 = X - X.dot(w).reshape(-1,1)*w\n\n# 相减得到的样本分布几乎垂直于原来的样本分布\nplt.scatter(X2[:,0],X2[:,1])\n\n\n# 求出第二主成分\nw2 = first_componet(X2,inital_w,eta)\nw2\n# 输出     array([-0.65068927,  0.75934411])\n\n# 因为w和w2都是单位向量，所以他们两个点乘得到的结果就是他们夹角的cos值，\n# 又因为w和w2应该是互相垂直的，所以他们夹角的cos值等于0\nw.dot(w2)\n# 输出5.1820671385094386e-06 几乎为0\n\ndef first_n_componet(n,X,eta = 0.01,n_iters = 1e4,epsilon = 1e-8):\n    X_pca = X.copy()\n    X_pca = demean(X_pca)\n\n    res = []\n    for i in range(n):\n        initial_w = np.random.random(X_pca.shape[1])\n        w = first_componet(X_pca,initial_w,eta)\n        res.append(w)\n        X_pca = X_pca - X_pca.dot(w).reshape(-1,1) *w\n    return res\n\nfirst_n_componet(2,X)\n# 输出  [array([0.75934077, 0.65069316]), array([-0.65068972,  0.75934372])]\n\n"},"PCA/5.高维数据向低维数据进行映射.html":{"url":"PCA/5.高维数据向低维数据进行映射.html","title":"5.高维数据向低维数据进行映射","keywords":"","body":"5. 高维数据向低维数据进行映射\n对于一个数据集X来说，这个X有m行n列，代表有m个样本n个特征，通过我们前面学习的主成分分析法，假设我们已经求出了针对这个数据来说的前k个主成分，每一个主成分对应一个单位方向，用W来表示,W也是一个矩阵，他有k行，代表我们求出的前K个主成分，每一行有n列，代表每一个主成分的坐标轴应该是有n个元素的。这是因为我们的主成分分析法主要就是将数据从一个坐标系转化成了另外一个坐标系，原来这个坐标系有n个维度，现在这个坐标系也应该有n个维度，只不过对于转化的坐标系来说，我们取出来前k个，这k个方向更加重要。\n如何将我们的样本X从n维转化成k维呢，回忆们之前学到的，对于一个X样本，与一个W进行点乘，其实就是讲一个样本映射到了w这个坐标轴，得到的模，如果讲这一个样本和这k个w分别做点乘，得到的就是这一个样本，在这k个方向上做映射后每一个方向上的大小，这k个元素合在一起，就代表这一个样本映射到新的k个轴所代表的坐标系上相应的这个样本的大小\nX1分别乘以W1到Wn，得到的k个数组成的向量，就是样本1映射到Wk这个坐标系上得到的k维的向量，由于kT**(为什么是转置呢，因为我们是拿X的每一行去和W的每一行做点乘的，但是矩阵乘法规定是拿X的每一行和W的每一列做乘法)\n\n我们得到新的降维后的矩阵Xk以后，是可以通过和Wk想乘回复回来的，但是由于我们在降维的过程中丢失了一部分信息，这时及时回复回来也和原来的矩阵不一样了，但是这个从数据角度成立的\n\n从高维数据向地维数据的映射\nclass PCA:\n\n    def __init__(self,n_components):\n        \"\"\"初始化PCA\"\"\"\n        assert n_components>=1, \"n_components must be vaild\"\n        self.n_components = n_components\n        self.components_ = None\n\n    def fit(self,X,eta=0.01,n_iters=1e4):\n        \"\"\"获得数据集X的前n个元素\"\"\"\n        assert self.n_componentsPCA 降维的基本原理:找到另外一个坐标系，这个坐标系每一个轴依次可以表达原来的样本他们的重要程度，也就是称为所有的主成分，我们取得前k个最重要的主成分，就可以将所有的样本映射到这k个轴上，获得一个低维的数据信息\n"},"PCA/6.sklearn中的PCA.html":{"url":"PCA/6.sklearn中的PCA.html","title":"6.sklearn中的PCA","keywords":"","body":"6.sklearn中的PCA\nimport matplotlib.pyplot as plt\nplt.scatter(X[:,0],X[:,1],color='b',alpha=0.5)\nplt.scatter(X_restore[:,0],X_restore[:,1],color='r',alpha=0.5)\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n加载书写识别数据集\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n\n# 分割数据集\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 666)\n\n使用64个维度的数据集，训练knn算法\n%%time\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train,y_train)\n\nCPU times: user 2.65 ms, sys: 1.46 ms, total: 4.11 ms\nWall time: 2.65 ms\nknn_clf.score(X_test,y_test)\n\n0.9866666666666667\n使用PCA进行降维，然后再训练knn算法\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca.fit(X_train)\nX_train_reduction = pca.transform(X_train)\nX_test_reduction = pca.transform(X_test)\n\n%%time\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train_reduction,y_train)\n\nCPU times: user 1.67 ms, sys: 935 µs, total: 2.61 ms\nWall time: 1.67 ms\n# 从64个维度降到两个维度以后，虽然运行速度提高了，但是识别精度大大降低了\nknn_clf.score(X_test_reduction,y_test)\n\n0.6066666666666667\nexplainedvariance_ratio-解释方差的比例\n\n0.14566817 代表第一个主成分可以解释14%的原数据\n0.13735469 代表第二个主成分可以解释13%的原数据\n两个主成分加起来可以解释百分之27的原数据，而其他的信息丢失了\n\n可以使用explainedvariance_ratio这个参数来查看每个主成分所解释的原数据，来判断要取多少个主成分\npca.explained_variance_ratio_\n\narray([0.14566817, 0.13735469])\npca = PCA(n_components=64)\npca.fit(X_train)\n# 这个数据可以近乎表示每个主成分轴的重要程度\npca.explained_variance_ratio_\n\narray([1.45668166e-01, 1.37354688e-01, 1.17777287e-01, 8.49968861e-02,\n5.86018996e-02, 5.11542945e-02, 4.26605279e-02, 3.60119663e-02,\n3.41105814e-02, 3.05407804e-02, 2.42337671e-02, 2.28700570e-02,\n1.80304649e-02, 1.79346003e-02, 1.45798298e-02, 1.42044841e-02,\n1.29961033e-02, 1.26617002e-02, 1.01728635e-02, 9.09314698e-03,\n8.85220461e-03, 7.73828332e-03, 7.60516219e-03, 7.11864860e-03,\n6.85977267e-03, 5.76411920e-03, 5.71688020e-03, 5.08255707e-03,\n4.89020776e-03, 4.34888085e-03, 3.72917505e-03, 3.57755036e-03,\n3.26989470e-03, 3.14917937e-03, 3.09269839e-03, 2.87619649e-03,\n2.50362666e-03, 2.25417403e-03, 2.20030857e-03, 1.98028746e-03,\n1.88195578e-03, 1.52769283e-03, 1.42823692e-03, 1.38003340e-03,\n1.17572392e-03, 1.07377463e-03, 9.55152460e-04, 9.00017642e-04,\n5.79162563e-04, 3.82793717e-04, 2.38328586e-04, 8.40132221e-05,\n5.60545588e-05, 5.48538930e-05, 1.08077650e-05, 4.01354717e-06,\n1.23186515e-06, 1.05783059e-06, 6.06659094e-07, 5.86686040e-07,\n1.71368535e-33, 7.44075955e-34, 7.44075955e-34, 7.15189459e-34])\n绘制曲线观察取前i个主成分的时候，所能解释的原数据比例\nplt.plot([i for i in range(X_train.shape[1])],\n[np.sum(pca.explained_variance_ratio_[:i+1]) for i in range(X_train.shape[1])])\n\n[]\n\nsklearn中的PCA算法支持传入一个小于1的数来表示我们希望能解释多少比例的主成分\npca = PCA(0.95)\npca.fit(X_train)\n# 说明前28个主成分表示了百分之95的信息\npca.n_components_\n\n28\nX_train_reduction = pca.transform(X_train)\nX_test_reduction = pca.transform(X_test)\n\n%%time\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train_reduction,y_train)\n\nCPU times: user 2.43 ms, sys: 1.21 ms, total: 3.63 ms\nWall time: 2.55 ms\n# 虽然训练出来的精度丢失了一些，但是效率却大大提高了\nknn_clf.score(X_test_reduction,y_test)\n\n0.98\nPCA降维到两维的意义-可以方便可视化展示，帮助人们理解\n下图每个颜色代表一个数字在降维到二维空间中的分布情况\n仔细观察后可以发现，很多数字的区分还是比较明细的\n比如如果只是区分蓝色的数字和紫色的数字，那么使用二个维度就足够了\npca = PCA(n_components=2)\npca.fit(X)\nX_reduction = pca.transform(X)\nfor i in range(10):\nplt.scatter(X_reduction[y==i,0],X_reduction[y==i,1],alpha=0.8)\n\n\nX_train_reduction.shape\n\n(1347, 2)\ny.shape\n\n(1797,)\n"},"PCA/试手MNIST数据集.html":{"url":"PCA/试手MNIST数据集.html","title":"7.试手MNIST数据集","keywords":"","body":"7.试手MNIST数据集\n1.加载MNIST数据集\nimport numpy as np\nfrom sklearn.datasets import fetch_mldata\n\nmnist = fetch_mldata(\"MNIST original\")\n\nX,y = mnist.data,mnist.target\n\nX_train = np.array(X[:60000],dtype=float)\ny_train = np.array(y[:60000],dtype=float)\nX_test = np.array(X[60000:],dtype=float)\ny_test = np.array(y[60000:],dtype=float)\n\n2.使用KNN\n\nsklearn 封装的KNeighborsClassifier，在fit过程中如果数据集较大，会以树结构的过程进行存储，以加快knn的预测过程，但是会导致fit过程变慢\n没有进行数据归一化，是因为这里的每个维度都标示的是每个像素点的亮度，他们的尺度是相同的，这个时候比较两个样本之间的距离是有意义的\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier()\n%time knn_clf.fit(X_train,y_train)\n\nCPU times: user 31.3 s, sys: 209 ms, total: 31.5 s\nWall time: 31.7 s\n\n\n\n\n\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n           weights='uniform')\n%time knn_clf.score(X_test,y_test)\n\nCPU times: user 10min 41s, sys: 2.55 s, total: 10min 44s\nWall time: 10min 47s\n\n\n\n\n\n0.9688\n3.PCA进行降维\nfrom sklearn.decomposition import PCA\n# 使用可以解释百分之90原数据集的主成分\npca = PCA(0.9)\npca.fit(X_train)\nX_train_reduction = pca.transform(X_train)\n# 从784维降到了87维，只用87维就可以解释百分之90的原数据集\nX_train.shape\n# (60000, 784)\nX_train_reduction\n\n(60000, 784)\nX_test_reduction = pca.transform(X_test)\nknn_clf = KNeighborsClassifier()\n%time knn_clf.fit(X_train_reduction,y_train)\n\nCPU times: user 352 ms, sys: 2.54 ms, total: 355 ms\nWall time: 356 ms\n\n\n\n\n\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n           weights='uniform')\n\n使用PCA进行降维后的数据集进行训练，不光时间变短了，准确度也变高了\n这是因为PCA的过程中，不仅仅是进行了降维，还在降维的过程中将数据包含的噪音给消除了\n这使得我们可以更加好的，更加准确的拿到我们数据集对应的特征，从而使得准确率大大提高\n\n%time knn_clf.score(X_test_reduction,y_test)\n\nCPU times: user 1min 2s, sys: 197 ms, total: 1min 2s\nWall time: 1min 2s\n\n\n\n\n\n0.9728\n"},"PCA/使用PCA对数据进行降噪.html":{"url":"PCA/使用PCA对数据进行降噪.html","title":"8.使用PCA对数据进行降噪","keywords":"","body":"8.使用PCA对数据进行降噪\n1.回忆我们之前的例子\nimport numpy as np\nimport matplotlib.pyplot as plt\nX = np.empty((100,2))\nX[:,0] = np.random.uniform(0.,100.,size=100)\n# 0.75倍的X[:,0]加上3加上一个噪音\nX[:,1] = 0.75*X[:,0]+3.+np.random.normal(0.,10.,size=100)\nplt.scatter(X[:,0],X[:,1],color='b',alpha=0.8)\n\n\n\n\n现在有一个问题：这个数据集展现出来这样的结果，可是是不是有这样一种情况，这个数据集就应该是一根直线呢\n换句话说，这个数据集展现的是在一根直线上下进行抖动式的分布，实际上这种抖动和这根直线本身的距离是噪音\n这个噪音的产生原因可能有很多，如测量人员的粗心，测量手段有问题等等原因，都会使得我们在现实世界中采集的数据\n是有噪音的\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=1)\npca.fit(X)\nX_reduction = pca.transform(X)\nX_restore = pca.inverse_transform(X_reduction)\nplt.scatter(X_restore[:,0],X_restore[:,1],color='b',alpha=0.8)\n\n\n\n\n我们使用PCA进行降维然后在反转回原来的维度，经过这样一个操作，可以发现此时这个数据\n就成为了一条直线，比较一下这两个图，我们可以说，经过这样的操作，我们将原有数据集的噪音\n给消除了\n当然，在实际情况下，我们不好说X_restore就是一点噪音都没有，也不好说原数据的所有的抖动全都是\n噪音，所以我们还是倾向于说从X到X_restore丢失了一些信息，不过我们丢失的信息很有可能有很大的一部分\n是噪音，这也解释了为什么我们在上一节降维处理以后，反而识别率提高了\n\n总结一句话就是：降低了维度，丢失了信息，同时也去除了一部分噪音\n2.手写识别的例子\nfrom sklearn import datasets\n\ndigits = datasets.load_digits()\nX,y = digits.data,digits.target\n\nnoisy_digits = X + np.random.normal(0,4,size=X.shape)\n\n# 获得每个标记加了噪音的10个元素，一共10个标记，公100个元素\nexample_digits = noisy_digits[y==0,:][:10]\nfor num in range(1,10):\n    X_num = noisy_digits[y==num,:][:10]\n    example_digits = np.vstack([example_digits,X_num])\nexample_digits.shape\n\n(100, 64)\n# 画出带噪音的图像\ndef plot_digits(data):\n    fig,axes = plt.subplots(10,10,figsize=(10,10),\n                           subplot_kw={'xticks':[],'yticks':[]},\n                            gridspec_kw=dict(hspace=0.1,wspace=0.1))\n    for i,ax in enumerate(axes.flat):\n        ax.imshow(data[i].reshape(8,8),\n                  cmap='binary',\n                  interpolation='nearest',\n                  clim=(0,16))\n\nplot_digits(example_digits)\n\n\n使用PCA降噪\n# 实际情况下，应该多试一些数字，找到最合适的数字\npca = PCA(0.5)\npca.fit(noisy_digits)\npca.n_components_\n\n12\ncomponents = pca.transform(noisy_digits)\nfiltered_digits = pca.inverse_transform(components)\nplot_digits(filtered_digits)\n# 相比之前，数字清楚了很多，平滑了很多，说明使用PCA进行降噪是可行的\n\n\n"},"PCA/特征脸.html":{"url":"PCA/特征脸.html","title":"9.人脸识别与特征脸","keywords":"","body":"9.人脸识别与特征脸\n1.高维数据向低维数据映射\n\n\n对于W这个矩阵来说，每一行代表一个方向，第一行是最重要的方向，第二行是次重要的方向\n如果将W中的每一行都看作一个样本的话，那么我们也可以说，第一行所代表的样本是最重要的那个样本，最能反应X这个矩阵原来的那个特征的样本\n在人脸识别领域中，X的每一行都是人脸，而W中的每一行，相应的也可以理解为是一个人脸，就是特征脸。之所以叫特征脸就是因为，每一行都能反应原来的样本的一个重要的特征。\n每一行实际上是一个主成分，他相当于表达了一部分原来的人脸数据中对应的一个特征\n\n2.实际编程用可视化的方式观察特征脸\n1.加载人脸数据库\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_lfw_people\n\nfaces = fetch_lfw_people()\n\nDownloading LFW metadata: https://ndownloader.figshare.com/files/5976012\nDownloading LFW metadata: https://ndownloader.figshare.com/files/5976009\nDownloading LFW metadata: https://ndownloader.figshare.com/files/5976006\nDownloading LFW data (~200MB): https://ndownloader.figshare.com/files/5976015\nfaces.data.shape\n\n(13233, 2914)\n# images是将我们的数据集以一个二维平面可视化的角度展现出来\n# 62*47 = 2914\nfaces.images.shape\n\n(13233, 62, 47)\n# 随机获取36张脸\nrandom_indexs = np.random.permutation(len(faces.data))\nX = faces.data[random_indexs]\nexample_faces = X[:36,:]\nexample_faces.shape\n\n(36, 2914)\ndef plot_digits(data):\n    fig,axes = plt.subplots(6,6,figsize=(10,10),\n                           subplot_kw={'xticks':[],'yticks':[]},\n                            gridspec_kw=dict(hspace=0.1,wspace=0.1))\n    for i,ax in enumerate(axes.flat):\n        ax.imshow(data[i].reshape(62,47),cmap='bone')\nplot_digits(example_faces)\n\n\n# 每张脸对应的人名\nfaces.target_names\n\narray(['AJ Cook', 'AJ Lamas', 'Aaron Eckhart', ..., 'Zumrati Juma',\n       'Zurab Tsereteli', 'Zydrunas Ilgauskas'], dtype='# 说明一共包含5749个不同的人的脸\nlen(faces.target_names)\n\n5749\n2.特征脸\n%%time\nX,y = faces.data,faces.target\nfrom sklearn.decomposition import PCA\n# 使用随机的方式来求解出PCA\n# 没有指定n_componets ，也就是说想求出所有的主成分\npca = PCA(svd_solver='randomized')\npca.fit(X)\n\nCPU times: user 1min 54s, sys: 2.73 s, total: 1min 57s\nWall time: 30.8 s\n# 一共2914个维度，所以求出了2914个主成分\npca.components_.shape\n\n(2914, 2914)\n使用所有的主成分绘制特征脸，\n\n可以看到排在前面的这些脸相应的比较笼统，排名第一的这张脸，告诉我们人脸大概就是这个位置，大概有这样一个轮廓\n越往后，鼻子眼睛的信息就清晰了起来\n\n通过求特征脸\n\n一方面我们可以方便直观的看出在人脸识别的过程中我们是怎么看到每一张脸相应的特征的\n另一方面，也可以看出来，其实每一张脸都是这些人脸的一个线性组合，而特征脸依据重要程度顺序的排在了这里\n\nplot_digits(pca.components_[:36])\n\n\n由于fetch_lfw_people这个库的人脸是分布不均匀的，有的人可能只有一张图片，有的人有几十张\n通过这个方法我们可以取出至少有60张脸的人的数据\nfaces2 = fetch_lfw_people(min_faces_per_person=60)\nX,y = faces2.data,faces2.target\n\n"},"多项式回归/":{"url":"多项式回归/","title":"7.多项式回归","keywords":"","body":"7.多项式回归\n"},"多项式回归/1.什么是多项式回归.html":{"url":"多项式回归/1.什么是多项式回归.html","title":"1.多项式回归简介","keywords":"","body":"1.多项式回归简介\n\n考虑下面的数据，虽然我们可以使用线性回归来拟合这些数据，但是这些数据更像是一条二次曲线,相应的方程是y=ax2+bx+c,这是式子虽然可以理解为二次方程，但是我们呢可以从另外一个角度来理解这个式子：\n如果将x2理解为一个特征，将x理解为另外一个特征,换句话说，本来我们的样本只有一个特征x，现在我们把他看成有两个特征的一个数据集。多了一个特征x2，那么从这个角度来看，这个式子依旧是一个线性回归的式子，但是从x的角度来看，他就是一个二次的方程\n\n\n以上这样的方式，就是所谓的多项式回归\n相当于我们为样本多添加了一些特征，这些特征是原来样本的多项式项，增加了这些特征之后，我们们可以使用线性回归的思路更好的我们的数据\n2.编程实验多项式回归\n1.模拟多项式回归的数据集\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.random.uniform(-3,3,size=100)\nX = x.reshape(-1,1)\n# 一元二次方程\ny = 0.5*x**2 + x + 2+np.random.normal(0,1,size=100)\n\nplt.scatter(x,y)\n\n\n\n2.使用线性回归拟合\n很明显，我们用一跟直线来拟合一根有弧度的曲线，效果是不好的\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X,y)\ny_predict = lin_reg.predict(X)\n\nplt.scatter(x,y)\nplt.plot(X,y_predict,color='r')\n\n[]\n\n3.解决方案，添加一个特征\n原来所有的数据都在X中，现在对X中每一个数据都进行平方，\n再将得到的数据集与原数据集进行拼接，\n在用新的数据集进行线性回归\n(X**2).shape\n\n(100, 1)\nX2 = np.hstack([X,X**2])\n\nlin_reg2 = LinearRegression()\nlin_reg2.fit(X2,y)\ny_predict2 = lin_reg2.predict(X2)\n\nplt.scatter(x,y)\n# 由于x是乱的，所以应该进行排序\nplt.plot(np.sort(x),y_predict2[np.argsort(x)],color='r')\n\n[]\n\n从上图可以看出，当我们添加了一个特征（原来特征的平方）之后，再从x的维度来看，就形成了一条曲线，显然这个曲线对原来数据集的拟合程度是更好的\n# 第一个系数是x前面的系数，第二个系数是x平方前面的系数\nlin_reg2.coef_\n\narray([1.08043759, 0.52423752])\nlin_reg2.intercept_\n\n1.9427736300237914\n3.总结\n多线性回归在机器学习算法上并没有新的地方，完全是使用线性回归的思路\n他的关键在于为原来的样本，添加新的特征。而我们得到新的特征的方式是原有特征的多项式的组合。\n采用这样的方式，我们就可以解决一些非线性的问题\n与此同时需要主要，我们在上一章所讲的PCA是对我们的数据进行降维处理，而我们这一章所讲的多项式回归显然在做一件相反的事情，他让我们的数据升维，在升维之后使得我们的算法可以更好的拟合高纬度的数据\n"},"多项式回归/scikit-learn中的多项式回归于pipeline.html":{"url":"多项式回归/scikit-learn中的多项式回归于pipeline.html","title":"2.scikit-learn中的多项式回归于pipeline","keywords":"","body":"2.scikit-learn中的多项式回归于pipeline\n1.使用scikit-learn中的多项式对数据进行预处理\n# sklearn中对数据进行预处理的函数都封装在preprocessing模块下，包括之前学的归一化StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures()\npoly.fit(X)\nX2 = poly.transform(X)\n# 第一列是sklearn为我们添加的X的零次方的特征\n# 第二列和原来的特征一样是X的一次方的特征\n# 第三列是添加的X的二次方的特征\nX2[:5]\n\narray([[ 1.        ,  2.5980174 ,  6.74969443],\n       [ 1.        ,  2.07484052,  4.30496317],\n       [ 1.        , -1.74999096,  3.06246837],\n       [ 1.        , -2.74141103,  7.51533441],\n       [ 1.        , -1.3420996 ,  1.80123135]])\n2.调用LinearRegression对X2进行预测\n\nlin_reg2 = LinearRegression()\nlin_reg2.fit(X2,y)\ny_predict2 = lin_reg2.predict(X2)\n\nplt.scatter(x,y)\n# 由于x是乱的，所以应该进行排序\nplt.plot(np.sort(x),y_predict2[np.argsort(x)],color='r')\n\n[]\n\nlin_reg2.coef_\n\narray([0.        , 1.08043759, 0.52423752])\n3.关于PolynomialFeatures\n# 测试多维的数据集\nX = np.arange(1,11).reshape(5,2)\nX.shape\n\n(5, 2)\nX\n\narray([[ 1,  2],\n       [ 3,  4],\n       [ 5,  6],\n       [ 7,  8],\n       [ 9, 10]])\npoly = PolynomialFeatures(degree=2)\npoly.fit(X)\nX2 = poly.transform(X)\nX2\n\narray([[  1.,   1.,   2.,   1.,   2.,   4.],\n       [  1.,   3.,   4.,   9.,  12.,  16.],\n       [  1.,   5.,   6.,  25.,  30.,  36.],\n       [  1.,   7.,   8.,  49.,  56.,  64.],\n       [  1.,   9.,  10.,  81.,  90., 100.]])\n将52的矩阵进行多项式转换后变成了56\n\n第一列是1 对应的是0次幂\n第二列和第三列对应的是原来的x矩阵，此时他有两列一次幂的项\n第四列是原来数据的第一列平方的结果\n第六列是原来数据的第二列平方的结果\n第五列是原来数据的两列相乘的结果\n\n可以想象如果将degree设置为3，那么将产生一下10个元素\n\n1,X1,X2\n\nX21,X22,X1*X2\n\nX13,X23,X12X2,X22X1 \n\n\n也就是说PolynomialFeatures会穷举出所有的多项式组合\n3.Pipline\npipline的英文名字是管道，那么 我们如何使用管道呢，先考虑我们多项式回归的过程\n1.使用```PolynomialFeatures```生成多项式特征的数据集\n2.如果生成数据幂特别的大，那么特征直接的差距就会很大，导致我们的搜索非常慢，这时候可以进行**数据归一化**\n3.进行线性回归\npipline 的作用就是把上面的三个步骤合并，使得我们不用一直重复这三步\nx = np.random.uniform(-3,3,size=100)\nX = x.reshape(-1,1)\n# 一元二次方程\ny = 0.5*x**2 + x + 2+np.random.normal(0,1,size=100)\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n# 传入每一步的对象名和类的实例化\npoly_reg = Pipeline([\n    ('poly',PolynomialFeatures(degree=2)),\n    ('std_scaler',StandardScaler()),\n    ('lin_reg',LinearRegression())\n])\n\npoly_reg.fit(X,y)\ny_predict = poly_reg.predict(X)\nplt.scatter(x,y)\nplt.plot(np.sort(x),y_predict[np.argsort(x)],color='r')\n\n[]\n\n"},"多项式回归/过拟合与前拟合.html":{"url":"多项式回归/过拟合与前拟合.html","title":"3.过拟合与前拟合","keywords":"","body":"3.过拟合与前拟合\n1.什么是过拟合和欠拟合\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nx = np.random.uniform(-3,3,size=100)\nX = x.reshape(-1,1)\ny = 0.5*x**2 + x + 2+np.random.normal(0,1,size=100)\n\nlin_reg = LinearRegression()\nlin_reg.fit(X,y)\ny_predict = lin_reg.predict(X)\n\nplt.scatter(x,y)\nplt.plot(X,y_predict,color='r')\n\n0.5406237455773699\n\n# 直接使用线性回归，显然分数太低\nlin_reg.score(X,y)\n\n0.5406237455773699\n\n使用均方误差来看拟合的结果，这是因为我们同样都是对一组数据进行拟合，所以使用不同的方法对数据进行拟合\n得到的均方误差的指标是具有可比性的，（但是对于多项式回归来说，使用r2score进行衡量是没有问题是）\n\nfrom sklearn.metrics import mean_squared_error\n\ny_predict = lin_reg.predict(X)\nmean_squared_error(y,y_predict)\n\n2.6112077267395803\n使用多项式回归\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# 使用Pipeline构建多项式回归\ndef PolynomialRegression(degree):\n    poly_reg = Pipeline([\n        ('poly',PolynomialFeatures(degree=degree)),\n        ('std_scaler',StandardScaler()),\n        ('lin_reg',LinearRegression())\n    ])\n    return poly_reg\npoly_reg2 = PolynomialRegression(2)\npoly_reg2.fit(X,y)\ny2_predict = poly_reg2.predict(X)\n# 显然使用多项式回归得到的结果是更好的\nmean_squared_error(y,y2_predict)\n\n1.000151338154146\nplt.scatter(x,y)\nplt.plot(np.sort(x),y2_predict[np.argsort(x)],color='r')\n\n[]\n\n使用更多的维度进行多项式回归\n# 使用10个维度\npoly_reg10 = PolynomialRegression(10)\npoly_reg10.fit(X,y)\ny10_predict = poly_reg10.predict(X)\nmean_squared_error(y,y10_predict)\n\n0.9394112675409493\nplt.scatter(x,y)\nplt.plot(np.sort(x),y10_predict[np.argsort(x)],color='r')\n\n[]\n\npoly_reg100 = PolynomialRegression(100)\npoly_reg100.fit(X,y)\ny100_predict = poly_reg100.predict(X)\n# 显然使用多项式回归得到的结果是更好的\nmean_squared_error(y,y100_predict)\n\n0.5431979125088253\nplt.scatter(x,y)\nplt.plot(np.sort(x),y100_predict[np.argsort(x)],color='r')\n\n[]\n\n\n这条曲线只是原来的点对应的y的预测值连接起来的曲线，不过有很多地方可能没有那个数据点，所以连接的结果和原来的曲线不一样\n下面尝试真正还原原来的曲线（构造均匀分布的原数据集）\n\nX_plot = np.linspace(-3,3,100).reshape(100,1)\ny_plot = poly_reg100.predict(X_plot)\n\nplt.scatter(x,y)\nplt.plot(X_plot,y_plot,color='r')\nplt.axis([-3 , 3 , -1,10 ])\n\n[-3, 3, -1, 10]\n\n说明总有一条曲线，他能拟合所有的样本点，使得均方误差的值为0\ndegree从2到10到100的过程中，虽然均方误差是越来越小的，从均方误差的角度来看是更加小的\n但是他真的能更好的预测我们数据的走势吗，例如我们选择2.5到3的一个x，使用上图预测出来的y的大小（0或者-1之间）显然不符合我们的数据\n\n换句话说，我们使用了一个非常高维的数据，虽然使得我们的样本点获得了更小的误差，但是这根曲线完全不是我们想要的样子\n他为了拟合我们所有的样本点，变的太过复杂了，这种情况就是过拟合【over-fitting】\n相反，在最开始，我们直接使用一根直线来拟合我们的数据，也没有很好的拟合我们的样本特征，当然他犯的错误不是太过复杂了，而是太过简单了\n这种情况，我们成为欠拟合-【under-fitting】\n\n对于现在的数据（基于二次方程构造），我们使用低于2项的拟合结果，就是欠拟合；高于2项的拟合结果，就是过拟合\n2.为什么要使用训练数据集和测试数据集\n模型的泛化能力\n使用上小节的过拟合结果，我们可以得知，虽然我们训练出的曲线将原来的样本点拟合的非常好，总体的误差非常的小，\n但是一旦来了新的样本点，他就不能很好的预测了，在这种情况下，我们就称我们得到的这条弯弯曲曲的曲线，他的泛化能力（由此及彼的能力）非常弱\n\n训练数据集和测试数据集的意义\n我们训练的模型目的是为了使得预测的数据能够尽肯能的准确，在这种情况下，我们观察训练数据集的拟合程度是没有意义的\n我们真正需要的是，我们得到的模型的泛化能力更高，解决这个问题的方法也就是使用训练数据集，测试数据集的分离\n\n\n测试数据对于我们的模型是全新的数据，如果使用训练数据获得的模型面对测试数据也能获得很好的结果，那么我们就说我们的模型泛化能力是很强的。\n如果我们的模型面对测试数据结果很差的话，那么他的泛化能力就很弱。事实上，这是训练数据集更大的意义\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y)\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train)\ny_predict = lin_reg.predict(X_test)\n# 训练模型使用的X_train，是预测的模型使用X_test，以计算模型的泛化能力\nmean_squared_error(y_test,y_predict)\n\n2.7714817137686794\npoly_reg2 = PolynomialRegression(2)\npoly_reg2.fit(X_train,y_train)\ny2_predict = poly_reg2.predict(X_test)\nmean_squared_error(y_test,y2_predict)\n\n0.7922037464116539\npoly_reg10 = PolynomialRegression(10)\npoly_reg10.fit(X_train,y_train)，\ny10_predict = poly_reg10.predict(X_test)\nmean_squared_error(y_test,y10_predict)\n\n1.336192585265726\n使用degree=10的时候得到的均方误差要大于degree=2的时候，说明当degree等于10的时候，他的模型泛化能力变弱了\npoly_reg100 = PolynomialRegression(100)\npoly_reg100.fit(X_train,y_train)\ny100_predict = poly_reg100.predict(X_test)\nmean_squared_error(y_test,y100_predict)\n\n4.192433747323001e+21\n刚刚我们进行的实验实际上在实验模型的复杂度，对于多项式模型来说，我们回归的阶数越高，我们的模型会越复杂，在这种情况下对于我们的机器学习算法来说，通常是有下面一张图的。横轴是模型复杂度（对于不同的算法来说，代表的是不同的意思，比如对于多项式回归来说，是阶数越高，越复杂；对于KNN来说，是K越小，模型越复杂，k越大，模型最简单，当k=n的时候，模型就简化成了看整个样本里，哪种样本最多，当k=1来说，对于每一个点，都要找到离他最近的那个点），另一个维度是模型准确率（也就是他能够多好的预测我们的曲线）\n\n通常对于这样一个图，会有两根曲线：\n\n一个是对于训练数据集来说的，模型越复杂，模型准确率越高，因为模型越复杂，对训练数据集的拟合就越好，相应的模型准确率就越高\n对于测试数据集来说，在模型很简单的时候，模型的准确率也比较低，随着模型逐渐变复杂，对测试数据集的准确率在逐渐的提升，提升到一定程度后，如果模型继续变复杂，那么我们的模型准确率将会进行下降（欠拟合->正合适->过拟合）\n\n欠拟合和过拟合的标准定义\n\n欠拟合：算法所训练的模型不能完整表述数据关系\n过拟合：算法所训练的模型过多的表达了数据间的噪音关系\n\n"},"多项式回归/学习曲线.html":{"url":"多项式回归/学习曲线.html","title":"4.学习曲线","keywords":"","body":"4.学习曲线\n1. 什么是学习曲线\n\n随着训练样本的主键增多，算法训练出的模型的表现能力\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(666)\nx = np.random.uniform(-3,3,size=100)\nX = x.reshape(-1,1)\ny = 0.5 * x**2 + x + 2 + np.random.normal(0,1,size=100)\n\nplt.scatter(x,y)\n\n\n\n2.实际编程实现学习曲线\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=10)\nX_train.shape\n\n(75, 1)\n2.1观察线性回归的学习曲线：观察线性回归模型，随着训练数据集增加，性能的变化\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\ndef plot_learning_curve(algo,X_train,X_test,y_train,y_test):\n\n    train_score = []\n    test_score = []\n\n    # 计算学习曲线数据\n    for i in range(1,len(X_train)+1):\n        algo.fit(X_train[:i],y_train[:i])\n\n        y_train_predict = algo.predict(X_train[:i])\n        train_score.append(mean_squared_error(y_train[:i],y_train_predict))\n\n        y_test_predict = algo.predict(X_test)\n        test_score.append(mean_squared_error(y_test,y_test_predict))\n\n    # 绘制学习曲线\n    plt.plot([i for i in range(1,len(X_train)+1)],np.sqrt(train_score),label = 'train')\n    plt.plot([i for i in range(1,len(X_train)+1)],np.sqrt(test_score),label = 'test')\n    plt.axis([0,len(X_train)+1,0,4])\n    plt.legend()\n\nplot_learning_curve(LinearRegression(),X_train,X_test,y_train,y_test)\n\n\n从趋势上看：\n\n在训练数据集上，误差是逐渐升高的。这是因为我们的训练数据越来越多，我们的数据点越难得到全部的累积，不过整体而言，在刚开始的时候误差变化的比较快，后来就几乎不变了\n在测试数据集上，在使用非常少的样本进行训练的时候，刚开始我们的测试误差非常的大，当训练样本大到一定程度以后，我们的测试误差就会逐渐减小，减小到一定程度后，也不会小太多，达到一种相对稳定的情况\n在最终，测试误差和训练误差趋于相等，不过测试误差还是高于训练误差一些，这是因为，训练数据在数据非常多的情况下，可以将数据拟合的比较好，误差小一些，但是泛化到测试数据集的时候，还是有可能多一些误差\n\n2.2 观察多项式回归的学习曲线\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# 使用Pipline构建多项式回归模型\ndef PolynomialRegression(degree):\n    return Pipeline([\n        (\"poly\",PolynomialFeatures(degree=degree)),\n        (\"std_scaler\",StandardScaler()),\n        (\"lin_reg\",LinearRegression())\n    ])\n\n# 使用二阶多项式回归\npoly2_reg = PolynomialRegression(2)\nplot_learning_curve(poly2_reg,X_train,X_test,y_train,y_test)\n\n\n首先整体从趋势上，和线性回归的学习曲线是类似的\n仔细观察，和线性回归曲线的不同在于，线性回归的学习曲线1.5，1.8左右；2阶多项式回归稳定在了1.0，0.9左右,2阶多项式稳定的误差比较低，说明\n使用二阶线性回归的性能是比较好的\n# 使用20阶多项式回归\npoly20_reg = PolynomialRegression(20)\nplot_learning_curve(poly20_reg,X_train,X_test,y_train,y_test)\n\n\n在使用20阶多项式回归训练模型的时候可以发现，在数据量偏多的时候，我们的训练数据集拟合的是比较好的，但是测试数据集的误差相对来说增大了很多，离训练数据集比较远，通常这就是过拟合的结果，他的泛化能力是不够的\n3.总结\n\n对于欠拟合比最佳的情况趋于稳定的那个位置要高一些，说明无论对于训练数据集还是测试数据集来说，误差都比较大。这是因为我们本身模型选的就不对，所以即使在训练数据集上，他的误差也是大的，所以才会呈现出这样的一种形态\n\n对于过拟合的情况，在训练数据集上，他的误差不大，和最佳的情况是差不多的，甚至在极端情况，如果degree取更高的话，那么训练数据集的误差会更低，但是问题在于，测试数据集的误差相对是比较大的，并且训练数据集的误差和测试数据集的误差相差比较大（表现在图上相差比较远），这就说明了此时我们的模型的泛化能力不够好，他的泛化能力是不够的\n"},"多项式回归/验证数据集与交叉验证.html":{"url":"多项式回归/验证数据集与交叉验证.html","title":"5.验证数据集与交叉验证","keywords":"","body":"5.验证数据集与交叉验证\n使用分割训练数据集和测试数据集来判断我们的机器学习性能的好坏，虽然是一个非常好的方案，但是会产生一个问题：针对特定测试数据集过拟合\n我们每次使用测试数据来分析性能的好坏。一旦发现结果不好，我们就换一个参数（可能是degree也可能是其他超参数）重新进行训练。这种情况下，我们的模型在一定程度上围绕着测试数据集打转。也就是说我们在寻找一组参数，使得这组参数训练出来的模型在测试结果集上表现的最好。但是由于这组测试数据集是已知的，我们相当于在针对这组测试数据集进行调参，那么他也有可能产生过拟合的情况，也就是我们得到的模型针对测试数据集过拟合了\n\n那么怎么解决这个问题呢？\n解决的方式其实就是：我们需要将我们的问题分为三部分，这三部分分别是训练数据集，验证数据集，测试数据集。\n我们使用训练数据集训练好模型之后，将验证数据集送给这个模型，看看这个训练数据集训练的效果是怎么样的，如果效果不好的话，我们重新换参数，重新训练模型。直到我们的模型针对验证数据来说已经达到最优了。\n这样我们的模型达到最优以后，再讲测试数据集送给模型，这样才能作为衡量模型最终的性能。换句话说，我们的测试数据集是不参与模型的创建的，而其他两个数据集都参与了训练。但是我们的测试数据集对于模型是完全不可知的，相当于我们在模型这个模型完全不知道的数据\n\n这种方法还会有一个问题。由于我们的模型可能会针对验证数据集过拟合，而我们只有一份验证数据集，一旦我们的数据集里有比较极端的情况，那么模型的性能就会下降很多，那么为了解决这个问题，就有了交叉验证。\n1.交叉验证 Cross Validation\n交叉验证相对来说是比较正规的、比较标准的在我们调整我们的模型参数的时候看我们的性能的方式\n交叉验证：在训练模型的时候，通常把数据分成k份，例如分成3份（ABC）（分成k分，k属于超参数），这三份分别作为验证数据集和训练数据集。这样组合后可以分别产生三个模型，这三个模型，每个模型在测试数据集上都会产生一个性能的指标，这三个指标的平均值作为当前这个算法训练处的模型衡量的标准是怎样的。\n由于我们有一个求平均的过程，所以不会由于一份验证数据集中有比较极端的数据而导致模型有过大的偏差，这比我们只分成训练、验证、测试数据集要更加准确\n2.编程实现\nimport numpy as np\nfrom sklearn import datasets\n\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n\n训练train_test_spilt\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,random_state =666)\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nbest_score,best_k,best_p = 0,0,0\n# k为k近邻中的寻找k个最近元素\nfor k in range(2,10):\n    # p为明科夫斯基距离的p\n    for p in range(1,5):\n        knn_clf = KNeighborsClassifier(weights='distance',n_neighbors=k,p=p)\n        knn_clf.fit(X_train,y_train)\n        score = knn_clf.score(X_test,y_test)\n        if score > best_score:\n            best_score,best_k,best_p = score,k,p\nprint(\"Best_score = \",best_score)    \nprint(\"Best_k = \",best_k)            \nprint(\"Best_p = \",best_p)\n\nBest_score =  0.9860917941585535\nBest_k =  3\nBest_p =  4\n使用交叉验证\n# 使用sklearn提供的交叉验证\nfrom sklearn.model_selection import cross_val_score\n\nknn_clf = KNeighborsClassifier()\n# 返回的是一个数组，有三个元素，说明cross_val_score方法默认将我们的数据集分成了三份\n# 这三份数据集进行交叉验证后产生了这三个结果\n\n# cv默认为3，可以修改改参数，修改修改不同分数的数据集\ncross_val_score(knn_clf,X_train,y_train,cv=3)\n\narray([0.98895028, 0.97777778, 0.96629213])\n# 使用交叉验证的方式来进行调参的过程\nbest_score,best_k,best_p = 0,0,0\n# k为k近邻中的寻找k个最近元素\nfor k in range(2,10):\n    # p为明科夫斯基距离的p\n    for p in range(1,5):\n        knn_clf = KNeighborsClassifier(weights='distance',n_neighbors=k,p=p)\n        scores = cross_val_score(knn_clf,X_train,y_train)\n        score = np.mean(scores)\n        if score > best_score:\n            best_score,best_k,best_p = score,k,p\nprint(\"Best_score = \",best_score)    \nprint(\"Best_k = \",best_k)            \nprint(\"Best_p = \",best_p)\n\nBest_score =  0.9823599874006478\nBest_k =  2\nBest_p =  2\n通过观察两组调参过程的结果可以发现\n1.两组调参得出的参数结果是不同的，通常这时候我们更愿意详细使用交叉验证的方式得出的结果。\n  因为使用train_test_split很有可能只是过拟合了测试数据集得出的结果\n2.使用交叉验证得出的最好分数0.982是小于使用分割训练测试数据集得出的0.986，因为在交叉验证的\n  过程中，通常不会过拟合某一组的测试数据，所以平均来讲这个分数会稍微低一些\n但是使用交叉验证得到的最好参数Best_score并不是真正的最好的结果，我们使用这种方式只是为了拿到\n一组超参数而已，拿到这组超参数后我们就可以训练处我们的最佳模型\nknn_clf = KNeighborsClassifier(weights='distance',n_neighbors=2,p=2)\n# 用我们找到的k和p。来对X_train,y_train整体fit一下，来看他对X_test,y_test的测试结果\nknn_clf.fit(X_train,y_train)\n# 注意这个X_test,y_test在交叉验证过程中是完全没有用过的，也就是说我们这样得出的结果是可信的\nknn_clf.score(X_test,y_test)\n\n0.980528511821975\n回顾网格搜素\n我们上面的操作，实际上在网格搜索的过程中已经进行了，只不过这个过程是sklean的网格搜索自带的一个过程\n# GridSearchCV里的cv实际上就是交叉验证的方式\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {\n        \"weights\":['distance'],\n        \"n_neighbors\":[i for i in range(2,10)],\n        \"p\":[i for i in range(1,6)]\n    }\n]\nknn_clf = KNeighborsClassifier()\n# cv默认为3，可以修改改参数，修改修改不同分数的数据集\ngrid_search = GridSearchCV(knn_clf,param_grid,verbose=1,cv=3)\ngrid_search.fit(X_train,y_train)\n\nFitting 3 folds for each of 40 candidates, totalling 120 fits\n\n\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  1.0min finished\n\n\n\n\n\nGridSearchCV(cv=None, error_score='raise',\n       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n           weights='uniform'),\n       fit_params=None, iid=True, n_jobs=1,\n       param_grid=[{'weights': ['distance'], 'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9], 'p': [1, 2, 3, 4, 5]}],\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=1)\nFitting 3 folds for each of 40 candidates, totalling 120 fits\n 的意思就是交叉验证中分割了三组数据集，而我们的参数组合为8*5=40中组合\n3组数据集，30种组合，一共要进行120次的训练\ngrid_search.best_score_\n# 0.9823747680890538 和我们上面得到的best_score 是吻合的\n\n0.9823747680890538\ngrid_search.best_params_\n\n{'n_neighbors': 2, 'p': 2, 'weights': 'distance'}\nbest_knn_clf = grid_search.best_estimator_\nbest_knn_clf.fit(X_train,y_train)\nbest_knn_clf.score(X_test,y_test)\n\n0.980528511821975\n3.总结\n\n虽然整体速度慢了，但是这个结果却是可信赖的\n极端情况下，K-folds cross validation可以叫做留一法\n\n"},"多项式回归/偏差方差均衡.html":{"url":"多项式回归/偏差方差均衡.html","title":"6.偏差方差均衡","keywords":"","body":"6.偏差方差均衡\n\n模型误差=偏差（Bias）均差(Variance)+不可避免的误差\n偏差\n\n方差\n模型没有完全的学到数据的中心，而学习到了很多噪音\n\n\n\n有一些算法天生是高方差算法。如KNN（过于依赖数据，一点选取的数据点有多数是不正确的，那么预测的结果就是错误的。导致有的很准确，有的非常不准确，方差非常大）\n非参数学习通常都是高方差的算法。因为不对数据做任何假设\n有一些算法天生是高偏差算法。如线性回归（用一条直线去拟合一条曲线，导致整体预测结果都距离真实数据查很大，偏差非常大）\n参数学习通常都是高偏差的算法。因为对数据具有极强的假设\n\n大多数算法具有相应的参数，可以调整偏差和方差\n\n如KNN中的k，线性回归中使用多项式回归\n偏差和方差是互相矛盾的。降低方差会提高偏差，降低偏差会提高方差\n\n\n机器学习的主要调整来源于方差（这是站在算法的角度上，而不是问题的角度上,比如对金融市场的理解，很多人尝试用历史的数据预测未来的金融走势，这样的尝试通常都不太理想。很有可能因为历史的金融趋势不能很好的反应未来的走向，这种预测方法本身带来的非常大的偏差）换句话说，我们很容易让模型变的很复杂，从而降低模型的偏差，但是由于这样的模型的方差非常的大，最终也没有很好的性能。\n\n解决高方差的通常手段：\n1.降低模型复杂度\n2.减少数据维度；降噪\n3.增加样本数（模型太过复杂，模型中的参数非常多，而样本数不足以支撑计算出这么复杂的参数）\n4.使用验证集\n5.模型正则化\n\n"},"多项式回归/模型正则化.html":{"url":"多项式回归/模型正则化.html","title":"7.模型正则化","keywords":"","body":"7.模型正则化-Regularization\n1.什么是模型正则化\n下图是我们之前使用多项式回归过拟合一个样本的例子，可以看到这条模型曲线非常的弯曲，而且非常的陡峭，可以想象这条曲线的一些θ系数会非常的大。\n模型正则化需要做的事情就是限制这些系数的大小\n\n模型正则化基本原理\n\n\n一些需要注意的细节：\n\n对于θ的求和i是从1到n,没有将θ0加进去，因为他不是任意一项的系数，他只是一个截距，决定了整个曲线的高低，但是不决定曲线每一部分的陡峭和缓和\nθ求和的系数二分之一是一个惯例，加不加都可以，加上的原因是因为，将来对θ2>求导的时候可以抵消系数2，方便计算。不要也是可以的\nα实际上是一个超参数，代表在我们模型正则化下新的损失函数中，我们要让每一个θ尽可能的小，小的程度占我们整个损失函数的多少，如果α等于0，相当于没有正则化；如果α是正无穷的话，那么我们主要的优化任务就是让每一个θ尽可能的小\n\n\n岭回归 Ridge Regression\n\n2.编程实现岭回归\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 模型样本\nnp.random.seed(42)\nx = np.random.uniform(-3.0,3.0,size=100)\nX = x.reshape(-1,1)\ny = 0.5 * x + 3 + np.random.normal(0,1,size=100)\n\n# 绘制样本曲线\nplt.scatter(x,y)\n\n\n\ny.shape\n\n(100,)\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\n\n# 定义多项式回归函数\ndef PolynomialRegression(degree):\n    return Pipeline([\n        (\"poly\",PolynomialFeatures(degree=degree)),\n        (\"std_scaler\",StandardScaler()),\n        (\"lin_reg\",LinearRegression())\n    ])\n\nfrom sklearn.model_selection import train_test_split\n\n# 分割数据集\nnp.random.seed(666)\nX_train,X_test,y_train,y_test = train_test_split(X,y)\n\nfrom sklearn.metrics import mean_squared_error\n\n# 多项式回归对样本进行训练，使用20个维度\npoly20_reg = PolynomialRegression(20)\npoly20_reg.fit(X_train,y_train)\n\ny20_predict = poly20_reg.predict(X_test)\nmean_squared_error(y_test,y20_predict)\n\n167.94010860151894\n# 定义绘图模型\ndef plot_module(module):\n    X_plot = np.linspace(-3,3,100).reshape(100,1)\n    y_plot = module.predict(X_plot)\n\n    plt.scatter(x,y)\n    plt.plot(X_plot[:,0],y_plot,color='r')\n    plt.axis([-3,3,0,6])\n\n# 绘制模型曲线--过拟合（非常的完全，两段有极端的情况）\nplot_module(poly20_reg)\n\n\n使用岭回归\nfrom sklearn.linear_model import Ridge\n\ndef RidgeRegression(degree,alpha):\n    return Pipeline([\n        (\"poly\",PolynomialFeatures(degree=degree)),\n        (\"std_scaler\",StandardScaler()),\n        (\"ridge_reg\",Ridge(alpha=alpha))\n    ])\n\n# 注意alpha后面的参数是所有theta的平方和，而对于多项式回归来说，岭回归之前得到的θ都非常大\n# 所以为了限制让他们比较小，我们前面系数可以取的小一些\nridge1_reg = RidgeRegression(degree=20,alpha=0.00001)\nridge1_reg.fit(X_train,y_train)\nridge1_predict = ridge1_reg.predict(X_test)\nmean_squared_error(y_test,ridge1_predict)\n\n1.387437803144217\n# 通过使用岭回归，使得我们的均方误差小了非常多,曲线也缓和了非常多\nplot_module(ridge1_reg)\n\n\nridge2_reg = RidgeRegression(degree=20,alpha=1)\nridge2_reg.fit(X_train,y_train)\nridge2_predict = ridge2_reg.predict(X_test)\nmean_squared_error(y_test,ridge2_predict)\n\n1.1888759304218461\n# 让ridge2_reg 的alpha值等于1，均差误差更加的缩小，并且曲线越来越趋近于一根倾斜的直线\nplot_module(ridge2_reg)\n\n\nridge3_reg = RidgeRegression(degree=20,alpha=100)\nridge3_reg.fit(X_train,y_train)\nridge3_predict = ridge3_reg.predict(X_test)\nmean_squared_error(y_test,ridge3_predict)\n\n1.31964561130862\n# 得到的误差依然是比较小，但是比之前的1.18大了些，说明正则化做的有些过头了\nplot_module(ridge3_reg)\n\n\nridge4_reg = RidgeRegression(degree=20,alpha=100000)\nridge4_reg.fit(X_train,y_train)\nridge4_predict = ridge4_reg.predict(X_test)\nmean_squared_error(y_test,ridge4_predict)\n# 当alpha非常大，我们的模型实际上相当于就是在优化θ的平方和这一项，使得其最小（因为MSE的部分相对非常小）\n# 而使得θ的平方和最小，就是使得每一个θ都趋近于0，这个时候曲线就趋近于一根直线了\nplot_module(ridge4_reg)\n\n\n"},"多项式回归/LASSO.html":{"url":"多项式回归/LASSO.html","title":"8.LASSO","keywords":"","body":"8.LASSO\n使用|θ|代替θ2来标示θ的大小\n\nSelection Operator -- 选择运算符\nLASSO回归有一些选择的功能\n\n1. 实际编程（准备代码参考上一节岭回归）\nfrom sklearn.linear_model import Lasso\n\ndef LassoRegression(degree,alpha):\n    return Pipeline([\n        (\"poly\",PolynomialFeatures(degree=degree)),\n        (\"std_scatter\",StandardScaler()),\n        (\"lasso_reg\",Lasso(alpha=alpha))\n    ])\n\n# 这里穿的alpha起始值比岭回归的时候大了很多，是由于现在是绝对值\nlasso1_reg = LassoRegression(degree=20,alpha=0.01)\nlasso1_reg.fit(X_train,y_train)\nlasso1_predict = lasso1_reg.predict(X_test)\nmean_squared_error(lasso1_predict,y_test)\n\n1.1496080843259968\nplot_module(lasso1_reg)\n\n\n# 增大alpha继续试验\nlasso2_reg = LassoRegression(degree=20,alpha=0.1)\nlasso2_reg.fit(X_train,y_train)\nlasso2_predict = lasso2_reg.predict(X_test)\nmean_squared_error(lasso2_predict,y_test)\n\n1.1213911351818648\n# 非常接近一根直线\nplot_module(lasso2_reg)\n\n\n# 增大alpha继续试验\nlasso3_reg = LassoRegression(degree=20,alpha=1)\nlasso3_reg.fit(X_train,y_train)\nlasso3_predict = lasso3_reg.predict(X_test)\nmean_squared_error(lasso3_predict,y_test)\n\n1.8408939659515595\n# alpha=1的时候正则化已经过头了\nplot_module(lasso3_reg)\n\n\n2. 总结Ridge和Lasso\n\nα=100的时候，使用Ridge的得到的模型曲线依旧是一根曲线，事实上，使用Ridge很难得到一根倾斜的直线，他一直是弯曲的形状\n但是使用LASSO的时候，当α=0.1，虽然得到的依然是一根曲线，但是他显然比Radge的程度更低，更像一根直线\n这是因为LASSO趋向于使得一部分theta值为0（而不是很小的值），所以可以作为特征选择用，LASSO的最后两个字母SO就是Selection Operator的首字母缩写\n使用LASSO的过程如果某一项θ等于0了，就说明LASSO Regression认为这个θ对应的特征是没有用的，剩下的那些不等于0的θ就说明LASSO Regression认为对应的这些特征有用，所以他可以当做特征选择用\n\n\n当使用Ridge的时候，当α趋近与无穷大，那么使用梯度下降法的J(θ)的导数如下图，J(θ)向0趋近的过程中，每个θ都是有值的\n\n\n\n但是LASSO不同,在LASSO的损失函数中，如果我们让α趋近于无穷，只看后面一部分的话，那么后面一部分的绝对值实际上是不可导的，我们可以使用一种sign函数刻画一下绝对值导数，如下图。那么这个时候，同样在J(θ)向0趋近的过程中，他会先走到θ等于0的y轴位置，然后再沿着y轴往下向零点的方向走\n\n\n这也说明了Ridge为什么叫岭回归，因为他更像是翻山越岭一样，在梯度下降法中一点一点找坡度缓的方向前进。而LASSO的路径就比较规则，会在训练的过程中碰到一些轴使得某些θ为0。\n所以从计算准确度上来说，我们应该更加倾向于Ridge，但是如果我们的维度比较多，样本非常大（比如多项式回归时degree=100）\n"},"多项式回归/L1,L2和弹性网络.html":{"url":"多项式回归/L1,L2和弹性网络.html","title":"9.L1,L2和弹性网络","keywords":"","body":"9.L1,L2和弹性网络\nRidge和LASSO都是在损失函数中添加一项，来调节θ的值使其尽可能的小，使得我们的模型泛化能力更好一些\n\n在机器学习领域中，我们会发明不同的名词来描述不同的标准，比如用Ridge和LASSO来衡量正则化的这一项；MSE和MAE用来衡量回归结果的好坏，欧拉距离和曼哈顿距离用来衡量两点之间的距离。但是他们背后的数学思想是非常的类似的，表达出的数学含义也是一致的。只不过应用到不同的场景中产生了不同的效果\n\n1.L1,L2正则\n对明克夫斯基距离进一步泛化\n\n\n对任意一个维度X，我们都可以求这样一个值，他的每一个值x都对他的p次方进行求和，再开p次方根。通常将这个式子成为LP范数。\n当P=1的时候就是L1范数（曼哈顿距离|Ridge Regression），当P=2的时候就是 L2范数（欧拉距离|LASSO Regression）\n\n\n注：有了L1,L2正则项，我们就可以进一步得到LN正则项，虽然实际应用中我们的n不会超过2，但是在数学推导中是有意义的\n2.L0正则项\n我们希望让θ的个数尽量小，描述的是非零θ元素的个数。我们用这样的方式来限制θ的数量尽可能的小，进而来限制我们的曲线不要太抖\n不过实际上我们是很少使用L0正则的，因为L0正则的优化是一个NP难的问题，我们不能使用诸如梯度下降法甚至数学公式来找到一个最优解。\n他是一个离散的值，我们需要穷举所有θ的值来找出哪些θ需要，哪些不需要。实际上可以用L1正则（LASS0 Regression）来替代，从而达到选择去掉一些θ的过程\n\n3.弹性网 Elastic NET\n\n在损失函数下，添加上一个L1正则项和一个L2正则项，并引入一个参数r来表示他们之间的比例。同时结合了岭回归和LASSO回归的优势\n\n\n实际应用中，通常应该先尝试一下岭回归（如果计算能力足够的话）。但是如果θ数量太大的话，消耗计算资源可能非常大，而LASSO由于有的时候急于把一些θ化为0，可能会导致得到的偏差比价大。这个时候需要使用弹性网\n回忆小批量梯度下降法也是将随机梯度下降法和批量梯度下降法结合到了一起。在机器学习领域中，经常使用这种方式来创造出一些新的方法，这些方法虽然名词非常的酷，但是他们背后的意义是非常简单的\n模型泛化的一个举例。我们在考试前会做很多练习题。我们做练习题不是为了把全部的练习题（训练数据集）都得到满分，而是为了在最后的那一场考试（真实数据）中得到满分\n"},"逻辑回归/":{"url":"逻辑回归/","title":"8.逻辑回归","keywords":"","body":"8.逻辑回归\n"},"逻辑回归/1.什么是逻辑回归.html":{"url":"逻辑回归/1.什么是逻辑回归.html","title":"1.什么是逻辑回归","keywords":"","body":"1.什么是逻辑回归\n逻辑回归：解决分类问题\n回归问题怎么解决分类问题？\n将样本的特征和样本发生的概率联系起来，概率是一个数。\n对于线性回归来说，我们得到一个函数f，将样本x输入f后，得到的值y就是要预测的值；\n而对于逻辑回归来说，我们要得到一个函数f，我们将样本x输入f以后，f会计算出y一个概率值p，之后我们使用这个概率值p来进行分类，如果p>=0.5,也就是有百分之50以上的概率发生的话，我们就让这个概率的值为1，否则让他为1，当然1和0在不同的场景下代表不同的意思。\n\n线性回归计算出来的值域是负无穷到正无穷，而我们使用逻辑回归得出来的p是只取0到之间的个值的。这使得我们不能直接使用线性回归的方法，单单从应用的角度来说，但是这样做不够好，因为我们的逻辑回归的值域是有限制的，使用线性回归或者多项式回归拟合出来的直线或者曲线肯定会比较差。\n我们可以在线性回归的结果基础上，添加一个σ函数，将结果转换成0到1之间\n\n\n绘制sigmoid函数\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef sigmoid(t):\n    return 1/(1+np.exp(-t))\n\nx = np.linspace(-10,10,100)\ny = sigmoid(x)\n\nplt.plot(x,y)\n\n\n"},"逻辑回归/2.逻辑回归的损失函数.html":{"url":"逻辑回归/2.逻辑回归的损失函数.html","title":"2.逻辑回归的损失函数","keywords":"","body":"2.逻辑回归的损失函数\n对于逻辑回归，定义他的损失函数比较困难。\n\n\n\n当p=0的时候，按照我们的分类，我们应该把样本分为y^=0(注意这里是预测值)这一类。但是这个样本实际是y=1(注意这里是真值)，显然我们分错了，此时，我们对他进行惩罚，这个惩罚是正无穷的；随着p逐渐变大，我们的损失越来越小。当p=1的时候，我们会将样本分类为y=1，此时和这个样本真实的y=1是一致的，那么此时-log(0)也就是没有任何损失。\n另一半式子反之亦然。\n\n（注意区别if 中的y=1是真值；我们使用σ函数求出来的是预测值）\n\n我们用两个损失函数训练模型是不太方便的，可以将他们合成一个式子如下。当y=1的时候，后半部分是0，所以就只剩下-log(p^).当y=0的时候，前半部分是0，就只剩下-log(1-p^)。\n\n\n"},"逻辑回归/3.逻辑回归函数损失的梯度.html":{"url":"逻辑回归/3.逻辑回归函数损失的梯度.html","title":"3.逻辑回归函数损失的梯度","keywords":"","body":"3.逻辑回归函数损失的梯度\n\n1.对σ(t)求导\n\n2.对log(σ(t))进行求导\n\n\n3.对J(θ)的前一部分求导\n\n4.对log(1-σ(t))进行求导\n\n5.对J(θ)的后一部分求导\n\n6.整合两部分的求导结果\n\n\n\n7.整合对J(θ)所有θ的求导结果\n\n8.对比之前的线性回归的损失函数的导数形式，找到相似点\n\n\n9.向量化\n\n"},"逻辑回归/4.实现逻辑回归算法.html":{"url":"逻辑回归/4.实现逻辑回归算法.html","title":"4.实现逻辑回归算法","keywords":"","body":"4.实现逻辑回归算法\n1.逻辑回归算法的封装实现\nclass LogisticRegression:\n\n    def __init__(self):\n        \"\"\"初始化Logistic Regression模型\"\"\"\n\n        # 系数向量（θ1,θ2,.....θn）\n        self.coef_ = None\n        # 截距 (θ0)\n        self.interception_ = None\n        # θ向量\n        self._theta = None\n\n    def _sigmoid(self, t):\n        return 1. / (1.+np.exp(-t))\n\n    def fit(self, X_train, y_train, eta=0.01, n_iters=1e4):\n        \"\"\"根据训练数据集X_train,y_train, 使用梯度下降法训练Logistic Regression 模型\"\"\"\n        assert X_train.shape[0] == y_train.shape[0], \\\n            \"the size of X_train must be equal to the size of y_train\"\n\n        def J(theta, X_b, y):\n            y_hat = self._sigmoid(X_b.dot(theta))\n            try:\n                return - np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) / len(y)\n            except:\n                return float('inf')\n\n        def dJ(theta, X_b, y):\n            return X_b.T.dot(self._sigmoid(X_b.dot(theta)) - y) / len(X_b)\n\n        def gradient_descent(X_b, y, initial_theta, eta, n_iters=n_iters, epsilon=1e-8):\n            \"\"\"\n            梯度下降法封装\n            X_b: X特征矩阵\n            y: 结果向量\n            initial_theta:初始化的theta值\n            eta:学习率η\n            n_iters: 最大循环次数\n            epsilon: 精度\n            \"\"\"\n            theta = initial_theta\n            i_iters = 0\n\n            while i_iters = 0.5\n\n    def score(self, X_test, y_test):\n        \"\"\"根据测试数据集 X_test 和 y_test 确定当前模型的准确度\"\"\"\n\n        y_predict = self.predict(X_test)\n        return accuracy_score(y_test, y_predict)\n\n    def __repr__(self):\n        return \"LinearRegression()\"\n2.测试我们的逻辑回归算法\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\niris = datasets.load_iris()\n\nX = iris.data\ny = iris.target\n\n# 只取前两个特征；只取y=0，1\nX = X[y\nplt.scatter(X[y==0,0],X[y==0,1],color='red')\nplt.scatter(X[y==1,0],X[y==1,1],color='blue')\n\n\n\n使用我们自己的逻辑回归算法\nfrom machine_learning.module_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y)\nfrom machine_learning.LogisticRegression import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\n\nLinearRegression()\n# 对于所有的测试数据，我们的LogisticRegression全都正确的进行了分类\nlog_reg.score(X_test,y_test)\n\n1.0\n观察逻辑回归预测的概率值p与y_test的关系\n\np越趋近于0，逻辑回归算法越愿意将数据预测为0\np越趋近于1，逻辑回归算法越愿意将数据预测为1\n\n如第一个测试数据的预测概率值为0.02945732，对应的y_test为0\nlog_reg.predict_proba(X_test)\n\narray([0.02945732, 0.10411811, 0.72452652, 0.95194827, 0.00436497,\n       0.04378511, 0.98773424, 0.15855562, 0.72452652, 0.18983267,\n       0.91446368, 0.92348443, 0.88028026, 0.053248  , 0.00987992,\n       0.98551291, 0.98002369, 0.98183066, 0.99639662, 0.07152432])\ny_test\n\narray([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0])\nlog_reg.coef_\n\narray([ 3.02265606, -5.07877569])\nlog_reg.interception_\n\n-0.7235198625270994\n"},"逻辑回归/5.决策边界.html":{"url":"逻辑回归/5.决策边界.html","title":"5.决策边界","keywords":"","body":"5.决策边界\n\n通过上面推导可以得出\n\n决策边界θT·Xb=0 表示的实际上是一条直线，如果有两个特征，那就是在二维平面上的一条直线，x1是横轴，x2是纵轴\n\n观察逻辑回归预测的概率值p与y_test的关系\n\np越趋近于0，逻辑回归算法越愿意将数据预测为0\np越趋近于1，逻辑回归算法越愿意将数据预测为1\n\n如第一个测试数据的预测概率值为0.02945732，对应的y_test为0\nlog_reg.predict_proba(X_test)\n\narray([0.02945732, 0.10411811, 0.72452652, 0.95194827, 0.00436497,\n       0.04378511, 0.98773424, 0.15855562, 0.72452652, 0.18983267,\n       0.91446368, 0.92348443, 0.88028026, 0.053248  , 0.00987992,\n       0.98551291, 0.98002369, 0.98183066, 0.99639662, 0.07152432])\ny_test\n\narray([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0])\nlog_reg.coef_\n\narray([ 3.02265606, -5.07877569])\nlog_reg.interception_\n\n-0.7235198625270994\n描述决策边界\ndef x2(x1):\n    return (-log_reg.coef_[0] * x1 - log_reg.interception_) / log_reg.coef_[1]\n\nx1_plot = np.linspace(4,8,1000)\nx2_plot = x2(x1_plot)\n\nplt.scatter(X[y==0,0],X[y==0,1],color='red')\nplt.scatter(X[y==1,0],X[y==1,1],color='blue')\nplt.plot(x1_plot,x2_plot)\n\n[]\n\n通过观察上面的图可以发现，如果新来一个数据点，落在了直线（决策边界）的上方，对应的x1θ1+x2θ2+θ>0，也就是p>0.5,那么我们就将他分类为1；反之，则分类为0。这就是决策曲线\n\n不过无论是KNN，还是逻辑回归算法，我们依然可以加入多项式项，使得他的决策边界不再是一根直线，对于这种情况，我们就不能简单的求出这根直线的方程。然后将整根直线画出来来看到这个决策的边界。这个时候我们需要一个绘制不规则的决策边界的方法\n在我们的决策平面上，分布着很多点，对于每一个点，我们都使用我们的模型来判断他分为哪一类。然后将这些颜色绘制出来得到的结果就是决策边界\n\n\n不规则的决策边界的绘制方法\ndef plot_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\nplot_decision_boundary(log_reg,axis=[4,7.5,1.5,4.5])\nplt.scatter(X[y==0,0],X[y==0,1],color='red')\nplt.scatter(X[y==1,0],X[y==1,1],color='blue')\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\nKNN的决策边界\n对于KNN来说，这个决策边界是没有表达式的，因为我们不是使用数学求解的方式。而是使用距离投票的方式\n但是我们依然可以使用上面的方式绘制决策边界\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train,y_train)\nknn_clf.score(X_test,y_test)\n\n1.0\n# 绘制KNN算法的决策边界。可以看到是一根弯弯曲曲的曲线 \nplot_decision_boundary(knn_clf,axis=[4,7.5,1.5,4.5])\nplt.scatter(X[y==0,0],X[y==0,1],color='red')\nplt.scatter(X[y==1,0],X[y==1,1],color='blue')\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n# 绘制三个不同类别\nknn_clf_all = KNeighborsClassifier()\nknn_clf_all.fit(iris.data[:,:2],iris.target)\n\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n           weights='uniform')\nplot_decision_boundary(knn_clf_all,axis=[4,8,1.5,4.5])\nplt.scatter(iris.data[iris.target==0,0],iris.data[iris.target==0,1])\nplt.scatter(iris.data[iris.target==1,0],iris.data[iris.target==1,1])\nplt.scatter(iris.data[iris.target==2,0],iris.data[iris.target==2,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n通过绘制的结果，可以看出，绘制出的结果是非常不规则的，甚至在黄色的部分还掺杂着一些蓝色的绿色的点。\n这就是一种过拟合的表现。可以看到上面的knn_clf_all的n_neighbors（k）为5。之前的讨论曾经说过，\nk越小，那么我们的模型就越复杂，在这里，我们可视化的看到了复杂的含义--就是模型非常的不规整\n# 调整k为50\nknn_clf_all = KNeighborsClassifier(n_neighbors=50)\nknn_clf_all.fit(iris.data[:,:2],iris.target)\n\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=50, p=2,\n           weights='uniform')\nplot_decision_boundary(knn_clf_all,axis=[4,8,1.5,4.5])\nplt.scatter(iris.data[iris.target==0,0],iris.data[iris.target==0,1])\nplt.scatter(iris.data[iris.target==1,0],iris.data[iris.target==1,1])\nplt.scatter(iris.data[iris.target==2,0],iris.data[iris.target==2,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n调整过后的样子已经比上面的决策边界规整了很多。整体分成了三大块，非常的清晰\n通过这样一个例子，再次印证了对于KNN算法来说k越大，模型越简单，对于决策边界的划分就是决策越规整，分块越明显\n"},"逻辑回归/6.在逻辑回归中使用多项式回归.html":{"url":"逻辑回归/6.在逻辑回归中使用多项式回归.html","title":"6.在逻辑回归中使用多项式回归","keywords":"","body":"6.在逻辑回归中使用多项式回归\n逻辑回归，实际上是在决策平面中找到一根直线，通过使用这根直线。用这条直线来分割所有样本的分类，用这样一个例子可以看到。为什么逻辑回归只能解决二分类问题，因为这根直线只能将我们的特征平面分成两部分。\n\n不过即使如此还是存在一个，就是直线太简单了，比如如下的情况 ，依然是在特征平面分成了两部分，但是对于这样的分布来说，是不可能使用一根直线来分割的。但是可以使用一个圆形。但是对于一个圆形，他的数学表达式是x12+x22-r2=0。\n这样，我们只要引入多项式项就好了。把x12和x22分别作为一个特征项。我们学习到的他们前面的系数都是1 。针对x12和x22依然是一个线性关系。但是对于x1，和x2就是曲线了\n\n逻辑回归中添加多项式项\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 模拟测试用例\nnp.random.seed(666)\n# 两个特征的样本X\nX = np.random.normal(0,1,size=(200,2))\ny = np.array(X[:,0]**2 + X[:,1]**2 \nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n\n\n使用逻辑回归\nfrom machine_learning.LogisticRegression import LogisticRegression\n\nlog_reg = LogisticRegression()\n# 直接使用逻辑回归。\nlog_reg.fit(X,y)\n\nLinearRegression()\nlog_reg.score(X,y)\n\n0.605\ndef plot_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\n# 显然有非常多的错误分类，所以导致我们的分类准确度只有0.605\nplot_decision_boundary(log_reg,axis=[-4,4,-4,4])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\n\ndef PolynomialLogisticRegression(degree):\n    return Pipeline([\n        ('poly',PolynomialFeatures(degree=degree)),\n        ('std_scaler',StandardScaler()),\n        ('log_reg',LogisticRegression())\n    ])\n\npoly_log_reg = PolynomialLogisticRegression(degree=2)\npoly_log_reg.fit(X,y)\n\nPipeline(memory=None,\n     steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LinearRegression())])\n# 使用多项式回归后分数达到了百分之95.结果非常好\npoly_log_reg.score(X,y)\n\n0.95\nplot_decision_boundary(poly_log_reg,axis=[-4,4,-4,4])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\npoly_log_reg2 = PolynomialLogisticRegression(degree=20)\npoly_log_reg2.fit(X,y)\n\nPipeline(memory=None,\n     steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LinearRegression())])\nprint(poly_log_reg.score(X,y))\nplot_decision_boundary(poly_log_reg2,axis=[-4,4,-4,4])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n0.95\n\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n使用degree=20得出的决策边界的外面变的很奇怪。出现这样的情况就是因为degree=20太大了。导致边界的形状非常的不规则。此时显然发生了过拟合\n线性回归添加了多项式项后。degree这个阶数越大，模型越复杂，就越容易发生过拟合\n"},"逻辑回归/7.scikit-learn中的逻辑会回归.html":{"url":"逻辑回归/7.scikit-learn中的逻辑会回归.html","title":"7.scikit-learn中的逻辑会回归","keywords":"","body":"7.scikit-learn中的逻辑会回归\n另一种正则化\n    C·J(θ) + L1\n    C·J(θ) + L2\n使用这种正则化。如果C很小。那我们的任务就是集中精力调整L1或者L2的大小；如果C很大，那我们的任务就是集中精力调整原损失函数J(θ)的大小\n使用这种正则化的好处是。我们不得不进行正则化；因为L1/L2前面的系数不能为0\nscikit-learn中的逻辑会回归\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(666)\nX = np.random.normal(0,1,size=(200,2))\n# 决策边界是一条抛物线\ny = np.array(X[:,0]**2+X[:,1]\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=666)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\n# C=1.0 说明默认的正则化超参数C=1.0;penalty=l2,说明默认使用l2正则化\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n# 0.82 因为我们生成的数据是具有二次项的。但是我们这里使用的是线性逻辑回归\nprint(log_reg.score(X_train,y_train))\nprint(log_reg.score(X_test,y_test))\n\n0.7933333333333333\n0.86\ndef plot_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\n# 描绘线性的决策边界\nplot_decision_boundary(log_reg,axis=[-4,4,-4,4])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n尝试使用多项式回归\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\n\ndef PolynomialLogisticRegression(degree):\n    return Pipeline([\n        ('poly',PolynomialFeatures(degree=degree)),\n        ('std_scaler',StandardScaler()),\n        ('log_reg',LogisticRegression())\n    ])\n\npoly_log_reg = PolynomialLogisticRegression(degree=2)\npoly_log_reg.fit(X_train,y_train)\n\nPipeline(memory=None,\n     steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False))])\npoly_log_reg.score(X_train,y_train)\n\n0.9133333333333333\npoly_log_reg.score(X_test,y_test)\n\n0.94\nplot_decision_boundary(poly_log_reg,axis=[-4,4,-4,4])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n尝试增大多项式项degree的值\npoly_log_reg2 = PolynomialLogisticRegression(degree=20)\npoly_log_reg2.fit(X_train,y_train)\n\nPipeline(memory=None,\n     steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False))])\npoly_log_reg2.score(X_train,y_train)\n\n0.94\n# 模型返回能力变脆。因为出现了过拟合\npoly_log_reg2.score(X_test,y_test)\n\n0.92\nplot_decision_boundary(poly_log_reg2,axis=[-4,4,-4,4])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n尝试调整正则化超参数C\ndef PolynomialLogisticRegression(degree,C):\n    return Pipeline([\n        ('poly',PolynomialFeatures(degree=degree)),\n        ('std_scaler',StandardScaler()),\n        ('log_reg',LogisticRegression(C=C))\n    ])\n\n# C=0.1 相当于让模型正则化那一项起更大的作用\npoly_log_reg3 = PolynomialLogisticRegression(degree=20,C=0.1)\npoly_log_reg3.fit(X_train,y_train)\n\nPipeline(memory=None,\n     steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False))])\npoly_log_reg3.score(X_train,y_train)\n\n0.8533333333333334\n# C=0.1 模型泛化能力并没有降低\npoly_log_reg3.score(X_test,y_test)\n\n0.92\n# 虽然边界还是比较奇怪，但是比之前degree=20要好很多\nplot_decision_boundary(poly_log_reg3,axis=[-4,4,-4,4])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n尝试使用L1正则项\ndef PolynomialLogisticRegression(degree,C,penalty='l2'):\n    return Pipeline([\n        ('poly',PolynomialFeatures(degree=degree)),\n        ('std_scaler',StandardScaler()),\n        ('log_reg',LogisticRegression(C=C,penalty=penalty))\n    ])\n\npoly_log_reg4 = PolynomialLogisticRegression(degree=20,C=0.1,penalty='l1')\npoly_log_reg4.fit(X_train,y_train)\n\nPipeline(memory=None,\n     steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False))])\npoly_log_reg4.score(X_train,y_train)\n\n0.8266666666666667\n# score变低是由于数据比较简单。主要看决策边界\npoly_log_reg4.score(X_test,y_test)\n\n0.9\n# 已经非常接近之前的正常的决策边界了\nplot_decision_boundary(poly_log_reg4,axis=[-4,4,-4,4])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n"},"逻辑回归/8.OvR与OvO.html":{"url":"逻辑回归/8.OvR与OvO.html","title":"8.OvR与OvO","keywords":"","body":"8.OvR与OvO\n1.什么是OvR与OvO\n逻辑回归只可以解决二分类问题。我们可以对逻辑回归稍加改造，让他解决多分类问题：\n\nOvR （One vs Rest -- 一针对剩余）\n假设一共有4个类别，选取其中的某一个类别（假设红色）（One），而对于剩下的类别，把他们融合在一起，把他称之为其他的类别（Rest），这样就把一个四分类问题转换成了二分类问题，转换成了使红色的概率是多少，是非红色的概率是多少\n\n以此类推，我们分别进行四次分类，哪次获得的类别得分最高，我们就任务他属于哪一个类别。对于逻辑回归来说，就是我们的概率P\n\n复杂度由T变成了N*T->O(N)\nOvO （One vs One -- 一对一的进行比较）\n每次就从N个类别挑出两个类别（比如这里挑出红蓝两个类别）,然后进行二分类任务，看对于这个任务来说，我们的样本点是属于哪个类别。然后依次类推进行扩展，如果我们有4个类别需要分类，那我们就能形成6个两两的对C42(排列组合公式4*3/2=6)，也就是6个二分类问题。对于这6个分类结果，判定他在哪个类别中数量最大，就判定他是哪个类别\n\n复杂度是O(N2),但是分类结果相比OVR是更加准确的，这是因为每次只用真实的两个类别进行比较，所以他更倾向于真实的样本属于哪个类别\n\n\n2.sklearn中的OvR与OvO\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\niris = datasets.load_iris()\n# 为了可视化，先使用两个类别\nX = iris.data[:,:2]\ny = iris.target\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=666)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\n# multi_class='ovr' sklearn 默认支持多分类任务，而且默认使用ovr方式\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n# 分数不好使由于我们只使用了两个维度\nlog_reg.score(X_test,y_test)\n\n0.6578947368421053\ndef plot_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\n# 描绘三分类的决策边界\nplot_decision_boundary(log_reg,axis=[4,8.5,1.5,4.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\nplt.scatter(X[y==2,0],X[y==2,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n使用OvO\n# sklearn中计算logistic不是简单的使用梯度下降法，他是使用更快的一种方法，所以需要修改solver参数\n# 默认solver='liblinear',为了正确的调用OvO，缓存newton-cg\nlog_reg2 = LogisticRegression(multi_class='multinomial',solver=\"newton-cg\")\n\nlog_reg2.fit(X_train,y_train)\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n          tol=0.0001, verbose=0, warm_start=False)\n# 分类准确度比使用OvR的时候要高了很多\nlog_reg2.score(X_test,y_test)\n\n0.7894736842105263\n# 从直观的角度看，决策边界也准确了很多\nplot_decision_boundary(log_reg2,axis=[4,8.5,1.5,4.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\nplt.scatter(X[y==2,0],X[y==2,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n尝试使用所有的数据特征\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=666)\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\nlog_reg.score(X_test,y_test)\n\n0.9473684210526315\nlog_reg2 = LogisticRegression(multi_class='multinomial',solver=\"newton-cg\")\nlog_reg2.fit(X_train,y_train)\n# 使用OvO的方式预测结果达到了百分之百；\n# 注意：这里由于数据集比较小，耗时的差别比较小\nlog_reg2.score(X_test,y_test)\n\n1.0\n使用sklearn中的OvO and OvR\nfrom sklearn.multiclass import OneVsRestClassifier\n\novr = OneVsRestClassifier(log_reg)\novr.fit(X_train,y_train)\novr.score(X_test,y_test)\n\n0.9473684210526315\nfrom sklearn.multiclass import OneVsOneClassifier\n\novo = OneVsOneClassifier(log_reg)\novo.fit(X_train,y_train)\novo.score(X_test,y_test)\n\n1.0\n"},"评价分类结果/":{"url":"评价分类结果/","title":"9.评价分类结果","keywords":"","body":"9.评价分类结果\n"},"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html":{"url":"评价分类结果/9.1 准确度的陷阱和混淆矩阵.html","title":"9.1 准确度的陷阱和混淆矩阵","keywords":"","body":"9.1 准确度的陷阱和混淆矩阵\n对于这样一个系统，我们什么都没有做，就可以达到99.9%的准确率\n\n如果我们真的进行了一个机器学习算法进行训练，而准确度是99.9%的话，那么说明我们的机器学习算法是失败的，因为他比纯粹的预测一个人是监控的准确率更低。\n\n\n对于极度偏斜的数据（Skeked Data），只使用分类准确度是远远不够的\n\n混淆矩阵 Confusion Matrix\n对于二分类问题来说，混淆矩阵是一个2*2的矩阵。每一行代表的是对于预测的问题来说真实值是多少，相应的每一列是分类算法进行预测的预测值是多少。\n\n行相当于二维数组的第一个维度，列相当于二维数组的第二个维度，相对来将，真实值（第一个维度）要在预测值（第二个维度）的前面\n\n每个维度都是按照顺序排列的。\n\n\n\n\n"},"评价分类结果/9.2 精准率和召回率.html":{"url":"评价分类结果/9.2 精准率和召回率.html","title":"9.2 精准率和召回率","keywords":"","body":"9.2 精准率和召回率\n\n精准率：预测数据为1，预测对了的概率\n\n\n在有偏的数据中，我们将分类1作为我们关注的对象，例如在医疗中，这个精准率就是指我们预测癌症预测的成功率\n精准率=40%：我们没做100次患病的预测，其中会有40次是对的\n\n\n召回率：我们关注的那个事件，真实的发生了的数据(分母)中，我们成功的预测了多少。\n\n\n在10000个癌症患者中，一共有10个癌症患者，我们成功的预测的预测出了8个\n\n\n我们通过精准率和召回率，判断出了这样做的预测算法是完全没有用的。\n在及其有偏的数据中，我们不看准确率，而看精准率和召回率，才能分析出算法的好坏\n\n"},"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html":{"url":"评价分类结果/9.3 实现混淆矩阵，精准率和召回率.html","title":"9.3 实现混淆矩阵，精准率和召回率","keywords":"","body":"9.3 实现混淆矩阵，精准率和召回率\nimport numpy as np \nfrom sklearn import datasets\n\ndigits = datasets.load_digits()\n\nX = digits.data\ny = digits.target.copy()\n\n# 手动将手写数据集变成及其偏斜的数据。不是9的y=0，是9的y=1\ny[digits.target==9] = 1\ny[digits.target!=9] = 0\n\ndigits.target\n\narray([0, 1, 2, ..., 8, 9, 8])\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=666)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\nlog_reg.score(X_test,y_test)\n\n0.9755555555555555\ny_log_predict = log_reg.predict(X_test)\n\ndef TN(y_true, y_predict):\n    assert len(y_true) == len(y_predict)\n    return np.sum((y_true==0)&(y_predict==0))\n\nTN(y_test,y_log_predict)\n\n403\ndef FP(y_true, y_predict):\n    assert len(y_true) == len(y_predict)\n    return np.sum((y_true==0)&(y_predict==1))\n\nFP(y_test,y_log_predict)\n\n2\ndef TP(y_true, y_predict):\n    assert len(y_true) == len(y_predict)\n    return np.sum((y_true==1)&(y_predict==1))\n\nTP(y_test,y_log_predict)\n\n36\ndef FN(y_true, y_predict):\n    assert len(y_true) == len(y_predict)\n    return np.sum((y_true==1)&(y_predict==0))\nFN(y_test,y_log_predict)\n\n9\ndef confusion_matrix(y_true, y_predict):\n    return np.array([\n        [TN(y_true, y_predict), FP(y_true,y_predict)],\n        [FN(y_true, y_predict), TP(y_true,y_predict)]\n    ])\nconfusion_matrix(y_test,y_log_predict)\n\narray([[403,   2],\n       [  9,  36]])\ndef precision_score(y_true, y_predict):\n    tp = TP(y_true, y_predict)\n    fp = FP(y_true, y_predict)\n    try:\n        return tp / (tp + fp)\n    except:\n        return 0\n\nprecision_score(y_test, y_log_predict)\n\n0.9473684210526315\ndef recall_score(y_true, y_predict):\n    tp = TP(y_true, y_predict)\n    fn = FN(y_true, y_predict)\n    try:\n        return tp / (tp + fn)\n    except:\n        return 0\n\nrecall_score(y_test, y_log_predict)\n\n0.8\nscikit-learn中的混淆矩阵，精准率和召回率\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\nconfusion_matrix(y_test,y_log_predict)\n\narray([[403,   2],\n       [  9,  36]])\nprecision_score(y_test, y_log_predict)\n\n0.9473684210526315\nrecall_score(y_test, y_log_predict)\n\n0.8\n"},"评价分类结果/F1 Score.html":{"url":"评价分类结果/F1 Score.html","title":"9.4 F1 Score","keywords":"","body":"9.4 F1 Score\n1.F1 Score的含义\n精准率和召回率是两个指标，有的时候精准率高一些，有的时候召回率高一些，在我们使用的时候，我们应该怎么解读这个精准率和召回率呢？\n这个问题的答案，和机器学习大多数的取舍是一样的，应该视情况而定。\n\n有的时候我们注重精准率，如股票预测。\n\n我们希望这个比例越高越好。如果我们预测股票升了，我们就要购买这个股票，如果我们犯了FP的错误（实际上股票将下来了，而我们预测升上来了），那么我们就就亏钱了。 但是对于这个应用来说，很有可能我们对召回率不是特别关注。可能有很多上升周期，但是我们落掉了一些上升的周期（本来为1，我们错误的判断为0），这对我们没有太多的损失，因为我们漏掉了他，也不会投钱进去。\n\n\n有的时候我们注重召回率，如病人诊断。\n\n召回率低意味着:本来一个病人得病了，但是我们没有把他预测出来，这就意味着这个病人的病情会继续恶化下去。所以召回率更加重要，我们希望把所有有病的患者都预测出来。但是精准率却不是特别重要，因为本来一个人没病，我们预测他有病，这时候让他去做进一步的检查，进行确诊就好了。我们犯了FP的错误，只是让他多做了一次检查而已。这个时候召回率比精准率重要。\n\n\n有的时候我们希望同时关注精准率和召回率，这个时候我们可以使用F1 Score，让我们兼顾精准率和召回率\n\n\n\n调和平均值的特点：如果一个值特别高，一个值特别低，那么我们得到的F1 Score 也将特别的低，只有二者都非常高，我们得到的值才会特别高\n\n2.F1 Score 的实现\nimport numpy as np\n\ndef f1_score(precision, recall):\n    return 2 * (precision * recall) / (precision + recall)\n\nprecision = 0.5\nrecall = 0.5\nf1_score(precision, recall)\n# 当二者相同，得到的就是这个相同的值\n\n0.5\nprecision = 0.1\nrecall = 0.9\nf1_score(precision, recall)\n# 有一个非常小，整体就非常小\n\n0.18000000000000002\nprecision = 0.9\nrecall = 0\nf1_score(precision, recall)\n\n0.0\nprecision = 0.9\nrecall = 0.95\nf1_score(precision, recall)\n# 只有都非常大，结果才会打\n\n0.9243243243243242\nfrom sklearn import datasets\ndigits = datasets.load_digits()\n\nX = digits.data\ny = digits.target.copy()\n\n# 手动将手写数据集变成及其偏斜的数据。不是9的y=0，是9的y=1\ny[digits.target==9] = 1\ny[digits.target!=9] = 0\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=666)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\nlog_reg.score(X_test,y_test)\n\ny_log_predict = log_reg.predict(X_test)\n\nfrom sklearn.metrics import f1_score\nf1_score(y_test, y_log_predict)\n\n0.8674698795180723\n0.8674698795180723 显然没有精准率和召回率高，这是因为首先我们的数据是有偏的，精准率和召回率都比准确率要低一些，在这里精准率和召回率能够更好的反应我们的结果。其次使用逻辑回归进行预测，明显召回率比较低，所以f1_score 被召回率拉低了，这个时候对于这个有偏的数据来说，我们更倾向认为f1_score 能更好的反应算法的水平\n"},"评价分类结果/9.5 Precision-Recall的平衡.html":{"url":"评价分类结果/9.5 Precision-Recall的平衡.html","title":"9.5 Precision-Recall的平衡","keywords":"","body":"9.5 Precision-Recall的平衡\n精准率和召回率这两个指标是互相矛盾的。我们要找到的是这两个指标直接的一个平衡。\n1.理解为什么二者是矛盾的\n回忆我们上一章学习的逻辑回归，通过一系列的推导，我们得出了决策边界:θT·Xb = 0\n\n如果我们设置任意一个常量-threshold。可不可以让这个threshold作为决策边界呢\n即θT·Xb = threshold。这样，相当于我们为我们的逻辑回归算法添加了一个新的超参数threshold，通过指定threshold，相对于可以平移决策边界的直线，进而影响逻辑回归的结果。\n\n由上图可知，不同的threshold对应的精准率和召回率的关系。随着threshold增大，精准率在不断的变高，而召回率在不断的降低。由此说明精准率和召回率是相互矛盾的\n2.直观的理解\n\n如果想让精准率变高的话，相应的其实就是我们只能讲那些特别有把握的数据分类为1。在这种情况下，实际上我们做的事情是让我们的算法做的事让其判断样本的概率是百分之90甚至百分之99的时候，我们才预测他为1，在这样的情况，很显然有很多真实为1的样本就被排除在了外面，所以召回率就会变低。\n如果要让召回率升高，我们就要降低算法判断的概率的threshold，这时却是召回率提高了，但是精准率下降了\n\n3.编码观察精准率和召回率的平衡\n# 结果即为对于每一个样本来说，在逻辑回归算法中对应的score值是什么\nlog_reg.decision_function(X_test)[:10]\n\narray([-22.05701166, -33.02939187, -16.21331661, -80.37908046,\n       -48.25125463, -24.54003391, -44.39167886, -25.04286766,\n        -0.97831023, -19.7173878 ])\n# 因为对于前10个样本来说，分数值都是小于0 的，所以预测值都是0\nlog_reg.predict(X_test)[:10]\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\ndscision_scores = log_reg.decision_function(X_test)\nprint(np.min(dscision_scores))\nprint(np.max(dscision_scores))\n\n-85.68606536672523\n19.889528301315465\n# 通过构建新的y_predict，来实现修改threshold为5\ny_predict2 = np.array(dscision_scores >= 5, dtype='int')\nconfusion_matrix(y_test, y_predict2)\n\narray([[404,   1],\n       [ 21,  24]])\n# 升高threshold后,精准率升高，召回率降低\nprint(precision_score(y_test, y_predict2))\nprint(recall_score(y_test, y_predict2))\n\n0.96\n0.5333333333333333\n"},"评价分类结果/9.6 精准率-召回率曲线.html":{"url":"评价分类结果/9.6 精准率-召回率曲线.html","title":"9.6 精准率-召回率曲线","keywords":"","body":"9.6 精准率-召回率曲线\n召回率曲线\nimport numpy as np \nfrom sklearn import datasets\n\ndigits = datasets.load_digits()\n\nX = digits.data\ny = digits.target.copy()\n\n# 手动将手写数据集变成及其偏斜的数据。不是9的y=0，是9的y=1\ny[digits.target==9] = 1\ny[digits.target!=9] = 0\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=666)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\ndecision_scores = log_reg.decision_function(X_test)\n\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\nprecisions = []\nrecalls = []\nthresholds = np.arange(np.min(decision_scores), np.max(decision_scores))\n\nfor threshold in thresholds:\n    y_predict = np.array(decision_scores >= threshold, dtype='int')\n    precisions.append(precision_score(y_test, y_predict))\n    recalls.append(recall_score(y_test, y_predict))\n\nfrom matplotlib import pyplot as plt\nplt.plot(thresholds, precisions)\nplt.plot(thresholds, recalls)\nplt.show()\n\n\nPrecision-Recall 曲线\nplt.plot(precisions, recalls)\n\n[]\n\n通过曲线再次印证了，精准率和召回率是相互制约的，这个曲线急剧下降的一个点，可能就是精准率和召回率平衡最好的一个位置\nscikit-learn中的Precision-Recall 曲线\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_test, decision_scores)\n\n# sklearn中会根据我们顶一个的decision_scores的值来取最合适的步长\nprecisions.shape\n\n(145,)\nrecalls.shape\n\n(145,)\n# The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. \n# This ensures that the graph starts on the x axis.\nthresholds.shape\n\n(144,)\nplt.plot(thresholds, precisions[:-1])\nplt.plot(thresholds, recalls[:-1])\n# 没有从最小值开始取，在sklearn的封装中，他自动寻找了他认为最重要的数据\nplt.show()\n\n\nplt.plot(precisions, recalls)\n\n[]\n\n\n我们用不同的超参数训练不同的模型，每得到一个新的模型，就可以得到一根不同的P-R曲线，如果这个P-R曲线更靠外（也就是与X轴，Y轴所包围的面积越大），那么说明这根曲线对应的模型越好，因为对应这个曲线上的每一个点，他的precision_score和recall_score都比另一个曲线要大\n"},"评价分类结果/9.7 ROC曲线.html":{"url":"评价分类结果/9.7 ROC曲线.html","title":"9.7 ROC曲线","keywords":"","body":"9.7 ROC曲线\n1.什么是ROC 曲线\n\nROC：Receiver Operation Characteristic Curve，描述TPR 和 FPR之间的关系\n\n\n\nTPR - True Positive Rate，FPR - False Positive Rate\n\n  TPR：预测为1，并且预测对了的数量占真实值为1的百分比是多少\n  FPR：预测为1，但是预测错了的数量占真实值为0的百分比是多少\n\nTPR和FPR之间的关系\n\n  随着threshold逐渐降低，TPR 和 FPR都是逐渐升高的，换句话说，TPR和FPR之间是存在着相一致的关系的\n  为了提高TPR，我们就要将threshold拉低，但是在拉低的过程中，犯FP的错误的概率也会增高\n\n\n编程实现ROC曲线\nROC曲线\nfrom machine_learning.metrics import TPR,FPR\n\nfprs = []\ntprs = []\n\nthresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)\nfor threshold in thresholds:\n    y_predict=np.array(decision_scores >= threshold,dtype='int')\n    fprs.append(FPR(y_test, y_predict))\n    tprs.append(TPR(y_test, y_predict))\n\n# 绘制ROC曲线\nplt.plot(fprs,tprs)\n\n[]\n\nscikit-learn中的ROC曲线\nfrom sklearn.metrics import roc_curve\n\nfprs, tprs, thresholds = roc_curve(y_test, decision_scores)\nplt.plot(fprs, tprs)\n\n[]\n\n对于ROC曲线，我们通常关注的是这根曲线下面的面积的大小，面积越大，代表分类算法效果越好。\n在fpr越低的时候（犯fp错误的次数越少的时候），相应的FPR越高的时候，这根曲线整体就会被抬的越高，\n他的面积相应的也会越大，这种情况下分类算法的效果就更好\n# 计算ROC曲线的面积\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test, decision_scores)\n\n0.9830452674897119\n对于ROC曲线来说，他对于有偏的数据并不是那么敏感，所以在有偏的数据集里，看一下精准率和召回率是非常有必要的\n而ROC的应用场景是比较两个模型的优劣\n\n"},"评价分类结果/9.8 多分类问题中的混淆矩阵.html":{"url":"评价分类结果/9.8 多分类问题中的混淆矩阵.html","title":"9.8 多分类问题中的混淆矩阵","keywords":"","body":"9.8 多分类问题中的混淆矩阵\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\n\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\n# sklearn 的 逻辑回归，如果我们传进来的数据集有多个分类，他讲使用OVR的方式来解决多分类的问题\nlog_reg.fit(X_train, y_train)\nlog_reg.score(X_test, y_test)\n\n0.9304589707927677\ny_predict =  log_reg.predict(X_test)\n\nfrom sklearn.metrics import precision_score\n\n# 通过传入average参数可以让precision_score处理多分类问题\nprecision_score(y_test, y_predict, average=\"micro\")\n\n0.9304589707927677\nfrom sklearn.metrics import confusion_matrix\n\n# sklearn 的混淆矩阵天然支持多分类问题\n# 第i行第j列的数值代表 真值为i而预测为j的样本数量有多少\nconfusion_matrix(y_test, y_predict)\n\narray([[147,   0,   0,   0,   1,   0,   0,   0,   0,   0],\n       [  0, 130,   2,   0,   0,   2,   0,   0,   7,   6],\n       [  0,   0, 139,   2,   0,   0,   0,   0,   1,   0],\n       [  0,   0,   0, 145,   0,   0,   0,   0,   3,   1],\n       [  0,   3,   0,   1, 135,   0,   0,   0,   0,   4],\n       [  1,   1,   1,   0,   0, 147,   0,   0,   0,   4],\n       [  0,   0,   0,   0,   2,   0, 133,   0,   0,   0],\n       [  0,   0,   0,   3,   5,   0,   0, 141,   1,   2],\n       [  1,  11,   3,   6,   1,   2,   1,   1, 105,   6],\n       [  1,   2,   0,   3,   3,   1,   0,   0,   5, 116]])\ncfm = confusion_matrix(y_test, y_predict)\n# 绘制混淆矩阵，越亮的地方说明数值越大\nplt.matshow(cfm, cmap=plt.cm.gray)\n\n\n\n# 每一行有多少样本,在列方向上求和\nrow_sums = np.sum(cfm, axis=1)\n# 矩阵中的每一行的数字都会除以这一行的数字和得到的一个百分比矩阵\nerr_matrix = cfm / row_sums\n# 将对角线上的数字全都填成是0，剩下的其他的格子就是犯错误的百分比\nnp.fill_diagonal(err_matrix, 0)\nerr_matrix\n\narray([[0.        , 0.        , 0.        , 0.        , 0.00699301,\n        0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.01408451, 0.        , 0.        ,\n        0.01298701, 0.        , 0.        , 0.05109489, 0.04580153],\n       [0.        , 0.        , 0.        , 0.01342282, 0.        ,\n        0.        , 0.        , 0.        , 0.00729927, 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.02189781, 0.00763359],\n       [0.        , 0.02040816, 0.        , 0.00671141, 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.03053435],\n       [0.00675676, 0.00680272, 0.00704225, 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.03053435],\n       [0.        , 0.        , 0.        , 0.        , 0.01398601,\n        0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.02013423, 0.03496503,\n        0.        , 0.        , 0.        , 0.00729927, 0.01526718],\n       [0.00675676, 0.07482993, 0.02112676, 0.04026846, 0.00699301,\n        0.01298701, 0.00740741, 0.00657895, 0.        , 0.04580153],\n       [0.00675676, 0.01360544, 0.        , 0.02013423, 0.02097902,\n        0.00649351, 0.        , 0.        , 0.03649635, 0.        ]])\n# 多分类问题中越亮的地方代表的就是犯错误越多的地方，并且通过横纵坐标可以看出具体的错误\nplt.matshow(err_matrix, cmap=plt.cm.gray)\n\n\n\n通过这样一个矩阵，我们就可以很清晰的发现分类的错误，并且更重要的是，可以看出具体的错误类型，比如有很多的8我们把他规约为了1，有很多1我们规约成了8，有了这些提示，我们就可以进一步改进我们的算法了。\n我们可以看到，这个分类的结果，我们又把他规约成了一个二分类的问题，我们现在的分类的结果很容易混淆1和9以及1和8，相应的我们可以微调1和9和1和8分类问题 中的阈值，来相应的调整多分类问题的准确度\n"},"支撑向量机SVM/":{"url":"支撑向量机SVM/","title":"10.支撑向量机 SVM","keywords":"","body":"10.支撑向量机 SVM\n"},"支撑向量机SVM/11.1 什么是SVM.html":{"url":"支撑向量机SVM/11.1 什么是SVM.html","title":"11.1 什么是SVM","keywords":"","body":"11.1 什么是SVM\n支持向量机-Support Vector Machine\n\n\n之前学习的逻辑回归（通过最小化损失函数找到一个决策边界，通过决策边界来分类数据）有一个非常大的不足，就是他的模型泛化能力非常弱，因为我们通过已知的数据求出了决策边界，而并没有考虑未知的数据。\n\n\n\n支持向量机的思想是找到一条决策边界，让这条边界理两部分数据都尽可能的远，并且能很好的分别这两个类别的数据点。也就是说这种方式不仅要将现在的数据进行一个很好的划分，同时还考虑了未来的泛化能力。\nSVM这种思想对于未来的泛化能力的考量没有集中在数据预处理阶段，或者是找到了这个模型之后在进行正则化。而是将这种考量放在了算法内部。也就是要找到一条决策边界离两类样本都尽可能的远，这样的决策边界，他的泛化能力就是好的\nSVM的决策边界的特点：理两个类别的数据点都尽可能的远，也就是这两个类别离决策边界最近的这些点离决策边界也尽肯能的远\n\n\n到现在我们所介绍的SVM，都必须在样本点中能求出一根确确实实的直线，满足以上的条件，也就是线性可分的--Hard Margin SVM\n对于那些线性不可分的情况，对应的解决办法是--Soft Margin SVM，其是在Hard Margin SVM进行改变得来的\n"},"支撑向量机SVM/11.2 SVM背后的最优化问题.html":{"url":"支撑向量机SVM/11.2 SVM背后的最优化问题.html","title":"11.2 SVM背后的最优化问题","keywords":"","body":"11.2 SVM背后的最优化问题\n\n我们只要找到d的表达式，也相应的能够求解SVM的问题。\n\n回忆解析几何，点到直线的距离\n\n有了这个点到直线的距离，我们就可以相应的得出SVM这个问题的表达式  \n  \n\n\nSVM的最终数学推导\n\n"},"支撑向量机SVM/11.3 Soft Margin SVM.html":{"url":"支撑向量机SVM/11.3 Soft Margin SVM.html","title":"11.3 Soft Margin SVM","keywords":"","body":"11.3 Soft Margin SVM\n在我们之前推导的SVM基础上，可以适当的将条件放宽松一些，允许我们的模型犯一些错误\n\n对于每一个样本点，都应该有一个℥。也就是说对于每一个数据点，我们都应该求出他的一个容错空间\n所以完整的Soft Margin SVM对应的表达式为\n\n如果C=1，代表两部分的重要程度是一样的，如果C在0-1之间，那么正规式子将以优化前半部分为主，如果C特别大，那么这个式子主要优化的目标就是后半部分\n如果C越大，对应的我们就是在逼迫着所有的℥为0，此时意味着模型的容错空间更小，Soft Margin SVM就退化成了Hard Margin SVM\n\n"},"支撑向量机SVM/11.4 scikit-learn中的SVM.html":{"url":"支撑向量机SVM/11.4 scikit-learn中的SVM.html","title":"11.4 scikit-learn中的SVM","keywords":"","body":"11.4 scikit-learn中的SVM\n使用SVM之前，和KNN一样，也是要做数据标准化处理的，因为SVM是涉及到距离的\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\niris = datasets.load_iris()\n\nX = iris.data\ny = iris.target\n\nX = X[y\nplt.scatter(X[y==0,0],X[y==0,1],color='red')\nplt.scatter(X[y==1,0],X[y==1,1],color='blue')\n\n\n\n# 数据标准化\nfrom sklearn.preprocessing import StandardScaler\n\nstandardScaler = StandardScaler()\nstandardScaler.fit(X)\nX_standard = standardScaler.transform(X)\n\n# SVC--Support Vector Classifier 使用支持向量机思想解决分类问题\nfrom sklearn.svm import LinearSVC\n# C越大，越偏向于是一个Hard Margin SVM，取值越小，相当于他的容错空间更大一些\nsvc = LinearSVC(C=1e9)\nsvc.fit(X_standard, y)\n\nLinearSVC(C=1000000000.0, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0)\ndef plot_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\n# 绘制决策边界---当C非常大，更趋近于Hard Margin SVM\nplot_decision_boundary(svc,axis=[-3, 3, -3, 3])\nplt.scatter(X_standard[y==0,0],X_standard[y==0,1],color='red')\nplt.scatter(X_standard[y==1,0],X_standard[y==1,1],color='blue')\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\nsvc2 = LinearSVC(C=0.01)\nsvc2.fit(X_standard, y)\n\nLinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0)\n# 绘制决策边界---C放小之后，容错空间变大，所以犯了一个错误\nplot_decision_boundary(svc2,axis=[-3, 3, -3, 3])\nplt.scatter(X_standard[y==0,0],X_standard[y==0,1],color='red')\nplt.scatter(X_standard[y==1,0],X_standard[y==1,1],color='blue')\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n# svc 天然的支持多分类的问题，如果是多分类问题，在特征平面内就会有多条直线，\n# 这里是二分类，所以只有一跟直线，放在了二维数组的第一个元素中\nsvc.coef_\n\narray([[ 4.03237972, -2.49296044]])\nsvc.intercept_\n\narray([0.95365304])\n# 绘制上下两条直线\ndef plot_svc_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\n    w = model.coef_[0]\n    b = model.intercept_[0]\n\n    # 决策边界的直线方程：w0 * x0 + w1 * x1 + b = 0\n    # => x1 = -w0/w1 * x0 + b\n    plot_x = np.linspace(axis[0], axis[1], 200)\n\n    # 决策边界上面的直线方程：x1 = -w0/w1 * x0 + b + 1/w1\n    up_y = -w[0]/w[1] * plot_x - b / w[1] + 1/w[1]\n    # 决策边界下面的直线方程：x1 = -w0/w1 * x0 + b - 1/w1\n    down_y = -w[0]/w[1] * plot_x - b / w[1] - 1/w[1]\n\n    # 过滤超出绘制范围的数据\n    up_index = (up_y >= axis[2]) & (up_y = axis[2]) & (down_y \nplot_svc_decision_boundary(svc,axis=[-3, 3, -3, 3])\nplt.scatter(X_standard[y==0,0],X_standard[y==0,1],color='red')\nplt.scatter(X_standard[y==1,0],X_standard[y==1,1],color='blue')\n\n# 落在两根直线上的这些点即为支撑向量，这两条直线中间的距离即为margin。\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\nplot_svc_decision_boundary(svc2,axis=[-3, 3, -3, 3])\nplt.scatter(X_standard[y==0,0],X_standard[y==0,1],color='red')\nplt.scatter(X_standard[y==1,0],X_standard[y==1,1],color='blue')\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\nsvc3 = LinearSVC(C=0.1)\nsvc3.fit(X_standard, y)\n\nLinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0)\nplot_svc_decision_boundary(svc3,axis=[-3, 3, -3, 3])\nplt.scatter(X_standard[y==0,0],X_standard[y==0,1],color='red')\nplt.scatter(X_standard[y==1,0],X_standard[y==1,1],color='blue')\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n"},"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html":{"url":"支撑向量机SVM/11.5 SVM中使用多项式特征和核函数.html","title":"11.5 SVM中使用多项式特征和核函数","keywords":"","body":"11.5 SVM中使用多项式特征和核函数\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\n\n# 生成不真实的非线性的数据集\nX, y = datasets.make_moons()\n\n/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\nX.shape\n\n(100, 2)\ny.shape\n\n(100,)\nplt.scatter(X[y==0, 0],X[y==0, 1])\nplt.scatter(X[y==1, 0],X[y==1, 1])\n\n\n\n# 为数据集添加一些噪音--在生成规则数据的基础上，增加标准差\nX, y = datasets.make_moons(noise=0.15, random_state=666)\n\nplt.scatter(X[y==0, 0],X[y==0, 1])\nplt.scatter(X[y==1, 0],X[y==1, 1])\n\n\n\n使用多项式特征的SVM\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\n\ndef polnomialSVC(degree, C=0.1):\n    return Pipeline([\n        (\"poly\", PolynomialFeatures(degree=degree)),\n        (\"std_scaler\", StandardScaler()),\n        (\"linearSVC\", LinearSVC(C=C))\n    ])\npoly_svc = polnomialSVC(degree=3)\n\npoly_svc.fit(X, y)\n\nPipeline(memory=None,\n     steps=[('poly', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linearSVC', LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0))])\ndef plot_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\n# 绘制决策边界\nplot_decision_boundary(poly_svc, axis=[-1.5, 2.5, -1.0, 1.5])\nplt.scatter(X[y==0, 0],X[y==0, 1])\nplt.scatter(X[y==1, 0],X[y==1, 1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n使用多项式核函数的SVM\nfrom sklearn.svm import SVC\n\n# 在SVM中，可以不使用现使用PolynomialFeatures再扔给LinearSVC这种方式进行多项式回归\ndef PolynomialKernelSVC(degree,C=1.0):\n    return Pipeline([\n        (\"std_scaler\", StandardScaler()),\n        (\"kernelSVC\", SVC(kernel=\"poly\",degree=degree,C=C))\n    ])\n\npoly_kernel_svc = PolynomialKernelSVC(3)\npoly_kernel_svc.fit(X,y)\nplot_decision_boundary(poly_kernel_svc, axis=[-1.5, 2.5, -1.0, 1.5])\nplt.scatter(X[y==0, 0],X[y==0, 1])\nplt.scatter(X[y==1, 0],X[y==1, 1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n"},"支撑向量机SVM/11.6 到底什么是核函数.html":{"url":"支撑向量机SVM/11.6 到底什么是核函数.html","title":"11.6 到底什么是核函数","keywords":"","body":"11.6 到底什么是核函数\n1.什么是核函数\n在我们之前推导的最优化函数基础上，需要进行进一步的变形\n\n在进行这一项变化之后，我们可以发现，在第二项中其中有一部分是xixj,也就是对于任意两个样本点的x值点乘。\n在多项式求解的时候需要对xi添加多项式项形成x`i,对xj添加多项式项形成x`j。然后再将二者进行点乘。\n\n如果能找到一个函数K，将xi和xj分别作为参数传入，返回的就是x`i和x`j点乘的结果，那么我们就不需要分别运算再相乘。并且也节省了空间（因为我们不需要将低维的数据变形为高维的数据进行存储，可以利用K函数直接计算出结果）。这个K就是核函数（Kernel Function），也叫核函数技巧\n\n\n核函数并不是SVM特有的技巧，只要我们的算法转换成了一个最优化问题，并且在求解这个最优化问题的过程中，存在xi点乘xj这样的式子或者类似这样的式子，我们都可以使用这种技巧\n2.以多项式核函数为例，看核函数时怎么运作的\n首先讨论二次方的多项式项核函数\n\n对于d次方的多项式\n\n"},"支撑向量机SVM/11.7 RBF核函数.html":{"url":"支撑向量机SVM/11.7 RBF核函数.html","title":"11.7 RBF核函数","keywords":"","body":"11.7 RBF核函数\n1.高斯核函数\n\n高斯核函数也叫RBF核（Radial Basis Function Kernel）\n我们之前说，对于多项式核函数来说，他的本质就是对于没一个数据点添加了多项式项，在将这些多项式的新的数据特征进行点乘就形成了新的多项式核函数。\n高斯核函数本质也应该是将原来的数据点映射成了新的特征向量，然后是这种新的特征向量点成的结果。\n事实上正是这样。只不过高斯函数表达出的这种数据的映射是非常复杂的。\n\n高斯函数是将每一个样本点映射到了无穷维的特征空间，这背后的变形是非常复杂的，但是变形之后再进行点乘的结果却是非常简单的。这再次显示了核函数这个工具的威力，他不需要我们具体的计算出来这个样本点怎么映射成新的样本点，我们只需要关注最终的点乘运算结果就可以了\n\n2.用一个简单的例子模拟高斯函数到底在座什么事情\n使用多项式特征为什么能处理非线性的数据问题：他的基本原理是依靠升维使得原本线性不可分的数据线性可分。\n\n对于这样一个一维的数据，他是线性不可分的，我们很难画一跟直线，将这些样本点区分开。\n但是如果我们添加上多项式特征的话，相当于我们在座的事情就是升维。让我们的数据点不但有横轴的值，还有第二个维度的值，也就是X2.一点我们这样做了，我们原来的数据点就变成了线性可分的。这就是升维的意义\n\n高斯函数的本质也是这样的。\n首先对高斯核函数进行一下改变，我们把y的值不取样本点，而取固定的的点，取两个固定的点分别叫l1，l2（landmark）。高斯核函数做的升维过程，就是对于每一个x的值，如果他有两个地标的话，就把他们升维成一个二维的样本点。\n\n3.使用程序直观理解高斯函数\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(-4,5,1)\n\nx\n\narray([-4, -3, -2, -1,  0,  1,  2,  3,  4])\ny = np.array((x>=-2) & (x\ny\n\narray([0, 0, 1, 1, 1, 1, 1, 0, 0])\nplt.scatter(x[y==0], [0]*len(x[y==0]))\nplt.scatter(x[y==1], [0]*len(x[y==1]))\n\n\n\ndef gaussian(x, l):\n    gamma = 1.0\n    return np.exp(-gamma * (x-l)**2)\n\nl1, l2 = -1, 1\nX_new = np.empty((len(x), 2))\nfor i, data in enumerate(x):\n    X_new[i, 0] = gaussian(data,l1) \n    X_new[i, 1] = gaussian(data,l2)\n\nplt.scatter(X_new[y==0,0],X_new[y==0,1])\nplt.scatter(X_new[y==1,0],X_new[y==1,1])\n\n\n\n这样我们就将高斯函数从一个一维的数据映射成了二维的数据，这里，很显然我们可以通过一根直线来区分两种类别，原来在一维空间中线性不可分的空间，在二维空间中变的线性可分了\n\n\n对于高斯函数，每一个数据点都是landmark，也就是对于每一个x，他都要尝试对于每一个样本y，进行核函数的计算，成为新的高维数据相对于的某一维的元素。\n他讲原本mn 的数据映射成了mm的数据，如果m非常的大，那么经过高斯核函数后，就映射成了一个非常非常高维空间的数据点。\n对于高斯函数时将样本点映射成了无穷维空间的理解。如果样本点可以有无穷多个，那么就是将每一个样本点映射到了无穷维的空间\n\n当我们使用高斯核函数的时候，其实这个计算开销是非常大的，也正是因为这样，在使用高斯核函数进行训练的时候，训练时间会比较长。\n尽管如此，还是有很多的场景是比较适合使用高斯函数的，比如样本的特征非常多，但是样本点的数量可能并不多，也就是m\n"},"支撑向量机SVM/11.8 RBF核函数中的gamma.html":{"url":"支撑向量机SVM/11.8 RBF核函数中的gamma.html","title":"11.8 RBF核函数中的gamma","keywords":"","body":"11.8 RBF核函数中的gamma\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\n\nX,y = datasets.make_moons(noise=0.15, random_state=666)\n\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\n\ndef RBFKernalSVC(gamma=1.0):\n    return Pipeline([\n        (\"std_scaler\",StandardScaler()),\n        (\"svc\",SVC(kernel=\"rbf\", gamma=gamma))\n    ])\n\nsvc = RBFKernalSVC()\nsvc.fit(X,y)\n\nPipeline(memory=None,\n     steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape='ovr', degree=3, gamma=1.0, kernel='rbf',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False))])\ndef plot_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\nplot_decision_boundary(svc,axis=[-1.5,2.5,-1.0,1.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\nsvc_gamma100 = RBFKernalSVC(gamma=100)\nsvc_gamma100.fit(X, y)\n\nPipeline(memory=None,\n     steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape='ovr', degree=3, gamma=100, kernel='rbf',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False))])\nplot_decision_boundary(svc_gamma100,axis=[-1.5,2.5,-1.0,1.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\nγ越大，正态分布对应的中型图案越窄。在这里RBF的kernal对应的γ变大了以后，这个决策边界针对其中的某一类，对于这一类他的每一个样本点的周围都形成了一个中型的图案，我们可以把这个图看成我们是在俯视这个中型图案，每个蓝色的点就是图案的尖。由于我们的γ值比较大，所以中型图案比较窄。在每一个蓝色的点周围都围绕了一定的区域，只有在这个区域内，我们才判断成蓝色的点，否则判断为红色的点。这也是高斯核函数的几何意义。\n当然这显然是过拟合了\nsvc_gamma10 = RBFKernalSVC(gamma=10)\nsvc_gamma10.fit(X, y)\nplot_decision_boundary(svc_gamma10,axis=[-1.5,2.5,-1.0,1.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n在我们调小gamma以后，可以想象成每一个蓝色点周围的中型图案变宽了一些，由于这些蓝色的点离的比较近，所以图案连在了一起\n这时候回过头来看gamma=1的时候，其实就是蓝色的点周围的中型图案变的更宽了\nsvc_gamma05 = RBFKernalSVC(gamma=0.5)\nsvc_gamma05.fit(X, y)\nplot_decision_boundary(svc_gamma05,axis=[-1.5,2.5,-1.0,1.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\nsvc_gamma01 = RBFKernalSVC(gamma=0.1)\nsvc_gamma01.fit(X, y)\nplot_decision_boundary(svc_gamma01,axis=[-1.5,2.5,-1.0,1.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n当gamma越来越小，决策边界越来越像一条直线，开始变的欠拟合。\n当我们使用SVM算法，我们的kernal选用高斯Kernal，我们的gamma值相当于在调整模型的复杂度。\ngamma越低，模型复杂度越低，欠拟合程度越低\ngamma越高，模型复杂度越高，过拟合程度越高\n"},"支撑向量机SVM/11.9 SVM思想解决回归问题.html":{"url":"支撑向量机SVM/11.9 SVM思想解决回归问题.html","title":"11.9 SVM思想解决回归问题","keywords":"","body":"11.9 SVM思想解决回归问题\n回归算法的本质就是找到一根直线或者一个曲线能够最大程度上的拟合我们的数据点，如何定义拟合，就是不同的回归算法的关键。比如线性回归的算法定义拟合的方式就是让数据点到直线的MSE的值最小。\n对于SVM的算法来说。对于拟合的定义是，我们要定义一个mergin值，在margin范围内，我们能够包含的样本点最多。也就代表我们这个范围能够比较好的来表达我们的样本数据点，在这种情况下，我们取中间的直线作为我们真正的回归的结果，用他来预测其他点，位置点的值。\n在这种情况下，SVM干的事，和解决分类算法是相反的过程。我们期望的是在margin范围里，包围的点越多越好。\n在具体训练svm回归问题的结果的时候，我们对margin是有一个指定的，所以在这里引入了一个超参数ɛ，代表margin范围的两根直线的任意一跟到中间直线的距离。\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\n\nboston = datasets.load_boston()\nX = boston.data\ny = boston.target\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 666)\n\nfrom sklearn.svm import LinearSVR # Support Vector Regression\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\ndef StandardLinearSVR(epsilon = 0.1):\n    return Pipeline([\n        (\"std_scaler\", StandardScaler()),\n        (\"linear_svr\", LinearSVR(epsilon= epsilon))\n    ])\n\nsvr = StandardLinearSVR()\nsvr.fit(X_train, y_train)\n\nPipeline(memory=None,\n     steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svr', LinearSVR(C=1.0, dual=True, epsilon=0.1, fit_intercept=True,\n     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n     random_state=None, tol=0.0001, verbose=0))])\nsvr.score(X_test, y_test)\n\n0.6360330630824074\n这里分数不是很高。但是使用SVR有很多超参数可以调节，比如C。如果我们使用SVR，还可以对不同的kernal还有不同的参数调节。对于高斯Kernal来说就需要对γ进行调节，对于polynomial来说有degree和C进行调节。\n"},"11jue-ce-shu.html":{"url":"11jue-ce-shu.html","title":"11.决策树","keywords":"","body":""},"121-shi-yao-shi-jue-ce-shu.html":{"url":"121-shi-yao-shi-jue-ce-shu.html","title":"12.1 什么是决策树","keywords":"","body":"12.1 什么是决策树\n\n这样的一个招聘过程形成了一个树结构，这颗树的所有叶子结点其实就是我们最终的决策。也相对于是对与输入（录用者信息）的分类（录用/考察）。这样的一个过程，就是决策树。\n对于这样的决策树来说，他具有数据结构利用树的所有性质。包括根结点，叶子结点，深度。在这里我们称决策树的深度为3\n使用sklearn中的决策树直观的感受\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\n\niris = datasets.load_iris()\n\nX = iris.data[:,2:]\ny = iris.target\n\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\nplt.scatter(X[y==2,0],X[y==2,1])\n\n\n\n# 引入决策树\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_cfl = DecisionTreeClassifier(max_depth=2, criterion=\"entropy\")\ndt_cfl.fit(X, y)\n\nDecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')\ndef plot_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\nplot_decision_boundary(dt_cfl, axis=[0.5, 7.5, 0, 3])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\nplt.scatter(X[y==2,0],X[y==2,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n\n这就是决策树在面对属性是数值特征的时候是怎么处理的。\n在每一个结点上，他首先选择一个维度以及这个维度上的一个阈值，分成两支，循环往复，来进行分类\n\n总结\n\n决策树是一个非参数学习的算法\n决策树可以解决分类问题\n天然的可以解决多分类问题\n可以解决回归问题（将最终预测数据点落在叶子节点所有数据点的平均值作为预测值）\n非常好的可解释性\n\n构建决策树的问题\n\n每个结点在哪个维度做划分\n某个维度在哪个值上做划分\n\n\n"},"122-xin-xi-shang.html":{"url":"122-xin-xi-shang.html","title":"12.2 信息熵","keywords":"","body":"12.2 信息熵\n\n\n熵在信息论中代表随机变量中不确定度的度量。\n- 熵越大，数据的不确定性越高\n- 熵越小，数据的不确定性越低\n\n\n\n二分类的信息熵函数\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef entropy(p):\n    return -p * np.log(p) - (1-p) * np.log(1-p)\n\nx = np.linspace(0.01, 0.99, 200)\n\nplt.scatter(x, entropy(x))\n\n\n\n当x=0.5 的时候，信息熵达到了最大值（当两类样本各占一半的时候），也就是说这个时候的样本是最不稳定的\n当系统每一个类别都是等概率的时候，其实是他最不确定的时候，此时他的信息熵是最高的。如果系统偏向于某一类，相当于有了约定向，信息熵逐渐降低，知道有一个类别占到了百分之百，此时信息熵达到最低值0\n\n了解了信息熵的概念，解决上面的两个问题就好说了。在每一个结点上，都希望在某一个维度上基于某一个阈值进行划分，在划分以后要做的事情就是要让我们的数据划分成两部分之后，相应我们的系统整体的信息熵降低（也就是让我们的系统变的更加确定）\n接下来的任务，就是找每个节点上游一个维度，在这个维度上有一个取值，根据这个取值进行划分，划分后是所有其他划分方式的信息熵中的最小值。我们就成当前的划分方式就是一个最好的划分。找到这个划分的方法就是对所有的可能性进行搜索。\n"},"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html":{"url":"123-shi-yong-xin-xi-shang-xun-zhao-zui-you-hua-fen.html","title":"12.3 使用信息熵寻找最优划分","keywords":"","body":"12.3 使用信息熵寻找最优划分\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\n\niris = datasets.load_iris()\n\nX = iris.data[:,2:]\ny = iris.target\n\n# 引入决策树\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_cfl = DecisionTreeClassifier(max_depth=2, criterion=\"entropy\")\ndt_cfl.fit(X, y)\n\nDecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')\ndef plot_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\nplot_decision_boundary(dt_cfl, axis=[0.5, 7.5, 0, 3])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\nplt.scatter(X[y==2,0],X[y==2,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n模拟使用信息熵进行划分\ndef spilt(X, y, d, value):\n    index_a = (X[:,d]  value)\n    return X[index_a], X[index_b], y[index_a], y[index_b]\n\nfrom collections import Counter\nfrom math import log\n\ndef entropy(y):\n    counter = Counter(y)\n    res = 0.0\n    for num in counter.values():\n        p = num / len(y)\n        res += -p*log(p)\n    return res\n\ndef try_spilt(X, y):\n    best_entropy = float('inf')\n    best_d, best_v = -1,-1\n    for d in range(X.shape[1]):\n        # 遍历每一个维度\n        sorted_index = np.argsort(X[:,d])\n        for i in range(1,len(X)):\n            # 计算每两个值之间的信息熵，所以从1开始\n            if X[sorted_index[i-1],d] != X[sorted_index[i],d]:\n                # 将两个数据点的平均值作为切分点\n                v = (X[sorted_index[i-1],d] + X[sorted_index[i],d]) / 2\n                X_l, X_r, y_l, y_r = spilt(X, y, d, v)\n                # 分别计算两部分的信息熵然后相加\n                e = entropy(y_l) + entropy(y_r)\n                if e\nbest_entropy, best_d, best_v = try_spilt(X, y)\nprint(best_entropy)\nprint(best_d)\nprint(best_v)\n# 对比sklearn的划分结果，就是在横轴上（第0个维度），大约2.45的位置进行了划分\n\n0.6931471805599453\n0\n2.45\nX1_l, X1_r, y1_l, y1_r = spilt(X, y ,best_d, best_v)\n\nentropy(y1_l)\n\n0.0\nentropy(y1_r)\n\n0.6931471805599453\nbest_entropy2, best_d2, best_v2 = try_spilt(x1_r, y1_r)\nprint(best_entropy2)\nprint(best_d2)\nprint(best_v2)\n# 在第一个维度0.75的位置进行划分，划分的结果的信息熵为0.41左右\n\n0.4132278899361904\n1\n1.75\nX2_l, x2_r, y2_l, y2_r = spilt(X1_r,y1_r,best_d2,best_v2)\n\nentropy(y2_l)\n\n0.30849545083110386\nentropy(y2_r)\n\n0.10473243910508653\n"},"124-ji-ni-xi-shu.html":{"url":"124-ji-ni-xi-shu.html","title":"12.4 基尼系数","keywords":"","body":"12.4 基尼系数\n基尼系数的意义和信息熵相同\n\n二分类基尼系数函数\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\n\niris = datasets.load_iris()\n\nX = iris.data[:,2:]\ny = iris.target\n\n# 引入决策树\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_cfl = DecisionTreeClassifier(max_depth=2, criterion=\"gini\")\ndt_cfl.fit(X, y)\n\nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')\ndef plot_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\nplot_decision_boundary(dt_cfl, axis=[0.5, 7.5, 0, 3])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\nplt.scatter(X[y==2,0],X[y==2,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n模拟使用基尼系数进行划分\n只需要把信息熵的函数改成基尼系数\ndef spilt(X, y, d, value):\n    index_a = (X[:,d]  value)\n    return X[index_a], X[index_b], y[index_a], y[index_b]\n\nfrom collections import Counter\nfrom math import log\n\ndef gini(y):\n    counter = Counter(y)\n    res = 1.0\n    for num in counter.values():\n        p = num / len(y)\n        res -= p**2\n    return res\n\ndef try_spilt(X, y):\n    best_g = float('inf')\n    best_d, best_v = -1,-1\n    for d in range(X.shape[1]):\n        # 遍历每一个维度\n        sorted_index = np.argsort(X[:,d]) #在每一个特征维度上排序\n        for i in range(1,len(X)):\n            # 计算每两个值之间的信息熵，所以从1开始\n            if X[sorted_index[i-1],d] != X[sorted_index[i],d]:\n                # 将两个数据点的平均值作为切分点\n                v = (X[sorted_index[i-1],d] + X[sorted_index[i],d]) / 2\n                X_l, X_r, y_l, y_r = spilt(X, y, d, v)\n                # 分别计算两部分的基尼系数然后相加\n                e = gini(y_l) + gini(y_r)\n                if e\nbest_g, best_d, best_v = try_spilt(X, y)\nprint(best_g)\nprint(best_d)\nprint(best_v)\n\n0.5\n0\n2.45\nX1_l, X1_r, y1_l, y1_r = spilt(X, y ,best_d, best_v)\n\ngini(y1_l)\n\n0.0\ngini(y1_r)\n\n0.5\nbest_g2, best_d2, best_v2 = try_spilt(X1_r, y1_r)\nprint(best_g2)\nprint(best_d2)\nprint(best_v2)\n\n0.2105714900645938\n1\n1.75\nX2_l, x2_r, y2_l, y2_r = spilt(X1_r,y1_r,best_d2,best_v2)\n\ngini(y2_l)\n\n0.1680384087791495\ngini(y2_r)\n\n0.04253308128544431\n\n总结\n\n熵信息的计算比基尼系数稍慢\nscikit-learn中默认使用基尼系数\n大多数时候二者没有特别的效果优劣\n\n\n"},"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html":{"url":"crat-yu-jue-ce-shu-zhong-de-chao-can-shu.html","title":"CRAT 与决策树中的超参数","keywords":"","body":"12.5 CRAT 与决策树中的超参数\n\nCRAT： Classification And Regression Tree \n根据某一个维度d和某一个阈值v进行二分\nscikit-learn的决策实现：CRAT\nID3，C4.5，C5.0等是使用其他的方式实现决策树\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\n\nX, y = datasets.make_moons(noise=0.25, random_state=666)\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# 默认使用基尼系数划分数据\n# 不传max_depth会一直划分直到基尼系数为0为止\ndt_clf = DecisionTreeClassifier()\ndt_clf.fit(X, y)\n\nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')\ndef plot_decision_boundary(model,axis):\n    \"\"\"\n    model：模型\n    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围\n    \"\"\"\n    # 使用linspace将x轴，y轴划分成无数的小点\n    x0,x1 = np.meshgrid(\n        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(),x1.ravel()]\n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)\n\n# 决策边界非常不规则，产生了过拟合\nplot_decision_boundary(dt_clf, axis=[-1.5, 2.5, -1.0, 1.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\ndt_clf2 = DecisionTreeClassifier(max_depth=2)\ndt_clf2.fit(X, y)\n\n# 很显然，现在这个样子相比上面的形状不在有过拟合，有了非常清晰的边界（不会针对某几个特别的样本点进行特殊的变化）\nplot_decision_boundary(dt_clf2, axis=[-1.5, 2.5, -1.0, 1.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n这种情况下，很有可能存在欠拟合，所以我们要对这些参数进行比较精细的调整，让他既不过拟合也不欠拟合\n# 对于一个结点来说，至少要有多少个样本数据，我们才对他继续进行拆分下去\ndt_clf3 = DecisionTreeClassifier(min_samples_split=10)\ndt_clf3.fit(X, y)\n\nplot_decision_boundary(dt_clf3, axis=[-1.5, 2.5, -1.0, 1.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n# 对于叶子节点来说，至少要有几个样本\ndt_clf4 = DecisionTreeClassifier(min_samples_leaf=6)\ndt_clf4.fit(X, y)\n\nplot_decision_boundary(dt_clf4, axis=[-1.5, 2.5, -1.0, 1.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n# 决策树最多的叶子节点\ndt_clf5 = DecisionTreeClassifier(max_leaf_nodes=4)\ndt_clf5.fit(X, y)\n\nplot_decision_boundary(dt_clf5, axis=[-1.5, 2.5, -1.0, 1.5])\nplt.scatter(X[y==0,0],X[y==0,1])\nplt.scatter(X[y==1,0],X[y==1,1])\n\n/anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'\n  s)\n\n\n\n\n\n\n\n\n"},"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html":{"url":"126-jue-ce-shu-jie-jue-hui-gui-wen-ti.html","title":"12.6 决策树解决回归问题","keywords":"","body":"12.6 决策树解决回归问题\n在决策树建立之后，每个叶子结点都包含了一些数据。\n\n如果我们的数据输出的是类别的话，我们就让这些叶子结点包含的数据进行投票，票数最多的即为我们输出的分类\n如果我们的数据输出的是数据的话，那就是回归问题所解决的问题，一个预测的数据来临之后，通过决策树到达了某一个叶子节点，我们就可以将这些叶子节点包含数据的平均值作为我们的预测值\n\nscikit-learn封装的决策树解决回归问题\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\n\nboston = datasets.load_boston()\nX = boston.data\ny = boston.target\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 666)\n\nDecision Tree Regression\nfrom sklearn.tree import DecisionTreeRegressor\n\ndt_reg = DecisionTreeRegressor()\ndt_reg.fit(X_train, y_train)\n\nDecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best')\ndt_reg.score(X_test, y_test)\n\n0.5782292347434448\n# 在训练数据集上预测的准确率是百分之百的---过拟合\ndt_reg.score(X_train, y_train)\n\n1.0\n"},"127-jue-ce-shu-de-ju-xian-xing.html":{"url":"127-jue-ce-shu-de-ju-xian-xing.html","title":"12.7 决策树的局限性","keywords":"","body":"12.7 决策树的局限性\n\n决策边界必须是横平竖直的\n对数据敏感--案例\n\n"},"13ji-cheng-xue-xi-he-sui-ji-sen-lin.html":{"url":"13ji-cheng-xue-xi-he-sui-ji-sen-lin.html","title":"13.集成学习和随机森林","keywords":"","body":""},"131-shi-yao-shi-ji-cheng-xue-xi.html":{"url":"131-shi-yao-shi-ji-cheng-xue-xi.html","title":"13.1 什么是集成学习","keywords":"","body":"13.1 什么是集成学习\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\n\nX, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=42)\nplt.scatter(X[y==0, 0],X[y==0, 1])\nplt.scatter(X[y==1, 0],X[y==1, 1])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression()\nlog_clf.fit(X_train, y_train)\nlog_clf.score(X_test, y_test)\n\n0.824\nfrom sklearn.svm import SVC\n\nsvm_clf = SVC()\nsvm_clf.fit(X_train, y_train)\nsvm_clf.score(X_test, y_test)\n\n0.88\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, y_train)\ndt_clf.score(X_test, y_test)\n\n0.84\n# 手动完成集成学习的过程\ny_predict1 = log_clf.predict(X_test)\ny_predict2 = svm_clf.predict(X_test)\ny_predict3 = dt_clf.predict(X_test)\n\n# 只有至少两个算法预测结果为1，三个相加才会大于2\ny_predict = np.array((y_predict1+y_predict2+y_predict3) >= 2, dtype='int')\n\ny_predict[:10]\n\narray([1, 1, 0, 0, 0, 1, 0, 1, 0, 1])\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, y_predict)\n\n0.896\n使用Voting Classifier\nfrom sklearn.ensemble import VotingClassifier\n\n# hard 少数服从多数\nvoting_clf = VotingClassifier(estimators=[\n    (\"log_clf\", LogisticRegression()),\n    (\"svm_clf\", SVC()),\n    (\"dt_clf\", DecisionTreeClassifier())\n], voting=\"hard\")\n\nvoting_clf.fit(X_train, y_train)\nvoting_clf.score(X_test, y_test)\n\n/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n  if diff:\n\n\n\n\n\n0.88\n"},"132-softvoting-classifier.html":{"url":"132-softvoting-classifier.html","title":"13.2 Softvoting Classifier","keywords":"","body":"13.2 Softvoting Classifier\n更合理的投票，应该有权值\n\nHard Voting\n\nSoft Voting\n\n要求集合的每一个模型都能估算概率\n逻辑回归可以估算概率\nKNN可以估算规律，结果占离他最近的k个点\n决策树，叶子结点中占比例最大的类别数据占整个叶子结点量的比值\nSVM算法：probability: boolean, optional(default=False)-> True \nhttp://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n\n\n\nvoting_clf2 = VotingClassifier(estimators=[\n    (\"log_clf\", LogisticRegression()),\n    (\"svm_clf\", SVC(probability=True)),\n    (\"dt_clf\", DecisionTreeClassifier(random_state=666))\n], voting=\"soft\")\n\nvoting_clf2.fit(X_train, y_train)\nvoting_clf2.score(X_test, y_test)\n0.921\n"},"133-bagging-and-pasting.html":{"url":"133-bagging-and-pasting.html","title":"13.3 Bagging and Pasting","keywords":"","body":"13.4 Bagging and Pasting\n虽然有很多机器学习方法\n但是从投票的角度看，还是不够多\n创建更多的子模型！集成更多的子模型的意见\n子模型之间不能一直！模型之间要有差异\n如何创建差异性？\n只看样本的一部分\n例如: -共有500个样本数据;每个子模型只看100个样本数据，每个子模型不需要太高的准确率\n如果每个子模型只有51%的准确率\n\n如果每个子模型有60%的准确率\n\n取样：放回取样（Bagging），不放回取样（Pasting）\nBagging更常用\n统计学中，放回取样：bootstrap\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n# n_estimators 集成几个模型\n# max_samples每个子模型看几个样本数据\n# bootstrap 是否放回取样\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n                                n_estimators=500,\n                                max_samples=100,\n                                bootstrap=True)\nbagging_clf.fit(X_train, y_train)\nbagging_clf.score(X_test, y_test)\n0.904\n\nbagging_clf2 = BaggingClassifier(DecisionTreeClassifier(),\n                                n_estimators=5000,\n                                max_samples=100,\n                                bootstrap=True)\nbagging_clf2.fit(X_train, y_train)\nbagging_clf2.score(X_test, y_test)\n\n0.92\n\n"},"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html":{"url":"134-oob-out-of-bag-he-guan-yu-bagging-de-geng-duo-tao-lun.html","title":"13.4 oob (Out-of-Bag) 和关于Bagging的更多讨论","keywords":"","body":"13.4 oob (Out-of-Bag) 和关于Bagging的更多讨论\n放回取样导致一部分样本很有可能没有取到\n平均大约有37%的样本没有取到。这些样本就叫Out-of-Bag.\n不使用测试数据集，而使用这部分没有取到的样本做测试/验证。\nsikit-learn：oob_score_\n使用oob\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n                                n_estimators=500,\n                                max_samples=100,\n                                bootstrap=True,\n                                oob_score=True)\nbagging_clf.fit(X, y)\nbagging_clf.oob_score_\n\n0.92\n\nBagging的思路极易并行化处理\nsikit-learn中对于并行处理的算法，可以传入n_jobs来调整使用几个核来处理\n使用n_jobs\n%%time\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n                                n_estimators=500,\n                                max_samples=100,\n                                bootstrap=True,\n                                oob_score=True)\nbagging_clf.fit(X, y)\n\nCPU times: user 575 ms, sys: 4.64 ms, total: 580 ms\nWall time: 578 ms\n\n%%time\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n                                n_estimators=500,\n                                max_samples=100,\n                                bootstrap=True,\n                                oob_score=True,\n                                n_jobs=-1)\nbagging_clf.fit(X, y)\n\nCPU times: user 166 ms, sys: 54 ms, total: 220 ms\nWall time: 480 ms\n\n使模型产生差异化\n针对特征进行随机采样\nRandom Subspaces\n既针对样本，又针对特征进行随机采样\nRandom Patches\n\n# max_features 对特征随机取样\n# bootstrap_features 对特征进行随机取样的方式\nrandom_subspaces_clf = BaggingClassifier(DecisionTreeClassifier(),\n                                n_estimators=500,\n                                max_samples=500,\n                                bootstrap=True,\n                                oob_score=True,\n                                n_jobs=-1,\n                                max_features=1,\n                                bootstrap_features=True)\nrandom_subspaces_clf.fit(X, y)\nrandom_subspaces_clf.oob_score_\n\n0.82\n\n# max_features 对特征随机取样\n# bootstrap_features 对特征进行随机取样的方式\nrandom_Patches_clf = BaggingClassifier(DecisionTreeClassifier(),\n                                n_estimators=500,\n                                max_samples=100,\n                                bootstrap=True,\n                                oob_score=True,\n                                n_jobs=-1,\n                                max_features=1,\n                                bootstrap_features=True)\nrandom_Patches_clf.fit(X, y)\nrandom_Patches_clf.oob_score_\n0.864\n\n使用决策树方式进行集成学习的方式也叫随机森林\n"},"135-sui-ji-sen-lin.html":{"url":"135-sui-ji-sen-lin.html","title":"13.5 随机森林","keywords":"","body":"13.5 随机森林\nBagging\nBase Estimator: Decision Tree\n决策树在节点划分上，在随机的特征子集上寻找最优划分特征\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=500, \n                                random_state=666, \n                                oob_score=True, \n                                n_jobs=-1)\nrf_clf.fit(X, y)\n\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n            oob_score=True, random_state=666, verbose=0, warm_start=False)\nrf_clf.oob_score_\n\n0.896\n# 随机森林包含决策树的参数\nrf_clf2 = RandomForestClassifier(n_estimators=500, \n                                random_state=666, \n                                max_leaf_nodes=10,\n                                oob_score=True, \n                                n_jobs=-1)\nrf_clf2.fit(X, y)\nrf_clf2.oob_score_\n\n0.912\nExtra-Trees\n决策树在节点划分上，使用随机的特征和随机的阈值(理论基础：根据BAGGING CLASSFIFER 的原理，只要大多数的决策树的决策能力比扔硬币的能力好一点就够了)\n提供额外的随机性，抑制过拟合，但增大了bias\n更快的训练速度\nfrom sklearn.ensemble import ExtraTreesClassifier\n\net_clf = ExtraTreesClassifier(n_estimators=500, bootstrap=True, oob_score=True, random_state=666)\net_clf.fit(X, y)\n\nExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n           max_depth=None, max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n           oob_score=True, random_state=666, verbose=0, warm_start=False)\n\net_clf.oob_score_\n0.892\n"},"136-ada-boosting-he-gradient-boosting.html":{"url":"136-ada-boosting-he-gradient-boosting.html","title":"13.6 Ada Boosting 和 Gradient Boosting","keywords":"","body":"13.6 Ada Boosting 和 Gradient Boosting\nAda Boosting\n集成多个模型\n每个模型都在尝试增强(Boosting)整体的效果\n\n使用每次增强训练出来的子模型进行投票得出最终的结果\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=500, random_state=666)\nada_clf.fit(X_train, y_train)\n\n\nAdaBoostClassifier(algorithm='SAMME.R',\n          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best'),\n          learning_rate=1.0, n_estimators=500, random_state=666)\n\nada_clf.score(X_test, y_test)\n0.864\n\nGradient Boosting\n训练一个模型m1,产生错误e1\n針対e1訓繚第二个模型m2,产生錯俣e2\n针对e2训练第三个模型m3,产生错误e3..\n最终预测结果是:m1 + m2 + m3 +\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngrad_clf = GradientBoostingClassifier(max_depth=2, n_estimators=30, random_state=666)\ngrad_clf.fit(X_train, y_train)\n\nGradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.1, loss='deviance', max_depth=2,\n              max_features=None, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=30,\n              presort='auto', random_state=666, subsample=1.0, verbose=0,\n              warm_start=False)\ngrad_clf.score(X_test, y_test)\n0.912\n"},"137-stacking.html":{"url":"137-stacking.html","title":"13.7 Stacking","keywords":"","body":"13.7 Stacking\n\n\n\n层次继续增多，就越来越像是一个神经网络\n"}}}