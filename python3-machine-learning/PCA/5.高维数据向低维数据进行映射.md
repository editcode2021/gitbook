# 5. 高维数据向低维数据进行映射
对于一个数据集X来说，这个X有m行n列，代表有m个样本n个特征，通过我们前面学习的主成分分析法，假设我们已经求出了针对这个数据来说的前k个主成分，每一个主成分对应一个单位方向，用W来表示,W也是一个矩阵，他有k行，代表我们求出的前K个主成分，每一行有n列，代表每一个主成分的坐标轴应该是有n个元素的。这是因为我们的主成分分析法主要就是将数据从一个坐标系转化成了另外一个坐标系，原来这个坐标系有n个维度，现在这个坐标系也应该有n个维度，只不过对于转化的坐标系来说，我们取出来前k个，这k个方向更加重要。

如何将我们的样本X从n维转化成k维呢，回忆们之前学到的，对于一个X样本，与一个W进行点乘，其实就是讲一个样本映射到了w这个坐标轴，得到的模，如果讲这一个样本和这k个w分别做点乘，得到的就是这一个样本，在这k个方向上做映射后每一个方向上的大小，这k个元素合在一起，就代表这一个样本映射到新的k个轴所代表的坐标系上相应的这个样本的大小

X1分别乘以W1到Wn，得到的k个数组成的向量，就是样本1映射到Wk这个坐标系上得到的k维的向量，由于k<n，所以我们就完成了一个样本从n维到k维的映射，这个过程依次类推从样本1到样本m都这么做，我们就将m个样本都从N维映射到了k维-----其实我们就是做了一个乘法**X·W<sup>T</sup>**(为什么是转置呢，因为我们是拿X的每一行去和W的每一行做点乘的，但是矩阵乘法规定是拿X的每一行和W的每一列做乘法)

![5-1](https://upload-images.jianshu.io/upload_images/7220971-8ab7c02a0b60d4cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们得到新的降维后的矩阵X<sub>k</sub>以后，是可以通过和W<sub>k</sub>想乘回复回来的，但是由于我们在降维的过程中丢失了一部分信息，这时及时回复回来也和原来的矩阵不一样了，但是这个从数据角度成立的

![5-2](https://upload-images.jianshu.io/upload_images/7220971-9124b3d1322fde52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 从高维数据向地维数据的映射
```
class PCA:
    
    def __init__(self,n_components):
        """初始化PCA"""
        assert n_components>=1, "n_components must be vaild"
        self.n_components = n_components
        self.components_ = None
        
    def fit(self,X,eta=0.01,n_iters=1e4):
        """获得数据集X的前n个元素"""
        assert self.n_components<=X.shape[1],\
            "n_components must be greater then the feature number of X"
        def f(w,X):
            return np.sum((X.dot(w)**2))/len(X)

        def df(w,X):
            return X.T.dot(X.dot(w)) * 2. /len(X)

        def direction(w):
            """计算单位向量"""
            return w / np.linalg.norm(w)

        def first_componet( X,inital_w,eta,n_iters = 1e4,epsilon=1e-8):
            w = direction(inital_w)
            cur_iter = 0

            while cur_iter < n_iters:
                gradient = df(w,X)
                last_w = w
                w = w + eta * gradient
                # 注意1：每次求单位向量
                w = direction(w) 
                if abs(f(w,X)-f(last_w,X)) < epsilon:
                    break

                cur_iter = cur_iter+1
            return w
        
        X_pca = demean(X)

        self.components_ = np.empty(shape=(self.n_components,X.shape[1]))
        for i in range(self.n_components):
            initial_w = np.random.random(X_pca.shape[1])
            w = first_componet(X_pca,initial_w,eta)
            self.components_[i,:] = w
            
            X_pca = X_pca - X_pca.dot(w).reshape(-1,1) *w
        return self
    
    
    def transform(self,X):
        """将给定的X映射到各个主成分分量中"""
        assert X.shape[1] == self.components_.shape[1]
        
        return X.dot(self.components_.T)
    
    def inverse_transform(self,X):
        """将给定的X反映射会原来的特征空间"""
        assert X.shape[1] == self.components_.shape[0]
        
        return X.dot(self.components_)

    def __repr__(self):
        return "PCA(n_components=%d)" % self.n_components
    

```


 ##### PCA 降维的基本原理:找到另外一个坐标系，这个坐标系每一个轴依次可以表达原来的样本他们的重要程度，也就是称为所有的主成分，我们取得前k个最重要的主成分，就可以将所有的样本映射到这k个轴上，获得一个低维的数据信息