# 6.sklearn中的PCA
```python
import matplotlib.pyplot as plt
plt.scatter(X[:,0],X[:,1],color='b',alpha=0.5)
plt.scatter(X_restore[:,0],X_restore[:,1],color='r',alpha=0.5)
```




<matplotlib.collections.PathCollection at 0x1a1e701198>




![7-1](https://upload-images.jianshu.io/upload_images/7220971-0d47afdaf5041a3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
```

#### 加载书写识别数据集


```python
digits = datasets.load_digits()
X = digits.data
y = digits.target
```


```python
# 分割数据集
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 666)
```

#### 使用64个维度的数据集，训练knn算法



```python
%%time
from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train,y_train)
```

CPU times: user 2.65 ms, sys: 1.46 ms, total: 4.11 ms
Wall time: 2.65 ms



```python
knn_clf.score(X_test,y_test)
```




0.9866666666666667



#### 使用PCA进行降维，然后再训练knn算法


```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca.fit(X_train)
X_train_reduction = pca.transform(X_train)
X_test_reduction = pca.transform(X_test)
```


```python
%%time
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train_reduction,y_train)
```

CPU times: user 1.67 ms, sys: 935 µs, total: 2.61 ms
Wall time: 1.67 ms



```python
# 从64个维度降到两个维度以后，虽然运行速度提高了，但是识别精度大大降低了
knn_clf.score(X_test_reduction,y_test)
```




0.6066666666666667



#### explained_variance_ratio_-解释方差的比例
- 0.14566817 代表第一个主成分可以解释14%的原数据
- 0.13735469 代表第二个主成分可以解释13%的原数据
两个主成分加起来可以解释百分之27的原数据，而其他的信息丢失了

可以使用explained_variance_ratio_这个参数来查看每个主成分所解释的原数据，来判断要取多少个主成分


```python
pca.explained_variance_ratio_
```




array([0.14566817, 0.13735469])




```python
pca = PCA(n_components=64)
pca.fit(X_train)
# 这个数据可以近乎表示每个主成分轴的重要程度
pca.explained_variance_ratio_
```




array([1.45668166e-01, 1.37354688e-01, 1.17777287e-01, 8.49968861e-02,
5.86018996e-02, 5.11542945e-02, 4.26605279e-02, 3.60119663e-02,
3.41105814e-02, 3.05407804e-02, 2.42337671e-02, 2.28700570e-02,
1.80304649e-02, 1.79346003e-02, 1.45798298e-02, 1.42044841e-02,
1.29961033e-02, 1.26617002e-02, 1.01728635e-02, 9.09314698e-03,
8.85220461e-03, 7.73828332e-03, 7.60516219e-03, 7.11864860e-03,
6.85977267e-03, 5.76411920e-03, 5.71688020e-03, 5.08255707e-03,
4.89020776e-03, 4.34888085e-03, 3.72917505e-03, 3.57755036e-03,
3.26989470e-03, 3.14917937e-03, 3.09269839e-03, 2.87619649e-03,
2.50362666e-03, 2.25417403e-03, 2.20030857e-03, 1.98028746e-03,
1.88195578e-03, 1.52769283e-03, 1.42823692e-03, 1.38003340e-03,
1.17572392e-03, 1.07377463e-03, 9.55152460e-04, 9.00017642e-04,
5.79162563e-04, 3.82793717e-04, 2.38328586e-04, 8.40132221e-05,
5.60545588e-05, 5.48538930e-05, 1.08077650e-05, 4.01354717e-06,
1.23186515e-06, 1.05783059e-06, 6.06659094e-07, 5.86686040e-07,
1.71368535e-33, 7.44075955e-34, 7.44075955e-34, 7.15189459e-34])



#### 绘制曲线观察取前i个主成分的时候，所能解释的原数据比例


```python
plt.plot([i for i in range(X_train.shape[1])],
[np.sum(pca.explained_variance_ratio_[:i+1]) for i in range(X_train.shape[1])])
```




[<matplotlib.lines.Line2D at 0x1a1fc6eef0>]




![7-2](https://upload-images.jianshu.io/upload_images/7220971-a4d163dccaf770a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



#### sklearn中的PCA算法支持传入一个小于1的数来表示我们希望能解释多少比例的主成分


```python
pca = PCA(0.95)
pca.fit(X_train)
# 说明前28个主成分表示了百分之95的信息
pca.n_components_
```




28




```python
X_train_reduction = pca.transform(X_train)
X_test_reduction = pca.transform(X_test)
```


```python
%%time
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train_reduction,y_train)
```

CPU times: user 2.43 ms, sys: 1.21 ms, total: 3.63 ms
Wall time: 2.55 ms



```python
# 虽然训练出来的精度丢失了一些，但是效率却大大提高了
knn_clf.score(X_test_reduction,y_test)
```




0.98



#### PCA降维到两维的意义-可以方便可视化展示，帮助人们理解
下图每个颜色代表一个数字在降维到二维空间中的分布情况
仔细观察后可以发现，很多数字的区分还是比较明细的
比如如果只是区分蓝色的数字和紫色的数字，那么使用二个维度就足够了


```python
pca = PCA(n_components=2)
pca.fit(X)
X_reduction = pca.transform(X)
for i in range(10):
plt.scatter(X_reduction[y==i,0],X_reduction[y==i,1],alpha=0.8)
```


![7-3](https://upload-images.jianshu.io/upload_images/7220971-4eb39be87a98e901.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




```python
X_train_reduction.shape
```




(1347, 2)




```python
y.shape
```




(1797,)
