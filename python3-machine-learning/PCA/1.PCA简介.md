# 1.PCA简介
PCA（Principal Component Analysis）：也是一个梯度分析的应用，不仅是机器学习的算法，也是统计学的经典算法
![1-1](https://upload-images.jianshu.io/upload_images/7220971-e5208d3422668f29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 1.1 举个栗子
例如下面一个两个特征的一个训练集，我们可以选择一个特征，扔掉一个特征
![1.1-1](https://upload-images.jianshu.io/upload_images/7220971-375fc4de759f8eb6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
下图分别是扔掉了特征一和特征二的两种方案，很明显右边这种的效果会更好一些，因为访问二扔掉特征二以后，点之间的分布情况更接近与原图，但是这不是更好的
![1.1-2](https://upload-images.jianshu.io/upload_images/7220971-aa4527420a30ce4f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
我们希望有一根直线，是斜着的，我们希望将所有的点都映射到这条直线上，那么这个时候我们就成功的将二维降到了一维，与此同时，这些点更加趋近与原来的点的分布情况，换句话说，点和点之间的距离比无论是映射到x还是映射到y周，他们之间的区分度都更加的大，也就更加容易区分
![1.1-3](https://upload-images.jianshu.io/upload_images/7220971-e9baadaac406f499.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 1.2 总结
那么如何找到这个让样本间间距最大的轴?
如何定义样本间间距?
事实上有一个指标可以之间定义样本间的距离，就是方差（Variance）（方差：描述样本整体之间的疏密的一个指标，方差越大，代表样本之间越稀疏，方差越小，代表样本之间越紧密）
![1.2-1：方差](https://upload-images.jianshu.io/upload_images/7220971-5b85b7cc0c62c555.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![1-6](https://upload-images.jianshu.io/upload_images/7220971-677c3c4fd9bbc02d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 第一步: 将样例的均值归为0 (demean)（归0：所有样本都减去他们的均值），使得均值为0，这样可以简化方差的公式
![1.2-2](https://upload-images.jianshu.io/upload_images/7220971-82777ccfb6c62775.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 1.3 推导
![1.3-1](https://upload-images.jianshu.io/upload_images/7220971-4143ed69b327f4d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
进行均值归0操作以后，就是下面的式子
![1.3-2](https://upload-images.jianshu.io/upload_images/7220971-16b93305129b3edd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

注：|X<sub>project</sub>|的平均值也是一个向量

X<sup>(i)</sup>映射到w的距离实际上就是X<sup>(i)</sup>与w的点乘（蓝色的线），根据定义推导，其值实际上就是X<sub>project</sub>

![1.3-3](https://upload-images.jianshu.io/upload_images/7220971-f5c3cc35287a9c5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
此时我们的目标函数就可以化简成


![1.3-4](https://upload-images.jianshu.io/upload_images/7220971-1d87968b9d8167bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这是一个目标函数的最优化问题，使用梯度上升法解决。
当然我们也可以之间使用数学原理推导出结果，这里我们主要关注使用搜索的策略来求解主成分分析法，这样我们对梯度上升发和梯度下降法也可以有一个更深刻的认识

## 1.4 与线性回归的区别

![1.4-1](https://upload-images.jianshu.io/upload_images/7220971-d83ef8e4bbfdcfb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![1.4-2](https://upload-images.jianshu.io/upload_images/7220971-677c3c4fd9bbc02d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
1.主成分分析法的两个轴都是特征，线性回归y轴是目标结果值
2.主成分分析法的点是垂直于方差轴直线的，线性回归的点事垂直于x轴的



## 1.5 对PCA和线性代数中的特征向量的对比
>一个矩阵可以把一个向量拉伸或者缩短λ倍，这个向量就是特征向量，λ是特征值；
那么能够拉伸最长的特征向量，实际上就是这个矩阵的第一主成分；
一组线性不相关的特征向量可以组成一个特征向量空间；
所以主成分分析实际上就是把矩阵转换到了一个特殊的特征向量空间；
这个特征向量空间是由能够被拉伸最长的前k个特征向量组成的；而且这k个特征向量相互正交；