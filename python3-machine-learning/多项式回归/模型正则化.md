# 7.模型正则化-Regularization

## 1.什么是模型正则化

下图是我们之前使用多项式回归过拟合一个样本的例子，可以看到这条模型曲线非常的弯曲，而且非常的陡峭，可以想象这条曲线的一些θ系数会非常的大。
模型正则化需要做的事情就是限制这些系数的大小
![image.png](https://upload-images.jianshu.io/upload_images/7220971-5d3e62f5cf7f3bfd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 模型正则化基本原理

![image.png](https://upload-images.jianshu.io/upload_images/7220971-12abd8f03963b71d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

>一些需要注意的细节：
1. 对于θ的求和i是从1到n,没有将θ<sub>0</sub>加进去，因为他不是任意一项的系数，他只是一个截距，决定了整个曲线的高低，但是不决定曲线每一部分的陡峭和缓和
2. θ求和的系数二分之一是一个惯例，加不加都可以，加上的原因是因为，将来对θ<sup>2</sup>>求导的时候可以抵消系数2，方便计算。不要也是可以的
3. α实际上是一个超参数，代表在我们模型正则化下新的损失函数中，我们要让每一个θ尽可能的小，小的程度占我们整个损失函数的多少，如果α等于0，相当于没有正则化；如果α是正无穷的话，那么我们主要的优化任务就是让每一个θ尽可能的小

### 岭回归 Ridge Regression
![image.png](https://upload-images.jianshu.io/upload_images/7220971-b3aca68e141af312.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 2.编程实现岭回归



```python
import numpy as np
import matplotlib.pyplot as plt

# 模型样本
np.random.seed(42)
x = np.random.uniform(-3.0,3.0,size=100)
X = x.reshape(-1,1)
y = 0.5 * x + 3 + np.random.normal(0,1,size=100)

# 绘制样本曲线
plt.scatter(x,y)
```




    <matplotlib.collections.PathCollection at 0x1a159a6c88>




![image.png](https://upload-images.jianshu.io/upload_images/7220971-654f0b109af9bb03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



```python
y.shape
```




    (100,)




```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

# 定义多项式回归函数
def PolynomialRegression(degree):
    return Pipeline([
        ("poly",PolynomialFeatures(degree=degree)),
        ("std_scaler",StandardScaler()),
        ("lin_reg",LinearRegression())
    ])
```


```python
from sklearn.model_selection import train_test_split

# 分割数据集
np.random.seed(666)
X_train,X_test,y_train,y_test = train_test_split(X,y)
```


```python
from sklearn.metrics import mean_squared_error

# 多项式回归对样本进行训练，使用20个维度
poly20_reg = PolynomialRegression(20)
poly20_reg.fit(X_train,y_train)

y20_predict = poly20_reg.predict(X_test)
mean_squared_error(y_test,y20_predict)
```




    167.94010860151894




```python
# 定义绘图模型
def plot_module(module):
    X_plot = np.linspace(-3,3,100).reshape(100,1)
    y_plot = module.predict(X_plot)

    plt.scatter(x,y)
    plt.plot(X_plot[:,0],y_plot,color='r')
    plt.axis([-3,3,0,6])
```


```python
# 绘制模型曲线--过拟合（非常的完全，两段有极端的情况）
plot_module(poly20_reg)
```


![image.png](https://upload-images.jianshu.io/upload_images/7220971-1a0cece989e72e78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



### 使用岭回归


```python
from sklearn.linear_model import Ridge

def RidgeRegression(degree,alpha):
    return Pipeline([
        ("poly",PolynomialFeatures(degree=degree)),
        ("std_scaler",StandardScaler()),
        ("ridge_reg",Ridge(alpha=alpha))
    ])
```


```python
# 注意alpha后面的参数是所有theta的平方和，而对于多项式回归来说，岭回归之前得到的θ都非常大
# 所以为了限制让他们比较小，我们前面系数可以取的小一些
ridge1_reg = RidgeRegression(degree=20,alpha=0.00001)
ridge1_reg.fit(X_train,y_train)
ridge1_predict = ridge1_reg.predict(X_test)
mean_squared_error(y_test,ridge1_predict)
```




    1.387437803144217




```python
# 通过使用岭回归，使得我们的均方误差小了非常多,曲线也缓和了非常多
plot_module(ridge1_reg)
```


![image.png](https://upload-images.jianshu.io/upload_images/7220971-7bbf7b07408202a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



```python
ridge2_reg = RidgeRegression(degree=20,alpha=1)
ridge2_reg.fit(X_train,y_train)
ridge2_predict = ridge2_reg.predict(X_test)
mean_squared_error(y_test,ridge2_predict)
```




    1.1888759304218461




```python
# 让ridge2_reg 的alpha值等于1，均差误差更加的缩小，并且曲线越来越趋近于一根倾斜的直线
plot_module(ridge2_reg)
```


![image.png](https://upload-images.jianshu.io/upload_images/7220971-09c9738492d5b5c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



```python
ridge3_reg = RidgeRegression(degree=20,alpha=100)
ridge3_reg.fit(X_train,y_train)
ridge3_predict = ridge3_reg.predict(X_test)
mean_squared_error(y_test,ridge3_predict)
```




    1.31964561130862




```python
# 得到的误差依然是比较小，但是比之前的1.18大了些，说明正则化做的有些过头了
plot_module(ridge3_reg)
```


![image.png](https://upload-images.jianshu.io/upload_images/7220971-e53c1bb7b0ec539b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




```python
ridge4_reg = RidgeRegression(degree=20,alpha=100000)
ridge4_reg.fit(X_train,y_train)
ridge4_predict = ridge4_reg.predict(X_test)
mean_squared_error(y_test,ridge4_predict)
# 当alpha非常大，我们的模型实际上相当于就是在优化θ的平方和这一项，使得其最小（因为MSE的部分相对非常小）
# 而使得θ的平方和最小，就是使得每一个θ都趋近于0，这个时候曲线就趋近于一根直线了
plot_module(ridge4_reg)
```


![image.png](https://upload-images.jianshu.io/upload_images/7220971-b0409e70fa67c836.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


