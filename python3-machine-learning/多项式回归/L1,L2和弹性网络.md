# 9.L1,L2和弹性网络

Ridge和LASSO都是在损失函数中添加一项，来调节θ的值使其尽可能的小，使得我们的模型泛化能力更好一些

---------------------
在机器学习领域中，我们会发明不同的名词来描述不同的标准，比如用Ridge和LASSO来衡量正则化的这一项；MSE和MAE用来衡量回归结果的好坏，欧拉距离和曼哈顿距离用来衡量两点之间的距离。但是他们背后的数学思想是非常的类似的，表达出的数学含义也是一致的。只不过应用到不同的场景中产生了不同的效果

![image.png](https://upload-images.jianshu.io/upload_images/7220971-3d6faab813f9a61e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



## 1.L1,L2正则
对明克夫斯基距离进一步泛化
![image.png](https://upload-images.jianshu.io/upload_images/7220971-41f7284c3119d63b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
> 对任意一个维度X，我们都可以求这样一个值，他的每一个值x都对他的p次方进行求和，再开p次方根。通常将这个式子成为LP范数。
当P=1的时候就是L1范数（曼哈顿距离|Ridge Regression），当P=2的时候就是 L2范数（欧拉距离|LASSO Regression）


![image.png](https://upload-images.jianshu.io/upload_images/7220971-4b55431c117b3981.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

注：有了L1,L2正则项，我们就可以进一步得到LN正则项，虽然实际应用中我们的n不会超过2，但是在数学推导中是有意义的


## 2.L0正则项
我们希望让θ的个数尽量小，描述的是非零θ元素的个数。我们用这样的方式来限制θ的数量尽可能的小，进而来限制我们的曲线不要太抖
不过实际上我们是很少使用L0正则的，因为L0正则的优化是一个NP难的问题，我们不能使用诸如梯度下降法甚至数学公式来找到一个最优解。
他是一个离散的值，我们需要穷举所有θ的值来找出哪些θ需要，哪些不需要。实际上可以用L1正则（LASS0 Regression）来替代，从而达到选择去掉一些θ的过程
![image.png](https://upload-images.jianshu.io/upload_images/7220971-aabdcbcab60c8eab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


## 3.弹性网 Elastic NET
> 在损失函数下，添加上一个L1正则项和一个L2正则项，并引入一个参数r来表示他们之间的比例。同时结合了岭回归和LASSO回归的优势

![image.png](https://upload-images.jianshu.io/upload_images/7220971-8200c6459275f87a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


实际应用中，通常应该先尝试一下岭回归（如果计算能力足够的话）。但是如果θ数量太大的话，消耗计算资源可能非常大，而LASSO由于有的时候急于把一些θ化为0，可能会导致得到的偏差比价大。这个时候需要使用弹性网

回忆小批量梯度下降法也是将随机梯度下降法和批量梯度下降法结合到了一起。在机器学习领域中，经常使用这种方式来创造出一些新的方法，这些方法虽然名词非常的酷，但是他们背后的意义是非常简单的




### 模型泛化的一个举例。我们在考试前会做很多练习题。我们做练习题不是为了把全部的练习题（训练数据集）都得到满分，而是为了在最后的那一场考试（真实数据）中得到满分