# 6.偏差方差均衡


> 模型误差=偏差（Bias）均差(Variance)+不可避免的误差
### 偏差
![image.png](https://upload-images.jianshu.io/upload_images/7220971-446e53ac30ff6070.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
### 方差
模型没有完全的学到数据的中心，而学习到了很多噪音
![image.png](https://upload-images.jianshu.io/upload_images/7220971-4ce2c54696a5ab2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 有一些算法天生是高方差算法。如KNN（过于依赖数据，一点选取的数据点有多数是不正确的，那么预测的结果就是错误的。导致有的很准确，有的非常不准确，方差非常大）
- 非参数学习通常都是高方差的算法。因为不对数据做任何假设
- 有一些算法天生是高偏差算法。如线性回归（用一条直线去拟合一条曲线，导致整体预测结果都距离真实数据查很大，偏差非常大）
- 参数学习通常都是高偏差的算法。因为对数据具有极强的假设

- 大多数算法具有相应的参数，可以调整偏差和方差
- 如KNN中的k，线性回归中使用多项式回归
- 偏差和方差是互相矛盾的。降低方差会提高偏差，降低偏差会提高方差

> 机器学习的主要调整来源于方差（这是站在算法的角度上，而不是问题的角度上,比如对金融市场的理解，很多人尝试用历史的数据预测未来的金融走势，这样的尝试通常都不太理想。很有可能因为历史的金融趋势不能很好的反应未来的走向，这种预测方法本身带来的非常大的偏差）换句话说，我们很容易让模型变的很复杂，从而降低模型的偏差，但是由于这样的模型的方差非常的大，最终也没有很好的性能。

### 解决高方差的通常手段：
1.降低模型复杂度
2.减少数据维度；降噪
3.增加样本数（模型太过复杂，模型中的参数非常多，而样本数不足以支撑计算出这么复杂的参数）
4.使用验证集
**5.模型正则化**

![image.png](https://upload-images.jianshu.io/upload_images/7220971-919d6dd8eed22a4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)