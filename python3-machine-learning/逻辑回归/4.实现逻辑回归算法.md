# 4.实现逻辑回归算法

## 1.逻辑回归算法的封装实现
```
class LogisticRegression:

    def __init__(self):
        """初始化Logistic Regression模型"""

        # 系数向量（θ1,θ2,.....θn）
        self.coef_ = None
        # 截距 (θ0)
        self.interception_ = None
        # θ向量
        self._theta = None

    def _sigmoid(self, t):
        return 1. / (1.+np.exp(-t))

    def fit(self, X_train, y_train, eta=0.01, n_iters=1e4):
        """根据训练数据集X_train,y_train, 使用梯度下降法训练Logistic Regression 模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

        def J(theta, X_b, y):
            y_hat = self._sigmoid(X_b.dot(theta))
            try:
                return - np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) / len(y)
            except:
                return float('inf')

        def dJ(theta, X_b, y):
            return X_b.T.dot(self._sigmoid(X_b.dot(theta)) - y) / len(X_b)

        def gradient_descent(X_b, y, initial_theta, eta, n_iters=n_iters, epsilon=1e-8):
            """
            梯度下降法封装
            X_b: X特征矩阵
            y: 结果向量
            initial_theta:初始化的theta值
            eta:学习率η
            n_iters: 最大循环次数
            epsilon: 精度
            """
            theta = initial_theta
            i_iters = 0

            while i_iters < n_iters:
                """
                如果theta两次变化之间的损失函数值的变化小于我们定义的精度
                则可以说明我们已经找到了最低的损失函数值和对应的theta
                
                如果循环次数超过了我们设置的循环次数，
                则说明可能由于η设置的过大导致无止境的循环
                """
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient

                if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:
                    break

                i_iters += 1

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta)

        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def predict_proba(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果概率向量"""
        assert self.coef_ is not None and self.interception_ is not None,\
            "must fit before predict"
        assert X_predict.shape[1] == len(self.coef_),\
            "the feature number of X_predict must be equal to X_train"

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return self._sigmoid(X_b.dot(self._theta))

    def predict(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果向量"""
        assert self.coef_ is not None and self.interception_ is not None,\
            "must fit before predict"
        assert X_predict.shape[1] == len(self.coef_),\
            "the feature number of X_predict must be equal to X_train"

        proba = self.predict_proba(X_predict)
        return proba >= 0.5

    def score(self, X_test, y_test):
        """根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(X_test)
        return accuracy_score(y_test, y_predict)

    def __repr__(self):
        return "LinearRegression()"

```

## 2.测试我们的逻辑回归算法

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
```


```python
X = iris.data
y = iris.target
```


```python
# 只取前两个特征；只取y=0，1
X = X[y<2,:2]
y = y[y<2]
```


```python
plt.scatter(X[y==0,0],X[y==0,1],color='red')
plt.scatter(X[y==1,0],X[y==1,1],color='blue')
```




    <matplotlib.collections.PathCollection at 0x1a1d231ba8>




![image.png](https://upload-images.jianshu.io/upload_images/7220971-a5d958c9dc8b2f5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


### 使用我们自己的逻辑回归算法


```python
from machine_learning.module_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y)
from machine_learning.LogisticRegression import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train,y_train)
```




    LinearRegression()




```python
# 对于所有的测试数据，我们的LogisticRegression全都正确的进行了分类
log_reg.score(X_test,y_test)
```




    1.0



### 观察逻辑回归预测的概率值p与y_test的关系
- p越趋近于0，逻辑回归算法越愿意将数据预测为0
- p越趋近于1，逻辑回归算法越愿意将数据预测为1

如第一个测试数据的预测概率值为0.02945732，对应的y_test为0


```python
log_reg.predict_proba(X_test)
```




    array([0.02945732, 0.10411811, 0.72452652, 0.95194827, 0.00436497,
           0.04378511, 0.98773424, 0.15855562, 0.72452652, 0.18983267,
           0.91446368, 0.92348443, 0.88028026, 0.053248  , 0.00987992,
           0.98551291, 0.98002369, 0.98183066, 0.99639662, 0.07152432])




```python
y_test
```




    array([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0])




```python
log_reg.coef_
```




    array([ 3.02265606, -5.07877569])




```python
log_reg.interception_
```




    -0.7235198625270994
