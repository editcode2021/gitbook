# 5.决策边界

![image.png](https://upload-images.jianshu.io/upload_images/7220971-d07abf9a7805e6a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
通过上面推导可以得出
![image.png](https://upload-images.jianshu.io/upload_images/7220971-a64bf9af0d9d460b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

决策边界θ<sup>T</sup>·X<sub>b</sub>=0 表示的实际上是一条直线，如果有两个特征，那就是在二维平面上的一条直线，x1是横轴，x2是纵轴
![image.png](https://upload-images.jianshu.io/upload_images/7220971-ef279794800d3d7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 观察逻辑回归预测的概率值p与y_test的关系
- p越趋近于0，逻辑回归算法越愿意将数据预测为0
- p越趋近于1，逻辑回归算法越愿意将数据预测为1

如第一个测试数据的预测概率值为0.02945732，对应的y_test为0


```python
log_reg.predict_proba(X_test)
```




    array([0.02945732, 0.10411811, 0.72452652, 0.95194827, 0.00436497,
           0.04378511, 0.98773424, 0.15855562, 0.72452652, 0.18983267,
           0.91446368, 0.92348443, 0.88028026, 0.053248  , 0.00987992,
           0.98551291, 0.98002369, 0.98183066, 0.99639662, 0.07152432])




```python
y_test
```




    array([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0])




```python
log_reg.coef_
```




    array([ 3.02265606, -5.07877569])




```python
log_reg.interception_
```




    -0.7235198625270994



### 描述决策边界


```python
def x2(x1):
    return (-log_reg.coef_[0] * x1 - log_reg.interception_) / log_reg.coef_[1]
```


```python
x1_plot = np.linspace(4,8,1000)
x2_plot = x2(x1_plot)
```


```python
plt.scatter(X[y==0,0],X[y==0,1],color='red')
plt.scatter(X[y==1,0],X[y==1,1],color='blue')
plt.plot(x1_plot,x2_plot)
```




    [<matplotlib.lines.Line2D at 0x1a1dcb6d68>]




![image.png](https://upload-images.jianshu.io/upload_images/7220971-7788191d2b6404d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



通过观察上面的图可以发现，如果新来一个数据点，落在了直线（决策边界）的上方，对应的x1*θ1+x2*θ2+θ>0，也就是p>0.5,那么我们就将他分类为1；反之，则分类为0。这就是决策曲线

> 不过无论是KNN，还是逻辑回归算法，我们依然可以加入多项式项，使得他的决策边界不再是一根直线，对于这种情况，我们就不能简单的求出这根直线的方程。然后将整根直线画出来来看到这个决策的边界。这个时候我们需要一个绘制不规则的决策边界的方法

> 在我们的决策平面上，分布着很多点，对于每一个点，我们都使用我们的模型来判断他分为哪一类。然后将这些颜色绘制出来得到的结果就是决策边界

![image.png](https://upload-images.jianshu.io/upload_images/7220971-62b17d42f499bf0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


### 不规则的决策边界的绘制方法


```python
def plot_decision_boundary(model,axis):
    """
    model：模型
    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围
    """
    
    # 使用linspace将x轴，y轴划分成无数的小点
    x0,x1 = np.meshgrid(
        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),
        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)
    )
    X_new = np.c_[x0.ravel(),x1.ravel()]
    
    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)
    
    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)
```


```python
plot_decision_boundary(log_reg,axis=[4,7.5,1.5,4.5])
plt.scatter(X[y==0,0],X[y==0,1],color='red')
plt.scatter(X[y==1,0],X[y==1,1],color='blue')
```

    /anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'
      s)





    <matplotlib.collections.PathCollection at 0x1a1e76f0f0>




![image.png](https://upload-images.jianshu.io/upload_images/7220971-e5b90eb0bd6c8703.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




### KNN的决策边界
对于KNN来说，这个决策边界是没有表达式的，因为我们不是使用数学求解的方式。而是使用距离投票的方式
但是我们依然可以使用上面的方式绘制决策边界


```python
from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train,y_train)
knn_clf.score(X_test,y_test)
```




    1.0




```python
# 绘制KNN算法的决策边界。可以看到是一根弯弯曲曲的曲线 
plot_decision_boundary(knn_clf,axis=[4,7.5,1.5,4.5])
plt.scatter(X[y==0,0],X[y==0,1],color='red')
plt.scatter(X[y==1,0],X[y==1,1],color='blue')
```

    /anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'
      s)





    <matplotlib.collections.PathCollection at 0x1a204d5dd8>




![image.png](https://upload-images.jianshu.io/upload_images/7220971-2be492fa53941b78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



```python
# 绘制三个不同类别
knn_clf_all = KNeighborsClassifier()
knn_clf_all.fit(iris.data[:,:2],iris.target)
```




    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
               metric_params=None, n_jobs=1, n_neighbors=5, p=2,
               weights='uniform')




```python
plot_decision_boundary(knn_clf_all,axis=[4,8,1.5,4.5])
plt.scatter(iris.data[iris.target==0,0],iris.data[iris.target==0,1])
plt.scatter(iris.data[iris.target==1,0],iris.data[iris.target==1,1])
plt.scatter(iris.data[iris.target==2,0],iris.data[iris.target==2,1])
```

    /anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'
      s)





    <matplotlib.collections.PathCollection at 0x1a1ef88f98>




![image.png](https://upload-images.jianshu.io/upload_images/7220971-90586a01b4c5f6fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


通过绘制的结果，可以看出，绘制出的结果是非常不规则的，甚至在黄色的部分还掺杂着一些蓝色的绿色的点。
这就是一种过拟合的表现。可以看到上面的knn_clf_all的n_neighbors（k）为5。之前的讨论曾经说过，
k越小，那么我们的模型就越复杂，在这里，我们可视化的看到了复杂的含义--就是模型非常的不规整


```python
# 调整k为50
knn_clf_all = KNeighborsClassifier(n_neighbors=50)
knn_clf_all.fit(iris.data[:,:2],iris.target)
```




    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
               metric_params=None, n_jobs=1, n_neighbors=50, p=2,
               weights='uniform')




```python
plot_decision_boundary(knn_clf_all,axis=[4,8,1.5,4.5])
plt.scatter(iris.data[iris.target==0,0],iris.data[iris.target==0,1])
plt.scatter(iris.data[iris.target==1,0],iris.data[iris.target==1,1])
plt.scatter(iris.data[iris.target==2,0],iris.data[iris.target==2,1])
```

    /anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'
      s)





    <matplotlib.collections.PathCollection at 0x1a20376a58>




![image.png](https://upload-images.jianshu.io/upload_images/7220971-b9b82d03883bbdce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


调整过后的样子已经比上面的决策边界规整了很多。整体分成了三大块，非常的清晰
通过这样一个例子，再次印证了对于KNN算法来说k越大，模型越简单，对于决策边界的划分就是决策越规整，分块越明显
