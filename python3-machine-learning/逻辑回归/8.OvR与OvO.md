# 8.OvR与OvO

## 1.什么是OvR与OvO
逻辑回归只可以解决二分类问题。我们可以对逻辑回归稍加改造，让他解决多分类问题：
- ### OvR （One vs Rest -- 一针对剩余）
假设一共有4个类别，选取其中的某一个类别（假设红色）（One），而对于剩下的类别，把他们融合在一起，把他称之为其他的类别（Rest），这样就把一个四分类问题转换成了二分类问题，转换成了使红色的概率是多少，是非红色的概率是多少
![image.png](https://upload-images.jianshu.io/upload_images/7220971-80e4677a6b481b16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
以此类推，我们分别进行四次分类，哪次获得的类别得分最高，我们就任务他属于哪一个类别。对于逻辑回归来说，就是我们的概率P
![image.png](https://upload-images.jianshu.io/upload_images/7220971-57e9ab4aa9c2d273.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
复杂度由T变成了N*T->O(N)
- ### OvO （One vs One -- 一对一的进行比较）
每次就从N个类别挑出两个类别（比如这里挑出红蓝两个类别）,然后进行二分类任务，看对于这个任务来说，我们的样本点是属于哪个类别。然后依次类推进行扩展，如果我们有4个类别需要分类，那我们就能形成6个两两的对C42(排列组合公式4*3/2=6)，也就是6个二分类问题。对于这6个分类结果，判定他在哪个类别中数量最大，就判定他是哪个类别
![image.png](https://upload-images.jianshu.io/upload_images/7220971-cd9d173b2f9ab48e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

 复杂度是O(N<sup>2</sup>),但是分类结果相比OVR是更加准确的，这是因为每次只用真实的两个类别进行比较，所以他更倾向于真实的样本属于哪个类别
 
## 2.sklearn中的OvR与OvO


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
# 为了可视化，先使用两个类别
X = iris.data[:,:2]
y = iris.target
```


```python
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=666)  
```


```python
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train,y_train)
# multi_class='ovr' sklearn 默认支持多分类任务，而且默认使用ovr方式
```




    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
              verbose=0, warm_start=False)




```python
# 分数不好使由于我们只使用了两个维度
log_reg.score(X_test,y_test)
```




    0.6578947368421053




```python
def plot_decision_boundary(model,axis):
    """
    model：模型
    axis:坐标轴的范围；0123对应的就是x轴和y轴的范围
    """
    
    # 使用linspace将x轴，y轴划分成无数的小点
    x0,x1 = np.meshgrid(
        np.linspace(axis[0],axis[1],int((axis[1]-axis[0])*100)).reshape(-1,1),
        np.linspace(axis[2],axis[3],int((axis[3]-axis[2])*100)).reshape(-1,1)
    )
    X_new = np.c_[x0.ravel(),x1.ravel()]
    
    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)
    
    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0,x1,zz,linspace=5,cmap=custom_cmap)
```


```python
# 描绘三分类的决策边界
plot_decision_boundary(log_reg,axis=[4,8.5,1.5,4.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.scatter(X[y==2,0],X[y==2,1])
```

    /anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'
      s)





    <matplotlib.collections.PathCollection at 0x113f38ba8>




![image.png](https://upload-images.jianshu.io/upload_images/7220971-30c72c6d14f7c97e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


### 使用OvO


```python
# sklearn中计算logistic不是简单的使用梯度下降法，他是使用更快的一种方法，所以需要修改solver参数
# 默认solver='liblinear',为了正确的调用OvO，缓存newton-cg
log_reg2 = LogisticRegression(multi_class='multinomial',solver="newton-cg")
```


```python
log_reg2.fit(X_train,y_train)
```




    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
              intercept_scaling=1, max_iter=100, multi_class='multinomial',
              n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',
              tol=0.0001, verbose=0, warm_start=False)




```python
# 分类准确度比使用OvR的时候要高了很多
log_reg2.score(X_test,y_test)
```




    0.7894736842105263




```python
# 从直观的角度看，决策边界也准确了很多
plot_decision_boundary(log_reg2,axis=[4,8.5,1.5,4.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.scatter(X[y==2,0],X[y==2,1])
```

    /anaconda3/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'linspace'
      s)





    <matplotlib.collections.PathCollection at 0x111b0b6d8>




![image.png](https://upload-images.jianshu.io/upload_images/7220971-c3e34d226cf16eb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


### 尝试使用所有的数据特征


```python
iris = datasets.load_iris()
X = iris.data
y = iris.target

X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=666)  
```


```python
log_reg = LogisticRegression()
log_reg.fit(X_train,y_train)
log_reg.score(X_test,y_test)
```




    0.9473684210526315




```python
log_reg2 = LogisticRegression(multi_class='multinomial',solver="newton-cg")
log_reg2.fit(X_train,y_train)
# 使用OvO的方式预测结果达到了百分之百；
# 注意：这里由于数据集比较小，耗时的差别比较小
log_reg2.score(X_test,y_test)
```




    1.0



### 使用sklearn中的OvO and OvR


```python
from sklearn.multiclass import OneVsRestClassifier

ovr = OneVsRestClassifier(log_reg)
ovr.fit(X_train,y_train)
ovr.score(X_test,y_test)
```




    0.9473684210526315




```python
from sklearn.multiclass import OneVsOneClassifier

ovo = OneVsOneClassifier(log_reg)
ovo.fit(X_train,y_train)
ovo.score(X_test,y_test)
```




    1.0





