# 3.衡量线性回归算法的指标

## 3.1 衡量标准
![3.1](https://upload-images.jianshu.io/upload_images/7220971-c440c129cd28b499.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
其中衡量标准是和m有关的，因为越多的数据量产生的误差和可能会更大，但是毫无疑问越多的数据量训练出来的模型更好，为此需要一个取消误差的方法，如下

![3.2](https://upload-images.jianshu.io/upload_images/7220971-31090a2f956341f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
MSE 的缺点，量纲不准确，如果y的单位是万元，平方后就变成了万元的平方，这可能会给我们带来一些麻烦
![3.3](https://upload-images.jianshu.io/upload_images/7220971-250c94798e4c458c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![3.4](https://upload-images.jianshu.io/upload_images/7220971-ace4bb2857d3417f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
RMSE 平方累加后再开根号，如果某些预测结果和真实结果相差非常大，那么RMSE的结果会相对变大，所以RMSE有放大误差的趋势，而MAE没有，他直接就反应的是预测结果和真实结果直接的差距，正因如此，从某种程度上来说，想办法我们让RMSE变的更小小对于我们来说比较有意义，因为这意味着整个样本的错误中，那个最值相对比较小，而且我们之前训练样本的目标，就是RMSE根号里面1/m的这一部分，而这一部分的本质和优化RMSE是一样的

## 3.2 MSE,RMSE,MAE的实现
```
def mean_squared_error(y_true, y_predict):
    """计算y_true和y_predict之间的MSE"""
    assert len(y_true) == len(y_predict), \
        "the size of y_true must be equal to the size of y_predict"

    return np.sum((y_true - y_predict)**2) / len(y_true)


def root_mean_squared_error(y_true, y_predict):
    """计算y_true和y_predict之间的RMSE"""

    return sqrt(mean_squared_error(y_true, y_predict))


def mean_absolute_error(y_true, y_predict):
    """计算y_true和y_predict之间的RMSE"""
    assert len(y_true) == len(y_predict), \
        "the size of y_true must be equal to the size of y_predict"

    return np.sum(np.absolute(y_true - y_predict)) / len(y_true)
```
## 3.3 调用sikit learn 的实现
```
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
mean_squared_error(y_test,y_predict)
```