# 5 随机梯度下降法
## 5.1 随机梯度下降法介绍
![5.1](https://upload-images.jianshu.io/upload_images/7220971-eefbdfef2cac4e5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
批量梯度下降法带来的一个问题是η的值需要设置的比较小，在样本数比较多的时候导致不是速度特别慢，这时候观察随机梯度下降法损失函数的求导公式，可以发现，我们对每一个Xb都做了求和操作，又在最外面除以了m，那么可以考虑将求和和除以m的两个运算约掉，采用每次使用一个随机的Xb
![5.2](https://upload-images.jianshu.io/upload_images/7220971-7402b36b21032c4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![5.3](https://upload-images.jianshu.io/upload_images/7220971-8213f918a891c5aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
由于我们使用的事随机梯度下降法，所以导致我们的最终结果不会像批量梯度下降法一样准确的朝着一个方向运算，而是曲线行下降，这时候我们就希望，越到下面，η值相应减小，事运算次数变多，从而精确计算结果
![5-4](https://upload-images.jianshu.io/upload_images/7220971-e0ba27e5a2bc9c18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
这里使用了模拟退火的思想
![5-5](https://upload-images.jianshu.io/upload_images/7220971-637c93f4b299d0a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 5.2 随机梯度下降法实现
```
def dJ_sgd(theta,X_b_i,y_i):
    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2

def sgd(X_b,y,initial_theta,n_iters):
    
    t0 = 5
    t1 = 50
    
    def learning_rate(t):
        return t0 / (t + t1)
    
    theta = initial_theta
    for cur_iter in range(n_iters):
        rand_i = np.random.randint(len(X_b))
        gradient = dJ_sgd(theta,X_b[rand_i],y[rand_i])
        theta = theta - learning_rate(cur_iter) * gradient
    return theta
```
```
        %%time
        eta = 0.01
        X_b = np.hstack([np.ones((len(X), 1)), X])
        initial_theta = np.zeros(X_b.shape[1])
        # 随机的检查了3分之一个样本总量的样本
        _theta = sgd(X_b, y, initial_theta, n_iters=len(X_b)//3)
        # 输出 CPU times: user 318 ms, sys: 5.22 ms, total: 323 ms
        # Wall time: 337 ms
        # _theta: array([3.03182269, 3.93118623])
```
## 5.3 随机梯度下降法的封装和测试
```
    def fit_sgd(self, X_train, y_train, n_iters=5, t0=5, t1=50):
        """
        根据训练数据集X_train, y_train, 使用随机梯度下降法训练Linear Regression模型
        :param X_train:
        :param y_train:
        :param n_iters: 在随机梯度下降法中，n_iters代表所有的样本会被看几圈
        :param t0:
        :param t1:
        :return:
        """
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"
        assert n_iters >= 1

        def dJ_sgd(theta, X_b_i, y_i):
            """
            去X_b,y 中的随机一个元素进行导数公式的计算
            :param theta:
            :param X_b_i:
            :param y_i:
            :return:
            """
            return X_b_i * (X_b_i.dot(theta) - y_i) * 2.

        def sgd(X_b, y, initial_theta, n_iters, t0=5, t1=50):

            def learning_rate(t):
                """
                计算学习率，t1 为了减慢变化速度，t0为了增加随机性
                :param t: 第t次循环
                :return:
                """
                return t0 / (t + t1)

            theta = initial_theta
            m = len(X_b)

            for cur_iter in range(n_iters):
                # 对X_b进行一个乱序的排序
                indexes = np.random.permutation(m)
                X_b_new = X_b[indexes]
                y_new = y[indexes]

                # 对整个数据集看一遍
                for i in range(m):
                    gradient = dJ_sgd(theta, X_b_new[i], y_new[i])
                    theta = theta - learning_rate(cur_iter * m + i) * gradient

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.random.randn(X_b.shape[1])
        self._theta = sgd(X_b, y_train, initial_theta, n_iters, t0, t1)

        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self
```
### 模拟数据进行测试
![5.3-1](https://upload-images.jianshu.io/upload_images/7220971-b323495c7676afa2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 真实数据波士顿房价进行测试
![5.3-2](https://upload-images.jianshu.io/upload_images/7220971-04577a1901413fb9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 5.4 使用Sklearn中的 随机梯度下降法
![5.4-1](https://upload-images.jianshu.io/upload_images/7220971-aa5395b1bc30a036.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
需要注意的是sklearn中的梯度下降法比我们自己的算法要复杂的多，性能和计算准确度上都比我们的要好，我们的算法只是用来演示过程，具体生产上的使用还是应该使用Sklearn提供的