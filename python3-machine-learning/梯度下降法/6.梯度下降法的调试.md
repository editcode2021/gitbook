# 6 梯度下降法 的调试
## 6.1 梯度下降法调试的原理
可能我们计算出梯度下降法的公式，并使用python编程实现，预测的过程中程序并没有报错，但是可能我们需要求的梯度的结果是错误的，这个时候需要怎么样去调试发现错误呢。


首先以二维坐标平面为例，一个点（O）的导数就是曲线在这个点的切线的斜率，在这个点两侧各取一个点（AB），那么AB两点对应的直线的斜率应该大体等于O的切线的斜率，并且这A和B的距离越近，那么两条直线的斜率就越接近
事实上，这也正是导数的定义，当函数y=f(x)的自变量x在一点x<sub>0</sub>上产生一个增量Δx时，函数输出值的增量Δy与自变量增量Δx的比值在Δx趋于0时的极限a如果存在，a即为在x<sub>0</sub>处的导数，记作f'(x<sub>0</sub>)或df(x<sub>0</sub>)/dx
![6-1](https://upload-images.jianshu.io/upload_images/7220971-94b82f4a9d5a36b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


扩展到多维维度则如下

![6-2](https://upload-images.jianshu.io/upload_images/7220971-c45cb87e253bf41f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


## 梯度下降法调试的实现
```python
np.random.seed(666)
X = np.random.normal(size=(1000,10))
X_b = np.hstack([np.ones((len(X),1)),X])

# 真实的θ值
true_theta = np.arange(1,12,dtype=float)
# np.random.normal(size=1000) 添加噪音
y = X_b.dot(true_theta) + np.random.normal(size=1000)
```


```python
# 实现J(θ)函数
def J(theta,X_b,y):
    try:
        return np.sum((y-X_b.dot(theta))**2)/len(X_b)
    except:
        return float('inf')
    
# 实现数学推导出的dJ(θ)
def d_J_main(theta,X_b,y):
    return X_b.T.dot(X_b.dot(theta) - y) * 2. /len(X_b)

# 实现debug模式的dJ(θ)
def d_J_debug(theta,X_b,y,epsilon=0.01):
    res = np.empty(len(theta))
    for i in range(len(theta)):
        theta_1 = theta.copy()
        theta_1[i] += epsilon
        theta_2 = theta.copy()
        theta_2[i] -= epsilon
        res[i] = (J(theta_1,X_b,y)-J(theta_2,X_b,y))/(2*epsilon)
    return res
```


```python
# 批量梯度下降法，d_J为求导函数，作为一个参数传入，用于切换求导策略
def gradient_descent(d_J,X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
            theta = initial_theta
            i_iters = 0

            while i_iters < n_iters:
                gradient = d_J(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient

                if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:
                    break

                i_iters += 1

            return theta
```


```python
X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01
# 使用d_J_debug调试模式求出theta
%time theta = gradient_descent(d_J_debug,X_b, y, initial_theta, eta)
print(theta)
# 使用数学解求出theta
%time theta = gradient_descent(d_J_main,X_b, y, initial_theta, eta)
print(theta)
```
    # 输出结果
    CPU times: user 531 ms, sys: 214 ms, total: 745 ms
    Wall time: 613 ms
    [ 0.94575233  1.98082712  3.06882065  3.94835863  4.97139932  5.9859077
      7.01077392  7.99250414  8.99151383  9.97525811 10.99758484]
    CPU times: user 67 ms, sys: 27.6 ms, total: 94.6 ms
    Wall time: 76.7 ms
    [ 0.94575233  1.98082712  3.06882065  3.94835863  4.97139932  5.9859077
      7.01077392  7.99250414  8.99151383  9.97525811 10.99758484]


由此可以看出，我们的d_J_debug和d_J_main的结果是相近的，所以我们的d_J_main的数学推导是没问题的。
我们可以在真正的机器学习之前，先使用d_J_debug这种调试方式来验证一下我们的d_J_main的结果是否正确，然后再进行机器学习。

d_J_debug是通用的，可以放在任何求导的debug过程中，所以可以作为我们机器学习的工具箱来使用