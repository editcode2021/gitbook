# 7.梯度下降法的总结
## 7.1 小批量

- 批量梯度下降法
- 随机梯度下降法
下面来看下二者的对比

维度|批量梯度下降法|随机梯度下降法
---|---|---
计算方式|每次对所有的样本看一遍才可以计算出梯度|每一次只需观察一个样本
速度|慢|快
稳定性|高，一定可以先向损失函数下降的方式前进|低，每一次的方式不确定，甚至向反方向前进

综合二者的优缺点，有一种新的梯度下降法

![7-1](https://upload-images.jianshu.io/upload_images/7220971-df669192ed579b23.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
小批量梯度下降法：即，我们每一次不看全部样本那么多，也不是只看一次样本那么少，每次只看k个样本
对于小批量梯度下降法，由多了一个超参数

```
def fit_lit_sgd(self, X_train, y_train, n_iters=5, t0=5, t1=50,k=10):
        """
        根据训练数据集X_train, y_train, 使用随机梯度下降法训练Linear Regression模型
        :param X_train:
        :param y_train:
        :param n_iters: 在随机梯度下降法中，n_iters代表所有的样本会被看几圈
        :param t0:
        :param t1:
        :param k: 小批量随机下降法的超参数k
        :return:
        """
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"
        assert n_iters >= 1

        def dJ_sgd(theta, X_b_k, y_k):
            """
            去X_b,y 中的随机选择k个元素进行导数公式的计算
            :param theta:
            :param X_b_i:
            :param y_i:
            :return:
            """
            return np.sum((X_b_k * (X_b_k.dot(theta) - y_k) ))* 2/len(X_b_k).

        def sgd(X_b, y, initial_theta, n_iters, t0=5, t1=50):

            def learning_rate(t):
                """
                计算学习率，t1 为了减慢变化速度，t0为了增加随机性
                :param t: 第t次循环
                :return:
                """
                return t0 / (t + t1)

            theta = initial_theta
            m = len(X_b)

            for cur_iter in range(n_iters):
                # 每次看k个元素
                i =0
                while i < m:
                    X_b_new = X_b[i:i+k]
                    y_new = y[i:i+k]
                    gradient = dJ_sgd(theta, X_b_new, y_new)
                    theta = theta - learning_rate(cur_iter * m + i+k) * gradient
                    i = i+k

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.random.randn(X_b.shape[1])
        self._theta = sgd(X_b, y_train, initial_theta, n_iters, t0, t1)

        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

```

## 7.2 随机
随机梯度下降法的优点
![7.2-1](https://upload-images.jianshu.io/upload_images/7220971-9f6fd87f6a1afe77.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 7.3 梯度上升法
![7.3-1](https://upload-images.jianshu.io/upload_images/7220971-2fe14d94eaa11b87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![7.3-2](https://upload-images.jianshu.io/upload_images/7220971-8e742627ef8ead2b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![7.3-3](https://upload-images.jianshu.io/upload_images/7220971-ce9cc2307f3ac8e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)





