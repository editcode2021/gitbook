# 11.9 SVM思想解决回归问题

回归算法的本质就是找到一根直线或者一个曲线能够最大程度上的拟合我们的数据点，如何定义拟合，就是不同的回归算法的关键。比如线性回归的算法定义拟合的方式就是让数据点到直线的MSE的值最小。

对于SVM的算法来说。对于拟合的定义是，我们要定义一个mergin值，在margin范围内，我们能够包含的样本点最多。也就代表我们这个范围能够比较好的来表达我们的样本数据点，在这种情况下，我们取中间的直线作为我们真正的回归的结果，用他来预测其他点，位置点的值。

在这种情况下，SVM干的事，和解决分类算法是相反的过程。我们期望的是在margin范围里，包围的点越多越好。

在具体训练svm回归问题的结果的时候，我们对margin是有一个指定的，所以在这里引入了一个超参数ɛ，代表margin范围的两根直线的任意一跟到中间直线的距离。

![image.png](https://upload-images.jianshu.io/upload_images/7220971-4d5df56627e8cd17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




```python
import numpy as np
import matplotlib.pyplot as plt
```


```python
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target
```


```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 666)
```


```python
from sklearn.svm import LinearSVR # Support Vector Regression
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def StandardLinearSVR(epsilon = 0.1):
    return Pipeline([
        ("std_scaler", StandardScaler()),
        ("linear_svr", LinearSVR(epsilon= epsilon))
    ])
```


```python
svr = StandardLinearSVR()
svr.fit(X_train, y_train)
```




    Pipeline(memory=None,
         steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svr', LinearSVR(C=1.0, dual=True, epsilon=0.1, fit_intercept=True,
         intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,
         random_state=None, tol=0.0001, verbose=0))])




```python
svr.score(X_test, y_test)
```




    0.6360330630824074



这里分数不是很高。但是使用SVR有很多超参数可以调节，比如C。如果我们使用SVR，还可以对不同的kernal还有不同的参数调节。对于高斯Kernal来说就需要对γ进行调节，对于polynomial来说有degree和C进行调节。
